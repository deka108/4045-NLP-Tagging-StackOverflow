Question-28818506, answer-38560612
This is part of JDK 9 in the form of or , which takes a Supplier<T> . Your example would then be: return serviceA(args) .or(args) .or(args); For details see the (preliminary) Javadoc or this post I wrote.

Question-24603186, answer-24603187
The Javadocs for Collection.(parallelS|s)tream(args) and Stream itself don't answer the question, so it's off to the mailing lists for the rationale. I went through the lambda-libs-spec-observers archives and found one thread specifically about Collection.parallelStream() and another thread that touched on whether java.util.Arrays should provide parallelStream() to match (or actually, whether it should be removed). There was no once-and-for-all conclusion, so perhaps I've missed something from another list or the matter was settled in private discussion. (Perhaps Brian Goetz , one of the principals of this discussion, can fill in anything missing.) The participants made their points well, so this answer is mostly just an organization of the relevant quotes, with a few clarifications in [brackets] , presented in order of importance (as I interpret it). parallelStream() covers a very common case Brian Goetz in the first thread, explaining why Collections.parallelStream(args) is valuable enough to keep even after other parallel stream factory methods have been removed: #blockquote Brian Goetz stands by this position in the later discussion about Arrays.parallelStream(args) : #blockquote parallelStream() is more performant Brian Goetz : #blockquote In response to Kevin Bourrillion's skepticism about whether the effect is significant, Brian again : #blockquote Doug Lea follows up , but hedges his position: #blockquote Indeed, the later discussion about Arrays.parallelStream(args) takes notice of lower Stream.parallel() cost . stream().parallel() statefulness complicates the future At the time of the discussion, switching a stream from sequential to parallel and back could be interleaved with other stream operations. Brian Goetz, on behalf of Doug Lea , explains why sequential/parallel mode switching may complicate future development of the Java platform: #blockquote This mode switching was removed after further discussion . In the current version of the library, a stream pipeline is either sequential or parallel; last call to sequential(args) / parallel(args) wins. Besides side-stepping the statefulness problem, this change also improved the performance of using parallel(args) to set up a parallel pipeline from a sequential stream factory. exposing parallelStream() as a first-class citizen improves programmer perception of the library, leading them to write better code Brian Goetz again , in response to Tim Peierls's argument that Stream.parallel(args) allows programmers to understand streams sequentially before going parallel: #blockquote

Question-14534767, answer-14534798
It should be r.append(args); But I recommend you to do as below, r.append(args); System.getProperty(args) gives you system-dependent newline in java. Also from Java 7 there's a method that returns the value directly: System.lineSeparator()

Question-24609564, answer-24609841
If you will read comment above the code, It is mentioned that... If the caller is not a ForkJoinTask , this method is behaviorally equivalent to #pre So it is just another form to implement above code in else part...!! In Style notes it is mentioned that, #blockquote And if you will see implementation of ManagedLocker#isReleasable , It is updating the lock and returns true if blocking is unnecessary. Interpretation : Blank while loops are used to provide an interrupt until some condition reset to true/false. Here, do { } while(args) is a blocker/interrupt until blocker.block(args) will be true when blocker.isReleasable(args) is false . Loop will continue execution while blocker is not releasable ( !blocker.isReleasable(args) ) and blocker is not blocked !! Execution will be out of loop as soon as blocker.block(args) will set to true. Note that, do{ } while(args) does not update CAS variable, but it guarantee that program will wait until variable gets updated (force to wait until variable gets updated).

Question-6094575, answer-6094600
You can use Class.forName() to get a Class object of the desired class. Then use getConstructor() to find the desired Constructor object. Finally, call newInstance() on that object to get your new instance. #pre

Question-27482579, answer-27483288
#blockquote No . It will never throw a compilation error. This is much similar to what a simple getter and setter does or a copy constructor does. Remember we can access private members using this. #pre Your testPrivate method accepts an instance of MyClass. Since testPrivate is a method inside MyClass , it will have access to private properties. #pre Methods defined inside the class will always have access to it's private members, through this. and instance variable. But if you define testPrivate outside of MyClass then, you won't have access to private members. There you will have to use a method or a setter or a getter.

Question-15430247, answer-15432106
This is to avoid possible efforts for reallocation for every single object. int newCapacity = (oldCapacity * 3)/2 + 1; internally new Object[] is created. JVM needs effort to create new Object[] when you add element in the arraylist. If you don't have above code(any algo you think) for reallocation then every time when you invoke arraylist.add(args) then new Object[] has to be created which is pointless and we are loosing time for increasing size by 1 for each and every objects to be added. So it is better to increase size of Object[] with following formula. (JSL has used forcasting formula given below for dynamically growing arraylist instead of growing by 1 every time. Because to grow it takes effort by JVM) int newCapacity = (oldCapacity * 3)/2 + 1;

Question-12468764, answer-24537575
I've found a very nice and concise solution, especially useful when you cannot modify enum classes as it was in my case. Then you should provide a custom ObjectMapper with a certain feature enabled. Those features are available since Jackson 1.6. So you only need to write toString(args) method in your enum. #pre There are more enum-related features available, see here: #a #a

Question-18777989, answer-18778228
You made a good summery of the practical differences in use and implementation but did not say anything about the difference in meaning. An interface is a description of the behaviour an implementing class will have. The implementing class ensures, that it will have these methods that can be used on it. It is basically a contract or a promise the class has to make. An abstract class is a basis for different subclasses that share behaviour which does not need to be repeatedly be created. Subclasses must complete the behaviour and have the option to override predefine behaviour (as long as it is not defined as final or private ). You will find good examples in the java.util package which includes interfaces like List and abstract classes like AbstractList which already implements the interface. The official documentation describes the AbstractList as follows: #blockquote

Question-8881291, answer-14060804
#li Strings are immutable in Java if you store password as plain text it will be available in memory until Garbage collector clears it and since Strings are used in String pool for re-usability there is pretty high chance that it will be remain in memory for long duration, which pose a security threat. Since any one who has access to memory dump can find the password in clear text #li Java recommendation using getPassword(args) method of JPasswordField which returns a char[] and deprecated getText(args) method which returns password in clear text stating security reason. #li toString() there is always a risk of printing plain text in log file or console but if use Array you won't print contents of array instead its memory location get printed. #pre #blockquote Final thoughts: Though using char[] is not just enough you need to erase content to be more secure. I also suggest working with hash'd or encrypted password instead of plaintext and clearing it from memory as soon as authentication is completed.

Question-21105403
I've written a factory to produce java.sql.Connection objects: #pre I'd like to validate the parameters passed to DriverManager.getConnection , but I don't know how to mock a static method. I'm using JUnit 4 and Mockito for my test cases. Is there a good way to mock/verify this specific use-case?

Question-31922866, answer-31923105
Oh, those coding styles are to be taken with a bit of salt. #li (+) Passing an Option result to another method, without any semantic analysis; leaving that to the method, is quite alright. #li (-) Using Optional parameters causing conditional logic inside the methods is literally contra-productive. #li (-) Needing to pack an argument in an Optional, is suboptimal for the compiler, and does an unnecessary wrapping. #li (-) In comparison to nullable parameters Optional is more costly. In general: Optional unifies two states, which have to be unraveled. Hence better suited for result than input, for the complexity of the data flow.

Question-7348711, answer-7353473
InetAddress.getLocalHost(args).getHostName(args) is better (as explained by Nick), but still not very good One host can be known under many different hostnames. Usually you'll be looking for the hostname your host has in a specific context. For example, in a web application, you might be looking for the hostname used by whoever issued the request you're currently handling. How to best find that one depends on which framework you're using for your web application. In some kind of other internet-facing service, you'll want the hostname your service is available through from the 'outside'. Due to proxies, firewalls etc this might not even be a hostname on the machine your service is installed on - you might try to come up with a reasonable default, but you should definitely make this configurable for whoever installs this.

Question-31370403
I'm currently in the process of writing a paint program in java, designed to have flexible and comprehensive functionalities. It stemmed from my final project, that I wrote overnight the day before. Because of that, it's got tons and tons of bugs, which I've been tackling one by one (e.g. I can only save files that will be empty, my rectangles don't draw right but my circles do...). This time, I've been trying to add undo/redo functionality to my program. However, I can't "undo" something that I have done. Therefore, I got an idea to save copies of my BufferedImage each time a mouseReleased event was fired. However, with some of the images going to 1920x1080 resolution, I figured that this wouldn't be efficient: storing them would probably take gigabytes of memory. The reason for why I can't simply paint the same thing with the background colour to undo is because I have many different brushes, which paint based on Math.random(args) , and because there are many different layers (in a single layer). Then, I've considered cloning the Graphics objects that I use to paint to the BufferedImage . Like this: #pre I haven't done this before, so I have a couple questions: #li Would I still be wasting pointless memory by doing this, like cloning my BufferedImages ? #li Is there necessarily a different way I can do this?

Question-33358248
Currently the BlendModes (Subtract, Exclusion etc) use the LauncherImage as the mask. Can I apply these BlendModes to a ColorMatrix? I'm using the GPUImageLibrary colorMatrix[ 0.393, 0.7689999, 0.18899999, 0, 0, 0.349, 0.6859999, 0.16799999, 0, 0, 0.272, 0.5339999, 0.13099999, 0, 0, 0, 0, 0, 1, 0]; SubtractBlendFilter.java #pre GPUIMageTwoInputFilter.java #pre My guess it involves changing something with String SUBTRACT_BLEND_GRAGMENT_SHADER & String VERTEX_SHADER .

Question-6608795, answer-20069798
All these answers around here, as well as the answers in this question , suggest that loading absolute URLs, like "/foo/bar.properties" treated the same by class.getResourceAsStream(args) and class.getClassLoader(args).getResourceAsStream(args) . This is NOT the case, at least not in my Tomcat configuration/version (currently 7.0.40). #pre Sorry, I have absolutely no satisfying explanation, but I guess that tomcat does dirty tricks and his black magic with the classloaders and cause the difference. I always used class.getResourceAsStream(args) in the past and haven't had any problems. PS: I also posted this over here

Question-8579657, answer-8641412
Thread.start(args) code registers the Thread with scheduler and the scheduler calls the run(args) method. Also, Thread is class while Runnable is an interface.

Question-13102045, answer-7056786
It's because when you enter a number then press Enter , input.nextInt(args) consumes only the number, not the "end of line". When input.nextLine(args) executes, it consumes the "end of line" still in the buffer from the first input. Instead, use input.nextLine(args) immediately after input.nextInt(args)

Question-19431234, answer-23884116
the following seems to work when converting from new API LocalDateTime into java.util.date: Date.from(args); the reverse conversion can be (hopefully) achieved similar way... hope it helps...

Question-33147339, answer-33148103
Looking at the code: int newCapacity = oldCapacity + (oldCapacity >> 1); If oldCapacity is quite large, this will overflow, and newCapacity will be a negative number. A comparison like newCapacity < oldCapacity will incorrectly evaluate true and the ArrayList will fail to grow. Instead, the code as written ( newCapacity - minCapacity < 0 returns false) will allow the negative value of newCapacity to be further evaluated in the next line, resulting in recalculating newCapacity by invoking hugeCapacity ( newCapacity = hugeCapacity(args); ) to allow for the ArrayList to grow up to MAX_ARRAY_SIZE . This is what the // overflow-conscious code comment is trying to communicate, though rather obliquely. So, bottom line, the new comparison protects against allocating an ArrayList larger than the predefined MAX_ARRAY_SIZE while allowing it to grow right up to that limit if needed.

Question-29494800, answer-29495065
#li It is checking if newType is array of Objects or not: #pre Why to do that? Because new Object[n] is faster than Array.newInstance #li Array.newInstance(args) creates an array of types defined by the first argument, eg String.class -> String[] . Note that String[].class.getComponentType(args) returns String.class #li You cannot use it like that, but it can be like this Integer[] nums = Arrays.copyOf(args); in this case it depends only on actual type of elements, eg Arrays.copyOf(args); will fail, you cannot write in Integer[] anything but Integer

Question-29922296, answer-29923793
I think the shortest code to get a Stream of enum constants is Stream.of(args) . It's not as nice as Letter.values(args).stream(args) but that's an issue with arrays, not specifically enums. #blockquote You are right that the nicest possible call would be Letter.stream(args) . Unfortunately a class cannot have two methods with the same signature, so it would not be possible to implicitly add a static method stream(args) to every enum (in the same way that every enum has an implicitly added static method values(args) ) as this would break every existing enum that already has a static or instance method without parameters called stream(args) . #blockquote I think so. The drawback is that stream is a static method, so there is no way to avoid code duplication; it would have to be added to every enum separately.

Question-7283338, answer-7283475
Because any particular implementation of Set may or may not be random access . You can always get an iterator and step through the Set, using the iterators' next(args) method to return the result you want once you find the equal element. This works regardless of the implementation. If the implementation is NOT random access (picture a linked-list backed Set), a get(args) method in the interface would be deceptive, since it would have to iterate the collection to find the element to return, and a get(args) would seem to imply this would be necessary, that the Set could jump directly to the element to get. contains(args) may or may not have to do the same thing, of course, depending on the implementation, but the name doesn't seem to lend itself to the same sort of misunderstandings.

Question-14491966
I made a class called QuickRandom , and its job is to produce random numbers quickly. It's really simple: just take the old value, multiply by a double , and take the decimal part. Here is my QuickRandom class in its entirety: #pre And here is the code I wrote to test it: #pre It is a very simple algorithm that simply multiplies the previous double by a "magic number" double. I threw it together pretty quickly, so I could probably make it better, but strangely, it seems to be working fine. This is sample output of the commented-out lines in the main method: #pre Hm. Pretty random. In fact, that would work for a random number generator in a game. Here is sample output of the non-commented out part: #pre Wow! It performs almost 4 times faster than Math.random . I remember reading somewhere that Math.random used System.nanoTime(args) and tons of crazy modulus and division stuff. Is that really necessary? My algorithm performs a lot faster and it seems pretty random. I have two questions: #li Is my algorithm "good enough" (for, say, a game, where really random numbers aren't too important)? #li Why does Math.random do so much when it seems just simple multiplication and cutting out the decimal will suffice?

Question-5785745, answer-15962949
If you want to make a copy of: int[] a = {1,2,3,4,5}; This is the way to go: int[] b = Arrays.copyOf(args); Arrays.copyOf may be faster than a.clone(args) on small arrays. Both copy elements equally fast but clone() returns Object so the compiler has to insert an implicit cast to int[] . You can see it in the bytecode, something like this: #pre

Question-17489250, answer-17489453
Java treats String as a special class, you can initialize in both ways #li Directly assigning literal String a = StringLiteral; #li As other Objects using new keyword String a = new String(args); You need to take special care when you wants to compare with == sign: #pre That is because in first case the objects a and b are kept in something called literal pool and they both are referencing same object so they are equal in both ways. But in second case a and b references different objects like when we initialize any other objects. so they are unequal when compared with == operator whereas they are equal in values.

Question-29815636, answer-29815698
The first form is called Javadoc . You use this when you're writing formal APIs for your code, which are generated by the javadoc tool. For an example, the Java 7 API page uses Javadoc and was generated by that tool. Some common elements you'd see in Javadoc include: #li @param : this is used to indicate what parameters are being passed to a method, and what value they're expected to have #li @return : this is used to indicate what result the method is going to give back #li @throws : this is used to indicate that a method throws an exception or error in case of certain input #li @since : this is used to indicate the earliest Java version this class or function was available in As an example, here's Javadoc for the compare method of Integer : #pre The second form is a block (multi-line) comment. You use this if you want to have multiple lines in a comment. I will say that you'd only want to use the latter form sparingly ; that is, you don't want to overburden your code with block comments that don't describe what behaviors the method/complex function is supposed to have. Since Javadoc is the more descriptive of the two, and you can generate actual documentation as a result of using it, using Javadoc would be more preferable to simple block comments.

Question-31922866, answer-31924845
There are almost no good reasons for not using Optional as parameters. The arguments against this rely on arguments from authority (see Brian Goetz - his argument is we can't enforce non null optionals) or that the Optional arguments may be null (essentially the same argument). Of course, any reference in Java can be null, we need to encourage rules being enforced by the compiler, not programmers memory (which is problematic and does not scale). Functional programming languages encourage Optional parameters. One of the best ways of using this is to have multiple optional parameters and using liftM2 to use a function assuming the parameters are not empty and returning an optional (see #a ). Java 8 has unfortunately implemented a very limited library supporting optional. As Java programmers we should only be using null to interact with legacy libraries.

Question-8725387, answer-34405789
Set and Map are non-linear data structure. List is linear data structure. The tree data structure SortedSet and SortedMap interfaces implements TreeSet and TreeMap respectively using used Red-Black tree implementation algorithm. So it ensure that there are no duplicated items (or keys in case of Map ). #li Tree by definition cannot contain duplicates. #li In List we can have duplicates, so there is no TreeList . So if we want to sort the list we have to use java.util.Collections.sort(args) .

Question-24651403, answer-24651855
OK, first of all, your setExits(args) method will not work as intended, chained if-elseif will maximally execute 1 branch of code, for example: #pre Even if e contains both N and W , only bexits[0] will be set. Also this method will only add exits (for example calling setExits(args) will not delete any existing exits. I would change that method to: #pre Also, i definetly wouldn't remember that north is on index 0, west in on 1, ... so a common practice is to name your indexes using final static constants: #pre Then you can write in your setExits method: #pre (much more readible) Finally, if you want your code even more well-arranged, you can make a Exits class representing avaliable exits, and backed by boolean array. Then on place where you create your String, you could create this class instead and save yourself work with generating and then parsing a string. EDIT: as @gexicide answers, there is a really handy class EnumSet which would be probably better for representing the exits than bollean array.

Question-25903137, answer-25903190
#pre It's not quite as nice as the list code. You can't construct new Map.Entry s in a map(args) call so the work is mixed into the collect(args) call.

Question-31993377, answer-31993533
The reason is that hasNext(args) checks if there are any more non-whitespace characters available. hasNextLine(args) checks to see if there is another line of text available. Your text file probably has a newline at the end of it so it has another line but no more characters that are not whitespace. Many text editors automatically add a newline to the end of a file if there isn't one already. In other words, your input file is not this (the numbers are line numbers): #pre It is actually this: #pre

Question-14846920, answer-14847046
Most of the time we use a constructor to create a new empty map . But the Collections methods offer a couple of advantages to create an empty map using static method java.util.Collections.emptyMap(args) #blockquote

Question-4772425, answer-18953213
remove one y form SimpleDateFormat dt1 = new SimpleDateFormat(args); should be SimpleDateFormat dt1 = new SimpleDateFormat(args);

Question-30581531
What is the maximum number of parameters that a method in Java can have and why? I am using Java 1.8 on a 64-bit Windows system. All the answers on StackOverflow about this say that the technical limit is 255 parameters without specifying why. To be precise, 255 for static and 254 for non-static ( this will be the 255th in this case) methods. I thought this could be specified in some sort of specification and that this was simply a statically defined maximum number of parameters allowed. But this was only valid for int and all 4-bytes types . I did some tests with long parameters, and I was only able to declare 127 parameters in that case. With String parameters, the allowed number i deduced from testing is 255 (it may be because the reference size is 4 bytes in Java?). But since I am using a 64-bit system, references size should be 8 bytes wide and so with String parameters the maximum allowed number should be 127, similar to long types. How does this limit is exactly applied? Does the limit have anything to do with the stack size of the method? Note: I am not really going to use these many parameters in any method, but this question is only to clarify the exact behavior.

Question-34509566, answer-34509655
As you said internally the last concatenation is done to something similar to String e = new StringBuilder(args).append(args).append(args).toString(args); the implementation of toString(args) of StringBuilder creates a new String . Here is the implementation. #pre Comparing strings using == instead of .equals(args) returns true only if both strings are the same . In this case they are not the same because the second string is created as a new object of type String . The other concatenations are performed directly by the compiler so no new String is created.

Question-33147339, answer-33147610
a < b and a - b < 0 can mean two different things. Consider the following code: #pre When run, this will only print a - b < 0 . What happens is that a < b is clearly false, but a - b overflows and becomes -1 , which is negative. Now, having said that, consider that the array has a length that is really close to Integer.MAX_VALUE . The code in ArrayList goes like this: #pre oldCapacity is really close to Integer.MAX_VALUE so newCapacity (which is oldCapacity + 0.5 * oldCapacity ) might overflow and become Integer.MIN_VALUE (i.e. negative). Then, subtracting minCapacity underflows back into a positive number. This check ensures that the if is not executed. If the code were written as if(args) , it would be true in this case (since newCapacity is negative) so the newCapacity would be forced to minCapacity regardless of the oldCapacity . This overflow case is handled by the next if. When newCapacity has overflowed, this will be true : MAX_ARRAY_SIZE is defined as Integer.MAX_VALUE - 8 and Integer.MIN_VALUE - (Integer.MAX_VALUE - 8) > 0 is true . The newCapacity is therefore rightly handled: hugeCapacity method returns MAX_ARRAY_SIZE or Integer.MAX_VALUE . NB: this is what the // overflow-conscious code comment in this method is saying.

Question-32475166, answer-32476850
When you think about the type of the operands, the problem becomes more apparent: this.method != null ? this.method : this.constructor has as type the most specialized common type of both the operands, i.e. the most specialized type common to both this.method and this.constructor . In Java 7 this is java.lang.reflect.Member , however the Java 8 class library introduces a new type java.lang.reflect.Executable which is more specialized than the generic Member . Hence with a Java 8 class library the result type of the ternary expression is Executable rather than Member . Some (pre-release) versions of the Java 8 compiler seem to have produced an explicit reference to Executable inside generated code when compiling the ternary operator. This would trigger a class load, and thus in turn a ClassNotFoundException at runtime when running with a class library < JDK 8, because Executable only exists for JDK ≥ 8. As noted by Tagir Valeev in this answer , this is actually a bug in pre-release versions of JDK 8 and has since been fixed, so both the if-else workaround and the explanatory comment are now obsolete. Additional note: One might come to the conclusion that this compiler bug was present before Java 8. However, the byte code generated for the ternary by OpenJDK 7 is the same as the byte code generated by OpenJDK 8. In fact, the type of the expression goes completely unmentioned at runtime, the code is really only test, branch, load, return without any additional checks going on. So rest assured that this is not a problem (anymore) and indeed seems to have been a temporary problem during development of Java 8.

Question-32714194, answer-32714537
While currently most of Java compilers using StringBuilder chain, it's not specified that it should be always in this way. In particular there's a proposal to change this drastically in Java-9 replacing with single invokedynamic call and introduce new metafactory which will generate an appropriate MethodHandle in runtime to perform concatenation.

Question-15430247, answer-15430259
If you know in advance what the size of the ArrayList is going to be, it is more efficient to specify the initial capacity. If you don't do this, the internal array will have to be repeatedly reallocated as the list grows. The larger the final list, the more time you save by avoiding the reallocations. That said, even without pre-allocation, inserting n elements at the back of an ArrayList is guaranteed to take total O(args) time. In other words, appending an element is an amortized constant-time operation. This is achieved by having each reallocation increase the size of the array exponentially, typically by a factor of 1.5 . With this approach, the total number of operations can be shown to be O(n) .

Question-20363719, answer-34675066
I was trying to do this and found that, using the answers above, when using Functions.identity(args) for the key to the Map, then I had issues with using a local method like this::localMethodName to actually work because of typing issues. Functions.identity(args) actually does something to the typing in this case so the method would only work by returning Object and accepting a param of Object To solve this, I ended up ditching Functions.identity(args) and using s->s instead. So my code, in my case to list all directories inside a directory, and for each one use the name of the directory as the key to the map and then call a method with the directory name and return a collection of items, looks like: Map<T> items = Arrays.stream(args) .map(args) .collect(args);

Question-12539365, answer-29349910
I must say, that you are absolutely right from a classical approach's point of view: #li If you first create (or change) business logic of your application and then cover it with (adopt) tests ( Test-Last approach ), then it will be very painful and dangerous to let tests know anything about how your software works, other than checking inputs and outputs. #li If you are practicing a Test-Driven approach , then your tests are the first to be written, to be changed and to reflect the use cases of your software's functionality. The implementation depends on tests. That sometimes mean, that you want your software to be implemented in some particular way, e.g. rely on some other component's method or even call it a particular amount of times. That is where Mockito.verify() comes in handy! It is important to remember, that there are no universal tools. The type of software, it's size, company goals and market situation, team skills and many other things influence the decision on which approach to use at your particular case.

Question-25850328, answer-25853827
I would argue first-off that Serializable and Cloneable are bad examples of marker interfaces. Sure, they're interfaces with methods, but they imply methods, such as writeObject(args) . (The compiler will create a writeObject(args) method for you if you don't override it, and all objects already have clone(args) , but the compiler will again create a real clone(args) method for you but with caveats. Both of these are weird edge cases that really aren't good design examples.) Marker interfaces are generally used for one of two purposes: 1) As a shortcut to avoid an excessively long type, which can happen with lots of generics. For instance, say you have this method signature: public void doSomething(args) { ... } That's messy and annoying to type, and more importantly, difficult to understand. Consider this instead: public interface Widget extends Foobar<T> { } Then your method looks like this: public void doSomething(args) { ... } Not only is it clearer, but you can now Javadoc the Widget interface, and it's also easier to search for all occurrences in your code of Widget. 2) Marker interfaces can also be used as a way around Java's lack of intersection types. With a marker interface, you can require something to be of two different types, such as in a method signature. Say you have some interface Widget in your application, like we described above. If you have a method that requires a Widget that also happens to let you iterate over it (it's contrived, but work with me here), your only good solution is to create a marker interface that extends both interfaces: public interface IterableWidget extends Iterable<T>, Widget { } And in your code: #pre

Question-8275499, answer-8275508
Just use TheClassName.class instead of getClass(args) .

Question-20129762, answer-20130475
To convert a Stream to an Iterable , you can do #pre To pass a Stream to a method that expects Iterable , void foo(args) simply foo(args) however it probably looks funny; it might be better to be a little bit more explicit foo(args);

Question-22929237, answer-28564046
You can use java.sql.Date.valueOf(args) method as: Date date = java.sql.Date.valueOf(args); No need to add time and time zone info here because they are taken implicitly. See Simpliest java8 LocalDate to java.util.Date conversion and vice versa?

Question-22036885, answer-22037068
As said in API #blockquote It's because of cannot represent double value exactly. So you have to use BigDecimal bigDecimal = BigDecimal.valueOf(args); instead of BigDecimal bigDecimal = new BigDecimal(args);

Question-31170459
I was testing boundary conditions on some code involving a BigDecimal , and I noticed that when a BigDecimal is initialized with the String StringLiteral it behaves unexpectedly. It seems to have a value between 0 and 1e-2147483647 . When I try calling intValue(args) , I get a NegativeArraySizeException . I should note that 2147483647 is the max value of an integer on my system. Am I doing something wrong, or is this a problem with BigDecimal ? #pre

Question-30888581, answer-30897102
Once given more thought In hindsight it sounds like they were looking for the execute around pattern . They're usually used to do things like enforce closing of streams. This is also more relevant due to this line: #blockquote The idea is you give the thing that does the "executing around" some class to do somethings with. You'll probably use Runnable but it's not necessary. ( Runnable makes the most sense and you'll see why soon.) In your StopWatch class add some method like this #pre You would then call it like this #pre This makes it fool proof. You don't have to worry about handling stop before start or people forgetting to call one and not the other, etc. The reason Runnable is nice is because #li Standard java class, not your own or third party #li End users can put whatever they need in the Runnable to be done. (If you were using it to enforce stream closing then you could put the actions that need to be done with a database connection inside so the end user doesn't need to worry about how to open and close it and you simultaneously force them to close it properly.) If you wanted, you could make some StopWatchWrapper instead leave StopWatch unmodified. You could also make measureAction(args) not return a time and make getTime(args) public instead. The Java 8 way to calling it is even simpler #pre #blockquote Original answer #blockquote To me something like this seems to be good. #pre The reason I believe this to be good is the recording is during object creation so it can't be forgotten or done out of order (can't call stop(args) method if it doesn't exist). One flaw is probably the naming of stop(args) . At first I thought maybe lap(args) but that usually implies a restarting or some sort (or at least recording since last lap/start). Perhaps read(args) would be better? This mimics the action of looking at the time on a stop watch. I chose stop(args) to keep it similar to the original class. The only thing I'm not 100% sure about is how to get the time. To be honest that seems to be a more minor detail. As long as both ... in the above code obtain current time the same way it should be fine.

Question-22561614, answer-22561690
Comparator is a functional interface , and Integer::max complies with that interface (after autoboxing/unboxing is taken into consideration). It takes two int values and returns an int - just as you'd expect a Comparator<T> to (again, squinting to ignore the Integer/int difference). However, I wouldn't expect it to do the right thing, given that Integer.max doesn't comply with the semantics of Comparator.compare . And indeed it doesn't really work in general. For example, make one small change: #pre ... and now the max value is -20 and the min value is -1. Instead, both calls should use Integer::compare : #pre

Question-34172978, answer-34173658
If you want to stop either if number 5 is reached or 10 elements are collected, there's Stream.takeWhile(args) method added in Java-9: #pre

Question-23699371, answer-34731808
You can use the distinct(args) method in Eclipse Collections . #pre If you can refactor persons to implement an Eclipse Collections interface, you can call the method directly on the list. #pre HashingStrategy is simply a strategy interface that allows you to define custom implementations of equals and hashcode. #pre Note: I am a committer for Eclipse Collections.

Question-14846920, answer-14846943
It is, in my personal experience admittedly, very useful in cases where an API requires a collection of parameters, but you have nothing to provide. For example you may have an API that looks something like this, and does not allow null references: public ResultSet executeQuery(args); If you have a query that doesn't take any parameters, it's certainly a bit wasteful to create a HashMap, which involves allocating an array, when you could just pass in the 'Empty Map' which is effectively a constant, the way it's implemented in java.util.Collections .

Question-14809293, answer-14821488
The Collections.unmodifiableList has already been mentioned - the Arrays.asList(args) strangely not! My solution would also be to use the list from the outside and wrap the array as follows: #pre The problem with copying the array is: if you're doing it every time you access the code and the array is big, you'll create a lot of work for the garbage collector for sure. So the copy is a simple but really bad approach - I'd say "cheap", but memory-expensive! Especially when you're having more than just 2 elements. If you look at the source code of Arrays.asList and Collections.unmodifiableList there is actually not much created. The first just wraps the array without copying it, the second just wraps the list, making changes to it unavailable.

Question-5585779, answer-36394016
Whenever there is the slightest possibility that the given String does not contain an Integer, you have to handle this special case. Sadly, the standard Java methods Integer::parseInt and Integer::valueOf throw a NumberFormatException to signal this special case. Thus, you have to use exceptions for flow control, which is generally considered bad coding style. In my opinion, this special case should be handled by returning an Optional<T> . Since Java does not offer such a method, I use the following wrapper: #pre Usage: #pre While this is still using exceptions for flow control internally, the usage code becomes very clean.

Question-7520432, answer-29226136
Also note that .equals(args) normally contains == for testing as this is the first thing you would wish to test for if you wanted to test if two objects are equal. And == actually does look at values for primitive types, for objects it checks the reference.

Question-25222811, answer-26676046
To begin with ( and unrelated ), instantiating the Application class by yourself does not seem to be its intended use. From what one can read from its source , you are rather expected to use the static instance returned by getApplication(args) . Now let's get to the error Eclipse reports. I've ran into a similar issue recently: Access restriction: The method ... is not API(args) . I called the method in question as a method of an object which inherited that method from a super class. All I had to do was to add the package the super class was in to the packages imported by my plugin. However, there is a lot of different causes for errors based on "restriction on required project/library ". Similar to the problem described above, the type you are using might have dependencies to packages that are not exported by the library or might not be exported itself. In that case you can try to track down the missing packages and export them my yourself, as suggested here , or try Access Rules. Other possible scenarios include: #li Eclipse wants to keep you from using available packages that are not part of the public Java API (solution 1 , 2 ) #li Dependencies are satisfied by multiple sources, versions are conflicting etc. (solution 1 , 2 , 3 ) #li Eclipse is using a JRE where a JDK is necessary (which might be the case here, from what your errors say; solution ) or JRE/JDK version in project build path is not the right one This ended up as more like a medley of restriction-related issues than an actual answer. But since restriction on required projects is such a versatile error to be reported, the perfect recipe is probably still to be found.

Question-27683759
I just studied about generic programming, the List<T> interface, and ArrayList , so I can understand the statement below. ArrayList<T> list = new ArrayList<T>(args); But I don't understand the next statement which I saw while surfing the web. List<T> list2 = Collections.<String>emptyList(args); #li What is Collections ? Why isn't it Collections<T> or Collections<T> ? #li Why is <String> placed before the method name emptyList ? (Isn't emptyList<T>(args) correct for Generic?) #li What does the statement mean?

Question-10977992
#pre We can use Iterator to traverse a Set or a List or a Map . But ListIterator can only be used to traverse a List , it can't traverse a Set . Why? I know that the main difference is that with iterator we can travel in only one direction but with ListIterator we can travel both directions. Are there any other differences? And any advantages of ListIterator over Iterator ?

Question-8725387, answer-8725518
Because the concept of a List is incompatible with the concept of an automatically sorted collection. The point of a List is that after calling list.add(args) , a call to list.get(args) will return elem . With an auto-sorted list, the element could end up in an arbitrary position.

Question-15336477, answer-32364785
Examples from Apache ( link ) use this: HttpClient httpclient = HttpClients.createDefault(args); The class org.apache.http.impl.client.HttpClients is there since version 4.3. The code for HttpClients.createDefault(args) is the same as the accepted answer in here.

Question-27430092, answer-27449187
The standard use case for BigInteger.isProbablePrime(int) is in cryptography. Specifically, certain cryptographic algorithms, such as RSA , require randomly chosen large primes. Importantly, however, these algorithms don't really require these numbers to be guaranteed to be prime — they just need to be prime with a very high probability. How high is very high? Well, in a crypto application, one would typically call .isProbablePrime(args) with an argument somewhere between 128 and 256. Thus, the probability of a non-prime number passing such a test is less than one in 2 128 or 2 256 . Let's put that in perspective: if you had 10 billion computers, each generating 10 billion probable prime numbers per second (which would mean less than one clock cycle per number on any modern CPU), and the primality of those numbers was tested with .isProbablePrime(args) , you would, on average, expect one non-prime number to slip in once in every 100 billion years . That is, that would be the case, if those 10 billion computers could somehow all run for hundreds of billions of years without experiencing any hardware failures. In practice, though, it's a lot more likely for a random cosmic ray to strike your computer at just the right time and place to flip the return value of .isProbablePrime(args) from false to true, without causing any other detectable effects, than it is for a non-prime number to actually pass the probabilistic primality test at that certainty level. Of course, the same risk of random cosmic rays and other hardware faults also applies to deterministic primality tests like AKS . Thus, in practice, even these tests have a (very small) baseline false positive rate due to random hardware failures (not to mention all other possible sources of errors, such as implementation bugs). Since it's easy to push the intrinsic false positive rate of the Miller–Rabin primality test used by .isProbablePrime(args) far below this baseline rate, simply by repeating the test sufficiently many times, and since, even repeated so many times, the Miller–Rabin test is still much faster in practice than the best known deterministic primality tests like AKS, it remains the standard primality test for cryptographic applications. (Besides, even if you happened to accidentally select a strong pseudoprime as one of the factors of your RSA modulus, it would not generally lead to a catastrophic failure. Typically, such pseudoprimes would be products of two (or rarely more) primes of approximately half the length, which means that you'd end up with a multi-prime RSA key . As long as none of the factors were too small (and if they were, the primality test should've caught them), the RSA algorithm will still work just fine, and the key, although somewhat weaker against certain types of attacks than normal RSA keys of the same length, should still be reasonably secure if you didn't needlessly skimp on the key length.)

Question-10827872, answer-22578124
Old thread I know but I am sort of new to Java and discover one of it's strange things. I have used String.replaceAll(args) but get unpredictable results. Something like this mess up the string: sUrl = sUrl.replaceAll(args).replaceAll(args); So I designed this function to get around the weird problem: #pre Which make you able to do: #pre

Question-18571223, answer-18571348
The object your method decompressGZIP(args) needs is a byte[] . So the basic, technical answer to the question you have asked is: #pre However the problem you appear to be wrestling with is that this doesn't display very well. Calling toString(args) will just give you the default Object.toString(args) which is the class name + memory address. In your result [B@38ee9f13 , the [B means byte[] and 38ee9f13 is the memory address, separated by an @ . For display purposes you can use: Arrays.toString(args); But this will just display as a sequence of comma-separated integers, which may or may not be what you want. To get a readable String back from a byte[] , use: String string = new String(args); The reason the Charset version is favoured, is that all String objects in Java are stored internally as UTF-16. When converting to a byte[] you will get a different breakdown of bytes for the given glyphs of that String , depending upon the chosen charset.

Question-7032070, answer-7032208
clear(args) will go through the underlying Array and set each entry to null; removeAll(args) will go through the ArrayList checking for collection and remove(args) it if it exists. I would imagine that clear(args) is way faster then removeAll because it's not comparing, etc.

Question-23218874
I understand that with .stream(args) , I can use chain operations like .filter(args) or use parallel stream. But what is difference between them if I need to execute small operations (for example, printing the elements of the list)? #pre

Question-26791441, answer-26791724
Use the replaceEach() method from Apache Commons StringUtils : StringUtils.replaceEach(args)

Question-14534767
I have a StringBuilder object, #pre Now I want to append a newline character to the StringBuilder . How can I do it? result.append(args); Does not work. So, I was thinking about writing a newline using Unicode. Will this help? If so, how can I add one?

Question-27949213, answer-27949367
The strategy they describe is to simply reuse the internal character array of one String in possibly many equal Strings. There's no need for each String to have its own copy if they are equal. In order to more quickly determine if 2 strings are equal, the hash code is used as a first step, as it is a fast way to determine if Strings may be equal. Hence their statement: #blockquote This is to make a certain (but slower) comparison for equality once possible equality has been determined using the hash code. In the end, equal Strings will share a single underlying char array. Java has had String.intern(args) for a long time, to do more or less the same (i.e. save memory by deduplicating equal Strings). What's novel about this is that it happens during garbage collection time and can be externally controlled.

Question-27888429, answer-27888447
You can use Arrays.stream E.g. Arrays.stream(args); You can also use Stream.of as mentioned by @fge , which looks like #pre But note Stream.of(args) will return Stream<T> whereas Arrays.stream(args) will return IntStream providing you pass an array of type int[] . So in a nutshell for primitives type you can observe the difference between 2 methods E.g. #pre When you pass primitive array to Arrays.stream , the following code is invoked #pre and when you pass primitive array to Stream.of the following code is invoked #pre Hence you get different results. Updated : As mentioned by Stuart Marks comment The subrange overload of Arrays.stream is preferable to using Stream.of(args).skip(args).limit(args) because the former results in a SIZED stream whereas the latter does not. The reason is that limit(args) doesn't know whether the size is m or less than m, whereas Arrays.stream does range checks and knows the exact size of the stream You can read the source code for stream implementation returned by Arrays.stream(args) here , whereas for stream implementation returned by Stream.of(args).skip(args).limit(args) is within this method .

Question-14846920, answer-14846960
From Effective Java , Item #43 - StringLiteral demonstrates returning an empty collection and perhaps even demonstrates using these emptyList(args) , emptySet(args) , and emptyMap(args) methods on the Collections class to get an empty collection that also has the additional benefit of being immutable. From Item #15 StringLiteral . From Collections-emptySet-Collections-emptyList-Collections #blockquote Note: Below code is just an example (change it according to your use case): #pre These methods offer a couple of advantages: #li They're more concise because you don't need to explicitly type out the generic type of the collection - it's generally just inferred from the context of the method call. #li They're more efficient because they don't bother creating new objects; they just re-use an existing empty and immutable object. This effect is generally very minor, but it's occasionally (well, rarely) important.

Question-23932061, answer-23936723
There's a much better answer than using spliteratorUnknownSize directly, which is both easier and gets a better result. Iterable has a spliterator(args) method, so you should just use that to get your spliterator. In the worst case, its the same code (the default implementation uses spliteratorUnknownSize ), but in the more common case, where your Iterable is already a collection, you'll get a better spliterator, and therefore better stream performance (maybe even good parallelism.) Its also less code: StreamSupport.stream(args) .filter(args) .moreStreamOps(args); As you can see, getting a stream from an Iterable (see Why does Iterable<T> not provide stream() and parallelStream() methods? ) is not very painful.

Question-8275499, answer-8275761
getClass(args) method is defined in Object class with the following signature: #blockquote Since it is not defined as static , you can not call it within a static code block. See these answers for more information: Q1 , Q2 , Q3 . If you're in a static context, then you have to use the class literal expression to get the Class, so you basically have to do like: #blockquote This type of expression is called Class Literals and they are explained in Java Language Specification Book as follows: #blockquote You can also find information about this subject on API documentation for Class.

Question-24649842
What's the main difference between scheduleAtFixedRate and scheduleWithFixedDelay methods of ScheduledExecutorService ? #pre they print exact the same time, seems they are executed at exact the same interval.

Question-9510125, answer-9511578
Just IMHO. Not sure it's 100% correct. invalidate(args) marks the container as invalid. Means the content is somehow wrong and must be relayed out. But it's just a kind of mark/flag. It's possible that multiple invalid containers must be refreshed later. validate(args) performs relayout. It means invalid content is asked for all the sizes and all the subcomponents' sizes are set to proper values by LayoutManager . revalidate(args) is just sum of both. It marks the container as invalid and performs layout of the container.

Question-33804394, answer-33814549
In this sort of situation I would write #pre Interestingly you say that the "try statement will never fail", but you still took the trouble to write a statement e.printStackTrace(args); that you claim will never be executed. Why? Perhaps your belief is not that firmly held. That is good (in my opinion), since your belief is not based on the code you wrote, but rather on the expectation that your client will not violate the precondition. Better to program public methods defensively. By the way, your code won't compile for me. You can't call a.clone(args) even if the type of a is Cloneable . At least Eclipse's compiler says so. Expression a.clone(args) gives error #blockquote What I would do for your specific case is #pre Where PubliclyCloneable is defined by #pre Or, if you absolutely need the parameter type to be Cloneable , the following at least compiles. #pre

Question-34509566, answer-34509669
The difference between d and e is that when you concatenate string literals , the concatenation is performed at compile time. Java compiler treats StringLiteral + StringLiteral expression in the same way as StringLiteral expression, producing the same literal at compile time. Since all String literals get interned, d , which is a result of StringLiteral + StringLiteral , also ends up referencing the same object as a and b 's StringLiteral . The expression for e , which is c + StringLiteral , is evaluated at runtime . Even though it produces the same string, this fact is not used by the compiler. That's why a different String object is produced, resulting in failed comparison on == .

Question-20331163, answer-20331323
Another way of doing that is: String date = dateAndTime.substring(args); I'm not exactly certain, but I think this might be faster/use less memory than using the .split(args) method.

Question-10786042, answer-16226168
I would not use URLEncoder . Besides being incorrectly named ( URLEncoder has nothing to do with URLs), inefficient (it uses a StringBuffer instead of Builder and does a couple of other things that are slow) Its also way too easy to screw it up. Instead I would use URIBuilder or Spring's URIUtils or Commons Apache HttpClient . The reason being you have to escape the query parameters name (ie BalusC's answer q ) differently than the parameter value. The only downside to the above (that I found out painfully) is that URL's are not a true subset of URI's . Since I'm just linking to other answers I marked this as a community wiki. Feel free to edit.

Question-14014086, answer-20784007
Ken's answer is basically right but I'd like to chime in on the "why would you want to use one over the other?" part of your question. Basics The base interface you choose for your repository has two main purposes. First, you allow the Spring Data repository infrastructure to find your interface and trigger the proxy creation so that you inject instances of the interface into clients. The second purpose is to pull in as much functionality as needed into the interface without having to declare extra methods. The common interfaces The Spring Data core library ships with two base interfaces that expose a dedicated set of functionalities: #li CrudRepository - CRUD methods #li PagingAndSortingRepository - methods for pagination and sorting (extends CrudRepository ) Store-specific interfaces The individual store modules (e.g. for JPA or MongoDB) expose store-specific extensions of these base interfaces to allow access to store-specific functionality like flushing or dedicated batching that take some store specifics into account. An example for this is deleteInBatch(args) of JpaRepository which is different from delete(args) as it uses a query to delete the given entities which is more performant but comes with the side effect of not triggering the JPA-defined cascades (as the spec defines it). We generally recommend not to use these base interfaces as they expose the underlying persistence technology to the clients and thus tighten the coupling between them and the repository. Plus, you get a bit away from the original definition of a repository which is basically "a collection of entities". So if you can, stay with PagingAndSortingRepository . Custom repository base interfaces The downside of directly depending on one of the provided base interfaces is two-fold. Both of them might be considered as theoretical but I think they're important to be aware of: #li Depending on a a Spring Data repository interface couples your repository interface to the library. I don't think this is a particular issue as you'll probably use abstractions like Page or Pageable in your code anyway. Spring Data is not any different from any other general purpose library like commons-lang or Guava. As long as it provides reasonable benefit, it's just fine. #li By extending e.g. CrudRepository , you expose a complete set of persistence method at once. This is probably fine in most circumstances as well but you might run into situations where you'd like to gain more fine-grained control over the methods expose, e.g. to create a ReadOnlyRepository that doesn't include the save(args) and delete(args) methods of CrudRepository . The solution to both of these downsides is to craft your own base repository interface or even a set of them. In a lot of applications we've seen something like this: #pre The first repository interface is some general purpose base interface that actually only fixes point 1 but also ties the ID type to be Long for consistency. The second interface usually has all the find…(…) methods copied from CrudRepository and PagingAndSortingRepository but does not expose the manipulating ones. Read more on that approach in the reference documentation . Summary - tl;dr The repository abstraction allows you to pickt the base repository totally driven by you architectural and functional needs. Use the ones provided out of the box if they suit, craft your own repository base interfaces if necessary. Stay away from the store specific repository interfaces unless unavoidable.

Question-14491966, answer-14495128
If the Math.Random(args) function calls the operating system to get the time of day, then you cannot compare it to your function. Your function is a PRNG, whereas that function is striving for real random numbers. Apples and oranges. Your PRNG may be fast, but it does not have enough state information to achieve a long period before it repeats (and its logic is not sophisticated enough to even achieve the periods that are possible with that much state information). Period is the length of the sequence before your PRNG begins to repeat itself. This happens as soon as the PRNG machine makes a state transition to a state which is identical to some past state. From there, it will repeat the transitions which began in that state. Another problem with PRNG's can be a low number of unique sequences, as well as degenerate convergence on a particular sequence which repeats. There can also be undesirable patterns. For instance, suppose that a PRNG looks fairly random when the numbers are printed in decimal, but an inspection of the values in binary shows that bit 4 is simply toggling between 0 and 1 on each call. Oops! Take a look at the Mersenne Twister and other algorithms. There are ways to strike a balance between the period length and CPU cycles. One basic approach (used in the Mersenne Twister) is to cycle around in the state vector. That is to say, when a number is being generated, it is not based on the entire state, just on a few words from the state array subject to a few bit operations. But at each step, the algorithm also moves around in the array, scrambling the contents a little bit at a time.

Question-14491966, answer-14492043
'Random' is more than just about getting numbers.... what you have is pseudo-random If pseudo-random is good enough for your purposes, then sure, it's way faster (and XOR+Bitshift will be faster than what you have) Rolf Edit: OK, after being too hasty in this answer, let me answer the real reason why your code is faster: From the JavaDoc for Math.Random() #blockquote This is likely why your code is faster.

Question-20001427, answer-22245383
:: is called Method Reference. It is basically a reference to a single method. i.e. it refers to an existing method by name. Short Explanation : Below is an example of a reference to a static method: #pre square can be passed around just like object reference's and trigger when at need. In fact, it can be perfectly used as a reference to a normal method of an object and not just static ones. #pre Function above is a functional interface . Well to fully explain :: , it is important to understand Functional Interface. Plainly, Function interface is an interface with just one abstract method. For example: Runnable , Callable , ActionListener and so. Function above is a functional interface with just one method apply . It takes one argument and produces a result. The reason why :: are awesome is because: #blockquote i.e. Just like writing lambda body: Function<T> square = (Double x) -> x * x; You can simply do: Function<T> square = Hey::square; At runtime they behave exactly the same. The bytecode may/not be the same (For above case, it generates the same bytecode (compile above and check javap -c )) The only major criteria to satisfy is: the method you provide should have a similar signature to method of the FunctionalInterface you use as object reference. Below is illegal Supplier<T> p = Hey::square; //illegal square expects an argument and returns a double. get method in Supplier expects an argument but doesn't return anything. So it is an error. Method Reference refers to a method of the functional interface (As mentioned, functional interface can only have one method). Some more examples: accept method in Consumer takes an input but doesnt return anything. #pre Above getRandom takes no argument and returns a double. So any functional interface that satisfies the criteria of: take no argument and return double can be used. Another example: #pre In case of Parametrized Types : #pre Method Reference can be obtained in different styles, but fundamentally they all mean the same and can simply be visualized as a lambda: #li A static method ( ClassName::methName ) #li An instance method of a particular object ( instanceRef::methName ) #li A super method of a particular object ( super::methName ) #li An instance method of an arbitrary object of a particular type ( ClassName::methName ) #li A class constructor reference ( ClassName::new ) #li An array constructor reference ( TypeName[]::new ) For further reference: #a

Question-15336477, answer-15336606
It got deprecated in version 4.3-alpha1 which you use because of the LATEST version specification. If you take a look at the javadoc of the class, it tells you what to use instead: HttpClientBuilder . In the latest stable version ( 4.2.3 ) the DefaultHttpClient is not deprecated yet.

Question-18532488, answer-24296741
Here's another technique I ran across the other day: Collections.nCopies(args) .stream(args) .forEach(args); The Collections.nCopies call creates a List containing n copies of whatever value you provide. In this case it's the boxed Integer value 1. Of course it doesn't actually create a list with n elements; it creates a "virtualized" list that contains only the value and the length, and any call to get within range just returns the value. The nCopies method has been around since the Collections Framework was introduced way back in JDK 1.2. Of course, the ability to create a stream from its result was added in Java SE 8. Big deal, another way to do the same thing in about the same number of lines. However, this technique is faster than the IntStream.generate and IntStream.iterate approaches, and surprisingly, it's also faster than the IntStream.range approach. For iterate and generate the result is perhaps not too surprising. The streams framework (really, the Spliterators for these streams) is built on the assumption that the lambdas will potentially generate different values each time, and that they will generate an unbounded number of results. This makes parallel splitting particularly difficult. The iterate method is also problematic for this case because each call requires the result of the previous one. So the streams using generate and iterate don't do very well for generating repeated constants. The relatively poor performance of range is surprising. This too is virtualized, so the elements don't actually all exist in memory, and the size is known up front. This should make for a fast and easily parallelizable spliterator. But it surprisingly didn't do very well. Perhaps the reason is that range has to compute a value for each element of the range and then call a function on it. But this function just ignores its input and returns a constant, so I'm surprised this isn't inlined and killed. The Collections.nCopies technique has to do boxing/unboxing in order to handle the values, since there are no primitive specializations of List . Since the value is the same every time, it's basically boxed once and that box is shared by all n copies. I suspect boxing/unboxing is highly optimized, even intrinsified, and it can be inlined well. Here's the code: #pre And here are the JMH results: (2.8GHz Core2Duo) #pre There is a fair amount of variance in the ncopies version, but overall it seems comfortably 20x faster than the range version. (I'd be quite willing to believe that I've done something wrong, though.) I'm surprised at how well the nCopies technique works. Internally it doesn't do very much special, with the stream of the virtualized list simply being implemented using IntStream.range ! I had expected that it would be necessary to create a specialized spliterator to get this to go fast, but it already seems to be pretty good.

Question-18777989, answer-37905386
Many junior developers make the mistake of thinking of interfaces, abstract and concrete classes as slight variations of the same thing, and choose one of them purely on technical grounds: Do I need multiple inheritance? Do I need some place to put common methods? Do I need to bother with something other than just a concrete class? This is wrong, and hidden in these questions is the main problem: "I" . When you write code for yourself, by yourself, you rarely think of other present or future developers working on or with your code. Interfaces and abstract classes, although apparently similar from a technical point of view, have completely different meanings and purposes. Summary #li An interface defines a contract that some implementation will fulfill for you . #li An abstract class provides a default behavior that your implementation can reuse. These two points above is what I'm looking for when interviewing, and is a compact enough summary. Read on for more details. Alternative summary #li An interface is for defining public APIs #li An abstract class is for internal use, and for defining SPIs By example To put it differently: A concrete class does the actual work, in a very specific way. For example, an ArrayList uses a contiguous area of memory to store a list of objects in a compact manner which offers fast random access, iteration, and in-place changes, but is terrible at insertions, deletions, and occasionally even additions; meanwhile, a LinkedList uses double-linked nodes to store a list of objects, which instead offers fast iteration, in-place changes, and insertion/deletion/addition, but is terrible at random access. These two types of lists are optimized for different use cases, and it matters a lot how you're going to use them. When you're trying to squeeze performance out of a list that you're heavily interacting with, and when picking the type of list is up to you, you should carefully pick which one you're instantiating. On the other hand, high level users of a list don't really care how it is actually implemented, and they should be insulated from these details. Let's imagine that Java didn't expose the List interface, but only had a concrete List class that's actually what LinkedList is right now. All Java developers would have tailored their code to fit the implementation details: avoid random access, add a cache to speed up access, or just reimplement ArrayList on their own, although it would be incompatible with all the other code that actually works with List only. That would be terrible... But now imagine that the Java masters actually realize that a linked list is terrible for most actual use cases, and decided to switch over to an array list for their only List class available. This would affect the performance of every Java program in the world, and people wouldn't be happy about it. And the main culprit is that implementation details were available, and the developers assumed that those details are a permanent contract that they can rely on. This is why it's important to hide implementation details, and only define an abstract contract. This is the purpose of an interface: define what kind of input a method accepts, and what kind of output is expected, without exposing all the guts that would tempt programmers to tweak their code to fit the internal details that might change with any future update. An abstract class is in the middle between interfaces and concrete classes. It is supposed to help implementations share common or boring code. For example, AbstractCollection provides basic implementations for isEmpty based on size is 0, contains as iterate and compare, addAll as repeated add , and so on. This lets implementations focus on the crucial parts that differentiate between them: how to actually store and retrieve data. Another perspective: APIs versus SPIs Interfaces are low-cohesion gateways between different parts of code. They allow libraries to exist and evolve without breaking every library user when something changes internally. It's called Application Programming Interface Abstract classes are high-cohesion helpers to be used when implementing an interface, assuming some level of implementation details. Alternatively, abstract classes are used for defining SPIs, Service Provider Interfaces. The difference between an API and an SPI is subtle, but important: for an API, the focus is on who uses it, and for an SPI the focus is on who implements it. Adding methods to an API is easy, all existing users of the API will still compile. Adding methods to an SPI is hard, since every service provider (concrete implementation) will have to implement the new methods. If interfaces are used to define an SPI, a provider will have to release a new version whenever the SPI contract changes. If abstract classes are used instead, new methods could either be defined in terms of existing abstract methods, or as empty throw not implemented exception stubs, which will at least allow an older version of a service implementation to still compile and run. A note on Java 8 and default methods Although Java 8 introduced default methods for interfaces, which makes the line between interfaces and abstract classes even blurrier, this wasn't so that implementations can reuse code, but to make it easier to change interfaces that serve both as an API and as an SPI (or are wrongly used for defining SPIs instead of abstract classes). "Book knowledge" The technical details provided in the OP's answer are considered "book knowledge" because this is usually the approach used in school and in most technology books about a language: what a thing is, not how to use it in practice, especially in large scale applications. Here's an analogy: supposed the question was: #blockquote The technical answer sounds like: #blockquote That is all true, but completely misses the points that they are two completely different things, and both can be used at the same time for different purposes, and the "doing it" aspect is not the most important thing about either of the two options. The answer lacks perspective, it shows an immature way of thinking, while correctly presenting true "facts".

Question-5585779, answer-5585876
For example, here are two ways: #pre There is a slight difference between these methods: #li valueOf returns a new or cached instance of java.lang.Integer #li parseInt returns primitive int . The same is for all cases: Short.valueOf / parseShort , Long.valueOf / parseLong , etc.

Question-28818506, answer-28833677
The cleanest “try services” approach given the current API would be: Optional<T> o = Stream.<Supplier<T>>of(args) .map(args) .filter(args) .map(args) .findFirst(args); The important aspect is not the (constant) chain of operations you have to write once but how easy it is to add another service (or modify the list of services is general). Here, adding or removing a single ()->serviceX(args) is enough. Due to the lazy evaluation of streams, no service will be invoked if a preceding service returned a non-empty Optional .

Question-31188231
It has generally been the case the Java source code has been forward compatible. Until Java 8, as far as I know, both compiled classes and source have been forward compatible with later JDK/JVM releases. [Update: this is not correct, see comments re 'enum', etc, below.] However, with the addition of default methods in Java 8 this appears to no longer be the case. For example, a library I have been using has an implementation of java.util.List which includes a List<T> sort(args) . This method returns a copy of the contents of the list sorted. This library, deployed as a jar file dependency, worked fine in a project being built using JDK 1.8. However, later I had occasion to recompile the library itself using JDK 1.8 and I found the library no longer compiles: the List -implementing class with its own sort(args) method now conflicts with the Java 8 java.util.List.sort(args) default method. The Java 8 sort(args) default method sorts the list in place (returns void ); my library's sort(args) method - since it returns a new sorted list - has an incompatible signature. So my basic question is: #li Doesn't JDK 1.8 introduce a forward incompatibility for Java source code due to default methods? Also: #li Is this the first such forward incompatible change? #li Was this considered or discussed when default methods where designed and implemented? Is it documented anywhere? #li Was the (admittedly small) inconvenience discounted versus the benefits? The following is an example of some code that compiles and runs under 1.7 and runs under 1.8 - but does not compile under 1.8: #pre The following shows this code being compiled (or failing to) and being run. #pre

Question-31993377, answer-31993478
You are consuming the value of next(args) , but asking for hasNext(args) and hasNextLine(args) . next(args) , per default, returns everything to the next whitespace(args) . So you are iterating through all whitespace seperated strings, and after each of them you are asking about the nextLine(args) . i 1 1 -> hasNextLine(args) ? True. hasNext(args) ? Also true. 1 1 -> hasNextLine(args) ? True. hasNext(args) ? Also true (still a whitespace left) 1 -> hasNextLine(args) ? True (Line Seperator, probably). haxNext? False, no whitespace anymore.

Question-29095967, answer-29096777
The only solution I come up with for the moment is by implementing your own custom collector. Before reading the solution, I want to add a few notes about this. I took this question more as a programming exercise, I'm not sure if it can be done with a parallel stream. So you have to be aware that it'll silently break if the pipeline is run in parallel . This is not a desirable behavior and should be avoided . This is why I throw an exception in the combiner part (instead of (l1, l2) -> {l1.addAll(args); return l1;} ), as it's used in parallel when combining the two lists, so that you have an exception instead of a wrong result. Also this is not very efficient due to list copying (although it uses a native method to copy the underlying array). So here's the collector implementation: #pre and how to use it: List<T> ll = list.stream(args).collect(args); Output: [[a, b], [c], [d, e]] As the answer of Joop Eggen is out , it appears that it can be done in parallel (give him credit for that!). With that it reduces the custom collector implementation to: #pre which let the paragraph about parallelism a bit obsolete, however I let it as it can be a good reminder. Note that the Stream API is not always a substitute. There are tasks that are easier and more suitable using the streams and there are tasks that are not. In your case, you could also create a utility method for that: #pre and call it like List<T> list = splitBySeparator(args); . It can be improved for checking edge-cases.

Question-24630963
The Java 8 Collectors.toMap throws a NullPointerException if one of the values is 'null'. I don't understand this behaviour, maps can contain null pointers as value without any problems. Is there a good reason why values cannot be null for Collectors.toMap ? Also, is there a nice Java 8 way of fixing this, or should I revert to plain old for loop? An example of my problem: #pre Stacktrace: #pre

Question-17828584, answer-17842210
After digging around for a while, I can't say that I find the answer, but I think it's quite close now. First, we need to know when a StackOverflowError will be thrown. In fact, the stack for a java thread stores frames, which containing all the data needed for invoking a method and resume. According to Java Language Specifications for JAVA 6 , when invoking a method, #blockquote Second, we should make it clear what is " there is not sufficient memory available to create such an activation frame ". According to Java Virtual Machine Specifications for JAVA 6 , #blockquote So, when a frame is created, there should be enough heap space to create a stack frame and enough stack space to store the new reference which point to the new stack frame if the frame is heap allocated. Now let's go back to the question. From the above, we can know that when a method is execute, it may just costs the same amount of stack space. And invoking System.out.println (may) needs 5 level of method invocation, so 5 frames need to be created. Then when StackOverflowError is thrown out, it has to go back 5 times to get enough stack space to store 5 frames' references. Hence 4 is print out. Why not 5? Because you use cnt++ . Change it to ++cnt , and then you will get 5. And you will notice that when the size of stack go to a high level, you will get 50 sometimes. That is because the amount of available heap space need to be taken into consideration then. When the stack's size is too large, maybe heap space will run out before stack. And (maybe) the actual size of stack frames of System.out.println is about 51 times of main , therefore it goes back 51 times and print 50.

Question-26791441, answer-26791826
Search for the first word to be replaced. If it's in the string, recurse on the the part of the string before the occurrence, and on the part of the string after the occurrence. Otherwise, continue with the next word to be replaced. A naive implementation might look like this #pre Sample usage: #pre Output: Once upon a foo, there was a bar and a baz. A less-naive version: #pre Unfortunately, Java's String has no indexOf(args) method. I've omitted the implementation of indexOf here as I'm not certain it's correct, but it can be found on ideone , along with some rough timings of various solutions posted here.

Question-10827872
What's the difference between java.lang.String 's replace(args) and replaceAll(args) methods, other than later uses regex? For simple substitutions like, replace . with / , is there any difference?

Question-8777257, answer-8777279
array1.equals(args) is the same as array1 == array2 , i.e. is it the same array. As @alf points out it's not what most people expect. Arrays.equals(args) compares the contents of the arrays. Similarly array.toString(args) may not be very useful and you need to use Arrays.toString(args) .

Question-27625611, answer-27625722
Because nextDouble works like this: ( source ) #pre next(args) makes x random bits. Now why does this matter? Because about half the numbers generated by the first part (before the division) are less than 1L << 52 , and therefore their significand doesn't entirely fill the 53 bits that it could fill, meaning the least significant bit of the significand is always zero for those. Because of the amount of attention this is receiving, here's some extra explanation of what a double in Java (and many other languages) really looks like and why it mattered in this question. Basically, a double looks like this: ( source ) #img A very important detail not visible in this picture is that numbers are "normalized" 1 such that the 53 bit fraction starts with a 1 (by choosing the exponent such that it is so), that 1 is then omitted. That is why the picture shows 52 bits for the fraction (significand) but there are effectively 53 bits in it. The normalization means that if in the code for nextDouble the 53rd bit is set, that bit is the implicit leading 1 and it goes away, and the other 52 bits are copied literally to the significand of the resulting double . If that bit is not set however, the remaining bits must be shifted left until it becomes set. On average, half the generated numbers fall into the case where the significand was not shifted left at all (and about half those have a 0 as their least significant bit), and the other half is shifted by at least 1 (or is just completely zero) so their least significant bit is always 0. 1: not always, clearly it cannot be done for zero, which has no highest 1. These numbers are called denormal or subnormal numbers, see wikipedia:denormal number .

Question-22182669, answer-22222274
StackOverflowError occurs due to an application recurses too deeply (This is not an answer you are expecting). Now other things to happen to StackOverflowError is keep calling methods from methods till you get StackOverflowError , but nobody can program to get StackOverflowError and even if those programmer are doing so then they are not following coding standards for cyclomatic complixity that every programmer has to understand while programming. Such reason for 'StackOverflowError' will require much time to rectify it. But unknowingly coding one line or two line which causes StackOverflowError is understandable and JVM throws that and we can rectify it instantly. Here is my answer with picture for some other question.

Question-29922296, answer-29922831
Three questions: three-part answer: Is it okay from a design point of view? Absolutely. Nothing wrong with it. If you need to do lots of iterating over your enum, the stream API is the clean way to go and hiding the boiler plate behind a little method is fine. Although I’d consider OldCumudgeon ’s version even better. Is it okay from a performance point of view? It most likely doesn’t matter. Most of the time, enums are not that big. Therefore, whatever overhead there is for one method or the other probably doesn’t matter in 99.9% of the cases. Of course, there are the 0.1% where it does. In that case: measure properly, with your real-world data and consumers. If I had to bet, I’d expect the for each loop to be faster, since it maps more directly to the memory model, but don’t guess when talking performance, and don’t tune before there is actual need for tuning. Write your code in a way that is correct first, easy to read second and only then worry about performance of code style. Why aren’t Enums properly integrated into the Stream API? If you compare Java’s Stream API to the equivalent in many other languages, it appears seriously limited. There are various pieces that are missing (reusable Streams and Optionals as Streams, for example). On the other hand, implementing the Stream API was certainly a huge change for the API. It was postponed multiple times for a reason. So I guess Oracle wanted to limit the changes to the most important use cases. Enums aren’t used that much anyway. Sure, every project has a couple of them, but they’re nothing compared to the number of Lists and other Collections. Even when you have an Enum, in many cases you won’t ever iterate over it. Lists and Sets, on the other hand, are probably iterated over almost every time. I assume that these were the reasons why the Enums didn’t get their own adapter to the Stream world. We’ll see whether more of this gets added in future versions. And until then you always can use Arrays.stream .

Question-7569335, answer-39329245
As others have pointed out the preferred way is to use: new StringBuilder(args).reverse(args).toString(args) but if you want to implement this by youself, i'am afraid that the rest of responses have flaws. The reason is that String represent a list of Unicode points, encoded in a char[] array according to the variable-length encoding: UTF-16 . This means some code points use a single element of the array (one code unit) but others use two of them, so there might be pairs of characters that must be treated as a single unit (consecutive "high" and "low" surrogates) #pre

Question-28276423, answer-28276493
You can close the outer most stream, in fact you don't need to retain all the streams wrapped and you can use Java 7 try-with-resources. #pre If you subscribe to YAGNI, or you-aint-gonna-need-it, you should be only adding code you actually need. You shouldn't be adding code you imagine you might need but in reality doesn't do anything useful. Take this example and imagine what could possibly go wrong if you didn't do this and what the impact would be? #pre Lets start with FileOutputStream which calls open to do all the real work. /** * Opens a file, with the specified name, for overwriting or appending. * @param name name of file to be opened * @param append whether the file is to be opened in append mode */ private native void open(args) throws FileNotFoundException; If the file is not found, there is no underlying resource to close, so closing it won't make any difference. If The file exists, it should be throwing a FileNotFoundException. So there is nothing to be gained by trying to close the resource from this line alone. The reason you need to close the file is when the file is opened successfully, but you later get an error. Lets look at the next stream GZIPOutputStream There is code which can throw an exception #pre This writes the header of the file. Now it would be very unusual for you to be able to open a file for writing but not be able to write even 8 bytes to it, but lets imagine this could happen and we don't close the file afterwards. What does happen to a file if it is not closed? You don't get any unflushed writes, they are discarded and in this case, there is no successfully written bytes to the stream which isn't buffered at this point anyway. But a file which is not closed doesn't live forever, instead FileOutputStream has #pre If you don't close a file at all, it gets closed anyway, just not immediately (and like I said, data which is left in a buffer will be lost this way, but there is none at this point) What is the consequence of not closing the file immediately? Under normal conditions, you potentially lose some data, and you potentially run out of file descriptors. But if you have a system where you can create files but you can't write anything to them, you have a bigger problem. i.e. it hard to imagine why you are repeatedly trying to create this file despite the fact you are failing. Both OutputStreamWriter and BufferedWriter don't throw IOException in their constructors, so it not clear what problem they would cause. In The case of BufferedWriter, you could get an OutOfMemoryError. In this case it will immediately trigger a GC, which as we have seen will close the file anyway.

Question-8894258, answer-8894356
Looks like niether is faster or slower #pre For long strings I'll chose the first one. Why copy around long strings? Documentations says: #blockquote //Edit 1 I've changed the test to trick JIT optimisation. //Edit 2 Repeat test 10 times to let JVM warm up. //Edit 3 Conclusions: First of all str.toCharArray(args); copies entire string in memory. It can be memory consuming for long strings. Method String.charAt(args) looks up char in char array inside String class checking index before. It looks like for short enough Strings first method (i.e. chatAt method) is a bit slower due to this index check. But if the String is long enough, copying whole char array gets slower, and the first method is faster. The longer the string is, the slower toCharArray performs. Try to change limit in for(args) loop to see it. If we let JVM warm up code runs faster, but proportions are the same. After all it's just micro-optimisation.

Question-28671903, answer-28672583
The behaviour is (somewhat) documented in the javadoc : #blockquote What this means in practice, when you call source.removeAll(args); : #li if the removals collection is of a smaller size than source , the remove method of HashSet is called, which is fast. #li if the removals collection is of equal or larger size than the source , then removals.contains is called, which is slow for an ArrayList. Quick fix: Collection<T> removals = new HashSet<T>(args); Note that there is an open bug that is very similar to what you describe. The bottom line seems to be that it is probably a poor choice but can't be changed because it is documented in the javadoc. For reference, this is the code of removeAll (in Java 8 - haven't checked other versions): #pre

Question-6802483, answer-6802523
There is no direct way to do this - Java has no Map literals (yet - I think they were proposed for Java 8). Some people like this: #pre This creates an anonymous subclass of HashMap, whose instance initializer puts these values. (By the way, a map can't contain twice the same value, your second put will overwrite the first one. I'll use different values for the next examples.) The normal way would be this (for a local variable): #pre If your test map is an instance variable, put the initialization in a constructor or instance initializer: #pre If your test map is a class variable, put the initialization in a static initializer: #pre If you want your map to never change, you should after the initialization wrap your map by Collections.unmodifiableMap(args) . You can do this in a static initializer too: #pre (I'm not sure if you can now make test final ... try it out and report here.)

Question-6992608, answer-6992643
There's no built in type for ConcurrentHashSet because you can always derive a set from a map. Since there are many types of maps, you use a method to produce a set from a given map (or map class). Prior to Java 8, you produce a concurrent hash set backed by a concurrent hash map, by using Collections.newSetFromMap(map) In Java 8 (pointed out by @Matt), you can get a concurrent hash set view via ConcurrentHashMap.newKeySet() . This is a bit simpler than the old newSetFromMap which required you to pass in an empty map object. But it is specific to ConcurrentHashMap . Anyway, the Java designers could have created a new set interface every time a new map interface was created, but that pattern would be impossible to enforce when third parties create their own maps. It is better to have the static methods that derive new sets; that approach always works, even when you create your own map implementations.

Question-13102045, answer-15046253
It does that because input.nextInt(args); doesn't capture the newline. you could do like the others proposed by adding an input.nextLine(args); underneath. Alternatively you can do it C# style and parse a nextLine to an integer like so: int number = Integer.parseInt(args); Doing this works just as well, and it saves you a line of code.

Question-29140402, answer-38628203
In intellij you can auto generate toString method by pressing alt+inset and then selecting toString() here is an out put for a test class: #pre As you can see, it generates a String by concatenating, several attributes of the class, for primitives it will print their values and for reference types it will use their class type (in this case to string method of Test2).

Question-10631715, answer-32362691
For completeness, using the Guava library, you'd do: Splitter.on(args).split(args) Another example: #pre Splitter.split(args) returns an Iterable, so if you need a List, wrap it in Lists.newArrayList(args) as above. Otherwise just go with the Iterable, for example: #pre Note how trimResults(args) handles all your trimming needs without having to tweak regexes for corner cases , as with String.split(args) . If your project uses Guava already, this should be your preferred solution. See Splitter documentation in Guava User Guide or the javadocs for more configuration options.

Question-31188231, answer-31190722
We can draw a parallel with abstract class. An abstract class is intended to be subclassed so that the abstract methods can be implemented. The abstract class itself contains concrete methods that invoke the abstract methods. The abstract class is free to evolve by adding more concrete methods; and this practice may break subclasses. Therefore the exact problem you described existed even before Java8. The problem is much more manifested on Collection APIs because there are a lot of subclasses out in the wild. While the leading motivation of default method was to add some useful methods to existing Collection APIs without breaking subclasses, they had to exercise great self-control of doing it too much, for fear of breaking subclasses. A default method is added only if it's absolutely necessary. The real question here is, why List.sort is considered absolutely necessary. I think that is debatable. Regardless of why default method was introduced in the 1st place, it is now a great tool for API designers, and we ought to treat it the same as concrete methods in abstract classes - they need to be designed carefully up front; and new ones must be introduced with great caution.

Question-32175145, answer-32175224
Since equals(...) is not a final method of Object , yes, it is very well be possible in a different situation . #pre This question, however, specifically says that these are primitive wrappers (e.g. Integer , Boolean , etc.) and since these classes are final, you cannot extend them, thus a.equals(args) will always return true .

Question-7032070, answer-7032180
The clear(args) method removes all the elements of a single ArrayList . It's a fast operation, as it just sets some array elements to null . The removeAll(args) method, which is inherited from AbstractCollection , removes all the elements that are in the argument collection from the collection you call the method on. It's a relatively slow operation, as it has to search through one of the collections involved.

Question-32693704, answer-32745612
For me, the reason is a likely performance inprovement, at a negligible cost in terms of code clarity. Note that the implementation of the fill method is trivial, a simple for-loop setting each array element to null. So, replacing a call to it with the actual implementation does not cause any significant degradation in the clarity/conciseness of the caller method. The potential performance benefits are not so insignificant, if you consider everything that is involved: #li There will be no need for the JVM to resolve the Arrays class, plus loading and initializing it if needed. This is a non-trivial process where the JVM performs several steps. Firstly, it checks the class loader to see if the class is already loaded, and this happens every time a method is called; there are optimizations involved here, of course, but it still takes some effort. If the class is not loaded, the JVM will need to go through the expensive process of loading it, verifying the bytecode, resolving other necessary dependencies, and finally performing static initialization of the class (which can be arbitrarily expensive). Given that HashMap is such a core class, and that Arrays is such a huge class (3600+ lines), avoiding these costs may add up to noticeable savings. #li Since there is no Arrays.fill(args) method call, the JVM won't have to decide whether/when to inline the method into the caller's body. Since HashMap#clear(args) tends to get called a lot, the JVM will eventually perform the inlining, which requires JIT recompilation of the clear method. With no method calls, clear will always run at top-speed (once initially JITed). Another benefit of no longer calling methods in Arrays is that it simplifies the dependency graph inside the java.util package, since one dependency is removed.

Question-11359187
For the first time in my life I find myself in a position where I'm writing a Java API that will be open sourced. Hopefully to be included in many other projects. For logging I (and indeed the people I work with) have always used JUL (java.util.logging) and never had any issues with it. However now I need to understand in more detail what I should do for my API development. I've done some research on this and with the information I've got I just get more confused. Hence this post. Since I come from JUL I'm biased on that. My knowledge of the rest is not that big. From the research I've done I've come up with these reasons why people do not like JUL: #li "I started developing in Java long before Sun released JUL and it was just easier for me to continue with logging-framework-X rather than to learn something new" . Hmm. I'm not kidding, this is actually what people say. With this argument we could all be doing COBOL. (however I can certainly relate to this being a lazy dude myself) #li "I don't like the names of the logging levels in JUL" . Ok, seriously, this is just not enough of a reason to introduce a new dependency. #li "I don't like the standard format of the output from JUL" . Hmm. This is just configuration. You do not even have to do anything code-wise. (true, back in old days you may have had to create your own Formatter class to get it right). #li "I use other libraries that also use logging-framework-X so I thought it easier just to use that one" . This is a cyclic argument, isn't ? Why does 'everybody' use logging-framework-X and not JUL? #li "Everybody else is using logging-framework-X" . This to me is just a special case of the above. Majority is not always right. So the real big question is why not JUL? . What is it I have missed ? The raison d'être for logging facades (SLF4J, JCL) is that multiple logging implementations have existed historically and the reason for that really goes back to the era before JUL as I see it. If JUL was perfect then logging facades wouldn't exist, or what? Rather than embracing them shouldn't we question why they were necessary in the first place? (and see if those reasons still exist) Ok, my research so far has led to a couple of things that I can see may be real issues with JUL: #li Performance . Some say that performance in SLF4J is superior to the rest. This seems to me to be a case of premature optimization. If you need to log hundreds of megabytes per second then I'm not sure you are on the right path anyway. JUL has also evolved and the tests you did on Java 1.4 may no longer be true. You can read about it here and this fix has made it into Java 7. Many also talk about the overhead of string concatenation in logging methods. However template based logging avoids this cost and it exist also in JUL. Personally I never really write template based logging. Too lazy for that. For example if I do this with JUL: log.finest(args)); my IDE will warn me and ask permission that it should change it to: log.log(args); .. which I will of course accept. Permission granted ! Thank you for your help. So I don't actually write such statements myself, that is done by the IDE. In conclusion on the issue of performance I haven't found anything that would suggest that JUL's performance is not ok compared to the competition. #li Configuration from classpath . Out-of-the-box JUL cannot load a configuration file from the classpath. It is a few lines of code to make it do so. I can see why this may be annoying but the solution is short and simple. #li Availability of output handlers . JUL comes with 5 output handlers out-of-the-box: console, file stream, socket and memory. These can be extended or new ones can be written. This may for example be writing to UNIX/Linux Syslog and Windows Event Log. I have personally never had this requirement nor have I seen it used but I can certainly relate to why it may be a useful feature. Logback comes with an appender for Syslog for example. Still I would argue that #li 99.5% of the needs for output destinations are covered by what is in JUL out-of-the-box. #li Special needs could be catered for by custom handlers on top of JUL rather than on top of something else. There's nothing to me that suggests that it takes more time to write a Syslog output handler for JUL than it does for another logging framework. I'm really concerned that there's something I've overlooked. The use of logging facades and logging implementations other than JUL is so widespread that I have to come to the conclusion that it's me who just doesn't understand. That wouldn't be the first time, I'm afraid. :-) So what should I do with my API? I want it to become successful. I can of course just "go with the flow" and implement SLF4J (which seems the most popular these days) but for my own sake I still need to understand exactly what is wrong with the JUL of today that warrants all the fuzz? Will I sabotage myself by choosing JUL for my library ? Testing performance (section added by nolan600 on 07-JUL-2012) There's a reference below from Ceki about SLF4J's parametrization being 10 times or more faster than JUL's. So I've started doing some simple tests. At first glance the claim is certainly correct. Here are the preliminary results (but read on!): #li Execution time SLF4J, backend Logback: 1515 #li Execution time SLF4J, backend JUL: 12938 #li Execution time JUL: 16911 The numbers above are msecs so less is better. So 10 times performance difference is by first actually pretty close. My initial reaction: That is a lot ! Here is the core of the test. As can be seen an integer and a string is construted in a loop which is then used in the log statement: #pre (I wanted the log statement to have both a primitive data type (in this case an int) and a more complex data type (in this case a String). Not sure it matters but there you have it.) The log statement for SLF4J: logger.info(args); The log statement for JUL: logger.log(args); The JVM was 'warmed up' with the same test executed once before the actual measurement was done. Java 1.7.03 was used on Windows 7. Latest versions of SLF4J (v1.6.6) and Logback (v1.0.6) was used. Stdout and stderr was redirected to null device. However, careful now, it turns out JUL is spending most of its time in getSourceClassName(args) because JUL by default prints the source class name in the output, while Logback doesn't. So we are comparing apples and oranges. I have to do the test again and configure the logging implementations in a similar manner so that they actually output the same stuff. I do however suspect that SLF4J+Logback will still come out on top but far from the initial numbers as given above. Stay tuned. Btw: The test was first time I've actually worked with SLF4J or Logback. A pleasant experience. JUL is certainly a lot less welcoming when you are starting out. Testing performance (part 2) (section added by nolan600 on 08-JUL-2012) As it turns out it doesn't really matter for performance how you configure your pattern in JUL, i.e. whether or not it includes the source name or not. I tried with a very simple pattern: java.util.logging.SimpleFormatter.format=StringLiteral and that did not change the above timings at all. My profiler revealed that the logger still spent a lot of time in calls to getSourceClassName(args) even if this was not part of my pattern. The pattern doesn't matter. I'm therefore concluding on the issue of performance that at least for the tested template based log statement there seems to be roughly a factor of 10 in real performance difference between JUL (slow) and SLF4J+Logback (quick). Just like Ceki said. I can also see another thing namely that SLF4J's getLogger(args) call is a lot more expensive than JUL's ditto. (95 ms vs 0.3 ms if my profiler is accurate). This makes sense. SLF4J has to do some time on the binding of the underlying logging implementation. This doesn't scare me. These calls should be somewhat rare in the lifetime of an application. The fastness should be in the actual log calls. Final conclusion (section added by nolan600 on 08-JUL-2012) Thank you for all your answers. Contrary to what I initially thought I've ended up deciding to use SLF4J for my API. This is based on a number of things and your input: #li It gives flexibility to choose log implementation at deployment time. #li Issues with lack of flexibility of JUL's configuration when run inside an application server. #li SLF4J is certainly a lot faster as detailed above in particular if you couple it with Logback. Even if this was just a rough test I have reason to believe that a lot more effort has gone into optimization on SLF4J+Logback than on JUL. #li Documentation. The documentation for SLF4J is simply a lot more comprehensive and precise. #li Pattern flexibility. As I did the tests I set out to have JUL mimic the default pattern from Logback. This pattern includes the name of the thread. It turns out JUL cannot do this out of the box. Ok, I haven't missed it until now, but I don't think it is a thing that should be missing from a log framework. Period! #li Most (or many) Java projects today use Maven so adding a dependency is not that big a thing especially if that dependency is rather stable, i.e. doesn't constantly change its API. This seems to be true for SLF4J. Also the SLF4J jar and friends are small in size. So the strange thing that happened was that I actually got quite upset with JUL after having worked a bit with SLF4J. I still regret that it has to be this way with JUL. JUL is far from perfect but kind of does the job. Just not quite well enough. The same can be said about Properties as an example but we do not think about abstracting that so people can plug in their own configuration library and what have you. I think the reason is that Properties comes in just above the bar while the opposite is true for JUL of today ... and in the past it came in at zero because it didn't exist.

Question-20945049
We all know that String is immutable in Java, but check the following code: #pre Why does this program operate like this? And why is the value of s1 and s2 changed, but not s3 ?

Question-6667243
What is the best way to use the values stored in an Enum as String literals? For example: #pre Then later I could use Mode.mode1 to return its string representation as mode1 . Without having to keep calling Mode.model.toString(args) .

Question-6470651, answer-18544736
Threads are not collected until they terminate. They serve as roots of garbage collection. They are one of the few objects that won't be reclaimed simply by forgetting about them or clearing references to them. Consider: the basic pattern to terminate a worker thread is to set some condition variable seen by the thread. The thread can check the variable periodically and use that as a signal to terminate. If the variable is not declared volatile , then the change to the variable might not be seen by the thread, so it won't know to terminate. Or imagine if some threads want to update a shared object, but deadlock while trying to lock on it. If you only have a handful of threads these bugs will probably be obvious because your program will stop working properly. If you have a thread pool that creates more threads as needed, then the obsolete/stuck threads might not be noticed, and will accumulate indefinitely, causing a memory leak. Threads are likely to use other data in your application, so will also prevent anything they directly reference from ever being collected. As a toy example: #pre Call System.gc(args) all you like, but the object passed to leakMe will never die. (*edited*)

Question-12317668
According to the JLS, an int array should be filled by zeros just after initialization. However, I am faced with a situation where it is not. Such a behavior occurs first in JDK 7u4 and also occurs in all later updates (I use 64-bit implementation). The following code throws exception: #pre The exception occurs after the JVM performs compilation of the code block and does not arise with -Xint flag. Additionally, the Arrays.fill(args) statement (as all other statements in this code) is necessary, and the exception does not occurs if it is absent. It is clear that this possible bug is bounded with some JVM optimization. Any ideas for the reason of such a behavior? Update: I see this behavior on HotSpot 64-bit server VM, Java version from 1.7.0_04 to 1.7.0_10 on Gentoo Linux, Debian Linux (both kernel 3.0 version) and MacOS Lion. This error can always be reproduced with the code above. I did not test this problem with a 32-bit JDK or on Windows. I already sent a bug report to the Oracle (bug id 7196857) and it will appear in public Oracle bug database in few days. Update: Oracle published this bug at their public bug database: #a

Question-7348711
Which of the following is the best and most portable way to get the hostname of the current computer in Java? Runtime.getRuntime(args).exec(args) vs InetAddress.getLocalHost(args).getHostName(args)

Question-5439529, answer-9991624
You can use Integer.parseInt(args) and catch the NumberFormatException if the string is not a valid integer, in the following fashion (as pointed out by all answers): #pre However, note here that if the evaluated integer overflows, the same exception will be thrown. Your purpose was to find out whether or not, it was a valid integer. So its safer to make your own method to check for validity: #pre

Question-22718744, answer-27477312
The behavior of String.split (which calls Pattern.split ) changes between Java 7 and Java 8. Documentation Comparing between the documentation of Pattern.split in Java 7 and Java 8 , we observe the following clause being added: #blockquote The same clause is also added to String.split in Java 8 , compared to Java 7 . Reference implementation Let us compare the code of Pattern.split of the reference implemetation in Java 7 and Java 8. The code is retrieved from grepcode, for version 7u40-b43 and 8-b132. Java 7 #pre Java 8 #pre The addition of the following code in Java 8 excludes the zero-length match at the beginning of the input string, which explains the behavior above. #pre Maintaining compatibility Following behavior in Java 8 and above To make split behaves consistently across versions and compatible with the behavior in Java 8: #li If your regex can match zero-length string, just add (?!\A) at the end of the regex and wrap the original regex in non-capturing group (?:...) (if necessary). #li If your regex can't match zero-length string, you don't need to do anything. #li If you don't know whether the regex can match zero-length string or not, do both the actions in step 1. (?!\A) checks that the string does not end at the beginning of the string, which implies that the match is an empty match at the beginning of the string. Following behavior in Java 7 and prior There is no general solution to make split backward-compatible with Java 7 and prior, short of replacing all instance of split to point to your own custom implementation.

Question-19418427, answer-19418548
As per my research, all the final String are interned in Java. From one of the blog post: #blockquote So it means if you call String.intern(args) you can compare two strings using == operator. But here String.intern(args) is not necessary because in Java final String are internally interned. You can find more information String comparision using == operator and Javadoc for String.intern() method. Also refer this Stackoverflow post for more information.

Question-32714194
String is a special case in Java. It's a class, which I can examine in the source code , but it also has its own infix operator + , which seems to be syntactic sugar for StringBuilder . For example, StringLiteral + yourName; could become new StringBuilder(args).append(args).append(args).toString(args); There are no user-defined operators in Java, so where is + specified for String ? Could the same mechanism be used to make additional operators, such as for vectors?

Question-15655012, answer-15656208
This is favorite interview question . Interviewer tries to find out here, how much you understand about behavior of objects with respect to constructors, methods, class variables(static variables), instance variables. #pre In above case we have defined Test constructor and setFoo method. About constructor: Constructor can be invoked only one time per object creation by using new keyword. Programmer cannot invoke constructor many times because constructor are designed so. About method: Method can be invoked as many time as programmer wants and compiler knows it programmer may invoke method zero or multiple time. Scenario 1 private final List foo; // 1 foo is an instance variable. When we create Test class' object then instance variable foo will be copied inside object of Test class. If we assign foo inside constructor then compiler know that Constructor will be invoked only one time. so there is no problem to assign it inside constructor. If we assign foo inside method then compiler knows that method may be invoked multiple time. So value will have to be changed multiple time which is not allowed for final variable. You can assign value to the final variable only one time. So compiler decides constructor is good choice! Scenario 2 private static final List foo = new ArrayList(args); foo is now static variable. When we create object of Test class then foo will not be copied to the object of Test class because foo is static. Now foo is not the property of each object. This is the property of Test class. But foo can be seen by many objects and if every object which is created by using new keyword which ultimately invoke Test constructor then value will be changed at the time of multiple object creation (Remember static foo is not copied in every object now but is shared between multiple objects.) Scenario 3 t.foo.add(args); // Modification-2 Above Modification-2 is from your question. In above case you are not changing first referenced object but you are adding contents inside foo which is allowed. Compiler complains if you try to assign new ArrayList(args) object to the foo reference variable. Rule is that if you have initialized object to final reference variable then you cannot change it to refer different ArrayList object. (in this case ArrayList) final class cannot be subclassed final methods cannot be overridden. (This method is in superclass) final methods can override. (Read this in grammatical way. This method is in subclass)

Question-8894258, answer-22676782
Despite @Saint Hill's answer if you consider the time complexity of str.toCharArray() , the first one is faster even for very large strings. You can run the code below to see it for yourself. #pre output: #pre

Question-4709175, answer-4709284
#blockquote You can use an Enum to represent a smallish fixed set of constants or an internal class mode while increasing readability. Also, Enums can enforce a certain rigidity when used in method parameters. They offer the interesting possibility of passing information to a constructor like in the Planets example on Oracle's site and, as you've discovered, also allow a simple way to create a singleton pattern. ex: Locale.setDefault(args) reads better than Locale.setDefault(args) and enforces the use of fixed set of values shown in an IDE when you add the . separator instead of all integers.

Question-24885223, answer-29841832
Thanks to @marmor, I updated my answer. I think it's a good solution to handle the onClick() in the ViewHolder class constructor and pass it to the parent class via OnItemClickListener interface. MyAdapter.java #pre Usage of adapter in other classes: MyFragment.java #pre

Question-18571223, answer-18571352
You can use String.getBytes(args) which returns the byte[] array.

Question-10827872, answer-10827945
The replace(args) method is overloaded to accept both a primitive char and a CharSequence as arguments. Now as far as the performance is concerned, the replace(args) method is a bit faster than replaceAll(args) because the later first compiles the regex pattern and then matches before finally replacing whereas the former simply matches for the provided argument and replaces. Since we know the regex pattern matching is a bit more complex and consequently slower, then preferring replace(args) over replaceAll(args) is suggested whenever possible. For example, for simple substitutions like you mentioned, it is better to use: replace(args); instead of: replaceAll(args); Note: the above conversion method arguments are system-dependent.

Question-23114015
I am wondering why the Iterable interface does not provide the stream(args) and parallelStream(args) methods. Consider the following class: #pre It is an implementation of a Hand as you can have cards in your hand while playing a Trading Card Game. Essentially it wraps a List<T> , ensures a maximum capacity and offers some other useful features. It is better as implementing it directly as a List<T> . Now, for convienience I thought it would be nice to implement Iterable<T> , such that you can use enhanced for-loops if you want to loop over it. (My Hand class also provides a get(args) method, hence the Iterable<T> is justified in my opinion.) The Iterable interface provides the following (left out javadoc): #pre Now can you obtain a stream with: Stream<T> stream = StreamSupport.stream(args); So onto the real question: #li Why does Iterable<T> not provide a default methods that implement stream(args) and parallelStream(args) , I see nothing that would make this impossible or unwanted? A related question I found is the following though: Why does Stream<T> not implement Iterable<T>? Which is oddly enough suggesting it to do it somewhat the other way around.

Question-8725387, answer-8725453
Since all lists are already "sorted" by the order the items were added (FIFO ordering), you can "resort" them with another ordering, including the natural ordering of elements, using java.util.Collections.sort(args) . EDIT: Lists as data structures are based in what is interesting is the ordering in which the items where inserted. Sets do not have that information. If you want to order by addition time, use List . If you want to order by other criteria, use SortedSet .

Question-7935858, answer-7935873
java.lang.String.split splits on regular expressions, and . in a regular expression means "any character". Try temp.split(args) .

Question-28724850
Running the following stream example in Java8: System.out.println(args); yields: /a/b/c/d/e/f Which is - of course - no surprise. Due to #a it shouldn't matter whether the stream is executed sequentially or parallel: #blockquote AFAIK reduce(args) is deterministic and (s1, s2) -> s1 + StringLiteral + s2 is associative, so that adding parallel(args) should yield the same result: System.out.println(args); However the result on my machine is: /a//b//c//d//e//f What's wrong here? BTW: using (the preferred) .collect(args) instead of reduce(args) yields the same result a/b/c/d/e/f for sequential and parallel execution. JVM details: #pre

Question-29333689
The following is taken from a job interview: #blockquote I thought about using an HashSet, but it might complicate everything... Any ideas of a simple solution?

Question-34172978, answer-34173110
Flipping the limit and the filter clauses has different behaviors. If you put the limit first, the stream will first generate 10 integers [1..10], and then filter them leaving only those smaller than 5. In the original ordering, with the filter applied first, integers are generated and filtered until you reach 10 elements. This isn't an infinite operator, as i in the supplier will eventually overflow, but it will take a while, especially on a slow computer, to reach MAX_INT .

Question-5673260, answer-5673356
#pre Generally speaking, when you have response.getOutputStream(args) , you can write anything there. You can pass this output stream as a place to put generated PDF to your generator. Also, if you know what file type you are sending, you can set response.setContentType(args);

Question-18647214, answer-18647232
I would just do this. It basically takes the minimum between 100 (the max health) and what the health would be with 15 extra points. It ensures that the user's health does not exceed 100. #pre To ensure that hitpoints do not drop below zero, you can use a similar function: Math.max . #pre

Question-34250207
As I recall, before Java 8, the default capacity of ArrayList was 10. Surprisingly, the comment on the default (void) constructor still says: Constructs an empty list with an initial capacity of ten. From ArrayList.java : #pre

Question-29333689, answer-29337734
My intuition in Python is to use the collections.Counter as a multiset: #pre So we have a list of integers from 0-99 twice except for 66. #pre Which gets the least common (last item) from a list sorted by frequency. Such a function would be: #pre And usage: #pre This is a generalized solution that works for any number of items.

Question-5374311, answer-5374346
What is happening is that stock_list.toArray(args) is creating an Object[] rather than a String[] and hence the typecast is failing. The correct code would be: String [] stockArr = stockList.toArray(args); or even String [] stockArr = stockList.toArray(args); For more details, refer to the javadocs for the two overloads of List.toArray . (From a technical perspective, the reason for this API behaviour / design is that an implementation of the List<T>.toArray(args) method has no information of what the <T> is at runtime. All it knows is that the raw element type is Object . By contrast, in the other case, the array parameter gives the base type of the array. (If the supplied array is big enough, it is used. Otherwise a new array of the same type and a larger size will be allocated and returned as the result.)

Question-16635398, answer-16636899
I feel that I need to extend my comment a bit... About paradigm\style That's probably the most notable aspect. FP became popular due to what you can get avoiding side-effects. I won't delve deep into what pros\cons you can get from this, since this is not related to the question. However, I will say that the iteration using Iterable.forEach is inspired by FP and rather result of bringing more FP to Java (ironically, I'd say that there is no much use for forEach in pure FP, since it does nothing except introducing side-effects). In the end I would say that it is rather a matter of taste\style\paradigm you are currently writing in. About parallelism. From performance point of view there is no promised notable benefits from using Iterable.forEach over foreach(...). According to official docs on Iterable.forEach : #blockquote ... i.e. docs pretty much clear that there will be no implicit parallelism. Adding one would be LSP violation. Now, there are "parallell collections" that are promised in Java 8, but to work with those you need to me more explicit and put some extra care to use them (see mschenk74's answer for example). BTW: in this case Stream.forEach will be used, and it doesn't guarantee that actual work will be done in parallell (depends on underlying collection). UPDATE: might be not that obvious and a little stretched at a glance but there is another facet of style and readability perspective. First of all - plain old forloops are plain and old. Everybody already knows them. Second, and more important - you probably want to use Iterable.forEach only with one-liner lambdas. If "body" gets heavier - they tend to be not-that readable. You have 2 options from here - use inner classes (yuck) or use plain old forloop. People often gets annoyed when they see the same things (iteratins over collections) being done various vays/styles in the same codebase, and this seems to be the case. Again, this might or might not be an issue. Depends on people working on code.

Question-5673260, answer-5673375
You should be able to write the file on the response directly. Something like #pre and then write the file as a binary stream on response.getOutputStream(args) . Remember to do response.flush(args) at the end and that should do it.

Question-26318569
I am getting following exception while running the tests. I am using Mockito for mocking. The hints mentioned by Mockito library are not helping. #pre Test Code from DomainTestFactory. When I run the following test, I see the exception #pre

Question-31419029
I've tried to build my own Map to increase the performance for a special environment, and I realized something pretty interesting: Creating a new Hashmap<T>(args) is faster than new Object[2000] - no matter in which order I execute these commands. That's pretty confusing to me, esp. because the Hashmap constructor contains a table = new Entry[capacity] , according to this . Is there something wrong with my testbench? #pre I'd love to see the results of testing on another computer. I've got no clue why creating a HashMap is 10 times faster than creating a Object[] .

Question-18666710, answer-18792306
An important feature of parametric types is the ability to write polymorphic algorithms, i.e. algorithms that operate on a data structure regardless of its parameter value, such as Arrays.sort(args) . With generics, that's done with wildcard types: <E extends Comparable<T>> void sort(args); To be truly useful, wildcard types require wildcard capture, and that requires the notion of a type parameter. None of that was available at the time arrays were added to Java, and makings arrays of reference type covariant permitted a far simpler way to permit polymorphic algorithms: void sort(args); However, that simplicity opened a loophole in the static type system: #pre requiring a runtime check of every write access to an array of reference type. In a nutshell, the newer approach embodied by generics makes the type system more complex, but also more statically type safe, while the older approach was simpler, and less statically type safe. The designers of the language opted for the simpler approach, having more important things to do than closing a small loophole in the type system that rarely causes problems. Later, when Java was established, and the pressing needs taken care of, they had the resources to do it right for generics (but changing it for arrays would have broken existing Java programs).

Question-15655012, answer-24895623
First of all, the place in your code where you are initializing (i.e. assigning for the first time) foo is here: foo = new ArrayList(args); foo is an object (with type List) so it is a reference type, not a value type (like int). As such, it holds a reference to a memory location (e.g. 0xA7D2A834) where your List elements are stored. Lines like this foo.add(args); // Modification-1 do not change the value of foo (which, again, is just a reference to a memory location). Instead, they just add elements into that referenced memory location. To violate the final keyword, you would have to try to re-assign foo as follows again: foo = new ArrayList(args); That would give you a compilation error. Now, with that out of the way, think about what happens when you add the static keyword. When you do NOT have the static keyword, each object that instantiates the class has its own copy of foo. Therefore, the constructor assigns a value to a blank, fresh copy of the foo variable, which is perfectly fine. However, when you DO have the static keyword, only one foo exists in memory that is associated with the class. If you were to create two or more objects, the constructor would be attempting to re-assign that one foo each time, violating the final keyword.

Question-13744450
The interviewer asked me: #blockquote I wasn't aware of these terms. So, when I came back to home, then I started looking into Google about Observer and Observable and found some point from different resources : #blockquote I found this example: #pre But I'm still unable to figure out why do we need Observer and Observable ? What are the setChanged(args) and notifyObservers(args) methods for?

Question-16635398, answer-22502206
When reading this question one can get the impression, that Iterable#forEach in combination with lambda expressions is a shortcut/replacement for writing a traditional for-each loop. This is simply not true. This code from the OP: joins.forEach(args); is not intended as a shortcut for writing #pre and should certainly not be used in this way. Instead it is intended as a shortcut (although it is not exactly the same) for writing #pre And it is as a replacement for the following Java 7 code: #pre Replacing the body of a loop with a functional interface, as in the examples above, makes your code more explicit: You are saying that (1) the body of the loop does not affect the surrounding code and control flow, and (2) the body of the loop may be replaced with a different implementation of the function, without affecting the surrounding code. Not being able to access non final variables of the outer scope is not a deficit of functions/lambdas, it is a feature that distinguishes the semantics of Iterable#forEach from the semantics of a traditional for-each loop. Once one gets used to the syntax of Iterable#forEach , it makes the code more readable, because you immediately get this additional information about the code. Traditional for-each loops will certainly stay good practice (to avoid the overused term " best practice ") in Java. But this doesn't mean, that Iterable#forEach should be considered bad practice or bad style. It is always good practice, to use the right tool for doing the job, and this includes mixing traditional for-each loops with Iterable#forEach , where it makes sense. Since the downsides of Iterable#forEach have already been discussed in this thread, here are some reasons, why you might probably want to use Iterable#forEach : #li To make your code more explicit: As described above, Iterable#forEach can make your code more explicit and readable in some situations. #li To make your code more extensible and maintainable: Using a function as the body of a loop allows you to replace this function with different implementations (see Strategy Pattern ). You could e.g. easily replace the lambda expression with a method call, that may be overwritten by sub-classes: joins.forEach(args); Then you could provide default strategies using an enum, that implements the functional interface. This not only makes your code more extensible, it also increases maintainability because it decouples the loop implementation from the loop declaration. #li To make your code more debuggable: Seperating the loop implementation from the declaration can also make debugging more easy, because you could have a specialized debug implementation, that prints out debug messages, without the need to clutter your main code with if(args)System.out.println(args) . The debug implementation could e.g. be a delegate , that decorates the actual function implementation. #li To optimize performance-critical code: Contrary to some of the assertions in this thread, Iterable#forEach does already provide better performance than a traditional for-each loop, at least when using ArrayList and running Hotspot in "-client" mode. While this performance boost is small and negligible for most use cases, there are situations, where this extra performance can make a difference. E.g. library maintainers will certainly want to evaluate, if some of their existing loop implementations should be replaced with Iterable#forEach . To back this statement up with facts, I have done some micro-benchmarks with Caliper . Here is the test code (latest Caliper from git is needed): #pre And here are the results: #li Results for -client #li Results for -server When running with "-client", Iterable#forEach outperforms the traditional for loop over an ArrayList, but is still slower than directly iterating over an array. When running with "-server", the performance of all approaches is about the same. #li To provide optional support for parallel execution: It has already been said here, that the possibility to execute the functional interface of Iterable#forEach in parallel using streams , is certainly an important aspect. Since Collection#parallelStream(args) does not guarantee, that the loop is actually executed in parallel, one must consider this an optional feature. By iterating over your list with list.parallelStream(args).forEach(args); , you explicitly say: This loop supports parallel execution, but it does not depend on it. Again, this is a feature and not a deficit! By moving the decision for parallel execution away from your actual loop implementation, you allow optional optimization of your code, without affecting the code itself, which is a good thing. Also, if the default parallel stream implementation does not fit your needs, no one is preventing you from providing your own implementation. You could e.g. provide an optimized collection depending on the underlying operating system, on the size of the collection, on the number of cores, and on some preference settings: #pre The nice thing here is, that your loop implementation doesn't need to know or care about these details.

Question-26549659
Google Guava has a predicate that always returns true . Does Java 8 have something similar for its Predicate ? I know I could use (foo)->{return true;} , but I want something pre-made, analogous to Collections.emptySet(args) .

Question-5439529, answer-5439600
As an alternative to trying to parse the string and catching NumberFormatException , you could use a regex; e.g. #pre This is likely to be faster, especially if you precompile and reuse the regex. However, the catch is that Integer.parseInt(args) will still fail if str represents a number that is outside range of legal int values.

Question-7032070, answer-7032137
Unless there is a specific optimization that checks if the argument passed to removeAll(args) is the collection itself (and I highly doubt that such an optimization is there) it will be significantly slower than a simple .clear(args) . Apart from that (and at least equally important): arraylist.removeAll(args) is just obtuse, confusing code. It is a very backwards way of saying "clear this collection". What advantage would it have over the very understandable arraylist.clear(args) ?

Question-16324831, answer-16324963
Note: everything in this post is in the source of Java7-b147 Double.parseDouble(args) goes into a Sun library (in sun.misc.FloatingDecimal ) the first important thing that happens is: #pre Integer.parseInt(args) is done manually in the Integer class. The first important thing that happens is: #pre I would guess there are two different authors.

Question-4812570, answer-21765043
You can use the ExceptionUtils.getStackTrace(args); from Apache Commons 3 class org.apache.commons.lang3.exception.ExceptionUtils . #a

Question-10904911
I am using trying to use the toString(args) method, but I think I am doing it wrong: #a My code: #pre The output is: [I@23fc4bec Also I tried printing like this, but: #pre I took this code out of bigger and more complex code, but I can add it if needed. But this should give general information. I am looking for output, like in Oracle's documentation: #blockquote

Question-21488056, answer-36242382
You can use Predicates from Eclipse Collections #pre If you can't change the strings from List : #pre If you only need a negation of String.isEmpty(args) you can also use StringPredicates.notEmpty(args) . Note: I am a contributor to Eclipse Collections.

Question-5778658, answer-10768914
You will need an intermediate class which will buffer between. Each time InputStream.read(args) is called, the buffering class will fill the passed in byte array with the next chunk passed in from OutputStream.write(args) . Since the sizes of the chunks may not be the same, the adapter class will need to store a certain amount until it has enough to fill the read buffer and/or be able to store up any buffer overflow. This article has a nice breakdown of a few different approaches to this problem: #a

Question-32693704, answer-32744190
There is no actual difference in the functionality between the 2 version's loop. Arrays.fill does the exact same thing. So the choice to use it or not may not necessarily be considered a mistake. It is left up to the developer to decide when it comes to this kind of micromanagement. There are 2 separate concerns for each approach: #li using the Arrays.fill makes the code less verbose and more readable. #li looping directly in the HashMap code (like version 8) peformance wise is actually a better option. While the overhead that inserting the Arrays class is negligible it may become less so when it comes to something as widespread as HashMap where every bit of performance enhancement has a large effect(imagine the tiniest footprint reduce of a HashMap in fullblown webapp). Take into consideration the fact that the Arrays class was used only for this one loop. The change is small enough that it doesn't make the clear method less readable. The precise reason can't be found out without asking the developer who actually did this, however i suspect it's either a mistake or a small enhancement. better option. My opinion is it can be considered an enhancement, even if only by accident.

Question-23114015, answer-23177907
This was not an omission; there was detailed discussion on the EG list in June of 2013. The definitive discussion of the Expert Group is rooted at this thread . While it seemed "obvious" (even to the Expert Group, initially) that stream(args) seemed to make sense on Iterable , the fact that Iterable was so general became a problem, because the obvious signature: Stream<T> stream(args) was not always what you were going to want. Some things that were Iterable<T> would rather have their stream method return an IntStream , for example. But putting the stream(args) method this high up in the hierarchy would make that impossible. So instead, we made it really easy to make a Stream from an Iterable , by providing a spliterator(args) method. The implementation of stream(args) in Collection is just: #pre Any client can get the stream they want from an Iterable with: Stream s = StreamSupport.stream(args); In the end we concluded that adding stream(args) to Iterable would be a mistake.

Question-30384955, answer-30385343
This happens because of the way capture conversion works: #blockquote Note the end bit. So, what this means is that, given a type like this: #pre Only "outside" wildcards are captured. The Map key wildcard is captured, but the List element wildcard is not. This is why, for example, we can add to a List<T> , but not a List<T> . The placement of the wildcard is what matters. Carrying this over to TbinList , if we have an ArrayList<T> , the wildcard is in a place where it does not get captured, but if we have a TbinList<T> , the wildcard is in a place where it gets captured. As I alluded to in the comments, one very interesting test is this: ArrayList<T> test3 = new TbinList<T>(args); We get this error: #pre So there's no way to make it work as-is. One of the class declarations needs to be changed. Additionally, think about it this way. Suppose we had: #pre And since a wildcard allows subtyping, we can do this: TbinList<T> test4 = new TbinList<T>(args); Should we be able to add a Tbin<T> to test4 ? No, this would be heap pollution. We might end up with Derived2 s floating around in a TbinList<T> .

Question-28319064, answer-28319148
I prefer the second way. When you use the first way, if you decide to use a parallel stream to improve performance, you'll have no control over the order in which the elements will be added to the output list by forEach . When you use toList , the Streams API will preserve the order even if you use a parallel stream.

Question-5243754, answer-34295445
request.getParameter() We use request.getParameter(args) to extract request parameters (i.e. data sent by posting a html form ). The request.getParameter(args) always returns String value and the data come from client. request.getAttribute() We use request.getAttribute(args) to get an object added to the request scope on the server side i.e. using request.setAttribute(args) . You can add any type of object you like here, Strings , Custom objects, in fact any object. You add the attribute to the request and forward the request to another resource, the client does not know about this. So all the code handling this would typically be in JSP/servlets. You can use request.setAttribute(args) to add extra-information and forward/redirect the current request to another resource. For example,consider about first.jsp, #pre and second.jsp: #pre From your browser, run first.jsp?CLIENT=you and the output on your browser is #pre The basic difference between getAttribute(args) and getParameter(args) is that the first method extracts a (serialized) Java object and the other provides a String value. For both cases a name is given so that its value (be it string or a java bean) can be looked up and extracted.

Question-29328785, answer-29330741
It's not so hard to track down the reason for the odd behavior. The divide call goes to #pre This, internally, delegates to another divide method, based on the rounding mode: #pre In this case, the last call applies. Note that the intVal (which is a BigInteger that is stored in the BigDecimal ) is passed directly to this method as the first argument: #pre Finally, the path to the second divideAndRound is taken here, again passing the dividend on (which was the intVal of the original BigDecimal ), ending up with this code: #pre And this is where the error is introduced: The mdivididend is a mutable BigInteger , that was created as a mutable view on the mag array of the BigInteger that is stored in the BigDecimal x from the original call. The division modifies the mag field, and thus, the state of the (now not-so-immutable) BigDecimal . This is clearly a bug in the implementation of one of the divide methods. I already started tracking the change sets of the OpenJDK, but have not yet spotted the definite culprit. ( Edit: See updates below ) (A side note: Calling x.toString(args) before doing the division does not really avoid , but only hide the bug: It causes a string cache of the correct state to be created internally. The right value is printed, but the internal state is still wrong - which is concerning, to say the least...) #blockquote

Question-34172978
#pre Counter to my expectation, the collect call never returns. Setting limit before filter produces the expected result. Why?

Question-12575990, answer-12576219
A Java Date is a container for the number of milliseconds since January 1, 1970, 00:00:00 GMT. When you use something like System.out.println(args) , Java uses Date.toString(args) to print the contents. The only way to change it is to override Date and provide your own implementation of Date.toString(args) . Now before you fire up your IDE and try this, I wouldn't; it will only complicate matters. You are better off formatting the date to the format you want to use (or display). What you can do, is format the date. #pre These are actually the same date, represented differently.
Question-19998454, answer-19998652
This is being described in this article . Think about forEach of Collections. List<T> list = … list.forEach(args); #blockquote

Question-14526260, answer-14526289
just use File.getName() #pre using String methods : #pre

Question-32356762, answer-32357149
Disclaimer: I am not a JDK author. I agree that it's correct to write your own code to interfaces, but if you're going return a mutable collection to a third party, it's important to let the third party know what sort of List they're getting back. LinkedList and ArrayList are very different, performance wise, for various operations. For example, removing the first element of an ArrayList is O(args) , but removing the first element of a LinkedList is O(args) . By fully specifying the return type, the JDK authors are communicating extra information , in unambiguous code, about what sort of object they're giving back to you, so you can write your code to use this method properly. If you really need a LinkedList , you know that you have to specify one here. Finally, the main reason to code to an interface over an implementation is if you think that the implementation will change. The JDK authors probably figure they are never going to change this method; it's never going to return a LinkedList or a Collections.UnmodifiableList . However, in most cases, you would probably still do: List<T> list = Collections.list(args);

Question-33635717
I'm reading up about Java streams and discovering new things as I go along. One of the new things I found was the peek(args) function. Almost everything I've read on peek says it should be used to debug your Streams. What if I had a Stream where each Account has a username, password field and a login() and loggedIn() method. I also have Consumer<T> login = account -> account.login(args); and Predicate<T> loggedIn = account -> account.loggedIn(args); Why would this be so bad? #pre Now as far as I can tell this does exactly what it's intended to do. It; #li Takes a list of accounts #li Tries to log in to each account #li Filters out any account which aren't logged in #li Collects the logged in accounts into a new list What is the downside of doing something like this? Any reason I shouldn't proceed? Lastly, if not this solution then what? The original version of this used the .filter() method as follows; #pre

Question-7520432, answer-7520464
In general, the answer to your question is "yes", but... #li equals will only compare what it is written to compare, no more, no less. #li if a class does not override the equals method, then it defaults to the equals(args) method of the closest parent class that has overridden this method. #li If no parent classes have provided an override, then it defaults to the method from the ultimate parent class, Object, and so you're left with the Object#equals(args) method. Per the Object API this is the same as == ; that is, it returns true if and only if both variables refer to the same object, if their references are one and the same. Thus you will be testing for object equality and not functional equality . #li Always remember to override hashCode if you override equals so as not to "break the contract". As per the API, the result returned from the hashCode(args) method for two objects must be the same if their equals methods shows that they are equivalent. The converse is not necessarily true.

Question-14846920, answer-14846964
For one, you can get away with reference sharing. A new HashMap(args) etc will require an allocated object, and possibly some extra elements to hold the data, but you only need one copy of an immutable empty collection (list, set, map, or any other such). This makes it an obvious choice when a method you're calling needs to accept a Map but does not need to edit it. I suggest checking out Josh Bloch's Effective Java , which lists some very nice attributes of immutable objects (including thread safety).

Question-6684665, answer-6684935
If you want to convert the string back into a byte array you will need to use String.getBytes(args) (or equivalent Python function) and this will allow you print out the original byte array.

Question-6652687, answer-6652772
Use String#trim() method or String allRemoved = myString.replaceAll(args) for trim both the end. For left trim: String leftRemoved = myString.replaceAll(args); For right trim: String rightRemoved = myString.replaceAll(args);

Question-30081520
I am aware that it is better to call the equals method over using the == operator (see this question ). I want two strings to compare as equal if they are both null or if they represent the same string. Unfortunately the equals method will throw an NPE if the strings are null . My code is currently: #pre This is inelegant. What is the correct way to perform this test?

Question-20129762, answer-20130131
There are already people asked the same on the mailing list ☺. The main reason is Iterable also has a re-iterable semantic, while Stream is not. #blockquote

Question-4818699, answer-4818753
There are two main uses of AtomicInteger : #li As an atomic counter ( incrementAndGet(args) , etc) that can be used by many threads concurrently #li As a primitive that supports compare-and-swap instruction ( compareAndSet(args) ) to implement non-blocking algorithms. Here is an example of non-blocking random number generator from Brian Göetz's Java Concurrency In Practice : #pre As you can see, it basically works almost the same way as incrementAndGet(args) , but performs arbitrary calculation ( calculateNext(args) ) instead of increment (and processes the result before return).

Question-32046078
I have a server side implemented in Scala and React/Flux based front end. My services return Futures and they are handled within Scalatra's AsyncResult for JSON responses. For isomorphic/server side rendering setup I did not want to change services to be blocking so I started with Scala Future-> java.util.function.Function conversion shown here . But the dispatcher in Flux would like to have JS Promise. So far I found only rather complicated sounding way around this Slides 68-81 Is there any recommended way to deal with this Scala Future -> JS Promise conversion?

Question-17781150, answer-17781189
The only int value for which it works is Integer.MIN_VALUE . It's because integers are negated using the two's complement way . Using System.out.println(args); you see that Integer.MIN_VALUE is 10000000000000000000000000000000 Taking the negative value is done by first swapping 0 and 1 , which gives 01111111111111111111111111111111 and by adding 1 , which gives 10000000000000000000000000000000 As you can see in the link I gave, Wikipedia mentions the problem with the most negative numbers and specifies it's the sole exception : #blockquote Of course you have the same phenomenon for Long.Min_Value if you store it in a long variable. Note that this is only due to choices that were made regarding the binary storage of ints in Java . Another (bad) solution could for example have been to negate by simply changing the most significant bit and letting the other bits unchanged, this would have avoided this problem with MIN_VALUE but would have made 2 different 0 values and complicated binary arithmetic (how would you have incremented for example ?).

Question-5071040, answer-22547954
Always use either String.valueOf(args) or Integer.toString(args) . Using "" + number is an overhead and does the following: #pre

Question-4709175, answer-4709892
Something none of the other answers have covered that make enums particularly powerful are the ability to have template methods . Methods can be part of the base enum and overridden by each type. And, with the behavior attached to the enum, it often eliminates the need for if-else constructs or switch statements as this blog post demonstrates - where enum.method(args) does what originally would be executed inside the conditional. The same example also shows the use of static imports with enums as well producing much cleaner DSL like code. Some other interesting qualities include the fact that enums provide implementation for equals(args) , toString(args) and hashCode(args) and implement Serializable and Comparable . For a complete rundown of all that enums have to offer I highly recommend Bruce Eckel's Thinking in Java 4th edition which devotes an entire chapter to the topic. Particularly illuminating are the examples involving a Rock, Paper, Scissors (i.e. RoShamBo) game as enums.

Question-17828584
I was wondering what happens when you try to catch an StackOverflowError and came up with the following method: #pre Now my question: Why does this method print '4'? I thought maybe it was because System.out.println(args) needs 3 segments on the call stack, but I don't know where the number 3 comes from. When you look at the source code (and bytecode) of System.out.println(args) , it normally would lead to far more method invocations than 3 (so 3 segments on the call stack would not be sufficient). If it's because of optimizations the Hotspot VM applies (method inlining), I wonder if the result would be different on another VM. Edit : As the output seems to be highly JVM specific, I get the result 4 using Java(TM) SE Runtime Environment (build 1.6.0_41-b02) Java HotSpot(TM) 64-Bit Server VM (build 20.14-b01, mixed mode) Explanation why I think this question is different from Understanding java stack : My question is not about why there is a cnt > 0 (obviously because System.out.println(args) requires stack size and throws another StackOverflowError before something gets printed), but why it has the particular value of 4, respectively 0,3,8,55 or something else on other systems.

Question-19235606, answer-19235863
You should almost certainly never have to deal with Spliterator as a user; it should only be necessary if you're writing Collection types yourself and also intending to optimize parallelized operations on them. For what it's worth, a Spliterator is a way of operating over the elements of a collection in a way that it's easy to split off part of the collection, e.g. because you're parallelizing and want one thread to work on one part of the collection, one thread to work on another part, etc. You should essentially never be saving values of type Stream to a variable, either. Stream is sort of like an Iterator , in that it's a one-time-use object that you'll almost always use in a fluent chain, as in the Javadoc example: int sum = widgets.stream(args) .filter(args) .mapToInt(args) .sum(args); Collector is the most generalized, abstract possible version of a "reduce" operation a la map/reduce; in particular, it needs to support parallelization and finalization steps. Examples of Collector s include: #li summing, e.g. Collectors.reducing(args) #li StringBuilder appending, e.g. Collectors.of(args)

Question-16635398, answer-25855691
TL;DR : List.stream(args).forEach(args) was the fastest. I felt I should add my results from benchmarking iteration. I took a very simple approach (no benchmarking frameworks) and benchmarked 5 different methods: #li classic for #li classic foreach #li List.forEach(args) #li List.stream(args).forEach(args) #li List.parallelStream(args).forEach the testing procedure and parameters #pre The list in this class shall be iterated over and have some doIt(args) applied to all it's members, each time via a different method. in the Main class I run the tested method three times to warm up the JVM. I then run the test method 1000 times summing the time it takes for each iteration method (using System.nanoTime(args) ). After that's done i divide that sum by 1000 and that's the result, average time. example: #pre I ran this on a i5 4 core CPU, with java version 1.8.0_05 classic for #pre execution time: 4.21 ms classic foreach #pre execution time: 5.95 ms List.forEach(args) list.forEach(args); execution time: 3.11 ms List.stream(args).forEach(args) list.stream(args).forEach(args); execution time: 2.79 ms List.parallelStream(args).forEach list.parallelStream(args).forEach(args); execution time: 3.6 ms

Question-20358883
Is there a Utility method somewhere that can do this in 1 line? I can't find it anywhere in Collections , or List . #pre I don't want to re-invent the wheel unless I plan on putting fancy rims on it. Well... the type can be T , and not String . but you get the point. (with all the null checking, safety checks...etc)

Question-28319064, answer-28319221
Don't worry about any performance differences, they're going to be minimal in this case normally. Method 2 is preferable because #li it doesn't require mutating a collection that exists outside the lambda expression, #li it's more readable because the different steps that are performed in the collection pipeline are written sequentially (first a filter operation, then a map operation, then collecting the result), (for more info on the benefits of collection pipelines, see Martin Fowler's excellent article ) #li you can easily change the way values are collected by replacing the Collector that is used. In some cases you may need to write your own Collector , but then the benefit is that you can easily reuse that.

Question-31922866, answer-39005452
The best post I've seen on the topic was written by Daniel Olszewski and can be found at #a . While others mention when you should or should not use Optional, this post actually explains why . Cross-posting here in case the link goes down: #blockquote

