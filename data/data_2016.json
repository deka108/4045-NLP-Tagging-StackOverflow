{"has_more": true, "items": [{"body": "<p>We are compiling an embedded C/C++ application that is deployed in a shielded device in an environment bombarded with <a href=\"https://en.wikipedia.org/wiki/Ionizing_radiation\">ionizing radiation</a>. We are using GCC and cross-compiling for ARM. When deployed, our application generates some erroneous data and crashes more often that we would like. The hardware is designed for this environment, and our application has run on this platform for several years.</p>\n\n<p>Are there changes we can make to our code, or compile-time improvements that can be made to identify/correct <a href=\"https://en.wikipedia.org/wiki/Soft_error\">soft errors</a> and memory-corruption caused by <a href=\"https://en.wikipedia.org/wiki/Single_event_upset\">single event upsets</a>? Have any other developers had success in reducing the harmful effects of soft errors on a long-running application?</p>\n", "title": "Compiling an application for use in highly radioactive environments", "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"], "answer_count": 21, "link": "http://stackoverflow.com/questions/36827659/compiling-an-application-for-use-in-highly-radioactive-environments", "answers": [{"body": "<p>NASA has <a href=\"https://ti.arc.nasa.gov/m/pub-archive/1075h/1075%20(Mehlitz).pdf\">a paper on radiation-hardened</a> software. It describes three main tasks:</p>\n\n<ol>\n<li>Regular monitoring of memory for errors then scrubbing out those errors,</li>\n<li>robust error recovery mechanisms, and</li>\n<li>the ability to reconfigure if something no longer works.</li>\n</ol>\n\n<p>Note that the memory scan rate should be frequent enough that multi-bit errors rarely occur, as most <a href=\"https://en.wikipedia.org/wiki/RAM_parity#ECC_type_RAM\">ECC</a> memory can recovery from single-bit errors, not multi-bit errors.</p>\n\n<p>Robust error recovery includes control flow transfer (typically restarting a process at a point before the error), resource release, and data restoration.</p>\n\n<p>Their main recommendation for data restoration is to avoid the need for it, through having intermediate data be treated as temporary, so that restarting before the error also rolls back the data to a reliable state. This sounds similar to the concept of \"transactions\" in databases.</p>\n\n<p>They discuss techniques particularly suitable for object-oriented languages such as C++. For example</p>\n\n<ol>\n<li>Software-based ECCs for contiguous memory objects</li>\n<li><a href=\"https://en.wikipedia.org/wiki/Design_by_contract\">Programming by Contract</a>: verifying preconditions and postconditions, then checking the object to verify it is still in a valid state.</li>\n</ol>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36827916, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>You may also be interested in the rich literature on the subject of algorithmic fault tolerance. This includes the old assignment: Write a sort that correctly sorts its input when a constant number of comparisons will fail (or, the slightly more evil version, when the asymptotic number of failed comparisons scales as <code>log(n)</code> for <code>n</code> comparisons).</p>\n\n<p>A place to start reading is Huang and Abraham's 1984 paper \"<a href=\"https://www.computer.org/csdl/trans/tc/1984/06/01676475.pdf\">Algorithm-Based Fault Tolerance for Matrix Operations</a>\". Their idea is vaguely similar to homomorphic encrypted computation (but it is not really the same, since they are attempting error detection/correction at the operation level).</p>\n\n<p>A more recent descendant of that paper is Bosilca, Delmas, Dongarra, and Langou's \"<a href=\"http://www.sciencedirect.com/science/article/pii/S0743731508002141\">Algorithm-based fault tolerance applied to high performance computing</a>\".</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36828960, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Here are some thoughts and ideas:</p>\n\n<p><strong>Use ROM more creatively.</strong></p>\n\n<p>Store anything you can in ROM. Instead of calculating things, store look-up tables in ROM. (Make sure your compiler is outputting your look-up tables to the read-only section! Print out memory addresses at runtime to check!) Store your interrupt vector table in ROM. Of course, run some tests to see how reliable your ROM is compared to your RAM.</p>\n\n<p><strong>Use your best RAM for the stack.</strong></p>\n\n<p>SEUs in the stack are probably the most likely source of crashes, because it is where things like index variables, status variables, return addresses, and pointers of various sorts typically live.</p>\n\n<p><strong>Implement timer-tick and watchdog timer routines.</strong></p>\n\n<p>You can run a \"sanity check\" routine every timer tick, as well as a watchdog routine to handle the system locking up. Your main code could also periodically increment a counter to indicate progress, and the sanity-check routine could ensure this has occurred.</p>\n\n<p><strong>Implement <a href=\"https://en.wikipedia.org/wiki/Forward_error_correction\">error-correcting-codes</a> in software.</strong></p>\n\n<p>You can add redundancy to your data to be able to detect and/or correct errors. This will add processing time, potentially leaving the processor exposed to radiation for a longer time, thus increasing the chance of errors, so you must consider the trade-off.</p>\n\n<p><strong>Remember the caches.</strong></p>\n\n<p>Check the sizes of your CPU caches. Data that you have accessed or modified recently will probably be within a cache. I believe you can disable at least some of the caches (at a big performance cost); you should try this to see how susceptible the caches are to SEUs. If the caches are hardier than RAM then you could regularly read and re-write critical data to make sure it stays in cache and bring RAM back into line.</p>\n\n<p><strong>Use page-fault handlers cleverly.</strong></p>\n\n<p>If you mark a memory page as not-present, the CPU will issue a page fault when you try to access it. You can create a page-fault handler that does some checking before servicing the read request. (PC operating systems use this to transparently load pages that have been swapped to disk.)</p>\n\n<p><strong>Use assembly language for critical things (which could be everything).</strong></p>\n\n<p>With assembly language, you <em>know</em> what is in registers and what is in RAM; you <em>know</em> what special RAM tables the CPU is using, and you can design things in a roundabout way to keep your risk down.</p>\n\n<p>Use <code>objdump</code> to actually look at the generated assembly language, and work out how much code each of your routines takes up.</p>\n\n<p>If you are using a big OS like Linux then you are asking for trouble; there is just so much complexity and so many things to go wrong.</p>\n\n<p><strong>Remember it is a game of probabilities.</strong></p>\n\n<p>A commenter said</p>\n\n<blockquote>\n  <p>Every routine you write to catch errors will be subject to failing itself from the same cause.</p>\n</blockquote>\n\n<p>While this is true, the chances of errors in the (say) 100 bytes of code and data required for a check routine to function correctly is much smaller than the chance of errors elsewhere. If your ROM is pretty reliable and almost all the code/data is actually in ROM then your odds are even better.</p>\n\n<p><strong>Use redundant hardware.</strong></p>\n\n<p>Use 2 or more identical hardware setups with identical code. If the results differ, a reset should be triggered. With 3 or more devices you can use a \"voting\" system to try to identify which one has been compromised.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36829957, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Working for about 4-5 years with software/firmware development and environment testing of <a href=\"https://en.wikipedia.org/wiki/Miniaturized_satellite\">miniaturized satellites</a>*, I would like to share my experience here.</p>\n\n<p>*(<em>miniaturized satellites are a lot more prone to single event upsets than bigger satellites due to its relatively small, limited sizes for its electronic components</em>)</p>\n\n<blockquote>\n  <p>To be very concise and direct: there is no mechanism to recover from <em>detectable, erroneous\n  situation</em> by the software/firmware itself <em>without</em>, at least, one\n  <em>copy</em> of <em>minimum working version</em> of the software/firmware <em>somewhere</em> for <em>recovery</em> purpose - and with the <em>hardware supporting the recovery</em> (functional).</p>\n</blockquote>\n\n<p>Now, this situation is normally handled both in the hardware and software level. Here, as you request, I will share what we can do in the software level.</p>\n\n<ol>\n<li><p><strong>...recovery purpose...</strong>. Provide ability to update/recompile/reflash your software/firmware in real environment. This is an <em>almost must-have</em> feature for any software/firmware in highly ionized environment. Without this, you <em>could</em> have redundant software/hardware as many as you want but at one point, they are all going to blow up. So, prepare this feature!</p></li>\n<li><p><strong>...minimum working version...</strong> Have responsive, multiple copies, minimum version of the software/firmware in your code. This is like Safe mode in Windows. Instead of having only one, fully functional version of your software, have multiple copies of the minimum version of your software/firmware. The minimum copy will usually having much less size than the full copy and almost always have <em>only</em> the following two or three features: </p>\n\n<ol>\n<li>capable of listening to command from external system, </li>\n<li>capable of updating the current software/firmware, </li>\n<li>capable of monitoring the basic operation's housekeeping data.</li>\n</ol></li>\n<li><p><strong>...copy... somewhere...</strong> Have redundant software/firmware somewhere. </p>\n\n<ol>\n<li><p>You could, with <em>or</em> without redundant hardware, try to have redundant software/firmware in your ARM uC. This is normally done by having two or more identical software/firmware <em>in separate addresses</em> which sending heartbeat to each other - but only one will be active at a time. If one or more software/firmware is known to be unresponsive, switch to the other software/firmware. The benefit of using this approach is we can have functional replacement immediately after an error occurs - without any contact with whatever external system/party who is responsible to detect and to repair the error (in satellite case, it is usually the Mission Control Centre (MCC)). </p>\n\n<p><strong>[Edit]</strong> as shown in my response to <a href=\"http://stackoverflow.com/users/541686/mehrdad\">Mehrdad</a>'s comment: strictly speaking, without redundant hardware, the disadvantage of doing this is you actually <em>cannot</em> eliminate <em>all</em> single point of failures. At the very least, you will still have <em>one</em> single point of failure, which is <em>the switch itself</em> (or often the beginning of the code). Nevertheless, for a device limited by size in a highly ionized environment (such as pico/femto satellites), the reduction of the single point of failures to one point <em>without</em> additional hardware will still be worth considering. Somemore, the piece of code for the switching would certainly be much less than the code for the whole program - significantly reducing the risk of getting Single Event in it.</p></li>\n<li><p>But if you are not doing this, you should have at least one copy in your external system which can come in contact with the device and update the software/firmware (in the satellite case, it is again the mission control centre). </p></li>\n<li>You could also have the copy in your permanent memory storage in your device which can be triggered to restore the running system's software/firmware</li>\n</ol></li>\n<li><p><strong>...detectable erroneous situation..</strong> The error must be <em>detectable</em>, usually by the hardware <em>error correction/detection circuit</em> or by a small piece of code for error correction/detection. It is best to put such code small, multiple, and <em>independent</em> from the main software/firmware. Its main task is <em>only</em> for checking/correcting. If the hardware circuit/firmware is <em>reliable</em> (such as it is more radiation hardened than the rests - or having multiple circuits/logics), then you might consider making error-correction with it. But if it is not, it is better to make it as error-detection. The correction can be by external system/device. For the error correction, you could consider making use of a basic error correction algorithm like Hamming/Golay23, because they can be implemented more easily both in the circuit/software. But it ultimately depends on your team's capability. For error detection, normally CRC is used.</p></li>\n<li><p><strong>...hardware supporting the recovery</strong> Now, comes to the most difficult aspect on this issue. Ultimately, the recovery requires the hardware which is responsible for the recovery to be <em>at least</em> functional. If the hardware is permanently broken (normally happen after its <strong>Total ionizing dose</strong> reaches certain level), then there is (sadly) no way for the software to help in recovery. Thus, hardware is rightly the utmost importance concern for a device exposed to high radiation level (such as satellite). <strong>[Edit]</strong> For the importance of the redundant hardware, read the comments.</p></li>\n</ol>\n\n<p>In addition to the suggestion for above anticipating firmware's error due to single event upset, I would also like to suggest you to have:</p>\n\n<ol>\n<li>Error detection and/or error correction algorithm in the inter-subsystem communication protocol. This is another almost must have in order to avoid incomplete/wrong signals received from other system</li>\n<li>Filter in your ADC reading. Do <em>not</em> use the ADC reading directly. Filter it by median filter, mean filter, or any other filters - <em>never</em> trust single reading value. Sample more, not less - reasonably.</li>\n</ol>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36831344, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>It may be possible to use C to write programs that behave robustly in such environments, but only if most forms of compiler optimization are disabled.  Optimizing compilers are designed to replace many seemingly-redundant coding patterns with \"more efficient\" ones, and may have no clue that the reason the programmer is testing <code>x==42</code> when the compiler knows there's no way <code>x</code> could possibly hold anything else is because the programmer wants to prevent the execution of certain code with <code>x</code> holding some other value--even in cases where the only way it could hold that value would be if the system received some kind of electrical glitch.</p>\n\n<p>Declaring variables as <code>volatile</code> is often helpful, but may not be a panacea.\nOf particular importance, note that safe coding often requires that dangerous\noperations have hardware interlocks that require multiple steps to activate,\nand that code be written using the pattern:</p>\n\n<pre><code>... code that checks system state\nif (system_state_favors_activation)\n{\n  prepare_for_activation();\n  ... code that checks system state again\n  if (system_state_is_valid)\n  {\n    if (system_state_favors_activation)\n      trigger_activation();\n  }\n  else\n    perform_safety_shutdown_and_restart();\n}\ncancel_preparations();\n</code></pre>\n\n<p>If a compiler translates the code in relatively literal fashion, and if all\nthe checks for system state are repeated after the <code>prepare_for_activation()</code>,\nthe system may be robust against almost any plausible single glitch event,\neven those which would arbitrarily corrupt the program counter and stack.  If\na glitch occurs just after a call to <code>prepare_for_activation()</code>, that would imply\nthat activation would have been appropriate (since there's no other reason\n<code>prepare_for_activation()</code> would have been called before the glitch). If the\nglitch causes code to reach <code>prepare_for_activation()</code> inappropriately, but there\nare no subsequent glitch events, there would be no way for code to subsequently\nreach <code>trigger_activation()</code> without having passed through the validation check or calling cancel_preparations first [if the stack glitches, execution might proceed to a spot just before <code>trigger_activation()</code> after the context that called <code>prepare_for_activation()</code> returns, but the call to <code>cancel_preparations()</code> would have occurred between the calls to <code>prepare_for_activation()</code> and <code>trigger_activation()</code>, thus rendering the latter call harmless.</p>\n\n<p>Such code may be safe in traditional C, but not with modern C compilers.  Such compilers can be very dangerous in that sort of environment because aggressive they strive to only include code which will be relevant in situations that could come about via some well-defined mechanism and whose resulting consequences would also be well defined.  Code whose purpose would be to detect and clean up after failures may, in some cases, end up making things worse.  If the compiler determines that the attempted recovery would in some cases invoke undefined behavior, it may infer that the conditions that would necessitate such recovery in such cases cannot possibly occur, thus eliminating the code that would have checked for them.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36845821, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>How about running many instances of your application. If crashes are due to random memory bit changes, chances are some of your app instances will make it through and produce accurate results. It's probably quite easy (for someone with statistical background) to calculate how many instances do you need given bit flop probability to achieve as tiny overall error as you wish.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36846344, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>This is an extremely broad subject. Basically, you can't really recover from memory corruption, but you can at least try to <strong>fail promptly</strong>. Here are a few techniques you could use:</p>\n\n<ul>\n<li><p><strong>checksum constant data</strong>. If you have any configuration data which stays constant for a long time (including hardware registers you have configured), compute its checksum on initialization and verify it periodically. When you see a mismatch, it's time to re-initialize or reset.</p></li>\n<li><p><strong>store variables with redundancy</strong>. If you have an important variable <code>x</code>, write its value in <code>x1</code>, <code>x2</code> and <code>x3</code> and read it as <code>(x1 == x2) ? x2 : x3</code>.</p></li>\n<li><p>implement <strong>program flow monitoring</strong>. XOR a global flag with a unique value in important functions/branches called from the main loop. Running the program in a radiation-free environment with near-100% test coverage should give you the list of acceptable values of the flag at the end of the cycle. Reset if you see deviations.</p></li>\n<li><p><strong>monitor the stack pointer</strong>. In the beginning of the main loop, compare the stack pointer with its expected value. Reset on deviation.</p></li>\n</ul>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36846817, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>What you ask is quite complex topic - not easily answerable. Other answers are ok, but they covered just a small part of all the things you need to do.</p>\n\n<p><a href=\"http://stackoverflow.com/questions/36827659/compiling-an-application-for-use-in-highly-radio-active-environments#comment61226321_36827659\">As seen in comments</a>, it is not possible to fix hardware problems 100%, however it is possible with high probabily to reduce or catch them using various techniques.</p>\n\n<p>If I was you, I would create the software of the highest <a href=\"https://en.wikipedia.org/wiki/Safety_integrity_level\">Safety integrity level</a> level (SIL-4). Get the IEC 61513 document (for the nuclear industry) and follow it.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36864417, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>What could help you is a <a href=\"https://en.wikipedia.org/wiki/Watchdog_timer\">watchdog</a>. Watchdogs were used extensively in industrial computing in the 1980s. Hardware failures were much more common then - another answer also refers to that period.</p>\n\n<p>A watchdog is a combined hardware/software feature. The hardware is a simple counter that counts down from a number (say 1023) to zero. <a href=\"https://en.wikipedia.org/wiki/Transistor%E2%80%93transistor_logic\">TTL</a> or other logic could be used.</p>\n\n<p>The software has been designed as such that one routine monitors the correct operation of all essential systems. If this routine completes correctly = finds the computer running fine, it sets the counter back to 1023.</p>\n\n<p>The overall design is so that under normal circumstances, the software prevents that the hardware counter will reach zero. In case the counter reaches zero, the hardware of the counter performs its one-and-only task and resets the entire system. From a counter perspective, zero equals 1024 and the counter continues counting down again.</p>\n\n<p>This watchdog ensures that the attached computer is restarted in a many, many cases of failure. I must admit that I'm not familiar with hardware that is able to perform such a function on today's computers. Interfaces to external hardware are now a lot more complex than they used to be.</p>\n\n<p>An inherent disadvantage of the watchdog is that the system is not available from the time it fails until the watchdog counter reaches zero + reboot time. While that time is generally much shorter than any external or human intervention, the supported equipment will need to be able to proceed without computer control for that timeframe.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36877096, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Writing code for radioactive environments is not really any different than writing code for any mission-critical application. </p>\n\n<p>In addition to what has already been mentioned, here are some miscellaneous tips:</p>\n\n<ul>\n<li>Use everyday \"bread &amp; butter\" safety measures that should be present on any semi-professional embedded system: internal watchdog, internal low-voltage detect, internal clock monitor. These things shouldn't even need to be mentioned in the year 2016 and they are standard on pretty much every modern microcontroller.</li>\n<li>If you have a safety and/or automotive-oriented MCU, it will have certain watchdog features, such as a given time window, inside which you need to refresh the watchdog. This is preferred if you have a mission-critical real-time system.</li>\n<li>In general, use a MCU suitable for these kind of systems, and not some generic mainstream fluff you received in a packet of corn flakes. Almost every MCU manufacturer nowadays have specialized MCUs designed for safety applications (TI, Freescale, Renesas, ST, Infineon etc etc). These have lots of built-in safety features, including lock-step cores: meaning that there are 2 CPU cores executing the same code, and they must agree with each other.</li>\n<li><p>IMPORTANT: You must ensure the integrity of internal MCU registers. All control &amp; status registers of hardware peripherals that are writeable may be located in RAM memory, and are therefore vulnerable. </p>\n\n<p>To protect yourself against register corruptions, preferably pick a microcontroller with built-in \"write-once\" features of registers. In addition, you need to store default values of all hardware registers in NVM and copy-down those values to your registers at regular intervals. You can ensure the integrity of important variables in the same manner.</p>\n\n<p>Note: always use defensive programming. Meaning that you have to setup <em>all</em> registers in the MCU and not just the ones used by the application. You don't want some random hardware peripheral to suddenly wake up.</p></li>\n<li><p>There are all kinds of methods to check for errors in RAM or NVM: checksums, \"walking patterns\", software ECC etc etc. The best solution nowadays is to not use any of these, but to use a MCU with built-in ECC and similar checks. Because doing this in software is complex, and the error check in itself could therefore introduce bugs and unexpected problems.</p></li>\n<li>Use redundancy. You could store both volatile and non-volatile memory in two identical \"mirror\" segments, that must always be equivalent. Each segment could have a CRC checksum attached.</li>\n<li>Avoid using external memories outside the MCU.</li>\n<li>Implement a default interrupt service routine / default exception handler for all possible interrupts/exceptions. Even the ones you are not using. The default routine should do nothing except shutting off its own interrupt source.</li>\n<li><p>Understand and embrace the concept of defensive programming. This means that your program needs to handle all possible cases, even those that cannot occur in theory. <a href=\"http://stackoverflow.com/questions/35053371/what-is-the-benefit-of-terminating-if-else-if-constructs-with-an-else-clause\">Examples</a>. </p>\n\n<p>High quality mission-critical firmware detects as many errors as possible, and then ignores them in a safe manner.</p></li>\n<li>Never write programs that rely on poorly-specified behavior. It is likely that such behavior might change drastically with unexpected hardware changes caused by radiation or EMI. The best way to ensure that your program is free from such crap is to use a coding standard like MISRA, together with a static analyser tool. This will also help with defensive programming and with weeding out bugs (why would you not want to detect bugs in any kind of application?).</li>\n<li><p>IMPORTANT: Don't implement any reliance of the default values of static storage duration variables. That is, don't trust the default contents of the <code>.data</code> or <code>.bss</code>. There could be any amount of time between the point of initialization to the point where the variable is actually used, there could have been plenty of time for the RAM to get corrupted. Instead, write the program so that all such variables are set from NVM in run-time, just before the time when such a variable is used for the first time. </p>\n\n<p>In practice this means that if a variable is declared at file scope or as <code>static</code>, you should never use <code>=</code> to initialize it (or you could, but it is pointless, because you cannot rely on the value anyhow). Always set it in run-time, just before use. If it is possible to repeatedly update such variables from NVM, then do so.</p>\n\n<p>Similarly in C++, don't rely on constructors for static storage duration variables. Have the constructor(s) call a public \"set-up\" routine, which you can also call later on in run-time, straight from the caller application.</p>\n\n<p>If possible, remove the \"copy-down\" start-up code that initializes <code>.data</code> and <code>.bss</code> (and calls C++ constructors) entirely, so that you get linker errors if you write code relying on such. Many compilers have the option to skip this, usually called \"minimal/fast start-up\" or similar.</p>\n\n<p>This means that any external libraries have to be checked so that they don't contain any such reliance.</p></li>\n<li><p>Implement and define a safe state for the program, to where you will revert in case of critical errors. </p></li>\n<li>Implementing an error report/error log system is always helpful.</li>\n</ul>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36892379, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>You want 3+ slave machines with a master outside the radiation environment. All I/O passes through the master which contains a vote and/or retry mechanism. The slaves must have a hardware watchdog each and the call to bump them should be surrounded by CRCs or the like to reduce the probability of involuntary bumping. Bumping should be controlled by the master, so lost connection with master equals reboot within a few seconds.</p>\n\n<p>One advantage of this solution is that you can use the same API to the master as to the slaves, so redundancy becomes a transparent feature.</p>\n\n<p><strong>Edit:</strong> From the comments I feel the need to clarify the \"CRC idea.\" The possibilty of the slave bumping it's own watchdog is close to zero if you surround the bump with CRC or digest checks on random data from the master. That random data is only sent from master when the slave under scrutiny is aligned with the others. The random data and CRC/digest are immediately cleared after each bump. The master-slave bump frequency should be more than <a href=\"https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem\" rel=\"nofollow\">double</a> the watchdog timeout. The data sent from the master is uniquely generated every time.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36892503, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>If your hardware fails then you can use mechanical storage to recover it. If your code base is small and have some physical space then you can use a mechanical data store.</p>\n\n<p><a href=\"http://i.stack.imgur.com/MdkE9.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/MdkE9.jpg\" alt=\"Enter image description here\"></a></p>\n\n<p>There will be a surface of material which will not be affected by radiation. Multiple gears will be there. A mechanical reader will run on all the gears and will be flexible to move up and down. Down means it is 0 and up means it is 1. From 0 and 1 you can generate your code base.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36892709, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Since you specifically ask for software solutions, and you are using C++, why not use operator overloading to make your own, safe datatypes? For example:</p>\n\n<p>Instead of using <code>uint32_t</code> (and <code>double</code>, <code>int64_t</code> etc), make your own <code>SAFE_uint32_t</code> which contains a multiple (minimum of 3) of uint32_t. Overload all of the operations you want (* + - / &lt;&lt; >> = == != etc) to perform, and make the overloaded operations perform independently on each internal value, ie don't do it once and copy the result. Both before and after, check that all of the internal values match. If values don't match, you can update the wrong one to the value with the most common one. If there is no most-common value, you can safely notify that there is an error.</p>\n\n<p>This way it doesn't matter if corruption occurs in the ALU, registers, RAM, or on a bus, you will still have multiple attempts and a very good chance of catching errors. Note however though that this only works for the variables you can replace - your stack pointer for example will still be susceptible.</p>\n\n<p>A side story: I ran into a similar issue, also on an old ARM chip. It turned out to be a toolchain which used an old version of GCC that, together with the specific chip we used, triggered a bug in certain edge cases that would (sometimes) corrupt values being passed into functions. Make sure your device doesn't have any problems before blaming it on radio-activity, and yes, sometimes it is a compiler bug =)</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36894388, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p><em>This answer assumes you are concerned with having a system that works correctly, over and above having a system that is minimum cost or fast; most people playing with radioactive things value correctness / safety over speed / cost</em></p>\n\n<p>Several people have suggested hardware changes you can make (fine - there's lots of good stuff here in answers already and I don't intend repeating all of it), and others have suggested redundancy (great in principle), but I don't think anyone has suggested how that redundancy might work in practice. How do you fail over? How do you know when something has 'gone wrong'? Many technologies work on the basis everything will work, and failure is thus a tricky thing to deal with. However, some distributed computing technologies designed for scale <em>expect</em> failure (after all with enough scale, failure of one node of many is inevitable with any MTBF for a single node); you can harness this for your environment.</p>\n\n<p>Here are some ideas:</p>\n\n<ul>\n<li><p>Ensure that your entire hardware is replicated <code>n</code> times (where <code>n</code> is greater than 2, and preferably odd), and that each hardware element can communicate with each other hardware element. Ethernet is one obvious way to do that, but there are many other far simpler routes that would give better protection (e.g. CAN). Minimise common components (even power supplies). This may mean sampling ADC inputs in multiple places for instance.</p></li>\n<li><p>Ensure your application state is in a single place, e.g. in a finite state machine. This can be entirely RAM based, though does not preclude stable storage. It will thus be stored in several place.</p></li>\n<li><p>Adopt a quorum protocol for changes of state. See <a href=\"https://raft.github.io/\">RAFT</a> for example. As you are working in C++, there are well known libraries for this. Changes to the FSM would only get made when a majority of nodes agree. Use a known good library for the protocol stack and the quorum protocol rather than rolling one yourself, or all your good work on redundancy will be wasted when the quorum protocol hangs up.</p></li>\n<li><p>Ensure you checksum (e.g. CRC/SHA) your FSM, and store the CRC/SHA in the FSM itself (as well as transmitting in the message, and checksumming the messages themselves). Get the nodes to check their FSM regularly against these checksum, checksum incoming messages, and check their checksum matches the checksum of the quorum.</p></li>\n<li><p>Build as many other internal checks into your system as possible, making nodes that detect their own failure reboot (this is better than carrying on half working provided you have enough nodes). Attempt to let them cleanly remove themselves from the quorum during rebooting in case they don't come up again. On reboot have them checksum the software image (and anything else they load) and do a full RAM test before reintroducing themselves to the quorum.</p></li>\n<li><p>Use hardware to support you, but do so carefully. You can get ECC RAM, for instance, and regularly read/write through it to correct ECC errors (and panic if the error is uncorrectable). However (from memory) static RAM is far more tolerant of ionizing radiation than DRAM is in the first place, so it <em>may</em> be better to use static DRAM instead. See the first point under 'things I would not do' as well.</p></li>\n</ul>\n\n<p>Let's say you have an 1% chance of failure of any given node within one day, and let's pretend you can make failures entirely independent. With 5 nodes, you'll need three to fail within one day, which is a .00001% chance. With more, well, you get the idea.</p>\n\n<p>Things I would <em>not</em> do:</p>\n\n<ul>\n<li><p><em>Underestimate the value of not having the problem to start off with.</em> Unless weight is a concern, a large block of metal around your device is going to be a far cheaper and more reliable solution than a team of programmers can come up with. Ditto optical coupling of inputs of EMI is an issue, etc. Whatever, attempt when sourcing your components to source those rated best against ionizing radiation.</p></li>\n<li><p><em>Roll your own algorithms</em>. People have done this stuff before. Use their work. Fault tolerance and distributed algorithms are hard. Use other people's work where possible.</p></li>\n<li><p><em>Use complicated compiler settings in the naive hope you detect more failures.</em> If you are lucky, you may detect more failures. More likely, you will use a code-path within the compiler which has been less tested, particularly if you rolled it yourself.</p></li>\n<li><p><em>Use techniques which are untested in your environment.</em> Most people writing high availability software have to simulate failure modes to check their HA works correctly, and miss many failure modes as a result. You are in the 'fortunate' position of having frequent failures on demand. So test each technique, and ensure its application actual improves MTBF by an amount that exceeds the complexity to introduce it (with complexity comes bugs). Especially apply this to my advice re quorum algorithms etc.</p></li>\n</ul>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36894602, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>One point no-one seems to have mentioned.  You say you're developing in GCC and cross-compiling onto ARM.  How do you know that you don't have code which makes assumptions about free RAM, integer size, pointer size, how long it takes to do a certain operation, how long the system will run for continuously, or various stuff like that?  This is a very common problem.</p>\n\n<p>The answer is usually automated unit testing.  Write test harnesses which exercise the code on the development system, then run the same test harnesses on the target system.  Look for differences!</p>\n\n<p>Also check for errata on your embedded device.  You may find there's something about \"don't do this because it'll crash, so enable that compiler option and the compiler will work around it\".</p>\n\n<p>In short, your most likely source of crashes is bugs in your code.  Until you've made pretty damn sure this isn't the case, don't worry (yet) about more esoteric failure modes.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36895275, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Given supercat's comments, the tendencies of modern compilers, and other things, I'd be tempted to go back to the ancient days and write the whole code in assembly and static memory allocations everywhere. For this kind of utter reliability I think assembly no longer incurs a large percentage difference of the cost.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36898264, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Someone mentioned using slower chips to prevent ions from flipping bits as easily. In a similar fashion perhaps use a specialized cpu/ram that actually uses multiple bits to store a single bit. Thus providing a hardware fault tolerance because it would be very unlikely that all of the bits would get flipped. So 1 = 1111 but would need to get hit 4 times to actually flipped. (4 might be a bad number since if 2 bits get flipped its already ambiguous). So if you go with 8, you get 8 times less ram and some fraction slower access time but a much more reliable data representation. You could probably do this both on the software level with a specialized compiler(allocate x amount more space for everything) or language implementation (write wrappers for data structures that allocate things this way). Or specialized hardware that has the same logical  structure but does this in the firmware.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36904789, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Here are huge amount of replies, but I'll try to sum up my ideas about this.</p>\n\n<p>Something crashes or does not work correctly could be result of your own mistakes - then it should be easily to fix when you locate the problem. But there is also possibility of hardware failures - and that's difficult if not impossible to fix in overall.</p>\n\n<p>I would recommend first to try to catch the problematic situation by logging (stack, registers, function calls) - either by logging them somewhere into file, or transmitting them somehow directly (\"oh no - I'm crashing\").</p>\n\n<p>Recovery from such error situation is either reboot (if software is still alive and kicking) or hardware reset (e.g. hw watchdogs). Easier to start from first one.</p>\n\n<p>If problem is hardware related - then logging should help you to identify in which function call problem occurs and that can give you inside knowledge of what is not working and where.</p>\n\n<p>Also if code is relatively complex - it makes sense to \"divide and conquer\" it - meaning you remove / disable some function calls where you suspect problem is - typically disabling half of code and enabling another half - you can get \"does work\" / \"does not work\" kind of decision after which you can focus into another half of code. (Where problem is)</p>\n\n<p>If problem occurs after some time - then stack overflow can be suspected - then it's better to monitor stack point registers - if they constantly grows.</p>\n\n<p>And if you manage to fully minimize your code until \"hello world\" kind of application - and it's still failing randomly - then hardware problems are expected - and there needs to be \"hardware upgrade\" - meaning invent such cpu / ram / ... -hardware combination which would tolerate radiation better.</p>\n\n<p>Most important thing is probably how you get your logs back if machine fully stopped / resetted / does not work - probably first thing bootstap should do - is a head back home if problematic situation is entcovered.</p>\n\n<p>If it's possible in your environment also to transmit a signal and receive response - you could try out to construct some sort of online remote debugging environment, but then you must have at least of communication media working and some processor/ some ram in working state. And by remote debugging I mean either GDB / gdb stub kind of approach or your own implementation of what you need to get back from your application (e.g. download log files, download call stack, download ram, restart)</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36907593, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Perhaps it would help to know does it mean for the hardware to be \"designed for this environment\". How does it correct and/or indicates the presence of SEU errors ?</p>\n\n<p>At one space exploration related project, we had a custom MCU, which would raise an exception/interrupt on SEU errors, but with some delay, i.e. some cycles may pass/instructions be executed after the one insn which caused the SEU exception.</p>\n\n<p>Particularly vulnerable was the data cache, so a handler would invalidate the offending cache line and restart the program. Only that, due to the imprecise nature of the exception, the sequence of insns headed by the exception raising insn may not be restartable.</p>\n\n<p>We identified the hazardous (not restartable) sequences (like <code>lw $3, 0x0($2)</code>, followed by an insn, which modifies <code>$2</code> and is not data-dependent on <code>$3</code>), and I made modifications to GCC, so such sequences do not occur (e.g. as a last resort, separating the two insns by a <code>nop</code>).</p>\n\n<p>Just something to consider ...</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36908331, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Disclaimer: I'm not a professional nor worked for this kind of application. But I worked on soft errors and redundancy for long term archival of critical data, which is somewhat linked (same problem, different goals). And I stumbled on possible solutions to OP's problem, which I kept in my books just in case (and also because it was a highly interesting problem).</p>\n\n<p>To significantly reduce the rate of soft errors (at the expense of computational overhead since it will mostly be software-based solutions), you can either:</p>\n\n<ul>\n<li><p>rely on the good old <strong><a href=\"https://en.wikipedia.org/wiki/Redundancy_(engineering)\" rel=\"nofollow\">redundancy scheme</a></strong>, and more specifically the more efficient <strong><a href=\"https://en.wikiversity.org/wiki/Reed%E2%80%93Solomon_codes_for_coders\" rel=\"nofollow\">error correcting codes</a></strong> (same purpose, but cleverer algorithms so that you can recover more bits with less redundancy). With this kind of solution, you will have to store the full state of your program at any moment in a master variable/class (or a struct?), compute an ECC, and check that the ECC is correct before doing anything, and if not, repair the fields. This solution however does not guarantee that your software can work (simply that it will work correctly when it can, or stops working if not, because ECC can tell you if something is wrong, and in this case you can stop your software so that you don't get fake results).</p></li>\n<li><p>or you can use <strong>resilient algorithmic data structures</strong>, which guarantee, up to a some bound, that your program will still give correct results even in the presence of soft errors. These algorithms can be seen as a mix of common algorithmic structures with ECC schemes natively mixed in, but this is much more resilient than that, because the resiliency scheme is tightly bounded to the structure, so that you don't need to encode additional procedures to check the ECC, and usually they are a lot faster. These structures provide a way to ensure that your program will work under any condition, up to the theoretical bound of soft errors. You can also mix these resilient structures with the redundancy/ECC scheme for additional security (or encode your most important data structures as resilient, and the rest, the expendable data that you can recompute from the main data structures, as normal data structures with a bit of ECC or a parity check which is very fast to compute).</p></li>\n</ul>\n\n<p>If you are interested in resilient data structures (which is a recent, but exciting, new field in algorithmics and redundancy engineering), I advise you to read the following documents:</p>\n\n<ul>\n<li><p><a href=\"http://fr.slideshare.net/lrq3000/resilient-algorithms-data-structures-intro-by-giuseppe-fitaliano\" rel=\"nofollow\">Resilient algorithms data structures intro by Giuseppe F.Italiano, Universita di Roma \"Tor Vergata\"</a></p></li>\n<li><p>Christiano, P., Demaine, E. D., &amp; Kishore, S. (2011). Lossless fault-tolerant data structures with additive overhead. In Algorithms and Data Structures (pp. 243-254). Springer Berlin Heidelberg.</p></li>\n<li><p>Ferraro-Petrillo, U., Grandoni, F., &amp; Italiano, G. F. (2013). Data structures resilient to memory faults: an experimental study of dictionaries. Journal of Experimental Algorithmics (JEA), 18, 1-6.</p></li>\n<li><p>Italiano, G. F. (2010). Resilient algorithms and data structures. In Algorithms and Complexity (pp. 13-24). Springer Berlin Heidelberg.</p></li>\n</ul>\n\n<p>If you are interested in knowing more about the field of resilient data structures, you can checkout the works of <a href=\"https://scholar.google.be/scholar?q=resilient%20italiano\" rel=\"nofollow\">Giuseppe F. Italiano</a> (and work your way through the refs) and the <strong>Faulty-RAM model</strong> (introduced in Finocchi et al. 2005; Finocchi and Italiano 2008).</p>\n\n<p>/EDIT: I illustrated the prevention/recovery from soft-errors mainly for RAM memory and data storage, but I didn't talk about <strong>computation (CPU) errors</strong>. Other answers already pointed at using atomic transactions like in databases, so I will propose another, simpler scheme: <strong>redundancy and majority vote</strong>.</p>\n\n<p>The idea is that you simply <strong>do x times the same computation</strong> for each computation you need to do, and store the result in x different variables (with x >= 3). You can then <strong>compare your x variables</strong>:</p>\n\n<ul>\n<li>if they all agree, then there's no computation error at all.</li>\n<li>if they disagree, then you can use a majority vote to get the correct value, and since this means the computation was partially corrupted, you can also trigger a system/program state scan to check that the rest is ok.</li>\n<li>if the majority vote cannot determine a winner (all x values are different), then it's a perfect signal for you to trigger the failsafe procedure (reboot, raise an alert to user, etc.).</li>\n</ul>\n\n<p>This redundancy scheme is <strong>very fast</strong> compared to ECC (practically O(1)) and it provides you with a <strong>clear signal</strong> when you need to <strong>failsafe</strong>. The majority vote is also <strong>guaranteed to recover</strong> correctly from <strong>minor computation errors</strong>, because the probability that x computations give the same output is infinitesimal (because there is a huge amount of possible outputs, it's almost impossible to randomly get 3 times the same, even less if x > 3).</p>\n\n<p>The drawback is that you need to compute x times instead of once, so you have an additional computation cost, but's linear complexity so asymptotically you don't lose much for the benefits you gain.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36970660, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}, {"body": "<p>Firstly, <strong>design your application around failure</strong>. Ensure that as part of normal flow operation, it expects to reset (depending on your application and the type of failure either soft or hard). This is hard to get perfect: critical operations that require some degree of transactionality may need to be checked and tweaked at an assembly level so that an interruption at a key point cannot result in inconsistent external commands.\n<strong>Fail fast</strong> as soon as any <em>unrecoverable</em> memory corruption or control flow deviation is detected. Log failures if possible.</p>\n\n<p>Secondly, where possible, <strong>correct corruption and continue</strong>. This means checksumming and fixing constant tables (and program code if you can) often; perhaps before each major operation or on a timed interrupt, and storing variables in structures that autocorrect (again before each major op or on a timed interrupt take a majority vote from 3 and correct if is a single deviation). Log corrections if possible.</p>\n\n<p>Thirdly, <strong>test failure</strong>. Set up a <em>repeatable</em> test environment that flips bits in memory psuedo-randomly. This will allow you to replicate corruption situations and help design your application around them.</p>\n", "title": "Compiling an application for use in highly radioactive environments", "answer_id": 36980614, "question_id": 36827659, "tags": ["c++", "c", "gcc", "embedded", "fault-tolerance"]}], "question_id": 36827659}, {"body": "<p>I have created certificate to enable Push Services in my app, but every time I try to add certificate in my Keychain, after adding certificate it shows me following error:</p>\n\n<p><strong>This certificate has an invalid issuer</strong></p>\n\n<p><a href=\"http://i.stack.imgur.com/1DKBI.png\"><img src=\"http://i.stack.imgur.com/1DKBI.png\" alt=\"enter image description here\"></a></p>\n\n<p>Any help will be appreciated. Thanks.</p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"], "answer_count": 11, "link": "http://stackoverflow.com/questions/35390072/this-certificate-has-an-invalid-issuer-apple-push-services", "answers": [{"body": "<p>In Apple's Developer's portal, add a new certificate, and when asked \"What type of certificate do you need?\" choose \"WorldWide developer relations certificate\". Generate the new certificate, download and install. The moment you do that, you will no longer see the message you have described.</p>\n\n<p>Edit:</p>\n\n<p>The certificate can be downloaded from the following page:\n<a href=\"https://www.apple.com/certificateauthority/\">https://www.apple.com/certificateauthority/</a>\nYou can choose one of the following two certificates:\n\"WWDR Certificate (Expiring 02/07/23)\"\nor\n\"WWDR Certificate (Expiring 02/14/16)\"</p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35390182, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>Just try to set local date earlier than Feb 14. Works for me! Not a complete solution but temporary solve the problem.</p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35399456, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>I think I've figured this one out. I imported the <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">new WWDR Certificate that expires in 2023</a>, but I was still getting problems building and my developer certificates were still showing the invalid issuer error.</p>\n\n<ul>\n<li>In keychain access, go to <em>View</em> -> <em>Show Expired Certificates</em>. Then in your login keychain highlight the expired WWDR Certificate and delete it.</li>\n<li>I also had the same expired certificate in my System keychain, so I deleted it from there too (important).</li>\n</ul>\n\n<p>After deleting the expired certificate from the login and System keychains, I was able to build for Distribution again.</p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35399656, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>Here is how we fixed this.</p>\n\n<p>Step 1: Open Keychain access, delete \"Apple world wide Developer relations certification authority\" (which expires on 14th Feb 2016) from both \"<strong>Login</strong>\" and \"<strong>System</strong>\" sections.\nIf you can't find it, use \u201cShow Expired Certificates\u201d in the View menu.</p>\n\n<p>Step 2: Download <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">this</a> and add it to <em>Keychain access</em> -> <em>Certificates</em>\n(which expires on 8th Feb 2023).</p>\n\n<p>Step 3: Everything should be back to normal and working now.</p>\n\n<p>Reference: <a href=\"https://developer.apple.com/support/certificates/expiration/\">Apple Worldwide Developer Relations Intermediate Certificate Expiration</a></p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35400391, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<ol>\n<li>Download <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a>\nand double-click to install to Keychain.</li>\n<li>Select \"View\" -> \"Show Expired Certificates\" in Keychain app.</li>\n<li><p>Confirm \"Certificates\" category is selected.</p>\n\n<p><a href=\"http://i.stack.imgur.com/V0OMU.png\"><img src=\"http://i.stack.imgur.com/V0OMU.png\" alt=\"enter image description here\"></a></p></li>\n<li><p>Remove expired Apple Worldwide Developer Relations Certificate Authority certificates from \"login\" tab and \"System\" tab.</p></li>\n</ol>\n\n<hr>\n\n<p>Here's Apple's answer.</p>\n\n<blockquote>\n  <p>Thanks for bringing this to the attention of the community and apologies for the issues you\u2019ve been having. This issue stems from having a copy of the expired WWDR Intermediate certificate in both your System and Login keychains. To resolve the issue, you should first download and install the new WWDR intermediate certificate (by double-clicking on the file). Next, in the Keychain Access application, select the System keychain. Make sure to select \u201cShow Expired Certificates\u201d in the View menu and then delete the expired version of the Apple Worldwide Developer Relations Certificate Authority Intermediate certificate (expired on February 14, 2016). Your certificates should now appear as valid in Keychain Access and be available to Xcode for submissions to the App Store.</p>\n</blockquote>\n\n<p><a href=\"https://forums.developer.apple.com/thread/37208\">https://forums.developer.apple.com/thread/37208</a></p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35400712, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>Follow the below steps:</p>\n\n<ol>\n<li>Download and install from <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\" rel=\"nofollow\">here</a>. Double click and install it.</li>\n<li>Select \"View\" -> \"Show Expired Certificates\" in Keychain app.</li>\n<li>Remove Apple Worldwide Developer Relations Certificate Authority\ncertificates from \"login\" tab and \"System\" tab in Keychain app.</li>\n</ol>\n\n<p>If you don't find your WWDR certificate in <strong>Login</strong> or <strong>System</strong> tab, then select category \"<strong>All items</strong>\" on the left side. Most probably you will get to see an expired WWDR certificate here, and you can remove it. An expired certificate is always shown with a red asterisk.</p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35403162, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>If you are facing the \"This certificate has an invalid issuer\" error for all your certificates then do the following steps.</p>\n\n<p><strong>Steps:</strong></p>\n\n<ul>\n<li>Open Keychain and Click on Login -> All Items from the left panel.</li>\n<li>Now, Click on <em>View</em> -> <em>Show Expired Certificates</em> from the top navigation menu.</li>\n<li>Now search for \"Apple Worldwide Developer Relations Certification Authority\" and delete expired certificates. <a href=\"http://i.stack.imgur.com/cZgqG.png\"><img src=\"http://i.stack.imgur.com/cZgqG.png\" alt=\"\"></a></li>\n<li>After deleting expired certificates, visit the following URL and download the new certificate,\n<a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a>.</li>\n<li>Double click on the newly downloaded certificate, and install it in your keychain.</li>\n<li>Double check: List expired certificates by following step number 3.</li>\n<li>Now you have a valid \"Apple Worldwide Developer Relations Certification Authority\" having expiry date 2023-02-07.</li>\n</ul>\n\n<p><strong>Reference:</strong> </p>\n\n<ul>\n<li><em><a href=\"https://developer.apple.com/support/certificates/expiration/\">Apple Worldwide Developer Relations Intermediate Certificate Expiration</a></em></li>\n</ul>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35408924, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>This is not actually a development issue. It happens due to expiration of the <strong>Apple Worldwide Developer Relations Intermediate Certificate</strong> issued by <strong>Apple Worldwide Developer Relations Certificate Authority</strong>. WWDRCA issues the certificate to sign your software for Apple devices, allowing our systems to confirm that your software is delivered to users as intended and has not been modified.</p>\n\n<p>To resolve this issue, you have to follow the below steps:</p>\n\n<ol>\n<li>Open Keychain Access</li>\n<li>Go to <strong>View</strong> -> <strong>Show Expired Certificates</strong></li>\n</ol>\n\n<p><a href=\"http://i.stack.imgur.com/VRP4A.png\"><img src=\"http://i.stack.imgur.com/VRP4A.png\" alt=\"Enter image description here\"></a></p>\n\n<ol start=\"3\">\n<li><p>Go to <strong>System</strong> in Keychain\n<a href=\"http://i.stack.imgur.com/NDhCh.png\"><img src=\"http://i.stack.imgur.com/NDhCh.png\" alt=\"Enter image description here\"></a></p></li>\n<li><p>Here you find that \"Apple Worldwide Developer Relations Certificate Authority\" is marked as expired. So delete it. Also check under Login Tab and delete expired WWDRCA.</p></li>\n<li><p>Download new <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">WWDR Intermediate Certificate</a> from here.</p></li>\n<li><p>Install it by double clicking on it.</p></li>\n<li><p>It's done!!!</p></li>\n</ol>\n\n<p>If you still face any issue with your iOS apps, Mac apps, Safari extensions, Apple Wallet and Safari push notifications, then please follow <a href=\"https://developer.apple.com/support/certificates/expiration/\">this link of expiration</a>.</p>\n\n<blockquote>\n  <p>The Apple Worldwide Developer Relations Certification Intermediate\n  Certificate expires soon and we've issued a renewed certificate that\n  must be included when signing all new Apple Wallet Passes, push\n  packages for Safari Push Notifications, and Safari Extensions starting\n  February 14, 2016.</p>\n  \n  <p>While most developers and users will not be affected by the\n  certificate change, we recommend that all developers download and\n  install the renewed certificate on their development systems and\n  servers as a best practice. All apps will remain available on the App\n  Store for iOS, Mac, and Apple TV.</p>\n</blockquote>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35409835, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>As described in the <a href=\"https://developer.apple.com/support/certificates/expiration/\">Apple Worldwide Developer Relations Intermediate Certificate Expiration</a>:</p>\n\n<hr>\n\n<p>The previous Apple Worldwide Developer Relations Certification Intermediate Certificate expired on <strong>February 14, 2016</strong> and the renewed certificate must now be used when signing Apple Wallet Passes, push packages for Safari Push Notifications, Safari Extensions, and submissions to the App Store, Mac App Store, and App Store for Apple TV.</p>\n\n<p>All developers should download and install the <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">renewed certificate</a> on their development systems and servers. All apps will remain available on the App Store for iOS, Mac, and Apple TV.</p>\n\n<hr>\n\n<p><strong>The new valid certificate will look like the following:</strong></p>\n\n<p><a href=\"http://i.stack.imgur.com/NRXDt.png\"><img src=\"http://i.stack.imgur.com/NRXDt.png\" alt=\"Apple Worldwide Developer Relations Certification Authority\"></a></p>\n\n<p>It will display (this certificate is valid) with a green mark.</p>\n\n<p>So, <strong>go to your Key Chain Access</strong>. Just <strong>delete the old certificate</strong> and replace it with the new one (<a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">renewed certificate</a>) as Apple described in the document. Mainly the problem is only with the Apple push notification service and extensions as described in the Apple document.</p>\n\n<p>You can also check the listing of certificates in <a href=\"https://www.apple.com/certificateauthority/\">https://www.apple.com/certificateauthority/</a></p>\n\n<p>Certificate Revocation List:</p>\n\n<p><a href=\"http://i.stack.imgur.com/kydye.png\"><img src=\"http://i.stack.imgur.com/kydye.png\" alt=\"Certificate Revocation List\"></a></p>\n\n<p>Now this updated certificate will expire on 2023-02-08.</p>\n\n<hr>\n\n<p>If you could not see the old certificate then go to the <strong>System Keychains</strong> and from edit menu and select the option <em>Show Expired Certificates</em>.</p>\n\n<p><a href=\"http://i.stack.imgur.com/D1msE.png\"><img src=\"http://i.stack.imgur.com/D1msE.png\" alt=\"Show Expired Certificates\"></a></p>\n\n<p>Now you can see the following certificate that you have to delete:</p>\n\n<p><a href=\"http://i.stack.imgur.com/Z8JuS.png\"><img src=\"http://i.stack.imgur.com/Z8JuS.png\" alt=\"Delete This Certificate\"></a></p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35423724, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<p>You need to search the \"World\" from the top right search bar and delete the expired certificate. Make sure you selected \"Login\" and \"All items\".</p>\n\n<p><a href=\"http://i.stack.imgur.com/2lCHb.png\"><img src=\"http://i.stack.imgur.com/2lCHb.png\" alt=\"Enter image description here\"></a></p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 35424878, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}, {"body": "<ul>\n<li>All my certificates are installed and expire dates are fine.  </li>\n<li>I deleted and reinstalled all my certificates, still no luck</li>\n</ul>\n\n<p>In the end, I right-clicked on the certificate, and selected \"Get Info\". Under the Trust section, I selected \"Always Trust\" and this solved my problem.</p>\n", "title": "This certificate has an invalid issuer Apple Push Services", "answer_id": 38350908, "question_id": 35390072, "tags": ["ios", "xcode", "apple-push-notifications", "ssl-certificate", "keychain"]}], "question_id": 35390072}, {"body": "<p>C++17 is now feature complete, so unlikely to experience large changes. Hundreds of proposals were put forward for C++17.</p>\n\n<p>Which of those features were added to C++ in C++17?</p>\n\n<p>When using a C++ compiler that supports \"C++1z\", which of those features are going to be available when the compiler updates to C++17?</p>\n", "title": "What are the new features in C++17?", "tags": ["c++", "standards", "c++1z", "c++-faq"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38060436/what-are-the-new-features-in-c17", "answers": [{"body": "<h1>Language features:</h1>\n\n<h2>Templates and Generic Code</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0091r2.html\">Template argument deduction for class templates</a></p>\n\n<ul>\n<li>Like how functions deduce template arguments, now constructors can deduce the template arguments of the class</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0127r1.html\"><code>template &lt;auto&gt;</code></a></p>\n\n<ul>\n<li>Represents a value of any (non-type template argument) type</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4268.html\">Non-type template arguments fixes</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4051.html\"><code>template&lt;template&lt;class...&gt;typename bob&gt; struct foo {}</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4295.html\">( Folding + ... + expressions ) </a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3922.html\"><code>auto x{8};</code> is an <code>int</code></a></p></li>\n<li><p><a href=\"http://open-std.org/JTC1/SC22/WG21/docs/papers/2016/p0128r1.html\">constexpr if</a></p>\n\n<ul>\n<li>Much requested feature to simplify almost-generic code</li>\n</ul></li>\n</ul>\n\n<h2>Lambda</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4487.pdf\">constexpr lambdas</a></p>\n\n<ul>\n<li>Lambdas are implicitly constexpr if they qualify</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0018r3.html\">Capturing <code>*this</code> in lambdas</a></p>\n\n<ul>\n<li><code>[*this]{ std::cout &lt;&lt; could &lt;&lt; \" be \" &lt;&lt; useful &lt;&lt; '\\n'; }</code></li>\n</ul></li>\n</ul>\n\n<h2>Attributes</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0068r0.pdf\"><code>[[fallthrough]]</code>, <code>[[nodiscard]]</code>, <code>[[maybe_unused]]</code></a> attributes </p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4266.html\"><code>[[attributes]]</code> on <code>namespace</code>s and <code>enum { erator[[s]] }</code></a></p></li>\n</ul>\n\n<h2>Syntax cleanup</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4424.pdf\">Inline variables</a></p>\n\n<ul>\n<li>Like inline functions</li>\n<li>Compiler picks where the instance is instantiated?</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4230.html\"><code>namespace A::B</code></a></p></li>\n<li><p>Simple <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3928.pdf\"><code>static_assert(expression);</code></a> with no string</p></li>\n</ul>\n\n<h2>Cleaner multi-return and flow control</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0217r2.html\">Structured bindings</a></p>\n\n<ul>\n<li>Basically, first-class <code>std::tie</code> with <code>auto</code></li>\n<li>Example:\n\n<ul>\n<li><code>const auto [it, inserted] = map.insert( {\"foo\", bar} );</code></li>\n<li>Creates variables <code>it</code> and <code>inserted</code> with deduced type from the <code>pair</code> that <code>map::insert</code> returns.</li>\n</ul></li>\n<li>Works with tuple/pair-likes &amp; <code>std::array</code>s and relatively flat structs</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0305r0.html\"><code>if (init; condition)</code> and <code>switch (init; condition)</code></a></p>\n\n<ul>\n<li><code>if (const auto [it, inserted] = map.insert( {\"foo\", bar} ); inserted)</code></li>\n<li>Extends the <code>if(decl)</code> to cases where <code>decl</code> isn't convertible-to-bool sensibly</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0184r0.html\">Generalizing range-based for loops</a></p>\n\n<ul>\n<li>Appears to be mostly support for sentinals, or end iterators that are not the same type as begin iterators, which helps with null-terminated loops and the like.</li>\n</ul></li>\n</ul>\n\n<h2>Misc</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0245r1.html\">Hexadecimal float point literals</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0035r1.html\">Dynamic memory allocation for over-aligned data</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0135r0.html\">Guaranteed copy elision</a> (r1 accepted, not r0: have not found r1 on 'net yet)</p>\n\n<ul>\n<li>Finally!</li>\n<li>Not in all cases, but distingushes syntax where you are \"just creating something\" that was called elision, from \"genuine elision\".</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0145r2.pdf\">Fixed order-of-evaluation for (some) expressions</a></p>\n\n<ul>\n<li>Not including function arguments, but function argument evaluation interleaving now banned</li>\n<li>Makes a bunch of broken code work mostly, and makes <code>.then</code> on future work.</li>\n</ul></li>\n<li><p>Forward progress guarantees (FPG) (also, <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0299r0.html\">FPGs for parallel algorithms</a>)</p>\n\n<ul>\n<li>I think this is saying \"the implementation may not stall threads forever\"?</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4267.html\"><code>u8'U', u8'T', u8'F', u8'8'</code></a> character literals.</p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0012r1.html\">\"noexcept\" in the type system</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0061r1.html\"><code>__has_include</code></a></p>\n\n<ul>\n<li>Test if a header file include would be an error</li>\n<li>makes migrating from experimental to std almost seamless</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4261.html\">Arrays of pointer conversion fixes</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0136r1.html\">inherited constructors</a> fixes to some corner cases.</p></li>\n</ul>\n\n<h1>Library additions:</h1>\n\n<h2>Data types</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0088r2.html\"><code>std::variant&lt;Ts...&gt;</code></a></p>\n\n<ul>\n<li>Almost-always non-empty last I checked?</li>\n<li>Tagged union type</li>\n<li>{awesome|useful}</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#optional\"><code>std::optional</code></a></p>\n\n<ul>\n<li>Maybe holds one of something</li>\n<li>Ridiculously useful</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#any\"><code>std::any</code></a></p>\n\n<ul>\n<li>Holds one of anything (that is copyable)</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#string.view\"><code>std::string_view</code></a></p>\n\n<ul>\n<li><code>std::string</code> like reference-to-character-array or substring</li>\n<li>Never take a <code>string const&amp;</code> again.  Also can make parsing a bajillion times faster.</li>\n</ul></li>\n</ul>\n\n<h2>Invoke stuff</h2>\n\n<ul>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4169.html\"><code>std::invoke</code></a>\n\n<ul>\n<li>Call any callable (function pointer, function, member pointer) with one syntax.  From the standard INVOKE concept.</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#tuple.apply\"><code>std::apply</code></a>\n\n<ul>\n<li>Takes a function-like and a tuple, and unpacks the tuple into the call.</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0077r2.html\"><code>std::is_callable</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0209r1.pdf\"><code>std::make_from_tuple</code></a>, <code>std::apply</code> applied to object construction</li>\n</ul>\n\n<h2><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html\">File System TS v1</a></h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#class-path\"><code>[class.path]</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#Class-filesystem_error\"><code>[class.filesystem.error]</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#file_status\"><code>[class.file_status]</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#Class-directory_entry\"><code>[class.directory_entry]</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#Class-directory_iterator\"><code>[class.directory_iterator]</code></a> and <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#class.rec.dir.itr\"><code>[class.recursive_directory_iterator]</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0218r0.html#Operational-functions\"><code>[fs.ops.funcs]</code></a></p></li>\n<li><p>As of this writing, minor oops: no simple and portable way to open a file found this way.  Hopefully fixed shortly.</p></li>\n</ul>\n\n<h2>Threading</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4508.html\"><code>std::shared_mutex</code></a></p>\n\n<ul>\n<li>Untimed, which can be more efficient if you don't need it.</li>\n</ul></li>\n<li><p><code>atomic&lt;T&gt;</code><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0152r1.html\"><code>::is_always_lockfree</code></a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0156r0.html\"><code>lock_guard&lt;Mutexes...&gt;</code></a></p>\n\n<ul>\n<li>Saves some <code>std::lock</code> pain when locking more than one mutex at a time.</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/paper/2014/n4071.htm\">Parallelism TS v1</a></p>\n\n<ul>\n<li>The linked paper from 2014, may be out of date</li>\n<li>Parallel versions of <code>std</code> algorithms, and related machinery</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0154r0.html\">hardware_*_interference_size</a></p></li>\n</ul>\n\n<h2>(parts of) <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0220r1.html\">Library Fundamentals TS v1</a> not covered above or below</h2>\n\n<ul>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#func.searchers\"><code>[func.searchers]</code></a> and <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#alg.search\"><code>[alg.search]</code></a>\n\n<ul>\n<li>A searching algorithm and techniques</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#memory.type.erased.allocator\"><code>[pmr]</code></a></p>\n\n<ul>\n<li>Polymorphic allocator, like <code>std::function</code> for allocators</li>\n<li>And some <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#memory.resource.pool\">standard memory resources to go with it</a>.</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4480.html#alg.random.sample\"><code>std::sample</code></a>, sampling from a range?</p></li>\n</ul>\n\n<h2>Container Improvements</h2>\n\n<ul>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4279.html\"><code>try_emplace</code></a> and <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4279.html\"><code>insert_or_assign</code></a></p>\n\n<ul>\n<li>gives better guarantees in some cases where spurious move/copy would be bad</li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3586.pdf\">Splicing for <code>map&lt;&gt;</code>, <code>unordered_map&lt;&gt;</code>, <code>set&lt;&gt;</code>, and <code>unordered_set&lt;&gt;</code></a></p>\n\n<ul>\n<li>Paper from 2013, so maybe newer proposal accepted?</li>\n<li>move nodes berween containers cheaply.</li>\n</ul></li>\n<li><p>non-const <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0272r1.html\"><code>.data()</code></a> for string.</p></li>\n<li><p>non-member <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4280.pdf\"><code>std::size</code>, <code>std::empty</code>, <code>std::data</code></a></p>\n\n<ul>\n<li>like <code>std::begin</code>/<code>end</code></li>\n</ul></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4510.html\">Minimal incomplete type support in containers</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4284.html\">Contiguous iterator \"concept\"</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0031r0.html\"><code>constexpr</code> iterators</a></p></li>\n</ul>\n\n<h2>Smart pointer changes</h2>\n\n<ul>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4089.pdf\"><code>unique_ptr&lt;T[]&gt;</code> fixes</a> and other <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4366.html\"><code>unique_ptr</code></a> tweaks.</li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0033r1.html\"><code>weak_from_this</code></a> and some fixed to shared from this</li>\n</ul>\n\n<h2>Other <code>std</code> datatype improvements:</h2>\n\n<ul>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4387\"><code>{}</code> construction of <code>std::tuple</code> and other improvements</a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4277.html\">TriviallyCopyable reference_wrapper</a>, can be performance boost</li>\n</ul>\n\n<h2>Misc</h2>\n\n<ul>\n<li><p>C++17 library is based on <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0063r2.html\">C11 instead of C99</a></p></li>\n<li><p>Reserved <code>std[0-9]+</code> for <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0180r1.html\">future standard libraries</a></p></li>\n<li><p><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0040r2.html\"><code>destroy(_at|_n)</code>, <code>uninitialized_move(_n)</code>, <code>uninitialized_value_construct(_n)</code>, <code>uninitialized_default_construct(_n)</code></a></p>\n\n<ul>\n<li>utility code already in most <code>std</code> implementations exposed</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2003/n1422.html\">Special math functions</a>\n\n<ul>\n<li>scientists may like them</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0025r0.html\"><code>std::clamp()</code></a>\n\n<ul>\n<li><code>std::clamp( a, b, c ) == std::max( b, std::min( a, c ) )</code> roughly</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4259.pdf\"><code>std::uncaught_exceptions</code></a>\n\n<ul>\n<li>Required if you want to only throw if safe from destructors</li>\n</ul></li>\n<li><a href=\"http://open-std.org/JTC1/SC22/WG21/docs/papers/2015/p0007r1.html\"><code>std::as_const</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4389.html\"><code>std::bool_constant</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0006r0.html\">A whole bunch of <code>_v</code> template variables</a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n3911.pdf\"><code>std::void_t&lt;T&gt;</code></a>\n\n<ul>\n<li>Surprisingly useful when writing templates</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0074r0.html\"><code>std::owner_less&lt;void&gt;</code></a>\n\n<ul>\n<li>like <code>std::less&lt;void&gt;</code>, but for smart pointers to sort based on contents</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0092r1.html\"><code>std::chrono</code> polish</a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0013r1.html\"><code>std::conjunction</code>, <code>std::disjunction</code>, <code>std::negation</code></a> exposed</li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0005r4.html#6.10\"><code>std::not_fn</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4258.pdf\">Rules for noexcept within <code>std</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0258r1.html\">std::is_contiguous_layout</a>, useful for efficient hashing</li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0067r2.html\">std::to_chars/std::from_chars</a>, high performance, locale agnostic number conversion; finally a way to serialize/deserialize to human readable formats (JSON &amp; co) </li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0181r0.html\">std::default_order</a>, indirection over <code>std::less</code>.</li>\n</ul>\n\n<p><a href=\"https://isocpp.org/blog/category/standardization\">Isocpp.org has</a> a more comprehensive list of papers, some of which I pillaged for above.  At this point it isn't up to date on what is accepted into C++17.</p>\n\n<p>Naturally TS work continues in parallel, so there are some TS that are not-quite-ripe that will have to wait for the next iteration.  The target for the next iteration is C++20 as previously planned, not C++19 as some rumors implied.  C++1\u03a9 has been avoided.</p>\n\n<p>Initial list taken from <a href=\"https://www.reddit.com/r/cpp/comments/48zp05/what_we_added_to_the_c17_working_draft/\">this reddit post</a> and <a href=\"https://www.reddit.com/r/cpp/comments/4pmlpz/what_the_iso_c_committee_added_to_the_c17_working/\">this reddit post</a>, with links added via googling or from the above isocpp.org page.</p>\n\n<p>Additional entries pillaged from <a href=\"https://isocpp.org/std/standing-documents/sd-6-sg10-feature-test-recommendations\">SD-6</a> feature-test list.</p>\n\n<p><a href=\"http://clang.llvm.org/cxx_status.html#cxx17\">clang's feature list</a> and <a href=\"http://libcxx.llvm.org/cxx1z_status.html\">library feature list</a> are next to be pillaged.  This doesn't seem to be reliable, as it is C++1z, not C++17.</p>\n\n<p><a href=\"https://codeplay.com/public/uploaded/filehost/0cbdaf_c++17post-oulu2016.pdf\">these slides</a> had some features missing elsewhere.</p>\n\n<p>While \"what was removed\" was not asked, here is a short list of a few things ((mostly?) previous deprecated) that are removed in C++17 from C++:</p>\n\n<h1>Removed:</h1>\n\n<ul>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0001r1.html#6.10\"><code>register</code></a>, keyword reserved for future use</li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0002r1.html\"><code>bool b; ++b;</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4086.html\">trigraphs</a>\n\n<ul>\n<li>if you still need them, they are now part of your source file encoding, not part of language</li>\n</ul></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/p0004r1.html\">ios aliases</a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4190.htm\">auto_ptr, old <code>&lt;functional&gt;</code> stuff, <code>random_shuffle</code></a></li>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0302r1.html\">allocators in <code>std::function</code></a></li>\n</ul>\n\n<p>There were rewordings.  I am unsure if these have any impact on code, or if they are just cleanups in the standard:</p>\n\n<h1>Spec changes:</h1>\n\n<ul>\n<li><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4285.html\">exception specs and throw expressions</a></li>\n</ul>\n\n<h1>Further reference:</h1>\n\n<ul>\n<li><a href=\"http://www.open-std.org/JTC1/SC22/WG21/docs/papers/\">papers grouped by year; not all accepted</a></li>\n</ul>\n", "title": "What are the new features in C++17?", "answer_id": 38060437, "question_id": 38060436, "tags": ["c++", "standards", "c++1z", "c++-faq"]}], "question_id": 38060436}, {"body": "<p>I wanted to start a simple hello world app for AngularJS 2.</p>\n\n<p>When I followed the instructions in the official <a href=\"https://angular.io/docs/js/latest/quickstart.html\">quickstart</a> the installation created 32,000 files in my project.</p>\n\n<p>I figured this is some mistake or I missed something, so I decided to use <a href=\"https://cli.angular.io/\">angular-cli</a>, but after setting up the project I counted 41,000 files.</p>\n\n<p>Where did I go wrong? Am I missing something really really obvious?</p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "tags": ["javascript", "angular2"], "answer_count": 7, "link": "http://stackoverflow.com/questions/38718690/huge-number-of-files-generated-for-every-angularjs-2-project", "answers": [{"body": "<p>There is nothing wrong with your configuration.</p>\n\n<p>Angular 2 uses npm modules and dependencies for development. That's the sole reason you are seeing such a huge number of files.</p>\n\n<p>A basic setup of Angular 2 contains transpiler, typings dependencies which are <em>essential</em> for development purposes only.</p>\n\n<p>Once you are done with development, all you will need to do is to bundle this application.</p>\n\n<p>After bundling your application, there will be only one <code>bundle.js</code> file which you can then deploy on your server.</p>\n\n<p><strong><em>'transpiler'</strong> is just a compiler, thanks @omninonsense for adding that.</em></p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 38719154, "question_id": 38718690, "tags": ["javascript", "angular2"]}, {"body": "<p>There is nothing wrong with your <strong>development</strong> configuration.</p>\n\n<p>Something wrong with your <strong>production</strong> configuration.</p>\n\n<p>When you develop a \"Angular 2 Project\" or \"Any Project Which is based on JS\" you can use all files, you can try all files, you can import all files. But if you want to serve this project you need to <strong>COMBINE</strong> all structured files and get rid of useless files.</p>\n\n<p>There are a lot of options for combine these files together:</p>\n\n<ul>\n<li><a href=\"https://github.com/yui/yuicompressor/\">YUI Compressor</a></li>\n<li><a href=\"https://developers.google.com/closure/compiler/docs/gettingstarted_ui?csw=1\">Google Closure Compiler</a></li>\n<li>For server side (I think it is best) <a href=\"https://github.com/gulpjs/gulp/blob/master/docs/getting-started.md\">GULP</a></li>\n</ul>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 38722185, "question_id": 38718690, "tags": ["javascript", "angular2"]}, {"body": "<p>Angular 2 itself has lots of dependencies, and the beta version of CLI downloads four times more files. </p>\n\n<p>This is how to create a simple project will less files (\"only\" 10K files)\n<a href=\"https://yakovfain.com/2016/05/06/starting-an-angular-2-rc-1-project/\">https://yakovfain.com/2016/05/06/starting-an-angular-2-rc-1-project/</a></p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 38724475, "question_id": 38718690, "tags": ["javascript", "angular2"]}, {"body": "<p>As several people already mentioned: All files in your node_modules directory (NPM location for packages) are part of your project dependencies (So called direct dependencies). As an addition to that, your dependencies can also have their own dependencies and so on etc. (So called transitive dependencies). Several then thousand files are nothing special.</p>\n\n<p>Because you are only allowed to upload 10'000 files (See comments), I would go with a bundler engine. This engine will bundle all your JavaScript, CSS, HTML etc. and create a single bundle (or more if you specify them). Your index.html will load this bundle and that's it.</p>\n\n<p>I am a fan of webpack, so my webpack solution will create an application bundle and a vendor bundle (For the full working application see here <a href=\"https://github.com/swaechter/project-collection/tree/master/web-angular2-example\">https://github.com/swaechter/project-collection/tree/master/web-angular2-example</a>):</p>\n\n<p><strong>index.html</strong></p>\n\n<pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;base href=\"/\"&gt;\n    &lt;title&gt;Webcms&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;webcms-application&gt;Applikation wird geladen, bitte warten...&lt;/webcms-application&gt;\n&lt;script type=\"text/javascript\" src=\"vendor.bundle.js\"&gt;&lt;/script&gt;\n&lt;script type=\"text/javascript\" src=\"main.bundle.js\"&gt;&lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n\n<p><strong>webpack.config.js</strong></p>\n\n<pre><code>var webpack = require(\"webpack\");\nvar path = require('path');\n\nvar ProvidePlugin = require('webpack/lib/ProvidePlugin');\nvar CommonsChunkPlugin = require('webpack/lib/optimize/CommonsChunkPlugin');\nvar UglifyJsPlugin = require('webpack/lib/optimize/UglifyJsPlugin');\n\n/*\n * Configuration\n */\nmodule.exports = {\n    devtool: 'source-map',\n    debug: true,\n\n    entry: {\n        'main': './app/main.ts'\n    },\n\n    // Bundle configuration\n    output: {\n        path: root('dist'),\n        filename: '[name].bundle.js',\n        sourceMapFilename: '[name].map',\n        chunkFilename: '[id].chunk.js'\n    },\n\n    // Include configuration\n    resolve: {\n        extensions: ['', '.ts', '.js', '.css', '.html']\n    },\n\n    // Module configuration\n    module: {\n        preLoaders: [\n            // Lint all TypeScript files\n            {test: /\\.ts$/, loader: 'tslint-loader'}\n        ],\n        loaders: [\n            // Include all TypeScript files\n            {test: /\\.ts$/, loader: 'ts-loader'},\n\n            // Include all HTML files\n            {test: /\\.html$/, loader: 'raw-loader'},\n\n            // Include all CSS files\n            {test: /\\.css$/, loader: 'raw-loader'},\n        ]\n    },\n\n    // Plugin configuration\n    plugins: [\n        // Bundle all third party libraries\n        new CommonsChunkPlugin({name: 'vendor', filename: 'vendor.bundle.js', minChunks: Infinity}),\n\n        // Uglify all bundles\n        new UglifyJsPlugin({compress: {warnings: false}}),\n    ],\n\n    // Linter configuration\n    tslint: {\n        emitErrors: false,\n        failOnHint: false\n    }\n};\n\n// Helper functions\nfunction root(args) {\n    args = Array.prototype.slice.call(arguments, 0);\n    return path.join.apply(path, [__dirname].concat(args));\n}\n</code></pre>\n\n<p>Advantages:</p>\n\n<ul>\n<li>Full build line (TS linting, compiling, minification etc.)</li>\n<li>3 files for deployment --> Only a few Http request</li>\n</ul>\n\n<p>Disadvantages:</p>\n\n<ul>\n<li>Higher build time</li>\n<li>Not the best solution fot Http 2 projects (See disclaimer)</li>\n</ul>\n\n<p><strong>Disclaimer:</strong> This is a good solution for Http 1.*, because it minimizes the overhead for each Http request. You only have a request for your index.html and each bundle - but not for 100 - 200 files. At the moment, this is the way to go.</p>\n\n<p>Http 2 on the other hand tries to minimize the Http overhead, so it's based on a stream protocol. This stream is able to communicate in both direction (Client &lt;--> Server) and as a reason of that a more intelligent resource loading is possible (You only load the required files). The stream eliminates much of the Http overhead (Less Http round trips).</p>\n\n<p>But it's the same as with IPv6: It will take a few years until people will really use Http 2</p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 38728503, "question_id": 38718690, "tags": ["javascript", "angular2"]}, {"body": "<pre><code>                                Typical Angular2 Project\n</code></pre>\n\n<p><strong>NPM Package</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>Files (Development)</strong> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Real World Files (Deployment)</strong> </p>\n\n<pre><code>@angular                       3,236                             1\nrxJS                           1,349                             1*\ncore-js                        1,341                             2\ntypings                        1,488                             0\ngulp                           1,218                             0\ngulp-typescript                1,243                             0\nlite-server                    5,654                             0\nsystemjs-builder               6,470                             0\n__________________________________________________________________\nTotal                         21,999                             3  \n</code></pre>\n\n<p><code>*</code>: <em><code>bundled with @angular</code></em> </p>\n\n<p>[ <a href=\"http://stackoverflow.com/questions/37631098/angular-2-how-to-bundle-for-production-currently-rc4/37874643#37874643\">see this for bundling process &neArr;</a> ]</p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 38735149, "question_id": 38718690, "tags": ["javascript", "angular2"]}, {"body": "<p>You need to ensure that you're just deploying the dist (short for distributable) folder from your project generated by the <a href=\"https://cli.angular.io\">Angular CLI</a>. This allows the tool to take your source code and it's dependencies and only give you what you need in order to run your application.</p>\n\n<p>That being said there is/was an issue with the Angular CLI in regards to production builds via `ng build --prod</p>\n\n<p>Yesterday (August 2, 2016) a release was done which switched the build mechanism from <a href=\"https://github.com/broccolijs\">broccoli</a> + <a href=\"https://github.com/systemjs/systemjs\">systemjs</a> to <a href=\"https://github.com/webpack/webpack\">webpack</a> which successfully handles production builds.</p>\n\n<p>Based upon these steps:</p>\n\n<pre><code>ng new test-project\nng build --prod\n</code></pre>\n\n<p>I am seeing a <code>dist</code> folder size of <strong>1.1 MB</strong> across the <strong>14 files</strong> listed here:</p>\n\n<pre><code>./app/index.js\n./app/size-check.component.css\n./app/size-check.component.html\n./favicon.ico\n./index.html\n./main.js\n./system-config.js\n./tsconfig.json\n./vendor/es6-shim/es6-shim.js\n./vendor/reflect-metadata/Reflect.js\n./vendor/systemjs/dist/system.src.js\n./vendor/zone.js/dist/zone.js\n</code></pre>\n\n<p><strong>Note</strong> Currently to install the webpack version of the angular cli, you must run... <code>npm install angular-cli@webpack -g</code></p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 38745227, "question_id": 38718690, "tags": ["javascript", "angular2"]}, {"body": "<p>This is actually not Angular 2 specific, it happens with almost any project that uses the NodeJs / npm ecosystem for its tooling. </p>\n\n<p>Those project are inside your node_modules folders, and are the transititve dependencies that your direct dependencies need to run.</p>\n\n<p>In the node ecosystem modules are usually small, meaning that instead of developing things ourselves we tend to import most of what we need under the form of a module. This can include such small things like the famous left-pad function, why write it ourselves if not as an exercise ?</p>\n\n<p>So having a lot of files its actually a good thing, it means everything is very modular and module authors frequently reused other modules. This ease of modularity is probably one of the main reasons why the node ecosystem grew so fast.</p>\n\n<p>In principle this should not cause any issue, but it seems you run into a google app engine file count limit. In that I case I suggest to not upload node_modules to app engine.</p>\n\n<p>instead build the application locally and upload to google app engine only the bundled filesn but don't to the build in app engine itself.</p>\n", "title": "Huge number of files generated for every AngularJS 2 project", "answer_id": 39064238, "question_id": 38718690, "tags": ["javascript", "angular2"]}], "question_id": 38718690}, {"body": "<p>I'm trying to summarize my knowledge about the most popular JavaScript package managers, bundlers, and task runners. Correct me please if I'm wrong:</p>\n\n<ul>\n<li><code>npm</code> &amp; <code>bower</code> are package managers. They just download the dependencies and don't know how to build projects on their own. What they know is to call <code>webpack</code>/<code>gulp</code>/<code>grunt</code> after fetching all the dependencies.</li>\n<li><code>bower</code> is like <code>npm</code>, but builds flattened dependencies trees (unlike <code>npm</code> which do it recursively). Meaning <code>npm</code> fetches the dependencies for each dependency (may fetch the same a few times), while <code>bower</code> expects you to manually include sub-dependencies. Sometimes <code>bower</code> and <code>npm</code> are used together for frontend and backend respectively (since each megabyte might matter on frontend).</li>\n<li><code>grunt</code> and <code>gulp</code> are task runners to automate everything that can be automated (i.e. compile css/sass, optimize images, make a bundle and minify/transpile it) </li>\n<li><code>grunt</code> vs <code>gulp</code> (is like <code>maven</code> vs <code>gradle</code> or configuration vs code). Grunt is based on configuring separate independent tasks, each task opens/handles/closes file. Gulp requires less amount of code and is based on node streams, which allows it to build pipe chains (w/o reopening the same file) and makes it faster. </li>\n<li><code>webpack</code> (<code>webpack-dev-server</code>) - for me it's a task runner with hot reloading of changes which allows you to forget about all js/css watchers. </li>\n<li><code>npm</code>/<code>bower</code> + plugins may replace task runners. Their abilities often intersect so there are different implications if you need to use <code>gulp</code>/<code>grunt</code> over npm + plugins. But task runners are definitely better for complex tasks (i.e. \"on each build create bundle, transpile from ES6 to ES5, run it at all browsers emulators, make screenshots and deploy to dropbox through ftp\")\n\n<ul>\n<li><code>browserify</code> allows packaging node modules for browsers. <code>browserify</code> vs <code>node's require</code> is actually <a href=\"https://addyosmani.com/writing-modular-js/\">AMD vs CommonJS</a>.</li>\n</ul></li>\n</ul>\n\n<p><strong><em>Questions:</em></strong></p>\n\n<ol>\n<li><em>What is webpack &amp; webpack-dev-server?</em> Official doc says it's a module bundler but for me it's just a task runner. <em>What's the difference?</em></li>\n<li><em>Where would you use<code>browserify</code>? Can't we do the same with node/ES6 imports?</em> </li>\n<li><em>When would you use <code>gulp</code>/<code>grunt</code> over <code>npm + plugins</code>?</em></li>\n<li><em>Please provide examples when you need to use a combination</em></li>\n</ol>\n", "title": "npm vs bower vs browserify vs gulp vs grunt vs webpack", "tags": ["gruntjs", "npm", "gulp", "bower", "webpack"], "answer_count": 3, "link": "http://stackoverflow.com/questions/35062852/npm-vs-bower-vs-browserify-vs-gulp-vs-grunt-vs-webpack", "answers": [{"body": "<p>Webpack and Browserify do pretty much the same job, which is bundling your modules to be used in a browser environment. The module is a Node feature, which doesn't exist in the browser, and ES 6 modules are not implemented in any browser yet, which is why things need to be bundled. However, they differ in many ways, Webpack offers many tools by default (e.g. code splitting), while Browserify can do this only after downloading plugins, but using both leads to very similar results. It comes down to personal preference (I am used to Webpack). Webpack is not a task runner, it is just processor of your files run directly from CLI or by a task runner.</p>\n\n<p><code>webpack-dev-server</code> provides something like Browsersync - a server, where you can deploy your app and verify your FE developing progress immediately by dev-server automatically refreshing the browser or even propagating changes without it with hot deploy (e.g. React components).</p>\n\n<p>I've been using Gulp for its conciseness and easy task writing, but have later found out I need neither Gulp nor Grunt at all. Everything I have ever needed could have been done using npm scripts to run 3rd-party tools through their API. Choosing between Gulp, Grunt or npm scripts depends on your taste, JS experience and experience of developers working with you.</p>\n\n<p>While tasks in Gulp (or Grunt maybe) are easy to read even for people not so familiar with JS, it is yet another tool to require and learn, and I personally prefer to narrow my dependencies and make things simple. On the other hand, replacing these tasks with combination of npm scripts and run files (where configuration and execution function of tools like Webpack lies) is more challenging. But in the majority of cases, those three are equal in terms of results.</p>\n\n<p>As for the examples, I suggest you have a look at this <a href=\"https://github.com/kriasoft/react-starter-kit\">React starter project</a>, which shows you nice combination of npm scripts, Webpack and Browsersync. You can find those npm scripts in package.json in the root folder, in a property named <code>scripts</code>. There you will mostly encounter commands like <code>babel-node tools/run start</code>. Babel-node is a CLI tool (not ment for production use), which at first compiles ES6 file <code>tools/run</code> (run.js file located in <a href=\"https://github.com/kriasoft/react-starter-kit/tree/master/tools\">tools</a>) - basically a runner utility. This runner takes a function as an argument and runs it, which in this case is <code>start</code> - another utility (start.js) responsible for bundling source files (both client and server), starting node-express server and subsequently Browsersync, which serves as a proxy for propagating development changes to browser.</p>\n\n<p>Speaking more precisely, start.js imports Webpack config for client, manipulates it to add hot module replacement capabilities, then creates both client and server side bundles, starts node server through yet another utility named runServer.js and after successful start inits Browsersync, which looks something like this.</p>\n\n<pre><code>const bs = Browsersync.create();  \nbs.init({\n      ...(DEBUG ? {} : { notify: false, ui: false }),\n\n      proxy: {\n        target: host,\n        middleware: [wpMiddleware, ...hotMiddlewares],\n      },\n\n      // no need to watch '*.js' here, webpack will take care of it for us,\n      // including full page reloads if HMR won't work\n      files: ['build/content/**/*.*'],\n}, resolve)\n</code></pre>\n\n<p>The important part is <code>proxy.target</code>, where you set server address you want to proxy, which could be <a href=\"http://localhost:3000\">http://localhost:3000</a>, and Browsersync starts a server listening on <a href=\"http://localhost:3001\">http://localhost:3001</a>, where the same app is deployed, but with hot module replacement, so you can experience propagation of source file changes to browser immediately with or even without reloading. As you can see, there is another configuration property <code>files</code> with individual files or patterns Browsersync watches for changes and reloads browser, if some occur, but as the comment says, Webpack takes care of watching js sources by itself with HMR, so they cooperate there.</p>\n\n<p>Now I don't have any equivalent example of such Grunt or Gulp configuration, but with Gulp (and somewhat similarly with Grunt) you would write individual tasks in gulpfile.js like</p>\n\n<pre><code>gulp.task('bundle', function() {\n  // bundling source files with some gulp plugins like gulp-webpack maybe\n});\n\ngulp.task('start', function() {\n  // starting server and stuff\n});\n</code></pre>\n\n<p>where you would be doing essentially pretty much the same things as in the starter-kit, this time with task runner, which solves some problems for you, but presents its own issues and some difficulties during learning the usage, and as I say, the more dependencies you have, the more can go wrong. And that is the reason I like to get rid of such tools.</p>\n", "title": "npm vs bower vs browserify vs gulp vs grunt vs webpack", "answer_id": 35064297, "question_id": 35062852, "tags": ["gruntjs", "npm", "gulp", "bower", "webpack"]}, {"body": "<p>A small note about npm: npm3 tries install dependencies in a flat way</p>\n\n<p><a href=\"https://docs.npmjs.com/how-npm-works/npm3#npm-v3-dependency-resolution\">https://docs.npmjs.com/how-npm-works/npm3#npm-v3-dependency-resolution</a></p>\n", "title": "npm vs bower vs browserify vs gulp vs grunt vs webpack", "answer_id": 36628118, "question_id": 35062852, "tags": ["gruntjs", "npm", "gulp", "bower", "webpack"]}, {"body": "<p>You can find some technical comparison on <a href=\"https://npmcompare.com\">npmcompare</a></p>\n\n<p><a href=\"https://npmcompare.com/compare/browserify,grunt,gulp,webpack\">Comparing browserify vs. grunt vs. gulp vs. webpack</a></p>\n\n<p>As you can see webpack is very well maintained with a new version coming out every 4 days on average.\nBut Gulp seems to have the biggest community of them all (with over 20K stars on Github)\nGrunt seems a bit neglected (compared to the others)</p>\n\n<p>So if need to choose one over the other i would go with Gulp</p>\n", "title": "npm vs bower vs browserify vs gulp vs grunt vs webpack", "answer_id": 36964916, "question_id": 35062852, "tags": ["gruntjs", "npm", "gulp", "bower", "webpack"]}], "question_id": 35062852}, {"body": "<p>I'm writing some code in Java where, at some point, the flow of the program is determined by whether two int variables, \"a\" and \"b\", are non-zero (note: a and b are never negative, and never within integer overflow range).</p>\n\n<p>I can evaluate it with</p>\n\n<pre><code>if (a != 0 &amp;&amp; b != 0) { /* Some code */ }\n</code></pre>\n\n<p>Or alternatively</p>\n\n<pre><code>if (a*b != 0) { /* Some code */ }\n</code></pre>\n\n<p>Because I expect that piece of code to run millions of times per run, I was wondering which one would be faster. I did the experiment by comparing them on a huge randomly generated array, and I was also curious to see how the sparsity of the array (fraction of data = 0) would affect the results:</p>\n\n<pre><code>long time;\nfinal int len = 50000000;\nint arbitrary = 0;\nint[][] nums = new int[2][len];\n\nfor (double fraction = 0 ; fraction &lt;= 0.9 ; fraction += 0.0078125) {\n    for(int i = 0 ; i &lt; 2 ; i++) {\n        for(int j = 0 ; j &lt; len ; j++) {\n            double random = Math.random();\n\n            if(random &lt; fraction) nums[i][j] = 0;\n            else nums[i][j] = (int) (random*15 + 1);\n        }\n    }\n\n    time = System.currentTimeMillis();\n\n    for(int i = 0 ; i &lt; len ; i++) {\n        if( /*insert nums[0][i]*nums[1][i]!=0 or nums[0][i]!=0 &amp;&amp; nums[1][i]!=0*/ ) arbitrary++;\n    }\n    System.out.println(System.currentTimeMillis() - time);\n}\n</code></pre>\n\n<p>And the results show that if you expect \"a\" or \"b\" to be equal to 0 more than ~3% of the time, <code>a*b != 0</code> is faster than <code>a!=0 &amp;&amp; b!=0</code>:</p>\n\n<p><a href=\"http://i.stack.imgur.com/POHYD.png\"><img src=\"http://i.stack.imgur.com/POHYD.png\" alt=\"Graphical graph of the results of a AND b non-zero\"></a></p>\n\n<p>I'm curious to know why. Could anyone shed some light? Is it the compiler or is it at the hardware level?</p>\n\n<p><strong>Edit:</strong> <em>Out of curiosity...</em> now that I learned about branch prediction, I was wondering what the analog comparison would show for a <strong>OR</strong> b is non-zero:</p>\n\n<p><a href=\"http://i.stack.imgur.com/GpJoM.png\"><img src=\"http://i.stack.imgur.com/GpJoM.png\" alt=\"Graph of a or b non-zero\"></a> </p>\n\n<p>We do see the same effect of branch prediction as expected, interestingly the graph is somewhat flipped along the X-axis. </p>\n\n<h3>Update</h3>\n\n<p>1- I added <code>!(a==0 || b==0)</code> to the analysis to see what happens.</p>\n\n<p>2- I also included <code>a != 0 || b != 0</code>, <code>(a+b) != 0</code> and <code>(a|b) != 0</code> out of curiosity, after learning about branch prediction. But they are not logically equivalent to the other expressions, because only a <em>OR</em> b needs to be non-zero to return true, so they are not meant to be compared for processing efficiency.</p>\n\n<p>3- I also added the actual benchmark that I used for the analysis, which is just iterating an arbitrary int variable. </p>\n\n<p>4- Some people were suggesting to include <code>a != 0 &amp; b != 0</code> as opposed to <code>a != 0 &amp;&amp; b != 0</code>, with the prediction that it would behave more closely to <code>a*b != 0</code> because we would remove the branch prediction effect. I didn't know that <code>&amp;</code> could be used with boolean variables, I thought it was only used for binary operations with integers.</p>\n\n<p>Note: In the context that I was considering all this, int overflow is not an issue, but that's definitely an important consideration in general contexts.</p>\n\n<p>CPU: Intel Core i7-3610QM @ 2.3GHz  </p>\n\n<p>Java version: 1.8.0_45<br>\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)<br>\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)  </p>\n", "title": "Why is (a*b != 0) faster than (a != 0 &amp;&amp; b != 0) in Java?", "tags": ["java", "performance", "processing-efficiency", "microbenchmark", "branch-prediction"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35531369/why-is-ab-0-faster-than-a-0-b-0-in-java", "answers": [{"body": "<p>I'm ignoring the issue that your benchmarking <em>might</em> be flawed, and taking the result at face value.</p>\n\n<blockquote>\n  <p>Is it the compiler or is it at the hardware level?</p>\n</blockquote>\n\n<p>That latter, I think:</p>\n\n<pre><code>  if (a != 0 &amp;&amp; b != 0)\n</code></pre>\n\n<p>will compile to 2 memory loads and two conditional branches</p>\n\n<pre><code>  if (a * b != 0)\n</code></pre>\n\n<p>will compile to 2 memory loads, a multiply and one conditional branch.</p>\n\n<p>The multiply is likely to be faster than the second conditional branch if the hardware-level branch prediction is ineffective.  As you increase the ratio ... the branch prediction is becoming less effective.</p>\n\n<p>The reason that conditional branches are slower is that they cause the instruction execution pipeline to stall.  Branch prediction is about avoiding the stall by predicting which way the branch is going to go and speculatively choosing the next instruction based on that.  If the prediction fails, there is a delay while the instruction for the other direction is loaded.</p>\n\n<p>(Note: the above explanation is oversimplified.  For a more accurate explanation, you need to look at the literature provided by the CPU manufacturer for assembly language coders and compiler writers.  The Wikipedia page on <a href=\"https://en.wikipedia.org/wiki/Branch_predictor\">Branch Predictors</a> is good background.)</p>\n\n<hr>\n\n<p>However, there is one thing that you need to be careful about with this optimization.  Are there any values where <code>a * b != 0</code> will give the wrong answer?  Consider cases where computing the product results in integer overflow.</p>\n\n<hr>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Your graphs tend to confirm what I said.  </p>\n\n<ul>\n<li><p>There is also a \"branch prediction\" effect in the conditional branch <code>a * b != 0</code> case, and this comes out in the graphs.</p></li>\n<li><p>If you project the curves beyond 0.9 on the X-axis, it looks like 1) they will meet at about 1.0 and 2) the meeting point will be at roughly the same Y value as for X = 0.0.</p></li>\n</ul>\n\n<hr>\n\n<p><strong>UPDATE 2</strong></p>\n\n<p>I don't understand why the curves are different for the <code>a + b != 0</code> and the <code>a | b != 0</code> cases.  There <em>could be</em> something clever in the branch predictors logic.  Or it could indicate something else.</p>\n\n<p>(Note that this kind of thing can be specific to a particular chip model number or even version.  The results of your benchmarks could be different on other systems.)</p>\n\n<p>However, they both have the advantage of working for all non-negative values of <code>a</code> and <code>b</code>.</p>\n", "title": "Why is (a*b != 0) faster than (a != 0 &amp;&amp; b != 0) in Java?", "answer_id": 35531496, "question_id": 35531369, "tags": ["java", "performance", "processing-efficiency", "microbenchmark", "branch-prediction"]}, {"body": "<p>When we take the multiplication, even if one number is 0, then the product is 0. While writing </p>\n\n<pre><code>    (a*b != 0)\n</code></pre>\n\n<p>It evaluates the result of the product thereby eliminating the first few occurrences of the iteration starting from 0. As a result the comparisons are less than that when the condition is</p>\n\n<pre><code>   (a != 0 &amp;&amp; b != 0)\n</code></pre>\n\n<p>Where every element is compared with 0 and evaluated. Hence the time required is less. But I believe that the second condition might give you more accurate solution.</p>\n", "title": "Why is (a*b != 0) faster than (a != 0 &amp;&amp; b != 0) in Java?", "answer_id": 35531610, "question_id": 35531369, "tags": ["java", "performance", "processing-efficiency", "microbenchmark", "branch-prediction"]}, {"body": "<p>I think your benchmark has some flaws and may not be useful for inferring about real programs. Here are my thoughts:</p>\n\n<ul>\n<li><p><code>(a*b)!=0</code> will do the wrong thing for values that overflow, and <code>(a+b)!=0</code> will additionally do the wrong thing for positive and negative values that sum to zero, so you can't use either of those expressions in the general case, even if they work here.</p></li>\n<li><p><code>(a|b)!=0</code> and <code>(a+b)!=0</code> are testing if <em>either</em> value is non-zero, while <code>(a*b)!=0</code> and <code>a != 0 &amp;&amp; b != 0</code> are testing if <em>both</em> are non-zero. The two types of conditions will not be true on the same percentage of data.</p></li>\n<li><p>The VM will optimize the expression during the first few runs of the outer (<code>fraction</code>) loop, when <code>fraction</code> is 0, when the branches are almost never taken. The optimizer may do different things if you start <code>fraction</code> at 0.5.</p></li>\n<li><p>Unless the VM is able to eliminate some of the array bounds checks here, there are four other branches in the expression just due to the bounds checks, and that's a complicating factor when trying to figure out what's happening at a low level. You may get different results if you split the two-dimensional array into two flat arrays, changing <code>nums[0][i]</code> and <code>nums[1][i]</code> to <code>nums0[i]</code> and <code>nums1[i]</code>.</p></li>\n<li><p>CPU branch predictors try to detect short patterns in the data, or runs of all branches being taken or not taken. Your randomly generated benchmark data is the worst thing for a branch predictor to try to deal with. If your real data has a predictable pattern, or it has long runs of all-zero and all-non-zero values, the branches could cost a lot less.</p></li>\n<li><p>The particular code that is executed after the condition is met can affect the performance of evaluating the condition itself, because it affects things like whether or not the loop can be unrolled, which CPU registers are available, and if any of the fetched <code>nums</code> values need to be reused after evaluating the condition. Merely incrementing a counter in the benchmark is not a perfect placeholder for what real code would do.</p></li>\n<li><p><code>System.currentTimeMillis()</code> is on most systems not more accurate than +/- 10 ms. <code>System.nanoTime()</code> is usually more accurate.</p></li>\n</ul>\n\n<p>As you can see there lots of uncertainties, and it's always hard to say anything definite with these sort of micro-optimizations because a trick that is faster on one VM or CPU can be slower on another. If your VM is HotSpot, be aware that there are two further varieties, with the \"Client\" VM having different (weaker) optimizations compared to the \"Server\" VM.</p>\n\n<p>If you can <a href=\"https://wiki.openjdk.java.net/display/HotSpot/PrintAssembly\">disassemble the machine code generated by the VM</a>, do that rather than trying to guess what it does!</p>\n", "title": "Why is (a*b != 0) faster than (a != 0 &amp;&amp; b != 0) in Java?", "answer_id": 35538237, "question_id": 35531369, "tags": ["java", "performance", "processing-efficiency", "microbenchmark", "branch-prediction"]}, {"body": "<p>The answers here are good, though I had an idea that might improve things.</p>\n\n<p>Since the two branches and associated branch prediction are the likely culprit, we may be able to reduce the branching to a single branch without changing the logic at all.</p>\n\n<pre><code>bool aNotZero = (nums[0][i] != 0);\nbool bNotZero = (nums[1][i] != 0);\nif (aNotZero &amp;&amp; bNotZero) { /* Some code */ }\n</code></pre>\n\n<p>It may also work to do</p>\n\n<pre><code>int a = nums[0][i];\nint b = nums[1][i];\nif (a != 0 &amp;&amp; b != 0) { /* Some code */ }\n</code></pre>\n\n<p>The reason being, by the rules of short circuiting, if the first boolean is false, the second should not be evaluated. It has to perform an extra branch to avoid evaluating <code>nums[1][i]</code> if <code>nums[0][i]</code> was false. Now, you may not care that <code>nums[1][i]</code> gets evaluated, but the compiler can't be certain that it won't throw an out of range or null ref when you do. By reducing the if block to simple bools, the compiler may be smart enough to realize that evaluating the second boolean unnecessarily won't have negative side effects.</p>\n", "title": "Why is (a*b != 0) faster than (a != 0 &amp;&amp; b != 0) in Java?", "answer_id": 35544998, "question_id": 35531369, "tags": ["java", "performance", "processing-efficiency", "microbenchmark", "branch-prediction"]}, {"body": "<p>You are using randomized input data which makes the branches unpredictable. In practice branches are often (~90%) predictable so in real code the branchful code is likely to be faster.</p>\n\n<p>That said. I don't see how <code>a*b != 0</code> can be faster than <code>(a|b) != 0</code>. Generally integer multiplication is more expensive than a bitwise OR. But things like this occasionally get weird. See for example the \"Example 7: Hardware complexities\" example from <a href=\"http://igoro.com/archive/gallery-of-processor-cache-effects/\" rel=\"nofollow\">Gallery of Processor Cache Effects</a>.</p>\n", "title": "Why is (a*b != 0) faster than (a != 0 &amp;&amp; b != 0) in Java?", "answer_id": 35591852, "question_id": 35531369, "tags": ["java", "performance", "processing-efficiency", "microbenchmark", "branch-prediction"]}], "question_id": 35531369}, {"body": "<p>I stumbled over an interesting question in a forum a long time ago and I want to know the answer.</p>\n\n<p>Consider the following C function:</p>\n\n<h2>f1.c</h2>\n\n<pre><code>#include &lt;stdbool.h&gt;\n\nbool f1()\n{\n    int var1 = 1000;\n    int var2 = 2000;\n    int var3 = var1 + var2;\n    return (var3 == 0) ? true : false;\n}\n</code></pre>\n\n<p>This should always return <code>false</code> since <code>var3 == 3000</code>. The <code>main</code> function looks like this:</p>\n\n<h2>main.c</h2>\n\n<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\n\nint main()\n{\n    printf( f1() == true ? \"true\\n\" : \"false\\n\");\n    if( f1() )\n    {\n        printf(\"executed\\n\");\n    }\n    return 0;\n}\n</code></pre>\n\n<p>Since <code>f1()</code> should always return <code>false</code>, one would expect the program to print only one <em>false</em> to the screen. But after compiling and running it, <em>executed</em> is also displayed:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>$ gcc main.c f1.c -o test\n$ ./test\nfalse\nexecuted\n</code></pre>\n\n<p>Why is that? Does this code have some sort of undefined behavior?</p>\n\n<p>Note: I compiled it with <code>gcc (Ubuntu 4.9.2-10ubuntu13) 4.9.2</code>.</p>\n", "title": "This C function should always return false, but it doesn\u2019t", "tags": ["c", "gcc"], "answer_count": 4, "link": "http://stackoverflow.com/questions/36476178/this-c-function-should-always-return-false-but-it-doesn-t", "answers": [{"body": "<p>You don't have a prototype declared for <code>f1()</code> in main.c, so it is implicitly defined as <code>int f1()</code>, meaning it is a function that takes an unknown number of arguments and returns an <code>int</code>.</p>\n\n<p>If <code>int</code> and <code>bool</code> are of different sizes, this will result in <strong><a href=\"https://en.wikipedia.org/wiki/Undefined_behavior\">undefined behavior</a></strong>.  For example, on my machine, <code>int</code> is 4 bytes and <code>bool</code> is one byte.  Since the function is <em>defined</em> to return <code>bool</code>, it puts one byte on the stack when it returns.  However, since it's <em>implicitly declared</em> to return <code>int</code> from main.c, the calling function will try to read 4 bytes from the stack.</p>\n\n<p>The default compilers options in gcc won't tell you that it's doing this.  But if you compile with <code>-Wall -Wextra</code>, you'll get this:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>main.c: In function \u2018main\u2019:\nmain.c:6: warning: implicit declaration of function \u2018f1\u2019\n</code></pre>\n\n<p>To fix this, add a declaration for <code>f1</code> in main.c, before <code>main</code>:</p>\n\n<pre><code>bool f1(void);\n</code></pre>\n\n<p>Note that the argument list is explicitly set to <code>void</code>, which tells the compiler the function takes no arguments, as opposed to an empty parameter list which means an unknown number of arguments.  The definition <code>f1</code> in f1.c should also be changed to reflect this.</p>\n", "title": "This C function should always return false, but it doesn\u2019t", "answer_id": 36476347, "question_id": 36476178, "tags": ["c", "gcc"]}, {"body": "<p>Please compile with a command such as this one:</p>\n\n<pre><code>gcc -Wall -Wextra -Werror -std=gnu99 -o main.exe main.c\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>main.c: In function 'main':\nmain.c:14:5: error: implicit declaration of function 'f1' [-Werror=impl\nicit-function-declaration]\n     printf( f1() == true ? \"true\\n\" : \"false\\n\");\n     ^\ncc1.exe: all warnings being treated as errors\n</code></pre>\n\n<p>With such a message, you should know what to do to correct it.</p>\n\n<p>Edit: After reading a (now deleted) comment, I tried to compile your code without the flags. Well, This led me to linker errors with no compiler warnings instead of compiler errors. And those linker errors are more difficult to understand, so even if <code>-std-gnu99</code> is not necessary, please try to allways use at least <code>-Wall -Werror</code> it will save you a lot of pain in the ass.</p>\n", "title": "This C function should always return false, but it doesn\u2019t", "answer_id": 36476426, "question_id": 36476178, "tags": ["c", "gcc"]}, {"body": "<p>As noted in other answers, the problem is that you use <code>gcc</code> with no compiler options set. If you do this, it defaults to what is called \"gnu90\", which is a non-standard implementation of the old, withdrawn C90 standard from 1990.</p>\n\n<p>In the old C90 standard there was a major flaw in the C language: if you didn't declare a prototype before using a function, it would default to <code>int func ()</code> (where <code>( )</code> means \"accept any parameter\"). This changes the calling convention of the function <code>func</code>, but it doesn't change the actual function definition. Since the size of <code>bool</code> and <code>int</code> are different, your code invokes undefined behavior when the function is called.</p>\n\n<p>This dangerous nonsense behavior was fixed in the year 1999, with the release of the C99 standard. Implicit function declarations were banned.</p>\n\n<p>Unfortunately, GCC up to version 5.x.x still uses the old C standard by default. There is probably no reason why you should want to compile your code as anything but standard C. So you have to explicitly tell GCC that it should compile your code as modern C code, instead of some 25+ years old, non-standard GNU crap.</p>\n\n<p>Fix the problem by always compiling your program as:</p>\n\n<pre><code>gcc -std=c11 -pedantic-errors -Wall -Wextra\n</code></pre>\n\n<ul>\n<li><code>-std=c11</code> tells it to make a half-hearted attempt to compile according the (current) C standard (informally known as C11).</li>\n<li><code>-pedantic-errors</code> tells it to whole-heartedly do the above, and give compiler errors when you write incorrect code which violates the C standard.</li>\n<li><code>-Wall</code> means give me some extra warnings that might be good to have.</li>\n<li><code>-Wextra</code> means give me some other extra warnings that might be good to have.</li>\n</ul>\n", "title": "This C function should always return false, but it doesn\u2019t", "answer_id": 36477209, "question_id": 36476178, "tags": ["c", "gcc"]}, {"body": "<p>I think it's interesting to see where the size-mismatch mentioned in Lundin's excellent answer actually happens.</p>\n\n<p>If you compile with <code>--save-temps</code>, you will get assembly files that you can look at. Here's the part where <code>f1()</code> does the <code>== 0</code> comparison and returns its value:</p>\n\n<pre><code>cmpl    $0, -4(%rbp)\nsete    %al\n</code></pre>\n\n<p>The returning part is <code>sete %al</code>. In C's x86 calling conventions, return values 4 bytes or smaller (which includes <code>int</code> and <code>bool</code>) are returned via register <code>%eax</code>. <code>%al</code> is the lowest byte of <code>%eax</code>. So, the upper 3 bytes of <code>%eax</code> are left in an uncontrolled state.</p>\n\n<p>Now in <code>main()</code>:</p>\n\n<pre><code>call    f1\ntestl   %eax, %eax\nje  .L2\n</code></pre>\n\n<p>This checks whether the <em>whole</em> of <code>%eax</code> is zero, because it thinks it's testing an int.</p>\n\n<p>Adding an explicit function declaration changes <code>main()</code> to:</p>\n\n<pre><code>call    f1\ntestb   %al, %al\nje  .L2\n</code></pre>\n\n<p>which is what we want.</p>\n", "title": "This C function should always return false, but it doesn\u2019t", "answer_id": 36514954, "question_id": 36476178, "tags": ["c", "gcc"]}], "question_id": 36476178}, {"body": "<p>I'm learning about operator overloading in C++, and I see that <code>==</code> and <code>!=</code> are simply some special functions which can be customized for user-defined types. My concern is, though, why are there <em>two separate</em> definitions needed? I thought that if <code>a == b</code> is true, then <code>a != b</code> is automatically false, and vice versa, and there is no other possibility, because, by definition, <code>a != b</code> is <code>!(a == b)</code>. And I couldn't imagine any situation in which this wasn't true. But perhaps my imagination is limited or I am ignorant of something?</p>\n\n<p>I know that I can define one in terms of the other, but this is not what I'm asking about. I'm also not asking about the distinction between comparing objects by value or by identity. Or whether two objects could be equal and non-equal at the same time (this is definitely not an option! these things are mutually exclusive). What I'm asking about is this:</p>\n\n<p>Is there any situation possible in which asking questions about two objects being equal does make sense, but asking about them <em>not</em> being equal doesn't make sense? (either from the user's perspective, or the implementer's perspective)</p>\n\n<p>If there is no such possibility, then why on Earth does C++ have these two operators being defined as two distinct functions?</p>\n", "title": "Are `==` and `!=` mutually dependent?", "tags": ["c++", "operator-overloading", "equality", "equality-operator"], "answer_count": 15, "link": "http://stackoverflow.com/questions/37800001/are-and-mutually-dependent", "answers": [{"body": "<blockquote>\n  <p>Is there any situation possible in which asking questions about two\n  objects being equal does make sense, but asking about them not being\n  equal doesn't make sense? (either from the user's perspective, or the\n  implementer's perspective)</p>\n</blockquote>\n\n<p>That's an opinion. Maybe it doesn't. But the language designers, not being omniscient, decided not to restrict people who might come up with situations in which it might make sense (at least to them).</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800157, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<blockquote>\n  <p>[..] why are there two separate definitions needed?</p>\n</blockquote>\n\n<p>One thing to consider is that there might be the possibility of implementing one of these operators more efficiently than just using the negation of the other.</p>\n\n<p>(My example here was rubbish, but the point still stands, think of bloom filters, for example: They allow fast testing if something is <em>not</em> in a set, but testing if it's in may take a lot more time.)</p>\n\n<blockquote>\n  <p>[..] by definition, <code>a != b</code> is <code>!(a == b)</code>.</p>\n</blockquote>\n\n<p>And it's your responsibility as programmer to make that hold. Probably a good thing to write a test for. </p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800173, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>If the <code>==</code> and <code>!=</code> operators don't actually imply equality, in the same way that the <code>&lt;&lt;</code> and <code>&gt;&gt;</code> stream operators don't imply bit-shifting.  If you treat the symbols as if they mean some other concept, they don't have to be mutually exclusive.</p>\n\n<p>In terms of equality, it could make sense if your use-case warrants treating objects as non-comparable, so that every comparison should return false (or a non-comparable result type, if your operators return non-bool).  I can't think of a specific situation where this would be warranted, but I could see it being reasonable enough.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800254, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>You would <em>not</em> want the language to automatically rewrite <code>a != b</code> as <code>!(a == b)</code> when <code>a == b</code> returns something other than a <code>bool</code>. And there are a few reasons why you might make it do that.</p>\n\n<p>You may have expression builder objects, where <code>a == b</code> doesn't and isn't intended to perform any comparison, but simply builds some expression node representing <code>a == b</code>.</p>\n\n<p>You may have lazy evaluation, where <code>a == b</code> doesn't and isn't intended to perform any comparison directly, but instead returns some kind of <code>lazy&lt;bool&gt;</code> that can be converted to <code>bool</code> implicitly or explicitly at some later time to actually perform the comparison. Possibly combined with the expression builder objects to allow complete expression optimisation before evaluation.</p>\n\n<p>You may have some custom <code>optional&lt;T&gt;</code> template class, where given optional variables <code>t</code> and <code>u</code>, you want to allow <code>t == u</code>, but make it return <code>optional&lt;bool&gt;</code>.</p>\n\n<p>There's probably more that I didn't think of. And even though in these examples the operation <code>a == b</code> and <code>a != b</code> do both make sense, still <code>a != b</code> isn't the same thing as <code>!(a == b)</code>, so separate definitions are needed.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800271, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<pre><code>enum BoolPlus {\n    kFalse = 0,\n    kTrue = 1,\n    kFileNotFound = -1\n}\n\nBoolPlus operator==(File&amp; other);\nBoolPlus operator!=(File&amp; other);\n</code></pre>\n\n<p>I can't justify this operator overloading, but in the example above it is impossible to define <code>operator!=</code> as the \"opposite\" of <code>operator==</code>.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800307, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<blockquote>\n  <p>My concern is, though, why are there two separate definitions needed?</p>\n</blockquote>\n\n<p>You don't have to define both.<br>\nIf they are mutually exclusive, you can still be concise by only defining <code>==</code> and <code>&lt;</code> alongside <a href=\"http://en.cppreference.com/w/cpp/utility/rel_ops/operator_cmp\">std::rel_ops</a>  </p>\n\n<p>Fom cppreference:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;utility&gt;\n\nstruct Foo {\n    int n;\n};\n\nbool operator==(const Foo&amp; lhs, const Foo&amp; rhs)\n{\n    return lhs.n == rhs.n;\n}\n\nbool operator&lt;(const Foo&amp; lhs, const Foo&amp; rhs)\n{\n    return lhs.n &lt; rhs.n;\n}\n\nint main()\n{\n    Foo f1 = {1};\n    Foo f2 = {2};\n    using namespace std::rel_ops;\n\n    //all work as you would expect\n    std::cout &lt;&lt; \"not equal:     : \" &lt;&lt; (f1 != f2) &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"greater:       : \" &lt;&lt; (f1 &gt; f2) &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"less equal:    : \" &lt;&lt; (f1 &lt;= f2) &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"greater equal: : \" &lt;&lt; (f1 &gt;= f2) &lt;&lt; '\\n';\n}\n</code></pre>\n\n<blockquote>\n  <p>Is there any situation possible in which asking questions about two\n  objects being equal does make sense, but asking about them not being\n  equal doesn't make sense?</p>\n</blockquote>\n\n<p>We often associate these operators to equality.<br>\nAlthough that is how they behave on fundamental types, there is no obligation that this be their behaviour on custom data types.\nYou don't even have to return a bool if you don't want to.  </p>\n\n<p>I've seen people overload operators in bizarre ways, only to find that it makes sense for their domain specific application.  Even if the interface appears to show that they are mutually exclusive, the author may want to add specific internal logic.  </p>\n\n<blockquote>\n  <p>(either from the user's perspective, or the implementer's perspective)</p>\n</blockquote>\n\n<p>I know you want a specific example,<br>\nso here is one from the <a href=\"https://github.com/philsquared/Catch\">Catch testing framework</a> that I thought was practical:  </p>\n\n<pre><code>template&lt;typename RhsT&gt;\nResultBuilder&amp; operator == ( RhsT const&amp; rhs ) {\n    return captureExpression&lt;Internal::IsEqualTo&gt;( rhs );\n}\n\ntemplate&lt;typename RhsT&gt;\nResultBuilder&amp; operator != ( RhsT const&amp; rhs ) {\n    return captureExpression&lt;Internal::IsNotEqualTo&gt;( rhs );\n}\n</code></pre>\n\n<p>These operators are doing different things, and it would not make sense to define one method as a !(not) of the other.  The reason this is done, is so that the framework can print out the comparison made.  In order to do that, it needs to capture the context of what overloaded operator was used.  </p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800344, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<blockquote>\n  <p>If there is no such possibility, then why on Earth does C++ have these two operators being defined as two distinct functions?</p>\n</blockquote>\n\n<p>Because you can overload them, and by overloading them you can give them a totally different meaning from their original one.</p>\n\n<p>Take, for example, operator <code>&lt;&lt;</code>, originally the bitwise left shift operator, now commonly overloaded as an insertion operator, like in <code>std::cout &lt;&lt; something</code>; totally different meaning from the original one.</p>\n\n<p>So, if you accept that the meaning of an operator changes when you overload it, then there is no reason to prevent user from giving a meaning to operator <code>==</code> that is not exactly the <em>negation</em> of operator <code>!=</code>, though this might be confusing.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37800428, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>There are some very well-established conventions in which <code>(a == b)</code> and <code>(a != b)</code> are <strike>both false</strike> not necessarily opposites.  In particular, in SQL, any comparison to NULL yields NULL, not true or false.</p>\n\n<p>It's probably not a good idea to create new examples of this if at all possible, because it's so unintuitive, but if you're trying to model an  existing convention, it's nice to have the option to make your operators behave \"correctly\" for that context.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37803936, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>In response to the edit;</p>\n\n<blockquote>\n  <p>That is, if it is possible for some type to have the operator <code>==</code> but not the <code>!=</code>, or vice versa, and when does it make sense to do so.</p>\n</blockquote>\n\n<p>In <em>general</em>, no, it doesn't make sense. Equality and relational operators generally come in sets. If there is the equality, then the inequality as well; less than, then greater than and so on with the <code>&lt;=</code> etc. A similar approach is applied to the arithmetic operators as well, they also generally come in natural logical sets. </p>\n\n<p>This is evidenced in the <a href=\"http://en.cppreference.com/w/cpp/utility/rel_ops/operator_cmp\"><code>std::rel_ops</code></a> namespace. If you implement the equality and less than operators, using that namespace gives you the others, implemented in terms of your original implemented operators. </p>\n\n<p>That all said, <strong>are there conditions or situations where the one would not immediately mean the other, or could not be implemented in terms of the others? Yes there are</strong>, arguably few, but they are there; again, as evidenced in the <code>rel_ops</code> being a namespace of its own. For that reason, allowing them to be implemented independently allows you to leverage the language to get the semantics you require or need in a way that is still natural and intuitive for the user or client of the code.</p>\n\n<p>The lazy evaluation already mentioned is an excellent example of this. Another good example is giving them semantics that don't mean equality or in-equality at all. A similar example to this is the bit shift operators <code>&lt;&lt;</code> and <code>&gt;&gt;</code> being used for stream insertion and extraction. Although it may be frowned upon in general circles, in some domain specific areas it may make sense.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37819571, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>Maybe an uncomparable rule, where a != b was false and a == b was false like a stateless bit.</p>\n\n<pre><code>if( !(a == b || a != b) ){\n    // Stateless\n}\n</code></pre>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37820261, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>I will only answer the second part of your question, namely:</p>\n\n<blockquote>\n  <p>If there is no such possibility, then why on Earth does C++ have these two operators being defined as two distinct functions?</p>\n</blockquote>\n\n<p>One reason why it makes sense to allow the developer to overload both is performance. You might allow optimizations by implementing both <code>==</code> and <code>!=</code>. Then <code>x != y</code> might be cheaper than <code>!(x == y)</code> is. Some compilers may be able to optimize it for you, but perhaps not, especially if you have complex objects with a lot of branching involved.</p>\n\n<p>Even in Haskell, where developers take laws and mathematical concepts very seriously, one is still allowed to overload both <code>==</code> and <code>/=</code>, as you can see here (<a href=\"http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:-61--61-\">http://hackage.haskell.org/package/base-4.9.0.0/docs/Prelude.html#v:-61--61-</a>):</p>\n\n<pre><code>$ ghci\nGHCi, version 7.10.2: http://www.haskell.org/ghc/  :? for help\n\u03bb&gt; :i Eq\nclass Eq a where\n  (==) :: a -&gt; a -&gt; Bool\n  (/=) :: a -&gt; a -&gt; Bool\n        -- Defined in `GHC.Classes'\n</code></pre>\n\n<p>This would probably be considered micro-optimization, but it might be warranted for some cases.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37823590, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>In the end, what you are checking with those operators is that the expression a==b or a!=b is returning a boolean value. These expression returns a boolean value after comparison rather than being mutually exclusive.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37826475, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>By customizing the behavior of the operators, you can make them do what you want.</p>\n\n<p>You may wish to customize things.  For instance, you may wish to customize a class.  Objects of this class can be compared just by checking a specific property.  Knowing that this is the case, you can write some specific code that only checks the minimum things, instead of checking every single bit of every single property in the whole object.</p>\n\n<p>Imagine a case where you can figure out that something is different just as fast, if not faster, than you can find out something is the same.  Granted, once you figure out whether something is the same or different, then you can know the opposite simply by flipping a bit.  However, flipping that bit is an extra operation.  In some cases, when code gets re-executed a lot, saving one operation (multiplied by many times) can have an overall speed increase.  (For instance, if you save one operation per pixel of a megapixel screen, then you've just saved a million operations.  Multiplied by 60 screens per second, and you save even more operations.)</p>\n\n<p><A HREF=\"http://stackoverflow.com/a/37800271/4411648\">hvd's answer</A> provides some additional examples.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37892068, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>With great power comes great responsibly, or at least really good style guides.</p>\n\n<p><code>==</code> and <code>!=</code> can be overloaded to do whatever the heck you want.  It's both a blessing and a curse.  There's no guarantee that <code>!=</code> means <code>!(a==b)</code>.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 37899050, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}, {"body": "<p>Yes, because one means \"equivalent\" and another means \"non-equivalent\" and this terms are mutually exclusive. Any other meaning for this operators is confusing and should be avoided by all means.</p>\n", "title": "Are `==` and `!=` mutually dependent?", "answer_id": 38111431, "question_id": 37800001, "tags": ["c++", "operator-overloading", "equality", "equality-operator"]}], "question_id": 37800001}, {"body": "<p>I've been racking my brain for a week trying to complete this assignment and I'm hoping someone here can lead me toward the right path. Let me start with the instructor's instructions:</p>\n\n<blockquote>\n  <p>Your assignment is the opposite of our first lab assignment, which was to optimize a prime number program. Your purpose in this assignment is to pessimize the program, i.e. make it run slower. Both of these are CPU-intensive programs. They take a few seconds to run on our lab PCs. You may not change the algorithm.</p>\n  \n  <p>To deoptimize the program, use your knowledge of how the Intel i7 pipeline operates. Imagine ways to re-order instruction paths to introduce WAR, RAW, and other hazards. Think of ways to minimize the effectiveness of the cache. Be diabolically incompetent.</p>\n</blockquote>\n\n<p>The assignment gave a choice of Whetstone or Monte-Carlo programs.  The cache-effectiveness comments are mostly only applicable to Whetstone, but I chose the Monte-Carlo simulation program:</p>\n\n<pre><code>// Un-modified baseline for pessimization, as given in the assignment\n#include &lt;algorithm&gt;    // Needed for the \"max\" function\n#include &lt;cmath&gt;\n#include &lt;iostream&gt;\n\n// A simple implementation of the Box-Muller algorithm, used to generate\n// gaussian random numbers - necessary for the Monte Carlo method below\n// Note that C++11 actually provides std::normal_distribution&lt;&gt; in \n// the &lt;random&gt; library, which can be used instead of this function\ndouble gaussian_box_muller() {\n  double x = 0.0;\n  double y = 0.0;\n  double euclid_sq = 0.0;\n\n  // Continue generating two uniform random variables\n  // until the square of their \"euclidean distance\" \n  // is less than unity\n  do {\n    x = 2.0 * rand() / static_cast&lt;double&gt;(RAND_MAX)-1;\n    y = 2.0 * rand() / static_cast&lt;double&gt;(RAND_MAX)-1;\n    euclid_sq = x*x + y*y;\n  } while (euclid_sq &gt;= 1.0);\n\n  return x*sqrt(-2*log(euclid_sq)/euclid_sq);\n}\n\n// Pricing a European vanilla call option with a Monte Carlo method\ndouble monte_carlo_call_price(const int&amp; num_sims, const double&amp; S, const double&amp; K, const double&amp; r, const double&amp; v, const double&amp; T) {\n  double S_adjust = S * exp(T*(r-0.5*v*v));\n  double S_cur = 0.0;\n  double payoff_sum = 0.0;\n\n  for (int i=0; i&lt;num_sims; i++) {\n    double gauss_bm = gaussian_box_muller();\n    S_cur = S_adjust * exp(sqrt(v*v*T)*gauss_bm);\n    payoff_sum += std::max(S_cur - K, 0.0);\n  }\n\n  return (payoff_sum / static_cast&lt;double&gt;(num_sims)) * exp(-r*T);\n}\n\n// Pricing a European vanilla put option with a Monte Carlo method\ndouble monte_carlo_put_price(const int&amp; num_sims, const double&amp; S, const double&amp; K, const double&amp; r, const double&amp; v, const double&amp; T) {\n  double S_adjust = S * exp(T*(r-0.5*v*v));\n  double S_cur = 0.0;\n  double payoff_sum = 0.0;\n\n  for (int i=0; i&lt;num_sims; i++) {\n    double gauss_bm = gaussian_box_muller();\n    S_cur = S_adjust * exp(sqrt(v*v*T)*gauss_bm);\n    payoff_sum += std::max(K - S_cur, 0.0);\n  }\n\n  return (payoff_sum / static_cast&lt;double&gt;(num_sims)) * exp(-r*T);\n}\n\nint main(int argc, char **argv) {\n  // First we create the parameter list                                                                               \n  int num_sims = 10000000;   // Number of simulated asset paths                                                       \n  double S = 100.0;  // Option price                                                                                  \n  double K = 100.0;  // Strike price                                                                                  \n  double r = 0.05;   // Risk-free rate (5%)                                                                           \n  double v = 0.2;    // Volatility of the underlying (20%)                                                            \n  double T = 1.0;    // One year until expiry                                                                         \n\n  // Then we calculate the call/put values via Monte Carlo                                                                          \n  double call = monte_carlo_call_price(num_sims, S, K, r, v, T);\n  double put = monte_carlo_put_price(num_sims, S, K, r, v, T);\n\n  // Finally we output the parameters and prices                                                                      \n  std::cout &lt;&lt; \"Number of Paths: \" &lt;&lt; num_sims &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Underlying:      \" &lt;&lt; S &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Strike:          \" &lt;&lt; K &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Risk-Free Rate:  \" &lt;&lt; r &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Volatility:      \" &lt;&lt; v &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Maturity:        \" &lt;&lt; T &lt;&lt; std::endl;\n\n  std::cout &lt;&lt; \"Call Price:      \" &lt;&lt; call &lt;&lt; std::endl;\n  std::cout &lt;&lt; \"Put Price:       \" &lt;&lt; put &lt;&lt; std::endl;\n\n  return 0;\n}\n</code></pre>\n\n<p>The changes I have made seemed to increase the code running time by a second but I'm not entirely sure what I can change to stall the pipeline without adding code. A point to the right direction would be awesome, I appreciate any responses.</p>\n\n<hr>\n\n<h2>Update: <a href=\"http://meta.stackoverflow.com/a/323690/224132\">the professor who gave this assignment posted some details</a></h2>\n\n<p>The highlights are:</p>\n\n<ul>\n<li>It's a second semester architecture class at a community college (using the Hennessy and Patterson textbook).</li>\n<li>the lab computers have Haswell CPUs</li>\n<li>The students have been exposed to the <code>CPUID</code> instruction and how to determine cache size, as well as intrinsics and the <code>CLFLUSH</code> instruction.</li>\n<li>any compiler options are allowed, and so is inline asm.</li>\n<li>Writing your own square root algorithm was announced as being outside the pale</li>\n</ul>\n\n<p>Cowmoogun's comments on the meta thread indicate that <a href=\"http://meta.stackoverflow.com/questions/323603/is-it-possible-for-some-too-broad-questions-to-be-exceptions-to-the-rule/323690#comment347466_323612\">it wasn't clear compiler optimizations could be part of this, and assumed <code>-O0</code></a>, and that a 17% increase in run-time was reasonable.</p>\n\n<p>So it sounds like the goal of the assignment was to get students to re-order the existing work to reduce instruction-level parallelism or things like that, but it's not a bad thing that people have delved deeper and learned more.</p>\n\n<hr>\n\n<p>Keep in mind that this is a computer-architecture question, not a question about how to make C++ slow in general.</p>\n", "title": "Deoptimizing a program for the pipeline in Intel Sandybridge-family CPUs", "tags": ["c++", "optimization", "x86", "intel", "cpu-architecture"], "answer_count": 3, "link": "http://stackoverflow.com/questions/37361145/deoptimizing-a-program-for-the-pipeline-in-intel-sandybridge-family-cpus", "answers": [{"body": "<p>Important background reading: <strong><a href=\"http://agner.org/optimize/\">Agner Fog's microarch pdf</a></strong>, and probably also Ulrich Drepper's <a href=\"http://www.akkadia.org/drepper/cpumemory.pdf\">What Every Programmer Should Know About Memory</a>.  See also the other links in the <a href=\"/questions/tagged/x86\" class=\"post-tag\" title=\"show questions tagged &#39;x86&#39;\" rel=\"tag\">x86</a> tag wiki, especially Intel's optimization manuals, and David Kanter's <a href=\"http://www.realworldtech.com/haswell-cpu/\">analysis of the Haswell microarchitecture, with diagrams</a>.</p>\n\n<p>Very cool assignment; much better than the ones I've seen where <a href=\"http://stackoverflow.com/a/32001196/224132\">students were asked to optimize some code for <code>gcc -O0</code></a>, learning a bunch of tricks that don't matter in real code.  In this case, you're being asked to learn about the CPU pipeline and use that to guide your de-optimization efforts, not just blind guessing.  <strong>The most fun part of this one is justifying each pessimization with \"diabolical incompetence\", not intentional malice.</strong></p>\n\n<hr>\n\n<p><strong>Problems with the assignment wording and code</strong>:</p>\n\n<p>The uarch-specific options for this code are limited.  It doesn't use any arrays, and much of the cost is calls to <code>exp</code>/<code>log</code> library functions.  There isn't an obvious way to have more or less instruction-level parallelism, and the loop-carried dependency chain is very short.</p>\n\n<p><strong>I'd love to see an answer that attempted to get a slowdown from re-arranging the expressions to change the dependencies, to reduce <a href=\"https://en.wikipedia.org/wiki/Instruction-level_parallelism\">ILP</a> just from dependencies (hazards).</strong>  I haven't attempted it.</p>\n\n<p>Intel Sandybridge-family CPUs are aggressive out-of-order designs that spend lots of transistors and power to find parallelism and avoid hazards (dependencies) that would trouble <a href=\"https://en.wikipedia.org/wiki/Classic_RISC_pipeline\">a classic RISC in-order pipeline</a>.  Usually the only traditional hazards that slow it down are RAW \"true\" dependencies that cause throughput to be limited by latency.</p>\n\n<p><strong><a href=\"https://en.wikipedia.org/wiki/Hazard_(computer_architecture)\">WAR and WAW hazards</a> for registers are pretty much not an issue, thanks to register renaming</strong>.  (except for <code>popcnt</code>/<code>lzcnt</code>/<code>tzcnt</code>, which have a <a href=\"http://stackoverflow.com/questions/25078285/replacing-a-32-bit-loop-count-variable-with-64-bit-introduces-crazy-performance/25089720#25089720\">false dependency their destination on Intel CPUs</a>, even though it's write-only.  i.e. WAW being handled as a RAW hazard + a write).  For memory ordering, modern CPUs use <a href=\"https://en.wikipedia.org/wiki/Memory_disambiguation#Avoiding_WAR_and_WAW_dependencies\">store queues to delay commit into cache until retirement, also avoiding WAR and WAW hazards</a>.</p>\n\n<p>The \"i7\" brand-name was introduced with Nehalem (successor to Core2), and some Intel manuals even say \"Core i7\" when they seem to mean Nehalem, but they kept the \"i7\" branding <a href=\"https://en.wikipedia.org/wiki/Sandy_Bridge\">for Sandybridge</a> and later microarchitectures.  <a href=\"http://www.realworldtech.com/sandy-bridge/\">SnB is when the P6-family evolved into a new species, the SnB-family</a>.  In many ways, Nehalem has more in common with Pentium III than with Sandybridge (e.g. register read stalls and ROB-read stalls don't happen on SnB, because it changed to using a physical register file.  Also a uop cache and a different internal uop format).  <strong>The term \"i7 architecture\" is not useful</strong>, because it makes no sense to group the SnB-family with Nehalem but not Core2.  (Nehalem did introduce the shared inclusive L3 cache architecture for connecting multiple cores together, though.  And also integrated GPUs.  So chip-level, the naming makes more sense.)</p>\n\n<hr>\n\n<h1>Summary of the good ideas that diabolical incompetence can justify</h1>\n\n<p>Even the diabolically incompetent are unlikely to add obviously useless work or an infinite loop, and making a mess with C++/Boost classes is beyond the scope of the assignment.</p>\n\n<ul>\n<li>Multi-thread with a single <em>shared</em> <code>std::atomic&lt;uint64_t&gt;</code> loop counter, so the right total number of iterations happen.  Atomic uint64_t is especially bad with <code>-m32 -march=i586</code>.  For bonus points, arrange for it to be misaligned, and crossing a page boundary with an uneven split (not 4:4).</li>\n<li><strong>False sharing</strong> for some other non-atomic variable -> memory-order mis-speculation pipeline clears, as well as extra cache misses.</li>\n<li>Instead of using <code>-</code> on FP variables, XOR the high byte with 0x80 to flip the sign bit, causing <strong>store-forwarding stalls</strong>.</li>\n<li>Time each iteration independently, with something even heavier than <code>RDTSC</code>.   e.g. <code>CPUID</code> / <code>RDTSC</code> or a time function that makes a system call.  Serializing instructions are inherently pipeline-unfriendly.</li>\n<li>Change multiplies by constants to divides by their reciprocal (\"for ease of reading\").  <strong>div is slow and not fully pipelined.</strong></li>\n<li>Vectorize the multiply/sqrt with AVX (SIMD), but fail to use <code>vzeroupper</code> before calls to scalar math-library <code>exp()</code> and <code>log()</code> functions, causing <strong>AVX&lt;->SSE transition stalls</strong>.</li>\n<li>Store the RNG output in a linked list, or in arrays which you traverse out of order.  Same for the result of each iteration, and sum at the end.</li>\n</ul>\n\n<p>Also covered in this answer but excluded from the summary: suggestions that would be just as slow on a non-pipelined CPU, or that don't seem to be justifiable even with diabolical incompetence.  e.g. many gimp-the-compiler ideas that produce obviously different / worse asm.</p>\n\n<hr>\n\n<h2>Multi-thread badly</h2>\n\n<p>Maybe use OpenMP to multi-thread loops with very few iterations, with way more overhead than speed gain.  Your monte-carlo code has enough parallelism to actually get a speedup, though, esp. if we succeed at making each iteration slow.  (Each thread computes a partial <code>payoff_sum</code>, added at the end).  <code>#omp parallel</code> on that loop would probably be an optimization, not a pessimization.</p>\n\n<p><strong>Multi-thread but force both threads to share the same loop counter (with <code>atomic</code> increments so the total number of iterations is correct).</strong> This seems diabolically logical.  This means using a <code>static</code> variable as a loop counter.  This justifies use of <code>atomic</code> for loop counters, and creates actual <a href=\"http://www.artima.com/cppsource/how_to_go_slow.html\">cache-line ping-ponging</a> (as long as the threads don't run on the same physical core with hyperthreading; that might not be <em>as</em> slow).  Anyway, this is <em>much</em> slower than the un-contended case for <code>lock inc</code>.  And <code>lock cmpxchg8b</code> to atomically increment a contended <code>uint64_t</code> on a 32bit system will have to retry in a loop instead of having the hardware arbitrate an atomic <code>inc</code>.</p>\n\n<p>Also create <strong>false sharing</strong>, where multiple threads keep their private data (e.g. RNG state) in different bytes of the same cache line.  <a href=\"https://software.intel.com/en-us/articles/avoiding-and-identifying-false-sharing-among-threads/\">(Intel tutorial about it, including perf counters to look at)</a>.  <strong>There's a microarchitecture-specific aspect to this</strong>:  Intel CPUs speculate on memory mis-ordering <em>not</em> happening, and there's a <a href=\"http://www.jaist.ac.jp/iscenter-new/mpc/altix/altixdata/opt/intel/vtune/doc/users_guide/mergedProjects/analyzer_ec/mergedProjects/reference_olh/pentium4_hh/er4/memory_order_machine_clear_performance_impact.htm\">memory-order machine-clear perf event to detect this, at least on P4</a>.  The penalty might not be as large on Haswell.  As that link points out, a <code>lock</code>ed instruction assumes this will happen, avoiding mis-speculation.  A normal load speculates that other cores won't invalidate a cache line between when the load executes and when it retires in program-order (<a href=\"http://www.felixcloutier.com/x86/PAUSE.html\">unless you use <code>pause</code></a>).  True sharing without <code>lock</code>ed instructions is usually a bug.  It would be interesting to compare a non-atomic shared loop counter with the atomic case.  To really pessimize, keep the shared atomic loop counter, and cause false sharing in the same or a different cache line for some other variable.</p>\n\n<hr>\n\n<h1>Random uarch-specific ideas:</h1>\n\n<p>If you can introduce <strong>any unpredictable branches</strong>, that will pessimize the code substantially.  Modern x86 CPUs have quite long pipelines, so a mispredict costs ~15 cycles (when running from the uop cache).</p>\n\n<hr>\n\n<h3>Dependency chains:</h3>\n\n<p>I think this was one of the intended parts of the assignment.</p>\n\n<p>Defeat the CPU's ability to exploit instruction-level parallelism by choosing an order of operations that has one long dependency chain instead of multiple short dependency chains.   Compilers aren't allowed to change the order of operations for FP calculations unless you use <code>-ffast-math</code>, because that can change the results (as discussed below).</p>\n\n<p>To really make this effective, increase the length of a loop-carried dependency chain.  Nothing leaps out as obvious, though:  The loops as written have very short loop-carried dependency chains: just an FP add. (3 cycles).  Multiple iterations can have their calculations in-flight at once, because they can start well before the <code>payoff_sum +=</code> at the end of the previous iteration.  (<code>log()</code> and <code>exp</code> take many instructions, but not a lot more than <a href=\"http://www.realworldtech.com/haswell-cpu/3/\">Haswell's out-of-order window for finding parallelism: ROB size=192 fused-domain uops, and scheduler size=60 unfused-domain uops</a>.  As soon as execution of the current iteration progresses far enough to make room for instructions from the next iteration to issue, any parts of it that have their inputs ready (i.e. independent/separate dep chain) can start executing when older instructions leave the execution units free (e.g. because they're bottlenecked on latency, not throughput.).</p>\n\n<p>The RNG state will almost certainly be a longer loop-carried dependency chain than the <code>addps</code>.</p>\n\n<hr>\n\n<h3>Use slower/more FP operations (esp. more division):</h3>\n\n<p>Divide by 2.0 instead of multiplying by 0.5, and so on.  FP multiply is heavily pipelined in Intel designs, and has one per 0.5c throughput on Haswell and later.  <strong>FP <code>divsd</code>/<code>divpd</code> is only partially pipelined</strong>.  (Although Skylake has an impressive one per 4c throughput for <code>divpd xmm</code>, with 13-14c latency, vs not pipelined at all on Nehalem (7-22c)).</p>\n\n<p>The <code>do { ...; euclid_sq = x*x + y*y;  } while (euclid_sq &gt;= 1.0);</code> is clearly testing for a distance, so clearly it would be proper to <code>sqrt()</code> it.  :P  (<code>sqrt</code> is even slower than <code>div</code>).</p>\n\n<p>As @Paul Clayton suggests, rewriting expressions with associative/distributive equivalents can introduce more work (as long as you don't use <code>-ffast-math</code> to allow the compiler to re-optimize).  <code>(exp(T*(r-0.5*v*v))</code> could become <code>exp(T*r - T*v*v/2.0)</code>.  Note that while math on real numbers is associative, <a href=\"http://stackoverflow.com/a/36526143/224132\">floating point math is <em>not</em></a>, even without considering overflow/NaN (which is why <code>-ffast-math</code> isn't on by default).  See <a href=\"http://stackoverflow.com/questions/37361145/deoptimizing-a-c-program/37362225?noredirect=1#comment62243542_37362225\">Paul's comment</a> for a very hairy nested <code>pow()</code> suggestion.</p>\n\n<p>If you can scale the calculations down to very small numbers, then FP math ops take <strong>~120 extra cycles to trap to microcode when an operation on two normal numbers produces a denormal</strong>.  See Agner Fog's microarch pdf for the exact numbers and details.  This is unlikely since you have a lot of multiplies, so the scale factor would be squared and underflow all the way to 0.0.  I don't see any way to justify the necessary scaling with incompetence (even diabolical), only intentional malice.</p>\n\n<hr>\n\n<h3>If you can use intrinsics (<code>&lt;immintrin.h&gt;</code>)</h3>\n\n<p><a href=\"http://stackoverflow.com/questions/35516878/acquire-release-semantics-with-non-temporal-stores-on-x64#comment58847294_35571387\">Use <code>movnti</code> to evict your data from cache</a>.  Diabolical: it's new and weakly-ordered, so that should let the CPU run it faster, right?  Or see that linked question for a case where someone was in danger of doing exactly this (for scattered writes where only some of the locations were hot).  <code>clflush</code> is probably impossible without malice.</p>\n\n<p>Use integer shuffles between FP math operations to cause bypass delays.</p>\n\n<p><strong><a href=\"https://software.intel.com/en-us/articles/intel-avx-state-transitions-migrating-sse-code-to-avx\">Mixing SSE and AVX instructions without proper use of <code>vzeroupper</code> causes large stalls in pre-Skylake</a></strong> (and a different penalty <a href=\"http://www.agner.org/optimize/blog/read.php?i=415#477\">in Skylake</a>).  Even without that, vectorizing badly can be worse than scalar (more cycles spent shuffling data into/out of vectors than saved by doing the add/sub/mul/div/sqrt operations for 4 Monte-Carlo iterations at once, with 256b vectors).  add/sub/mul execution units are fully pipelined and full-width, but div and sqrt on 256b vectors aren't as fast as on 128b vectors (or scalars), so the speedup isn't dramatic for <code>double</code>.</p>\n\n<p><code>exp()</code> and <code>log()</code> don't have hardware support, so that part would require extracting vector elements back to scalar and calling the library function separately, then shuffling the results back into a vector.  libm is typically compiled to only use SSE2, so will use the legacy-SSE encodings of scalar math instructions.  If your code uses 256b vectors and calls <code>exp</code> without doing a <code>vzeroupper</code> first, then you stall.  After returning, an AVX-128 instruction like <code>vmovsd</code> to set up the next vector element as an arg for <code>exp</code> will also stall.  And then <code>exp()</code> will stall again when it runs an SSE instruction.  <strong>This is exactly what happened <a href=\"http://stackoverflow.com/questions/21960229/unexpectedly-good-performance-with-openmp-parallel-for-loop\">in this question</a>, causing a 10x slowdown.</strong>  (Thanks @ZBoson).</p>\n\n<p>See also <a href=\"https://news.ycombinator.com/item?id=11750294\">Nathan Kurz's experiments with Intel's math lib vs. glibc for this code</a>.  Future glibc will come with <a href=\"https://sourceware.org/glibc/wiki/libmvec\">vectorized implementations of <code>exp()</code> and so on.</a></p>\n\n<hr>\n\n<p>If targeting pre-IvB, or esp. Nehalem, try to get gcc to cause partial-register stalls with 16bit or 8bit operations followed by 32bit or 64bit operations.  In most cases, gcc will use <code>movzx</code> after an 8 or 16bit operation, but <a href=\"http://stackoverflow.com/questions/37256507/gcc-inline-assembly-whats-wrong-with-the-dynamic-allocated-register-r-in-inp/37267621#37267621\">here's a case where gcc modifies <code>ah</code> and then reads <code>ax</code></a></p>\n\n<hr>\n\n<h2>With (inline) asm:</h2>\n\n<p>With (inline) asm, you could break the uop cache: A 32B chunk of code that doesn't fit in three 6uop cache lines forces a switch from the uop cache to the decoders.  An incompetent <code>ALIGN</code> using many single-byte <code>nop</code>s instead of a couple long <code>nop</code>s on a branch target inside the inner loop might do the trick.  Or put the alignment padding after the label, instead of before. :P  This only matters if the frontend is a bottleneck, which it won't be if we succeeded at pessimizing the rest of the code.</p>\n\n<p>Use self-modifying code to trigger pipeline clears (aka machine-nukes).</p>\n\n<p><a href=\"https://software.intel.com/en-us/forums/intel-performance-bottleneck-analyzer/topic/328256\">LCP stalls</a> from 16bit instructions with immediates too large to fit in 8 bits are unlikely to be useful.  The uop cache on SnB and later means you only pay the decode penalty once.  On Nehalem (the first i7), it might work for a loop that doesn't fit in the 28 uop loop buffer.  gcc will sometimes generate such instructions, even with <code>-mtune=intel</code> and when it could have used a 32bit instruction.</p>\n\n<hr>\n\n<p><a href=\"http://stackoverflow.com/questions/12065721/why-isnt-rdtsc-a-serializing-instruction\">A common idiom for timing is <code>CPUID</code>(to serialize) then <code>RDTSC</code></a>.  Time every iteration separately with a <code>CPUID</code>/<code>RDTSC</code> to make sure the <code>RDTSC</code> isn't reordered with earlier instructions will be slow things down a <em>lot</em>.  (In real life, the smart way to time is to time all the iterations together, instead of timing each separately and adding them up).</p>\n\n<hr>\n\n<h1>Cause lots of cache misses and other memory slowdowns</h1>\n\n<p>Use a <code>union { double d; char a[8]; }</code> for some of your variables.   <a href=\"https://en.wikipedia.org/wiki/Memory_disambiguation#Store_to_load_forwarding\"><strong>Cause a store-forwarding stall</strong></a> by doing a narrow store (or Read-Modify-Write) to just one of the bytes.  (That wiki article also covers a lot of other microarchitectural stuff for load/store queues).  e.g. <strong>flip the sign of a <code>double</code> using XOR 0x80 on just the high byte</strong>, instead of a <code>-</code> operator.  The diabolically incompetent developer may have heard that FP is slower than integer, and thus try to do as much as possible using integer ops.  (A very good compiler targeting FP math in SSE registers may possibly compile this to an <code>xorps</code> with a constant in another xmm register, but the only way this isn't terrible for x87 is if the compiler realizes that it's negating the value and replaces the next add with a subtract.)</p>\n\n<hr>\n\n<p>Use <code>volatile</code> if you're compiling with <code>-O3</code> and not using <code>std::atomic</code>, to force the compiler to actually store/reload all over the place.  Global variables (instead of locals) will also force some stores/reloads, but <a href=\"http://preshing.com/20120930/weak-vs-strong-memory-models/\">the C++ memory model's weak ordering</a> doesn't require the compiler to spill/reload to memory all the time.</p>\n\n<p><strong>Replace local vars with members of a big struct, so you can control the memory layout.</strong></p>\n\n<p>Use arrays in the struct for padding (and storing random numbers, to justify their existence).</p>\n\n<p>Choose your memory layout so <a href=\"http://stackoverflow.com/questions/37099454/force-to-l1-cache-miss\">everything goes into a different line in the same \"set\" in the L1 cache</a>.  It's only 8-way associative, i.e. each set has 8 \"ways\".  Cache lines are 64B.</p>\n\n<p>Even better, <strong>put things exactly 4096B apart, since loads have a false dependency on stores to different pages but with the same offset within a page</strong>.  Aggressive out-of-order CPUs use <a href=\"https://en.wikipedia.org/wiki/Memory_disambiguation\">Memory Disambiguation to figure out when loads and stores can be reordered without changing the results</a>, and Intel's implementation has false-positives that prevent loads from starting early.  Probably they only check bits below the page offset, so the check can start before the TLB has translated the high bits from a virtual page to a physical page.  As well as Agner's guide, see <a href=\"http://stackoverflow.com/questions/7905760/matrix-multiplication-small-difference-in-matrix-size-large-difference-in-timi/7905949#7905949\">an answer from Stephen Canon</a>, and also a section near the end of @Krazy Glew's answer on the same question.  (Andy Glew was one of the architects of Intel's original P6 microarchitecture.)</p>\n\n<p>Use <code>__attribute__((packed))</code> to let you mis-align variables so they span cache-line or even page boundaries.  (So a load of one <code>double</code> needs data from two cache-lines).  Misaligned loads have no penalty in any Intel i7 uarch, except when crossing cache lines and page lines.  <a href=\"http://www.agner.org/optimize/blog/read.php?i=415#423\">Cache-line splits still take extra cycles</a>.  Skylake dramatically reduces the penalty for page split loads, <a href=\"http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-optimization-manual.pdf#page=35\">from 100 to 5 cycles. (Section 2.1.3)</a>.  Perhaps related to being able to do two page walks in parallel.</p>\n\n<p><strong>A page-split on an <code>atomic&lt;uint64_t&gt;</code> should be just about the worst case</strong>, esp. if it's 5 bytes in one page and 3 bytes in the other page, or anything other than 4:4.  Even splits down the middle are more efficient for cache-line splits with 16B vectors on some uarches, IIRC.  Put everything in a <code>alignas(4096) struct __attribute((packed))</code> (to save space, of course), including an array for storage for the RNG results.  Achieve the misalignment by using <code>uint8_t</code> or <code>uint16_t</code> for something before the counter.</p>\n\n<p>If you can get the compiler to use indexed addressing modes, that will <a href=\"http://stackoverflow.com/questions/26046634/micro-fusion-and-addressing-modes/31027695#31027695\">defeat uop micro-fusion</a>.  Maybe by using <code>#define</code>s to replace simple scalar variables with <code>my_data[constant]</code>.</p>\n\n<p>If you can introduce an extra level of indirection, so load/store addresses aren't known early, that can pessimize further.</p>\n\n<hr>\n\n<h3>Traverse arrays in non-contiguous order</h3>\n\n<p>I think we can come up with incompetent justification for introducing an array in the first place:  It lets us separate the random number generation from the random number use.  Results of each iteration could also be stored in an array, to be summed later (with more diabolical incompetence).</p>\n\n<p>For \"maximum randomness\", we could have a thread looping over the random array writing new random numbers into it.  The thread consuming the random numbers could generate a random index to load a random number from.  (There's some make-work here, but microarchitecturally it helps for load-addresses to be known early so any possible load latency can be resolved before the loaded data is needed.)  Having a reader and writer on different cores will cause memory-ordering mis-speculation pipeline clears (as discussed earlier for the false-sharing case).</p>\n\n<p>For maximum pessimization, loop over your array with a stride of 4096 bytes (i.e. 512 doubles).  e.g. </p>\n\n<pre><code>for (int i=0 ; i&lt;512; i++)\n    for (int j=i ; j&lt;UPPER_BOUND ; j+=512)\n        monte_carlo_step(rng_array[j]);\n</code></pre>\n\n<p>So the access pattern is 0, 4096, 8192, ...,<br>\n8, 4104, 8200, ...<br>\n16, 4112, 8208, ...</p>\n\n<p>This is what you'd get for accessing a 2D array like <code>double rng_array[MAX_ROWS][512]</code> in the wrong order (looping over rows, instead of columns within a row in the inner loop, as suggested by @JesperJuhl).  If diabolical incompetence can justify a 2D array with dimensions like that, garden variety real-world incompetence easily justifies looping with the wrong access pattern.  This happens in real code in real life.</p>\n\n<p>Adjust the loop bounds if necessary to use many different pages instead of reusing the same few pages, if the array isn't that big.  Hardware prefetching doesn't work (as well/at all) across pages.  The prefetcher can track one forward and one backward stream within each page (which is what happens here), but will only act on it if the memory bandwidth isn't already saturated with non-prefetch.</p>\n\n<p>This will also generate lots of TLB misses, unless the pages get merged into a hugepage (<a href=\"https://www.kernel.org/doc/Documentation/vm/transhuge.txt\">Linux does this opportunistically for anonymous (not file-backed) allocations like <code>malloc</code>/<code>new</code> that use <code>mmap(MAP_ANONYMOUS)</code></a>).</p>\n\n<p>Instead of an array to store the list of results, you could use a <strong>linked list</strong>.  Then every iteration would require a pointer-chasing load (a RAW true dependency hazard for the load-address of the next load).  With a bad allocator, you might manage to scatter the list nodes around in memory, defeating cache.  With a diabolically incompetent allocator, it could put every node at the beginning of its own page.  (e.g. allocate with <code>mmap(MAP_ANONYMOUS)</code> directly, without breaking up pages or tracking object sizes to properly support <code>free</code>).</p>\n\n<hr>\n\n<p>These aren't really microarchitecture-specific, and have little to do with the pipeline (most of these would also be a slowdown on a non-pipelined CPU).</p>\n\n<h3>Somewhat off-topic: make the compiler generate worse code / do more work:</h3>\n\n<p>Use C++11 <code>std::atomic&lt;int&gt;</code> and <code>std::atomic&lt;double&gt;</code> for the most pessimal code.  The MFENCEs and <code>lock</code>ed instructions are quite slow even without contention from another thread.</p>\n\n<p><code>-m32</code> will make slower code, because x87 code will be worse than SSE2 code.  The stack-based 32bit calling convention takes more instructions, and passes even FP args on the stack to functions like <code>exp()</code>.  <a href=\"http://gcc.godbolt.org/#compilers:!((compiler:g530,options:&#39;-std%3Dgnu%2B%2B11+-O3+-Wall+-fverbose-asm+-m32+-march%3Di586&#39;,source:&#39;%23include+%3Catomic%3E%0A%23include+%3Cstdint.h%3E%0A%0A//+-march%3Di386+just+calls+the+libgcc+helper+functions,+which+were+presumably+compiled+with+the+default+-march%0A//+-mi586+use+a+lock+cmpxchg8b+loop%0Astd::atomic%3Cuint64_t%3E+ll(0)%3B%0Avoid+foo_ll(void)+%7B+ll%2B%2B%3B+%7D%0A%0A//+-mfpmath%3Dsse+isn!&#39;t+the+default+on+-m32%0A//+gcc+-m32!&#39;s+trick+for+atomic+64bit+load/store+is+to+use+fild/fistp+to+bounce+data+to+the+stack.%0A//+in+this+case,+the+bit+pattern+being+moved+is+already+FP+data,+so+it+should+just+fld/fstp+from+d+directly%0A//+reported+as+https://gcc.gnu.org/bugzilla/show_bug.cgi%3Fid%3D71245%0Astd::atomic%3Cdouble%3E+d(5.0)%3B%0Avoid+foo_d(void)+%7B%0A++d+%3D++d+%2B+1.0%3B%0A++//+d%2B%3D1.0%3B++//+unimplemented%0A%7D%0A%0Astd::atomic%3Clong+double%3E+ld(5.0)%3B%0Avoid+foo_ld(void)+%7B+ld+%3D+ld+%2B+1.0%3B+%7D%0A&#39;)),filterAsm:(commentOnly:!t,directives:!t,intel:!t,labels:!t),version:3\"><code>atomic&lt;uint64_t&gt;::operator++</code> on <code>-m32</code> requires a <code>lock cmpxchg8B</code> loop</a> (i586).  (So use that for loop counters!  [Evil laugh]).</p>\n\n<p><code>-march=i386</code> will also pessimize (thanks @Jesper).  FP compares with <code>fcom</code> are slower than 686 <code>fcomi</code>.  Pre-586 doesn't provide an atomic 64bit store, (let alone a cmpxchg), so all 64bit <code>atomic</code> ops compile to libgcc function calls (which is probably compiled for i686, rather than actually using a lock).  Try it on the Godbolt Compiler Explorer link in the last paragraph.</p>\n\n<p>Use <code>long double</code> / <code>sqrtl</code> / <code>expl</code> for extra precision and extra slowness in ABIs where sizeof(<code>long double</code>) is 10 or 16 (with padding for alignment).  (IIRC, 64bit Windows uses 8byte <code>long double</code> equivalent to <code>double</code>.  (Anyway, load/store of 10byte (80bit) FP operands is 4 / 7 uops, vs. <code>float</code> or <code>double</code> only taking 1 uop each for <code>fld m64/m32</code>/<code>fst</code>).  Forcing x87 with <code>long double</code> defeats auto-vectorization even for gcc <code>-m64 -march=haswell -O3</code>.</p>\n\n<p>If not using <code>atomic&lt;uint64_t&gt;</code> loop counters, use <code>long double</code> for everything, including loop counters.</p>\n\n<p><code>atomic&lt;double&gt;</code> compiles, but read-modify-write operations like <code>+=</code> aren't supported for it (even on 64bit).  <code>atomic&lt;long double&gt;</code> has to call a library function just for atomic loads/stores.  It's probably really inefficient, <a href=\"http://stackoverflow.com/questions/36624881/why-is-integer-assignment-on-a-naturally-aligned-variable-atomic/36685056#36685056\">because the x86 ISA doesn't naturally support atomic 10byte loads/stores</a>, and the only way I can think of without locking (<code>cmpxchg16b</code>) requires 64bit mode.</p>\n\n<hr>\n\n<p>At <code>-O0</code>, breaking up a big expression by assigning parts to temporary vars will cause more store/reloads.  Without <code>volatile</code> or something, this won't matter with optimization settings that a real build of real code would use.</p>\n\n<p>C aliasing rules allow a <code>char</code> to alias anything, so storing through a <code>char*</code> forces the compiler to store/reload everything before/after the byte-store, even at <code>-O3</code>.  (This is a problem for auto-vectorizing <a href=\"http://stackoverflow.com/questions/37351236/vectorize-a-function-in-clang\">code that operates on an array of <code>uint8_t</code></a>, for example.)</p>\n\n<p>Try <code>uint16_t</code> loop counters, to force truncation to 16bit, probably by using 16bit operand-size (potential stalls) and/or extra <code>movzx</code> instructions (safe).  <a href=\"http://stackoverflow.com/questions/37325524/does-integer-overflow-cause-undefined-behavior-because-of-memory-corruption\">Signed overflow is undefined behaviour</a>, so unless you use <code>-fwrapv</code> or at least <code>-fno-strict-overflow</code>, <a href=\"http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html\">signed loop counters don't have to be re-sign-extended every iteration</a>, even if used as offsets to 64bit pointers.</p>\n\n<hr>\n\n<p>Force conversion from integer to <code>float</code> and back again.  And/or <code>double</code>&lt;=><code>float</code> conversions.  The instructions have greater-than-one latency, and scalar int->float (<code>cvtsi2ss</code>) is badly designed to not zero the rest of the xmm register.  (gcc inserts an extra <code>pxor</code> to break dependencies, for this reason.)</p>\n\n<hr>\n\n<p>Frequently <strong>set your CPU affinity to a different CPU</strong> (suggested by @Egwor).  diabolical reasoning: You don't want one core to get overheated from running your thread for a long time, do you?  Maybe swapping to another core will let that core turbo to a higher clock speed.  (In reality: they're so thermally close to each other that this is highly unlikely except in a multi-socket system).  Now just get the tuning wrong and do it way too often.  Besides the time spent in the OS saving/restoring thread state, the new core has cold L2/L1 caches, uop cache, and branch predictors.</p>\n\n<p>Introducing frequent unnecessary system calls can slow you down no matter what they are.  Although some important but simple ones like <code>gettimeofday</code> may be implemented in user-space with, with no transition to kernel mode.  (glibc on Linux does this with the kernel's help, since the kernel exports code in the <code>vdso</code>).</p>\n", "title": "Deoptimizing a program for the pipeline in Intel Sandybridge-family CPUs", "answer_id": 37362225, "question_id": 37361145, "tags": ["c++", "optimization", "x86", "intel", "cpu-architecture"]}, {"body": "<p>A few things that you can do to make things perform as bad as possible:</p>\n\n<ul>\n<li><p>compile the code for the i386 architecture. This will prevent the use of SSE and newer instructions and force the use of the x87 FPU.</p></li>\n<li><p>use <code>std::atomic</code> variables everywhere. This will make them very expensive due to the compiler being forced to insert memory barriers all over the place. And this is something an incompetent person might plausibly do to \"ensure thread safety\".</p></li>\n<li><p>make sure sure to access memory in the worst possible way for the prefetcher to predict (column major vs row major).</p></li>\n<li><p>to make your variables extra expensive you could make sure they all have 'dynamic storage duration' (heap allocated) by allocating them with <code>new</code> rather than letting them have 'automatic storage duration' (stack allocated).</p></li>\n</ul>\n\n<p>Note: This answer basically just summarizes my comments that @Peter Cordes already incorporated into his very good answer. Suggest he get's your upvote if you only have one to spare :)</p>\n", "title": "Deoptimizing a program for the pipeline in Intel Sandybridge-family CPUs", "answer_id": 37363240, "question_id": 37361145, "tags": ["c++", "optimization", "x86", "intel", "cpu-architecture"]}, {"body": "<p>You can use <code>long double</code> for computation. On x86 it should be the 80-bit format. Only the legacy, x87 FPU has support for this.</p>\n\n<p>Few shortcomings of x87 FPU:</p>\n\n<ol>\n<li>Lack of SIMD, may need more instructions.</li>\n<li>Stack based, problematic for super scalar and pipelined architectures.</li>\n<li>Separate and quite small set of registers, may need more conversion from other registers and more memory operations.</li>\n<li>On the Core i7 there are 3 ports for SSE and only 2 for x87, the processor can execute less parallel instructions.</li>\n</ol>\n", "title": "Deoptimizing a program for the pipeline in Intel Sandybridge-family CPUs", "answer_id": 37367727, "question_id": 37361145, "tags": ["c++", "optimization", "x86", "intel", "cpu-architecture"]}], "question_id": 37361145}, {"body": "<p><a href=\"https://twitter.com/weitzelb/status/718623065480019968\">A recent tweet</a> contained this snippet of JavaScript.</p>\n\n<p>Can someone please explain what is happening in it step by step?</p>\n\n<pre><code>&gt; function dis() { return this }\nundefined\n&gt; five = dis.call(5)\nNumber {[[PrimitiveValue]]: 5}\n&gt; five.wtf = 'potato'\n\"potato\"\n&gt; five.wtf\n\"potato\"\n&gt; five * 5\n25\n&gt; five.wtf\n\"potato\"\n&gt; five++\n5\n&gt; five.wtf\nundefined\n&gt; five.wtf = 'potato?'\n\"potato?\"\n&gt; five.wtf\nundefined\n&gt; five\n6\n</code></pre>\n\n<p>In particular, it is not clear to me:</p>\n\n<ul>\n<li>why the result of <code>dis.call(5)</code> is a <code>Number</code> with some kind of a <code>[[PrimitiveValue]]</code> property, but the results of <code>five++</code> and <code>five * 5</code> appear to just be the plain numbers <code>5</code> and <code>25</code> (not <code>Number</code>s)</li>\n<li>why the <code>five.wtf</code> property disappears after the <code>five++</code> increment</li>\n<li>why the <code>five.wtf</code> property is no longer even settable after the <code>five++</code> increment, despite the <code>five.wtf = 'potato?'</code> assignment apparently setting the value</li>\n</ul>\n", "title": "What is happening in this JavaScript snippet?", "tags": ["javascript"], "answer_count": 10, "link": "http://stackoverflow.com/questions/38637003/what-is-happening-in-this-javascript-snippet", "answers": [{"body": "<p>It's pretty simple.</p>\n\n<pre><code>function dis () { return this; }\n</code></pre>\n\n<p>This returns the <code>this</code> context. So, if you do <code>call(5)</code> you're passing the number as an object.</p>\n\n<p>The <code>call</code> function doesn't supply arguments, the first argument you give is the context of <code>this</code>. Usually if you want it on it's on context, you give it <code>{}</code> so <code>dis.call({})</code>, which means <code>this</code> in the function is an empty <code>this</code>. However, if you pass <code>5</code> it seems it will be converted to an object. See <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/call#Parameters\">.call</a></p>\n\n<p>So the return is <code>object</code></p>\n\n<p>When you do <code>five * 5</code>, JavaScript sees the object <code>five</code> as the primitive type, so is equivalent to <code>5 * 5</code>. Interestingly, do <code>'5' * 5</code>, it still equals <code>25</code>, so JavaScript is clearly casting under the hood. <strong>No changes to the underlying <code>five</code> type is done on this line</strong></p>\n\n<p>But when you do <code>++</code> it will convert the object to the primitive <code>number</code> type thus removing the <code>.wtf</code> property. <strong>Because you are affecting the underlying type</strong></p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38637228, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p>There are two different ways to represent a number:</p>\n\n<pre><code>var a = 5;\nvar b = new Number(5);\n</code></pre>\n\n<p>The first is a primitive, the second an object. For all intents and purposes both behave the same, except they look different when printed to the console. One important difference is that, as an object, <code>new Number(5)</code> accepts new properties just like any plain <code>{}</code>, while the primitive <code>5</code> does not:</p>\n\n<pre><code>a.foo = 'bar';  // doesn't stick\nb.foo = 'bar';  // sticks\n</code></pre>\n\n<p>As for the initial <code>dis.call(5)</code> part, please see <a href=\"http://stackoverflow.com/q/3127429/476\">How does the &quot;this&quot; keyword work?</a>. Let's just say that the first argument to <code>call</code> is used as the value of <code>this</code>, and that this operation forces the number into the more complex <code>Number</code> object form.* Later on <code>++</code> forces it back into the primitive form, because the addition operation <code>+</code> results in a new primitive.</p>\n\n<pre><code>&gt; five = dis.call(5)  // for all intents and purposes same as new Number(5)\nNumber {[[PrimitiveValue]]: 5}\n&gt; five.wtf = 'potato'\n\"potato\"\n&gt; five.wtf\n\"potato\"\n</code></pre>\n\n<p>A <code>Number</code> object accepts new properties.</p>\n\n<pre><code>&gt; five++\n</code></pre>\n\n<p><code>++</code> results in a new primitive <code>6</code> value...</p>\n\n<pre><code>&gt; five.wtf\nundefined\n&gt; five.wtf = 'potato?'\n\"potato?\"\n&gt; five.wtf\nundefined\n</code></pre>\n\n<p>...which does not have and does not accept custom attributes.</p>\n\n<p>* Note that in <em>strict mode</em> the <code>this</code> argument would be treated differently and would <em>not</em> be converted to a <code>Number</code>. See <a href=\"http://es5.github.io/#x10.4.3\">http://es5.github.io/#x10.4.3</a> for the implementation details.</p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38637551, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p>First off it looks like this is being run through the nodejs console.</p>\n\n<p>1.</p>\n\n<pre><code>    function dis() { return this }\n</code></pre>\n\n<p>creates the function dis(), but because it wasn't set as a <code>var</code> there was no value to return so <code>undefined</code> was the output, even though <code>dis()</code> was defined. On a sidenote, <code>this</code> wasn't returned because the function wasn't executed.</p>\n\n<p>2.</p>\n\n<pre><code>    five = dis.call(5)\n</code></pre>\n\n<p>This returns javascript's <code>Number</code> object because you just set the function <code>dis()</code>'s <code>this</code> value to the primitive five.</p>\n\n<p>3.</p>\n\n<pre><code>   five.wtf = 'potato'\n</code></pre>\n\n<p>The first returns <code>\"potato\"</code> because you just set the property <code>wtf</code> of <code>five</code> to <code>'potato'</code>. Javascript returns the value of a variable you set, making it easy to chain multiple variables and set them to the same value like this: <code>a = b = c = 2</code>.</p>\n\n<p>4.</p>\n\n<pre><code>    five * 5\n</code></pre>\n\n<p>This returns <code>25</code> because you just multiplied the primitive number <code>5</code> to <code>five</code>. The value of <code>five</code> was determined by the value of the <code>Number</code> object.</p>\n\n<p>5.</p>\n\n<pre><code>    five.wtf\n</code></pre>\n\n<p>I skipped this line before because I would be repeating it here.\nIt just returns the value of the property <code>wtf</code> that you set above.</p>\n\n<p>6.</p>\n\n<pre><code>    five++\n</code></pre>\n\n<p>As @Callum said, <code>++</code> will convert the type to <code>number</code> from the same value from the object <code>Number {[[PrimitiveValue]]: 5}}</code>.</p>\n\n<p>Now because <code>five</code> is a <code>number</code>, you can't set properties to it anymore until you do something like this:</p>\n\n<pre><code>    five = dis.call(five)\n    five.wtf = \"potato?\"\n</code></pre>\n\n<p>or</p>\n\n<pre><code>    five = { value: 6, wtf: \"potato?\" }\n</code></pre>\n\n<p>Also note that the second way will have different behavior than using the first method because it is defining a generic object instead of the <code>Number</code> object that was created before.</p>\n\n<p>I hope this helps, javascript likes to assume things, so it can get confusing when changing around from the <code>Number</code> object to a primitive <code>number</code>. \nYou can check what type something is by using the <code>typeof</code> keyword, writing\n        typeof five\nafter you initialize it returns <code>'object'</code>, and after you do <code>five++</code> it returns <code>'number'</code>.</p>\n\n<p>@deceze describes the difference between the Number object and the primitive number extremely well.</p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38637585, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p>The primitive values can't have property. But when you try to access to a property on primitive value, it transparently transtype to a temporary Number object.</p>\n\n<p>So:</p>\n\n<pre><code>&gt; function dis() { return this }\nundefined\n// Like five.dis(), so dis return the temporaty Number object and \n// reference it in five\n&gt; five = dis.call(5)\nNumber {[[PrimitiveValue]]: 5}\n\n// Write the wtf attribut on the Number object referenced by five\n&gt; five.wtf = 'potato'\n\"potato\"\n// Read the wtf attribut on the Number object referenced by five\n&gt; five.wtf\n\"potato\"\n\n// Return 5*5 but dont change the reference of five\n&gt; five * 5\n25\n// Read the same wtf attribut on the Number object referenced by five\n&gt; five.wtf\n\"potato\"\n\n// Change the five reference to a new primitive value (5+1). Five\n// reference a primitive now.\n&gt; five++\n5\n\n// Read the wtf attribut on a new temporary Number object construct from\n// the primitive referenced by five. So wtf does not exist.\n&gt; five.wtf\nundefined\n\n// Write the wtf attribut on a new temporary Number object construct from\n// the primitive referenced by five. But this object not referenced by\n// five. It will be lost.\n&gt; five.wtf = 'potato?'\n\"potato?\"\n\n// Read the wtf attribut on a new temporary Number object construct from\n// the primitive referenced by five. So wtf does not exist.\n&gt; five.wtf\nundefined\n&gt; five\n6\n</code></pre>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38637657, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p>Declare function <code>dis</code>. Function returns its context</p>\n\n<pre><code>function dis() { return this }\nundefined\n</code></pre>\n\n<p>Call <code>dis</code> with context <code>5</code>. Primitive values are boxed when passed as context in strict mode (<a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Strict_mode#Securing_JavaScript\">MDN</a>). So <code>five</code> now is object (boxed number).</p>\n\n<pre><code>five = dis.call(5)\nNumber {[[PrimitiveValue]]: 5}\n</code></pre>\n\n<p>Declare <code>wtf</code> property on <code>five</code> variable</p>\n\n<pre><code>five.wtf = 'potato'\n\"potato\"\n</code></pre>\n\n<p>Value of <code>five.wtf</code></p>\n\n<pre><code>five.wtf\n\"potato\"\n</code></pre>\n\n<p><code>five</code> is boxed <code>5</code>, so it's number and object at the same time (5 * 5 = 25). It doesn't changes <code>five</code>.</p>\n\n<pre><code>five * 5\n25\n</code></pre>\n\n<p>Value of <code>five.wtf</code></p>\n\n<pre><code>five.wtf\n\"potato\"\n</code></pre>\n\n<p>Unboxing <code>five</code> here. <code>five</code> now is just primitive <code>number</code>. It prints <code>5</code>, and then add <code>1</code> to <code>five</code>.</p>\n\n<pre><code>five++\n5\n</code></pre>\n\n<p><code>five</code> is primitive number <code>6</code> now, there are no properties in it.</p>\n\n<pre><code>five.wtf\nundefined\n</code></pre>\n\n<p>primitives cannot have properties, you can't set this</p>\n\n<pre><code>five.wtf = 'potato?'\n\"potato?\"\n</code></pre>\n\n<p>you can't read this, because it was not set</p>\n\n<pre><code>five.wtf\nundefined\n</code></pre>\n\n<p><code>five</code> is <code>6</code> because of post incrementing above</p>\n\n<pre><code>five\n6\n</code></pre>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38638027, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<pre><code>01 &gt; function dis() { return this }\n02 undefined\n03 &gt; five = dis.call(5)\n04 Number {[[PrimitiveValue]]: 5}\n05 &gt; five.wtf = 'potato'\n06 \"potato\"\n07 &gt; five.wtf\n08 \"potato\"\n09 &gt; five * 5\n10 25\n11 &gt; five.wtf\n12 \"potato\"\n13 &gt; five++\n14 5\n15 &gt; five.wtf\n16 undefined\n17 &gt; five.wtf = 'potato?'\n18 \"potato?\"\n19 &gt; five.wtf\n20 undefined\n21 &gt; five\n22 6\n</code></pre>\n\n<p><code>01</code> declares a function <code>dis</code> that returns the context object. What <code>this</code> represents changes depending on whether you're using strict mode or not. The entire example has different results if the function were declared as:</p>\n\n<pre><code>&gt; function dis() { \"use strict\"; return this }\n</code></pre>\n\n<p>This is detailed in <a href=\"http://es5.github.io/#x10.4.3\">section 10.4.3 in the ES5 specification</a></p>\n\n<blockquote>\n  <ol>\n  <li>If the function code is strict code, set the ThisBinding to thisArg.</li>\n  <li>Else if thisArg is null or undefined, set the ThisBinding to the global object.</li>\n  <li>Else if Type(thisArg) is not Object, set the ThisBinding to ToObject(thisArg).</li>\n  </ol>\n</blockquote>\n\n<p><code>02</code> is the return value of the function declaration. <code>undefined</code> should be self explanatory here.</p>\n\n<p><code>03</code> the variable <code>five</code> is initialized with the return value of <code>dis</code> when called in the context of the primitive value <code>5</code>. Because <code>dis</code> is not in strict mode, this line is identical to calling <code>five = Object(5)</code>.</p>\n\n<p><code>04</code> The odd <code>Number {[[PrimitiveValue]]: 5}</code> return value is the representation of the object that wraps the primitive value <code>5</code></p>\n\n<p><code>05</code> the <code>five</code> object's <code>wtf</code> property is assigned a string value of <code>'potato'</code></p>\n\n<p><code>06</code> is the return value of the assignment and should be self explanatory.</p>\n\n<p><code>07</code> the <code>five</code> object's <code>wtf</code> property is being examined</p>\n\n<p><code>08</code> as <code>five.wtf</code> was previously set to <code>'potato'</code> it returns <code>'potato'</code> here</p>\n\n<p><code>09</code> the <code>five</code> object is being multiplied by the primitive value <code>5</code>. This is no different from any other object being multiplied and is explained in <a href=\"http://es5.github.io/#x11.5\">section 11.5 of the ES5 specification</a>. Of particular note is how objects are cast to numeric values, which is covered in a few sections.</p>\n\n<p><a href=\"http://es5.github.io/#x9.3\">9.3 ToNumber</a>:</p>\n\n<blockquote>\n  <ol>\n  <li>Let primValue be ToPrimitive(input argument, hint Number).</li>\n  <li>Return ToNumber(primValue).</li>\n  </ol>\n</blockquote>\n\n<p><a href=\"http://es5.github.io/#x9.1\">9.1 ToPrimitive</a>:</p>\n\n<blockquote>\n  <p>Return a default value for the Object. The default value of an object is retrieved by calling the [[DefaultValue]] internal method of the object, passing the optional hint PreferredType. The behaviour of the [[DefaultValue]] internal method is defined by this specification for all native ECMAScript objects in <a href=\"http://es5.github.io/#x8.12.8\">8.12.8</a>.</p>\n</blockquote>\n\n<p><a href=\"http://es5.github.io/#x8.12.8\">8.12.8 [[DefaultValue]]</a>:</p>\n\n<blockquote>\n  <p>Let valueOf be the result of calling the [[Get]] internal method of object O with argument \"valueOf\".</p>\n  \n  <ol>\n  <li><p>If IsCallable(valueOf) is true then,</p>\n  \n  <ol>\n  <li>Let val be the result of calling the [[Call]] internal method of valueOf, with O as the this value and an empty argument list.</li>\n  <li>If val is a primitive value, return val.</li>\n  </ol></li>\n  </ol>\n</blockquote>\n\n<p>This is all a roundabout way of saying that the object's <code>valueOf</code> function gets called and the return value from that function is used in the equation. If you were to change the <code>valueOf</code> function you could change the results of the operation:</p>\n\n<pre><code>&gt; five.valueOf = function () { return 10 }\nundefined\n&gt; five * 5\n50\n</code></pre>\n\n<p><code>10</code> as <code>five</code>s <code>valueOf</code> function was unchanged, it returns the wrapped primitive value <code>5</code> so that <code>five * 5</code> evaluates to <code>5 * 5</code> which results in <code>25</code></p>\n\n<p><code>11</code> the <code>five</code> object's <code>wtf</code> property is evaluated <em>again</em> despite having been unchanged from when it was assigned on <code>05</code>.</p>\n\n<p><code>12</code> <code>'potato'</code></p>\n\n<p><code>13</code> the <a href=\"http://es5.github.io/#x11.3.1\">Postfix Increment Operator</a> is called on <code>five</code>, which gets the numeric value (<code>5</code>, we covered how earlier), stores the value so that it can be returned, adds <code>1</code> to the value (<code>6</code>), assigns the value to <code>five</code>, and returns the stored value (<code>5</code>)</p>\n\n<p><code>14</code> as before, the returned value is the value before it was incremented</p>\n\n<p><code>15</code> the <code>wtf</code> property of the primitive value (<code>6</code>) stored at the variable <code>five</code> is accessed. <a href=\"http://es5.github.io/#x15.7.5\">Section 15.7.5 of the ES5 specification</a> defines this behavior. Numbers get the properties from <code>Number.prototype</code>.</p>\n\n<p><code>16</code> <code>Number.prototype</code> doesn't have a <code>wtf</code> property, so <code>undefined</code> is returned</p>\n\n<p><code>17</code> <code>five.wtf</code> is assigned a value of <code>'potato?'</code>. <a href=\"http://es5.github.io/#x11.13.1\">Assignment is defined in 11.13.1 of the ES5 specification</a>. Basically the value assigned is returned but not stored.</p>\n\n<p><code>18</code> <code>'potato?'</code> was returned by the assignment operator</p>\n\n<p><code>19</code> again <code>five</code>, which has a value of <code>6</code> is accessed, and again <code>Number.prototype</code> doesn't have a <code>wtf</code> property</p>\n\n<p><code>20</code> <code>undefined</code> as explained above</p>\n\n<p><code>21</code> <code>five</code> is accessed</p>\n\n<p><code>22</code> <code>6</code> is returned as explained in <code>13</code></p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38638385, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<h2>A couple of concepts explain what happens</h2>\n\n<p><code>5</code> is a number, a primitive value</p>\n\n<p><code>Number {[[PrimitiveValue]]: 5}</code> is an instance of Number (let's call it object wrapper)</p>\n\n<p>Whenever you access a property/method on a primitive value, the JS engine will create an object wrapper of the appropriate type (<code>Number</code> for <code>5</code>, <code>String</code> for <code>'str'</code> and <code>Boolean</code> for <code>true</code>) and resolve the property access/method call on that object wrapper. This is what happens when you do <code>true.toString()</code> for example.</p>\n\n<p>When performing operations on objects, they are converted to primitive values (by using <code>toString</code> or <code>valueOf</code>) in order to resolve those operations - for example when doing</p>\n\n<pre><code>var obj = { a : 1 };\nvar string = 'mystr' + obj;\nvar number = 3 + obj;\n</code></pre>\n\n<p><code>string</code> will hold the string concatenation of <code>mystr</code> and <code>obj.toString()</code> and <code>number</code> will hold the addition of <code>3</code> and <code>obj.valueOf()</code>.</p>\n\n<h2>Now to put it all together</h2>\n\n<pre><code>five = dis.call(5)\n</code></pre>\n\n<p><code>dis.call(5)</code> behaves just like <code>(5).dis()</code> if <code>5</code> actually had the method <code>dis</code>. In order to resolve the method call, the object wrapper is created and the method call is resolved on it. At this point five points to an object wrapper around the primitive value 5.</p>\n\n<pre><code>five.wtf = 'potato'\n</code></pre>\n\n<p>Setting a property on an object, nothing fancy here.</p>\n\n<pre><code>five * 5\n</code></pre>\n\n<p>This is actually <code>five.valueOf() * 5</code> obtaining the primitive value from the object wrapper. <code>five</code> still points to the initial object.</p>\n\n<pre><code>five++\n</code></pre>\n\n<p>This is actually <code>five = five.valueOf() + 1</code>. <strong>Before this line <code>five</code> holds the object wrapper around the value 5, while after this point <code>five</code> holds the primitive value <code>6</code></strong>.</p>\n\n<pre><code>five.wtf\nfive.wtf = 'potato?'\nfive.wtf\n</code></pre>\n\n<p><code>five</code> is no longer an object. Each of those lines creates a new instance of Number in order to resolve the <code>.wtf</code> property access. The instances are independent, so setting a property on one will not be visible on another. The code is completely equivalent to this one:</p>\n\n<pre><code>(new Number(6)).wtf;\n(new Number(6)).wtf = 'potato?';\n(new Number(6)).wtf;\n</code></pre>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38641978, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p>OP here. Funny to see this on Stack Overflow :)</p>\n\n<p>Before stepping through the behaviour, its important to clarify a few things:</p>\n\n<ol>\n<li><p><a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-4.3.19\">Number value</a> and <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-4.3.21\">Number object</a> (<code>a = 3</code> vs <code>a = new Number(3)</code>) are very different. One is a primitive, the other is an object. You cannot assign attributes to primitives, but you can to objects.</p></li>\n<li><p>Coercion between the two is implicit.</p>\n\n<p>For example:</p>\n\n<pre><code>(new Number(3) === 3)  // returns false\n(new Number(3) == 3)   // returns true, as the '==' operator coerces\n(+new Number(3) === 3) // returns true, as the '+' operator coerces\n</code></pre></li>\n<li><p>Every <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-11\">Expression</a> has a return value. When the <a href=\"https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop\">REPL</a> reads and executes an expression, this is what it displays. The return values often don't mean what you think and imply things that just aren't true.</p></li>\n</ol>\n\n<p>Ok, here we go.</p>\n\n<p><img src=\"https://i.stack.imgur.com/2tqWo.jpg\" alt=\"Original image of the JavaScript code\"></p>\n\n<h3><em>The pledge.</em></h3>\n\n<pre><code>&gt; function dis() { return this }\nundefined\n&gt; five = dis.call(5)\n[Number: 5]\n</code></pre>\n\n<p>Define a function <code>dis</code> and <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/call\">call</a> it with <code>5</code>. This will execute the function with <code>5</code> as the context (<code>this</code>). Here it is coerced from a Number value to a Number object. It is very important to note that were we in <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Strict_mode\">strict mode</a> <strong>this would not have happened</strong>.</p>\n\n<pre><code>&gt; five.wtf = 'potato'\n'potato'\n&gt; five.wtf\n'potato'\n</code></pre>\n\n<p>Now we set the attribute <code>five.wtf</code> to <code>'potato'</code>, and with five as an object, sure enough it accepts the <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-11.13.1\">Simple Assignment</a>.</p>\n\n<pre><code>&gt; five * 5\n25\n&gt; five.wtf\n'potato'\n</code></pre>\n\n<p>With <code>five</code> as an object, I ensure it can still perform simple arithmetic operations. It can. Do its attributes still stick? Yes.</p>\n\n<h3><em>The turn.</em></h3>\n\n<pre><code>&gt; five++\n5\n&gt; five.wtf\nundefined\n</code></pre>\n\n<p>Now we check <code>five++</code>. The trick with <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-11.3\">postfix increment</a> is that the entire expression will evaluate against the original value <em>and then</em> increment the value. It looks like <code>five</code> is still five, but really the expression evaluated to five, then set <code>five</code> to <code>6</code>.</p>\n\n<p>Not only did <code>five</code> get set to <code>6</code>, but it was coerced back into a Number value, and all attributes are lost. Since primitives cannot hold attributes, <code>five.wtf</code> is undefined.</p>\n\n<pre><code>&gt; five.wtf = 'potato?'\n'potato?'\n&gt; five.wtf\nundefined\n</code></pre>\n\n<p>I again attempt to reassign an attribute <code>wtf</code> to <code>five</code>. The return value implies it sticks, but it in fact does not because <code>five</code> is a Number value, not a Number object. The expression evaluates to <code>'potato?'</code>, but when we check we see it was not assigned.</p>\n\n<h3><em>The prestige.</em></h3>\n\n<pre><code>&gt; five\n6\n</code></pre>\n\n<p>Ever since the postfix increment, <code>five</code> has been <code>6</code>.</p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38647682, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p><strong>There's Coercion in the JavaScript World - A Detective Story</strong></p>\n\n<p>Nathan, you have no idea what you've uncovered. </p>\n\n<p>I've been investigating this for weeks now. It all started on a stormy night last October. I accidentally stumbled upon the <code>Number</code> class - I mean, why in the world did JavaScript have a <code>Number</code> class?</p>\n\n<p>I wasn't prepared for what I was going to find out next.</p>\n\n<p>It turns out that JavaScript, without telling you, has been changing your numbers to objects and your objects to numbers right under your nose.</p>\n\n<p>JavaScript was hoping no one would catch on, but people have been reporting strange unexpected behavior, and now thanks to you and your question I have the evidence I need to blow this thing wide open.</p>\n\n<p>This is what we've found out so far. I don't know if I should even be telling you this - you might want to turn off your JavaScript.</p>\n\n<pre><code>&gt; function dis() { return this }\nundefined\n</code></pre>\n\n<p>When you created that function, you probably had no idea what was going to happen next. Everything looked fine, and everything was fine - for now. </p>\n\n<p>No error messages, just the word \"undefined\" in console output, exactly what you would expect. After all, this was a function declaration - it isn't supposed to return anything. </p>\n\n<p>But this was just the beginning. What happened next, no one could have predicted.</p>\n\n<pre><code>&gt; five = dis.call(5)\nNumber {[[PrimitiveValue]]: 5}\n</code></pre>\n\n<p>Yeah I know, you expected a <code>5</code>, but that's not what you got, was it - you got something else - something different.</p>\n\n<p>The same thing happened to me.</p>\n\n<p>I didn't know what to make of it. It drove me nuts. I couldn't sleep, I couldn't eat, I tried to drink it away, but no amount of Mountain Dew would make me forget. It just didn't make any sense!</p>\n\n<p>That's when I found out what was really going on - it was coercion, and it was happening right there in front of my eyes, but I was too blind to see it. </p>\n\n<p>Mozilla tried to bury it by putting it where they knew nobody would look - their <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/call\">documentation</a>.</p>\n\n<p>After hours of recursively reading and re-reading and re-re-reading I found this:</p>\n\n<blockquote>\n  <p>\"... and primitive values will be converted to objects.\"</p>\n</blockquote>\n\n<p>It was right there plain as can be spelled out in Open Sans font. It was the <code>call()</code> function - how could I be so stupid?!</p>\n\n<p>My number was no longer a number at all. The moment I passed it into <code>call()</code>, it became something else. It became... an object.</p>\n\n<p>I couldn't believe it at first. How could this be true? But I couldn't ignore the evidence that was mounting up around me. It's right there if you just look:</p>\n\n<pre><code>&gt; five.wtf = 'potato'\n\"potato\"\n\n&gt; five.wtf\n\"potato\"\n</code></pre>\n\n<p><code>wtf</code> was right. Numbers can't have custom properties - we all know that! It's the first thing they teach you at the academy.</p>\n\n<p>We should have known the moment we saw the console output - this was not the number we thought it was. This was an impostor - an object passing itself off as our sweet innocent number.</p>\n\n<p>This was... <code>new Number(5)</code>.</p>\n\n<p>Of course! It made perfect sense. <code>call()</code> had a job to do, he had to invoke a function, and to do that he needed to populate <code>this</code>, he knew he couldn't do that with a number - he needed an object and he was willing to do anything to get it, even if that meant coercing our number. When <code>call()</code> saw the number <code>5</code>, he saw an opportunity.</p>\n\n<p>It was the perfect plan: wait until no one was looking and swap out our number for an object that looks just like it. We get a number, the function gets invoked, and no one would be the wiser.</p>\n\n<p>It really was the perfect plan, but like all plans, even perfect ones, there was a hole in it, and we were about to fall right into it.</p>\n\n<p>See, what <code>call()</code> didn't understand was that he wasn't the only one in town who could coerce numbers. This was JavaScript after all - coercion was everywhere.</p>\n\n<p><code>call()</code> took my number, and I wasn't going to stop until I pulled the mask off of his little impostor and expose him to the whole Stack&nbsp;Overflow community.</p>\n\n<p>But how? I needed a plan. Sure it looks like a number, but I know it's not, there's gotta be a way to prove that. That's it! It <em>looks</em> like a number, but can it act like one?</p>\n\n<p>I told <code>five</code> I need him to become 5 times larger - he didn't ask why and I didn't explain. I then did what any good programmer would do: I multiplied. Surely there was no way he could fake his way out of this.</p>\n\n<pre><code>&gt; five * 5\n25\n&gt; five.wtf\n'potato'\n</code></pre>\n\n<p>Damn it! Not only did <code>five</code> multiply just fine <code>wtf</code> was still there. Damn this guy and his potato.</p>\n\n<p>What the hell was going on? Was I wrong about this whole thing? Is <code>five</code> really a number? No, I must be missing something, I know it, there's something I must be forgetting, something so simple and basic that I'm completely overlooking it.</p>\n\n<p>This was not looking good, I had been writing this answer for hours and I was still no closer to making my point. I couldn't keep this up, eventually people would stop reading, I had to think of something and I had to think of it fast.</p>\n\n<p>Wait that's it! <code>five</code> wasn't 25, 25 was the result, 25 was a completely different number. Of course, how could I forget? Numbers are immutable. When you multiply <code>5 * 5</code> nothing gets assigned to anything you just create a new number <code>25</code>.</p>\n\n<p>That must be what's happening here. Somehow when I multiply <code>five * 5</code>, <code>five</code> must be getting coerced into a number and that number must be the one used for the multiplication. It's the results of that multiplication that gets printed to the console, not the value of <code>five</code> itself. <code>five</code> never gets assigned anything - so of course it doesn't change.</p>\n\n<p>So then how do I get <code>five</code> to assign himself the result of an operation. I got it. Before <code>five</code> even had a chance to think, I yelled \"++\".</p>\n\n<pre><code>&gt; five++\n5\n</code></pre>\n\n<p>Aha! I had him! Everybody knows <code>5 + 1</code> is <code>6</code>, this was the evidence I needed to expose that <code>five</code> was not a number! It was an impostor! A bad impostor that didn't know how to count. And I could prove it. Here's how a real number acts:</p>\n\n<pre><code>&gt; num = 5\n5\n&gt; num++\n5\n</code></pre>\n\n<p>Wait? What was going on here? <em>sigh</em> I got so caught up in busting <code>five</code> that I forget how post operators work. When I use the <code>++</code> at the end of <code>five</code> I'm saying return the current value, then increment <code>five</code>. It's the value <em>before</em> the operation occurs that gets printed to the console. <code>num</code> was in fact <code>6</code> and I could prove it:</p>\n\n<pre><code>&gt;num\n6\n</code></pre>\n\n<p>Time to see what <code>five</code> really was:</p>\n\n<pre><code>&gt;five\n6\n</code></pre>\n\n<p>...it was exactly what it should be. <code>five</code> was good - but I was better. If <code>five</code> were still an object that would mean it would still have the property <code>wtf</code> and I was willing to bet everything it didn't.</p>\n\n<pre><code>&gt; five.wtf\nundefined\n</code></pre>\n\n<p>Aha! I was right. I had him! <code>five</code> was a number now - it wasn't an object anymore. I knew the multiplication trick wouldn't save it this time. See <code>five++</code> is really <code>five = five + 1</code>. Unlike the multiplication, the <code>++</code> operator assigns a value to <code>five</code>. More specifically, it assigns it the results of <code>five + 1</code> which just like in the case of multiplication returns a new immutable <strong>number</strong>.</p>\n\n<p>I knew I had him, and just to make sure he couldn't squirm his way out of it. I had one more test up my sleeve. If I was right, and <code>five</code> was really a number now, then this wouldn't work:</p>\n\n<pre><code>&gt; five.wtf = 'potato?'\n'potato?'\n</code></pre>\n\n<p>He wasn't going to fool me this time. I knew <code>potato?</code> was going to be printed to the console because that's output of the assignment. The real question is, will <code>wtf</code> still be there?</p>\n\n<pre><code>&gt; five.wtf\nundefined\n</code></pre>\n\n<p>Just as I suspected - nothing - because numbers can't be assigned properties. We learned that the first year at the academy ;)</p>\n\n<p>Thanks Nathan. Thanks to your courage in asking this question I can finally put all this behind me and move on to a new case.</p>\n\n<p>Like this one about the function <code>toValue()</code>. Oh dear god. Nooo!</p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38677222, "question_id": 38637003, "tags": ["javascript"]}, {"body": "<p>JavaScript scopes are made of Execution Contexts. Each Execution Context has a Lexical Environment (external/globally scoped values), a Variable Environment (locally scoped values), and a <em>this binding</em>.</p>\n\n<p>The <em>this binding</em> is a very important part of the Execution Context. Using <code>call</code> is one way to alter the <em>this binding</em>, and doing so will automatically create an object to populate the binding with.</p>\n\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/call\" rel=\"nofollow\"><strong>Function.prototype.call()</strong> (from MDN)</a></p>\n\n<blockquote>\n  <p><strong>Syntax</strong><br>\n  <code>fun.call(thisArg[, arg1[, arg2[, ...]]])</code></p>\n  \n  <p><strong>thisArg</strong><br>\n  The value of this provided for the call to fun. Note that this may not be the actual value seen by the method: if the method is a function in non-strict mode code, null and undefined will be replaced with the global object and <strong>primitive values will be converted to objects</strong>. (emphasis mine)</p>\n</blockquote>\n\n<p>Once it is evident that 5 is being converted into <code>new Number(5)</code>, the rest should be rather obvious. Note that other examples will also work so long as they are primitive values.</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"false\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>function primitiveToObject(prim){\r\n  return dis.call(prim);\r\n}\r\nfunction dis(){ return this; }\r\n\r\n//existing example\r\nconsole.log(primitiveToObject(5));\r\n\r\n//Infinity\r\nconsole.log(primitiveToObject(1/0));\r\n\r\n//bool\r\nconsole.log(primitiveToObject(1&gt;0));\r\n\r\n//string\r\nconsole.log(primitiveToObject(\"hello world\"));</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;img src=\"http://i.stack.imgur.com/MUyRV.png\" /&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p><a href=\"http://i.stack.imgur.com/MUyRV.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/MUyRV.png\" alt=\"enter image description here\"></a></p>\n", "title": "What is happening in this JavaScript snippet?", "answer_id": 38706559, "question_id": 38637003, "tags": ["javascript"]}], "question_id": 38637003}, {"body": "<p>In Java, using throw/catch as a part of logic when there's not actually an error is generally a bad idea (in part) because throwing and catching an exception is expensive, and doing it many times in a loop is usually far slower than other control structures which don't involve throwing exceptions.</p>\n\n<p>My question is, is the cost incurred in the throw/catch itself, or when creating the Exception object (since it gets a lot of runtime information including the execution stack)?</p>\n\n<p>In other words, if I do</p>\n\n<pre><code>Exception e = new Exception();\n</code></pre>\n\n<p>but don't throw it, is that most of the cost of throwing, or is the throw + catch handling what's costly?</p>\n\n<p>I'm not asking whether putting code in a try/catch block adds to the cost of executing that code, I'm asking whether catching the Exception is the expensive part, or creating (calling the constructor for) the Exception is the expensive part.</p>\n\n<p>Another way of asking this is, if I made one instance of Exception and threw and caught it over and over, would that be significantly faster than creating a new Exception every time I throw?</p>\n", "title": "Which part of throwing an Exception is expensive?", "tags": ["java", "performance", "exception"], "answer_count": 6, "link": "http://stackoverflow.com/questions/36343209/which-part-of-throwing-an-exception-is-expensive", "answers": [{"body": "<p>The creation of the <code>Exception</code> with a <code>null</code> stack trace takes about as much time as the <code>throw</code> and <code>try-catch</code> block together. However, <strong>filling the stack trace takes on average 5x longer</strong>. </p>\n\n<p>I created the following benchmark to demonstrate the impact on performance. I added the <code>-Djava.compiler=NONE</code> to the Run Configuration to disable compiler optimization. To measure the impact of building the stack trace, I extended the <code>Exception</code> class to take advantage of the stack-free constructor:</p>\n\n<pre><code>class NoStackException extends Exception{\n    public NoStackException() {\n        super(\"\",null,false,false);\n    }\n}\n</code></pre>\n\n<p>The benchmark code is as follows:</p>\n\n<pre><code>public class ExceptionBenchmark {\n\n    private static final int NUM_TRIES = 100000;\n\n    public static void main(String[] args) {\n\n        long throwCatchTime = 0, newExceptionTime = 0, newObjectTime = 0, noStackExceptionTime = 0;\n\n        for (int i = 0; i &lt; 30; i++) {\n            throwCatchTime += throwCatchLoop();\n            newExceptionTime += newExceptionLoop();\n            newObjectTime += newObjectLoop();\n            noStackExceptionTime += newNoStackExceptionLoop();\n        }\n\n        System.out.println(\"throwCatchTime = \" + throwCatchTime / 30);\n        System.out.println(\"newExceptionTime = \" + newExceptionTime / 30);\n        System.out.println(\"newStringTime = \" + newObjectTime / 30);\n        System.out.println(\"noStackExceptionTime = \" + noStackExceptionTime / 30);\n\n    }\n\n    private static long throwCatchLoop() {\n        Exception ex = new Exception(); //Instantiated here\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            try {\n                throw ex; //repeatedly thrown\n            } catch (Exception e) {\n\n                // do nothing\n            }\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n    private static long newExceptionLoop() {\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            Exception e = new Exception();\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n    private static long newObjectLoop() {\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            Object o = new Object();\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n    private static long newNoStackExceptionLoop() {\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            NoStackException e = new NoStackException();\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n}\n</code></pre>\n\n<p><strong>Output:</strong></p>\n\n<pre><code>throwCatchTime = 19\nnewExceptionTime = 77\nnewObjectTime = 3\nnoStackExceptionTime = 15\n</code></pre>\n\n<p>This implies that creating a <code>NoStackException</code> is approximately as expensive as repeatedly throwing the same <code>Exception</code>. It also shows that creating an <code>Exception</code> and filling its stack trace takes approximately <strong>4x</strong> longer. </p>\n", "title": "Which part of throwing an Exception is expensive?", "answer_id": 36343689, "question_id": 36343209, "tags": ["java", "performance", "exception"]}, {"body": "<p>The first operation in most <code>Throwable</code> constructors is to <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html#fillInStackTrace--\">fill in the stack trace,</a> which is where most of the expense is.</p>\n\n<p>There is, however, a protected constructor with a flag to disable the stack trace. <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/Exception.html#Exception-java.lang.String-java.lang.Throwable-boolean-boolean-\">This constructor</a> is accessible when extending <code>Exception</code> as well. If you create a custom exception type, you can avoid the stack trace creation and get better performance at the expense of less information.</p>\n\n<p>If you create a single exception of any type by normal means, you can re-throw it many times without the overhead of filling in the stack trace. However, its stack trace will reflect where it was constructed, not where it was thrown in a particular instance.</p>\n\n<p>Current versions of Java make some attempts to optimize stack trace creation. Native code is invoked to fill in the stack trace, which records the trace in a lighter-weight, native structure. Corresponding Java <a href=\"https://docs.oracle.com/javase/8/docs/api/java/lang/StackTraceElement.html\"><code>StackTraceElement</code></a> objects are lazily created from this record only when the <code>getStackTrace()</code>, <code>printStackTrace()</code>, or other methods that require the trace are called.</p>\n\n<p>If you eliminate stack trace generation, the other main cost is unwinding the stack between the throw and the catch. The fewer intervening frames encountered before the exception is caught, the faster this will be.</p>\n\n<p>Design your program so that exceptions are thrown only in truly exceptional cases, and optimizations like these are hard to justify.</p>\n", "title": "Which part of throwing an Exception is expensive?", "answer_id": 36344028, "question_id": 36343209, "tags": ["java", "performance", "exception"]}, {"body": "<p><strong>Creating</strong> an exception object is not more expensive than creating other regular objects. The main cost is hidden in native <a href=\"http://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html#fillInStackTrace--\"><code>fillInStackTrace</code></a> method which walks through the call stack and collects all required information to build a stack trace: classes, method names, line numbers etc.</p>\n\n<p>The myth about high exception costs comes from the fact that most of <code>Throwable</code> constructors implicitly call <code>fillInStackTrace</code>. However, there is one <a href=\"http://docs.oracle.com/javase/8/docs/api/java/lang/Throwable.html#Throwable-java.lang.String-java.lang.Throwable-boolean-boolean-\">constructor</a> to create a <code>Throwable</code> without a stack trace. It allows you to make throwables that are very fast to instantiate. Another way to create lightweight exceptions is to override <code>fillInStackTrace</code>.</p>\n\n<hr>\n\n<p>Now what about <strong>throwing</strong> an exception?<br>\nIn fact, it depends on where a thrown exception is <strong>caught</strong>.</p>\n\n<p>If it is caught in the same method (or, more precisely, in the same context, since the context can include several methods due to inlining), then <code>throw</code> is as fast and simple as <code>goto</code> (of course, after JIT compilation).</p>\n\n<p>However if a <code>catch</code> block is somewhere deeper in the stack, then JVM needs to unwind the stack frames, and this can take significantly longer. It takes even longer, if there are <code>synchronized</code> blocks or methods involved, because unwinding implies releasing of monitors owned by removed stack frames.</p>\n\n<hr>\n\n<p>I could confirm the above statements by proper benchmarks, but fortunately I don't need to to this, since all the aspects are already perfectly covered in the post of HotSpot performance engineer Alexey Shipil\u00ebv: <a href=\"http://shipilev.net/blog/2014/exceptional-performance/\">The Exceptional Performance of Lil' Exception</a>.</p>\n", "title": "Which part of throwing an Exception is expensive?", "answer_id": 36344265, "question_id": 36343209, "tags": ["java", "performance", "exception"]}, {"body": "<p>Using @AustinD's answer as a starting point, I made some tweaks. Code at the bottom.</p>\n\n<p>In addition to adding the case where one Exception instance is thrown repeatedly, I also turned off compiler optimization so that we can get accurate performance results. I added <code>-Djava.compiler=NONE</code> to the VM arguments, as per <a href=\"http://stackoverflow.com/a/5242430/1021426\">this answer</a>.  (In eclipse, edit the Run Configuration &rarr; Arguments to set this VM argument)</p>\n\n<p>The results:</p>\n\n<pre><code>new Exception + throw/catch = 643.5\nnew Exception only          = 510.7\nthrow/catch only            = 115.2\nnew String (benchmark)      = 669.8\n</code></pre>\n\n<p>So creating the exception costs about 5x as much as throwing + catching it. Assuming the compiler doesn't optimize away much of the cost.</p>\n\n<p>For comparison, here's the same test run without disabling optimization:</p>\n\n<pre><code>new Exception + throw/catch = 382.6\nnew Exception only          = 379.5\nthrow/catch only            = 0.3\nnew String (benchmark)      = 15.6\n</code></pre>\n\n<p>Code:</p>\n\n<pre><code>public class ExceptionPerformanceTest {\n\n    private static final int NUM_TRIES = 1000000;\n\n    public static void main(String[] args) {\n\n        double numIterations = 10;\n\n        long exceptionPlusCatchTime = 0, excepTime = 0, strTime = 0, throwTime = 0;\n\n        for (int i = 0; i &lt; numIterations; i++) {\n            exceptionPlusCatchTime += exceptionPlusCatchBlock();\n            excepTime += createException();\n            throwTime += catchBlock();\n            strTime += createString();\n        }\n\n        System.out.println(\"new Exception + throw/catch = \" + exceptionPlusCatchTime / numIterations);\n        System.out.println(\"new Exception only          = \" + excepTime / numIterations);\n        System.out.println(\"throw/catch only            = \" + throwTime / numIterations);\n        System.out.println(\"new String (benchmark)      = \" + strTime / numIterations);\n\n    }\n\n    private static long exceptionPlusCatchBlock() {\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            try {\n                throw new Exception();\n            } catch (Exception e) {\n                // do nothing\n            }\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n    private static long createException() {\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            Exception e = new Exception();\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n    private static long createString() {\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            Object o = new String(\"\" + i);\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n\n    private static long catchBlock() {\n        Exception ex = new Exception(); //Instantiated here\n        long start = System.currentTimeMillis();\n        for (int i = 0; i &lt; NUM_TRIES; i++) {\n            try {\n                throw ex; //repeatedly thrown\n            } catch (Exception e) {\n                // do nothing\n            }\n        }\n        long stop = System.currentTimeMillis();\n        return stop - start;\n    }\n}\n</code></pre>\n", "title": "Which part of throwing an Exception is expensive?", "answer_id": 36344302, "question_id": 36343209, "tags": ["java", "performance", "exception"]}, {"body": "<p>Theres a good write up on Exceptions here.</p>\n\n<p><a href=\"http://shipilev.net/blog/2014/exceptional-performance/\" rel=\"nofollow\">http://shipilev.net/blog/2014/exceptional-performance/</a></p>\n\n<p>The conclusion being that stack trace construction and stack unwinding are the expensive parts. The code below takes advantage of a feature in <code>1.7</code> where we can turn stack traces on and off. We can then use this to see what sort of costs different scenarios have</p>\n\n<p>The following are timings for Object creation alone. I've added <code>String</code> here so you can see that without the stack being written there's almost no difference in creating a <code>JavaException</code> Object and a <code>String</code>. With stack writing turned on the difference is dramatic ie at least one order of magnitude slower.</p>\n\n<pre><code>Time to create million String objects: 41.41 (ms)\nTime to create million JavaException objects with    stack: 608.89 (ms)\nTime to create million JavaException objects without stack: 43.50 (ms)\n</code></pre>\n\n<p>The following shows how long it took to return from a throw at a particular depth a million times.</p>\n\n<pre><code>|Depth| WriteStack(ms)| !WriteStack(ms)| Diff(%)|\n|   16|           1428|             243| 588 (%)|\n|   15|           1763|             393| 449 (%)|\n|   14|           1746|             390| 448 (%)|\n|   13|           1703|             384| 443 (%)|\n|   12|           1697|             391| 434 (%)|\n|   11|           1707|             410| 416 (%)|\n|   10|           1226|             197| 622 (%)|\n|    9|           1242|             206| 603 (%)|\n|    8|           1251|             207| 604 (%)|\n|    7|           1213|             208| 583 (%)|\n|    6|           1164|             206| 565 (%)|\n|    5|           1134|             205| 553 (%)|\n|    4|           1106|             203| 545 (%)|\n|    3|           1043|             192| 543 (%)| \n</code></pre>\n\n<p>The following is almost certainly a gross over simplification...</p>\n\n<p>If we take a depth of 16 with stack writing on then object creation is taking approximately ~40% of the time, the actual stack trace accounts for the vast majority of this. ~93% of instantiating the JavaException object is due to the stack trace being taken. This means that unwinding the stack in this case is taking the other 50% of the time.</p>\n\n<p>When we turn off the stack trace object creation accounts for a much smaller\nfraction ie 20% and stack unwinding now accounts for 80% of the time.</p>\n\n<p>In both cases stack unwinding takes a large portion of the overall time.</p>\n\n<pre><code>public class JavaException extends Exception {\n  JavaException(String reason, int mode) {\n    super(reason, null, false, false);\n  }\n  JavaException(String reason) {\n    super(reason);\n  }\n\n  public static void main(String[] args) {\n    int iterations = 1000000;\n    long create_time_with    = 0;\n    long create_time_without = 0;\n    long create_string = 0;\n    for (int i = 0; i &lt; iterations; i++) {\n      long start = System.nanoTime();\n      JavaException jex = new JavaException(\"testing\");\n      long stop  =  System.nanoTime();\n      create_time_with += stop - start;\n\n      start = System.nanoTime();\n      JavaException jex2 = new JavaException(\"testing\", 1);\n      stop = System.nanoTime();\n      create_time_without += stop - start;\n\n      start = System.nanoTime();\n      String str = new String(\"testing\");\n      stop = System.nanoTime();\n      create_string += stop - start;\n\n    }\n    double interval_with    = ((double)create_time_with)/1000000;\n    double interval_without = ((double)create_time_without)/1000000;\n    double interval_string  = ((double)create_string)/1000000;\n\n    System.out.printf(\"Time to create %d String objects: %.2f (ms)\\n\", iterations, interval_string);\n    System.out.printf(\"Time to create %d JavaException objects with    stack: %.2f (ms)\\n\", iterations, interval_with);\n    System.out.printf(\"Time to create %d JavaException objects without stack: %.2f (ms)\\n\", iterations, interval_without);\n\n    JavaException jex = new JavaException(\"testing\");\n    int depth = 14;\n    int i = depth;\n    double[] with_stack    = new double[20];\n    double[] without_stack = new double[20];\n\n    for(; i &gt; 0 ; --i) {\n      without_stack[i] = jex.timerLoop(i, iterations, 0)/1000000;\n      with_stack[i]    = jex.timerLoop(i, iterations, 1)/1000000;\n    }\n    i = depth;\n    System.out.printf(\"|Depth| WriteStack(ms)| !WriteStack(ms)| Diff(%%)|\\n\");\n    for(; i &gt; 0 ; --i) {\n      double ratio = (with_stack[i] / (double) without_stack[i]) * 100;\n      System.out.printf(\"|%5d| %14.0f| %15.0f| %2.0f (%%)| \\n\", i + 2, with_stack[i] , without_stack[i], ratio);\n      //System.out.printf(\"%d\\t%.2f (ms)\\n\", i, ratio);\n    }\n  }\n private int thrower(int i, int mode) throws JavaException {\n    ExArg.time_start[i] = System.nanoTime();\n    if(mode == 0) { throw new JavaException(\"without stack\", 1); }\n    throw new JavaException(\"with stack\");\n  }\n  private int catcher1(int i, int mode) throws JavaException{\n    return this.stack_of_calls(i, mode);\n  }\n  private long timerLoop(int depth, int iterations, int mode) {\n    for (int i = 0; i &lt; iterations; i++) {\n      try {\n        this.catcher1(depth, mode);\n      } catch (JavaException e) {\n        ExArg.time_accum[depth] += (System.nanoTime() - ExArg.time_start[depth]);\n      }\n    }\n    //long stop = System.nanoTime();\n    return ExArg.time_accum[depth];\n  }\n\n  private int bad_method14(int i, int mode) throws JavaException  {\n    if(i &gt; 0) { this.thrower(i, mode); }\n    return i;\n  }\n  private int bad_method13(int i, int mode) throws JavaException  {\n    if(i == 13) { this.thrower(i, mode); }\n    return bad_method14(i,mode);\n  }\n  private int bad_method12(int i, int mode) throws JavaException{\n    if(i == 12) { this.thrower(i, mode); }\n    return bad_method13(i,mode);\n  }\n  private int bad_method11(int i, int mode) throws JavaException{\n    if(i == 11) { this.thrower(i, mode); }\n    return bad_method12(i,mode);\n  }\n  private int bad_method10(int i, int mode) throws JavaException{\n    if(i == 10) { this.thrower(i, mode); }\n    return bad_method11(i,mode);\n  }\n  private int bad_method9(int i, int mode) throws JavaException{\n    if(i == 9) { this.thrower(i, mode); }\n    return bad_method10(i,mode);\n  }\n  private int bad_method8(int i, int mode) throws JavaException{\n    if(i == 8) { this.thrower(i, mode); }\n    return bad_method9(i,mode);\n  }\n  private int bad_method7(int i, int mode) throws JavaException{\n    if(i == 7) { this.thrower(i, mode); }\n    return bad_method8(i,mode);\n  }\n  private int bad_method6(int i, int mode) throws JavaException{\n    if(i == 6) { this.thrower(i, mode); }\n    return bad_method7(i,mode);\n  }\n  private int bad_method5(int i, int mode) throws JavaException{\n    if(i == 5) { this.thrower(i, mode); }\n    return bad_method6(i,mode);\n  }\n  private int bad_method4(int i, int mode) throws JavaException{\n    if(i == 4) { this.thrower(i, mode); }\n    return bad_method5(i,mode);\n  }\n  protected int bad_method3(int i, int mode) throws JavaException{\n    if(i == 3) { this.thrower(i, mode); }\n    return bad_method4(i,mode);\n  }\n  private int bad_method2(int i, int mode) throws JavaException{\n    if(i == 2) { this.thrower(i, mode); }\n    return bad_method3(i,mode);\n  }\n  private int bad_method1(int i, int mode) throws JavaException{\n    if(i == 1) { this.thrower(i, mode); }\n    return bad_method2(i,mode);\n  }\n  private int stack_of_calls(int i, int mode) throws JavaException{\n    if(i == 0) { this.thrower(i, mode); }\n    return bad_method1(i,mode);\n  }\n}\n\nclass ExArg {\n  public static long[] time_start;\n  public static long[] time_accum;\n  static {\n     time_start = new long[20];\n     time_accum = new long[20];\n  };\n}\n</code></pre>\n\n<p>The stack frames in this example are tiny compared to what you'd normally find. </p>\n\n<p>You can peek at the bytecode using javap </p>\n\n<pre><code>javap -c -v -constants JavaException.class\n</code></pre>\n\n<p>ie this is for method 4... </p>\n\n<pre><code>   protected int bad_method3(int, int) throws JavaException;\nflags: ACC_PROTECTED\nCode:\n  stack=3, locals=3, args_size=3\n     0: iload_1       \n     1: iconst_3      \n     2: if_icmpne     12\n     5: aload_0       \n     6: iload_1       \n     7: iload_2       \n     8: invokespecial #6                  // Method thrower:(II)I\n    11: pop           \n    12: aload_0       \n    13: iload_1       \n    14: iload_2       \n    15: invokespecial #17                 // Method bad_method4:(II)I\n    18: ireturn       \n  LineNumberTable:\n    line 63: 0\n    line 64: 12\n  StackMapTable: number_of_entries = 1\n       frame_type = 12 /* same */\n\nExceptions:\n  throws JavaException\n</code></pre>\n", "title": "Which part of throwing an Exception is expensive?", "answer_id": 36344430, "question_id": 36343209, "tags": ["java", "performance", "exception"]}, {"body": "<p>This part of the question...</p>\n\n<blockquote>\n  <p>Another way of asking this is, if I made one instance of Exception and\n  threw and caught it over and over, would that be significantly faster\n  than creating a new Exception every time I throw?</p>\n</blockquote>\n\n<p>Seems to be asking if creating an exception and caching it somewhere improves performance. Yes it does. It's the same as turning off the stack being written on object creation because it's already been done.</p>\n\n<p>These are timings I got, please read caveat after this...</p>\n\n<pre><code>|Depth| WriteStack(ms)| !WriteStack(ms)| Diff(%)|\n|   16|            193|             251| 77 (%)| \n|   15|            390|             406| 96 (%)| \n|   14|            394|             401| 98 (%)| \n|   13|            381|             385| 99 (%)| \n|   12|            387|             370| 105 (%)| \n|   11|            368|             376| 98 (%)| \n|   10|            188|             192| 98 (%)| \n|    9|            193|             195| 99 (%)| \n|    8|            200|             188| 106 (%)| \n|    7|            187|             184| 102 (%)| \n|    6|            196|             200| 98 (%)| \n|    5|            197|             193| 102 (%)| \n|    4|            198|             190| 104 (%)| \n|    3|            193|             183| 105 (%)| \n</code></pre>\n\n<p>Of course the problem with this is your stack trace now points to where you instantiated the object not where it was thrown from.</p>\n", "title": "Which part of throwing an Exception is expensive?", "answer_id": 36368009, "question_id": 36343209, "tags": ["java", "performance", "exception"]}], "question_id": 36343209}, {"body": "<p>I have a method that calls 4 other methods in sequence to check for specific conditions, and returns immediately (not checking the following ones) whenever one returns something Truthy.</p>\n\n<pre><code>def check_all_conditions():\n    x = check_size()\n    if x:\n        return x\n\n    x = check_color()\n    if x:\n        return x\n\n    x = check_tone()\n    if x:\n        return x\n\n    x = check_flavor()\n    if x:\n        return x\n    return None\n</code></pre>\n\n<p>This seems like a lot of baggage code. Instead of each 2-line if statement, I'd rather do something like:</p>\n\n<pre><code>x and return x\n</code></pre>\n\n<p>But that is invalid Python. Am I missing a simple, elegant solution here? Incidentally, in this situation, those four check methods may be expensive, so I do not want to call them multiple times.</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "tags": ["python"], "answer_count": 16, "link": "http://stackoverflow.com/questions/36117583/pythonic-way-to-avoid-if-x-return-x-statements", "answers": [{"body": "<p>You could use a loop:</p>\n\n<pre><code>conditions = (check_size, check_color, check_tone, check_flavor)\nfor condition in conditions:\n    result = condition()\n    if result:\n        return result\n</code></pre>\n\n<p>This has the added advantage that you can now make the number of conditions variable.</p>\n\n<p>You could use <a href=\"https://docs.python.org/3/library/functions.html#map\"><code>map()</code></a> + <a href=\"https://docs.python.org/3/library/functions.html#filter\"><code>filter()</code></a> (the Python 3 versions, use the <a href=\"https://docs.python.org/2/library/future_builtins.html\"><code>future_builtins</code> versions</a> in Python 2) to get the first such matching value:</p>\n\n<pre><code>try:\n    # Python 2\n    from future_builtins import map, filter\nexcept ImportError:\n    # Python 3\n    pass\n\nconditions = (check_size, check_color, check_tone, check_flavor)\nreturn next(filter(None, map(lambda f: f(), conditions)), None)\n</code></pre>\n\n<p>but if this is more readable is debatable.</p>\n\n<p>Another option is to use a generator expression:</p>\n\n<pre><code>conditions = (check_size, check_color, check_tone, check_flavor)\nchecks = (condition() for condition in conditions)\nreturn next((check for check in checks if check), None)\n</code></pre>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36117603, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>Alternatively to Martijn's fine answer, you could chain <code>or</code>. This will return the first truthy value, or <code>None</code> if there's no truthy value:</p>\n\n<pre><code>def check_all_conditions():\n    return check_size() or check_color() or check_tone() or check_flavor() or None\n</code></pre>\n\n<p>Demo:</p>\n\n<pre><code>&gt;&gt;&gt; x = [] or 0 or {} or -1 or None\n&gt;&gt;&gt; x\n-1\n&gt;&gt;&gt; x = [] or 0 or {} or '' or None\n&gt;&gt;&gt; x is None\nTrue\n</code></pre>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36117720, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>If you want the same code structure, you could use ternary statements!</p>\n\n<pre><code>def check_all_conditions():\n    x = check_size()\n    x = x if x else check_color()\n    x = x if x else check_tone()\n    x = x if x else check_flavor()\n\n    return x if x else None\n</code></pre>\n\n<p>I think this looks nice and clear if you look at it.</p>\n\n<p>Demo:</p>\n\n<p><img src=\"http://i.stack.imgur.com/gYcy8.png\" alt=\"Screenshot of it running\"></p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36121285, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>A slight variation on Martijns first example above, that avoids the if inside the loop:</p>\n\n<pre><code>Status = None\nfor c in [check_size, check_color, check_tone, check_flavor]:\n  Status = Status or c();\nreturn Status\n</code></pre>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36122716, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>This is a variant of Martijns first example. It also uses the \"collection of callables\"-style in order to allow short-circuiting.</p>\n\n<p>Instead of a loop you can use the builtin <code>any</code>. </p>\n\n<pre><code>conditions = (check_size, check_color, check_tone, check_flavor)\nreturn any(condition() for condition in conditions) \n</code></pre>\n\n<p>Note that <code>any</code> returns a boolean, so if you need the exact return value of the check, this solution will not work. <code>any</code> will not distinguish between <code>14</code>, <code>'red'</code>, <code>'sharp'</code>, <code>'spicy'</code> as return values, they will all be returned as <code>True</code>.</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36126903, "question_id": 36117583, "tags": ["python"]}, {"body": "<p><strong>Don't change it</strong></p>\n\n<p>There are other ways of doing this as the various other answers show. None are as clear as your original code.</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36129322, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>According to <a href=\"http://blog.codinghorror.com/curlys-law-do-one-thing/\">Curly's law</a>, you can make this code more readable by splitting two concerns:</p>\n\n<ul>\n<li>What things do I check?</li>\n<li>Has one thing returned true?</li>\n</ul>\n\n<p>into two functions:</p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def all_conditions():\n    yield check_size()\n    yield check_color()\n    yield check_tone()\n    yield check_flavor()\n\ndef check_all_conditions():\n    for condition in all_conditions():\n        if condition:\n            return condition\n    return None\n</code></pre>\n\n<p>This avoids:</p>\n\n<ul>\n<li>complicated logical structures</li>\n<li>really long lines</li>\n<li>repetition</li>\n</ul>\n\n<p>...while preserving a linear, easy to read flow.</p>\n\n<p>You can probably also come up with even better function names, according to your particular circumstance, which make it even more readable.</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36140014, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>I'm quite surprised nobody mentioned the built-in <a href=\"https://docs.python.org/2/library/functions.html#any\"><code>any</code></a> which is made for this purpose:</p>\n\n<pre><code>def check_all_conditions():\n    return any([\n        check_size(),\n        check_color(),\n        check_tone(),\n        check_flavor()\n    ])\n</code></pre>\n\n<p>Note that although this implementation is probably the clearest, it evaluates all the checks even if the first one is <code>True</code>.</p>\n\n<hr>\n\n<p>If you really need to stop at the first failed check, consider using <a href=\"https://docs.python.org/2/library/functions.html#reduce\"><code>reduce</code></a> which is made to convert a list to a simple value:</p>\n\n<pre><code>def check_all_conditions():\n    checks = [check_size, check_color, check_tone, check_flavor]\n    return reduce(lambda a, f: a or f(), checks, False)\n</code></pre>\n\n<blockquote>\n  <p><code>reduce(function, iterable[, initializer])</code> : Apply function of two\n  arguments cumulatively to the items of iterable, from left to right,\n  so as to reduce the iterable to a single value. The left argument, x,\n  is the accumulated value and the right argument, y, is the update\n  value from the iterable. If the optional initializer is present, it is\n  placed before the items of the iterable in the calculation</p>\n</blockquote>\n\n<p>In your case:</p>\n\n<ul>\n<li><code>lambda a, f: a or f()</code> is the function that checks that either the accumulator <code>a</code> or the current check <code>f()</code> is <code>True</code>. Note that if <code>a</code> is <code>True</code>, <code>f()</code> won't be evaluated.</li>\n<li><code>checks</code> contains check functions (the <code>f</code> item from the lambda)</li>\n<li><code>False</code> is the initial value, otherwise no check would happen and the result would always be <code>True</code></li>\n</ul>\n\n<p><code>any</code> and <code>reduce</code> are basic tools for functional programming. I strongly encourage you to train these out as well as <a href=\"https://docs.python.org/2/library/functions.html#map\"><code>map</code></a> which is awesome too!</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36141317, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>Have you considered just writing <code>if x: return x</code> all on one line?</p>\n\n<pre><code>def check_all_conditions():\n    x = check_size()\n    if x: return x\n\n    x = check_color()\n    if x: return x\n\n    x = check_tone()\n    if x: return x\n\n    x = check_flavor()\n    if x: return x\n\n    return None\n</code></pre>\n\n<p>This isn't any less <em>repetitive</em> than what you had, but IMNSHO it reads quite a bit smoother.</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36142616, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>Ideally, I would re-write the <code>check_</code>  functions to return <code>True</code> or <code>False</code> rather than a value. Your checks then become</p>\n\n<pre><code>if check_size(x):\n    return x\n#etc\n</code></pre>\n\n<p>Assuming your <code>x</code> is not immutable, your function can still modify it (although they can't reassign it) - but a function called <code>check</code> shouldn't really be modifying it anyway. </p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36146460, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>In effectively the same answer as timgeb, but you could use parenthesis for nicer formatting:</p>\n\n<pre><code>def check_all_the_things():\n    return (\n        one()\n        or two()\n        or five()\n        or three()\n        or None\n    )\n</code></pre>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36161683, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>I have seen some interesting implementations of switch/case statements with dicts in the past that led me to this answer. Using the example you've provided you would get the following. (It's madness <code>using_complete_sentences_for_function_names</code>, so <code>check_all_conditions</code> is renamed to <code>status</code>. See (1))</p>\n\n<pre><code>def status(k = 'a', s = {'a':'b','b':'c','c':'d','d':None}) :\n  select = lambda next, test : test if test else next\n  d = {'a': lambda : select(s['a'], check_size()  ),\n       'b': lambda : select(s['b'], check_color() ),\n       'c': lambda : select(s['c'], check_tone()  ),\n       'd': lambda : select(s['d'], check_flavor())}\n  while k in d : k = d[k]()\n  return k\n</code></pre>\n\n<p>The select function eliminates the need to call each <code>check_FUNCTION</code> twice i.e. you avoid <code>check_FUNCTION() if check_FUNCTION() else next</code> by adding another function layer. This is useful for long running functions. The lambdas in the dict delay execution of it's values until the while loop.</p>\n\n<p>As a bonus you may modify the execution order and even skip some of the tests by altering <code>k</code> and <code>s</code> e.g. <code>k='c',s={'c':'b','b':None}</code> reduces the number of tests and reverses the original processing order.</p>\n\n<p>The <code>timeit</code> fellows might haggle over the cost of adding an extra layer or two to the stack and the cost for the dict look up but you seem more concerned with the prettiness of the code.</p>\n\n<p>Alternatively a simpler implementation might be the following :</p>\n\n<pre><code>def status(k=check_size) :\n  select = lambda next, test : test if test else next\n  d = {check_size  : lambda : select(check_color,  check_size()  ),\n       check_color : lambda : select(check_tone,   check_color() ),\n       check_tone  : lambda : select(check_flavor, check_tone()  ),\n       check_flavor: lambda : select(None,         check_flavor())}\n  while k in d : k = d[k]()\n  return k\n</code></pre>\n\n<ol>\n<li>I mean this not in terms of pep8 but in terms of using one concise descriptive word in place of a sentence. Granted the OP may be following some coding convention, working one some existing code base or not care for terse terms in their codebase.</li>\n</ol>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36167334, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>This way is a little bit outside of the box, but I think the end result is simple, readable, and looks nice. </p>\n\n<p>The basic idea is to <code>raise</code> an exception when one of the functions evaluates as truthy, and return the result. Here's how it might look: </p>\n\n<pre><code>def check_conditions():\n    try:\n        assertFalsey(\n            check_size,\n            check_color,\n            check_tone,\n            check_flavor)\n    except TruthyException as e:\n        return e.trigger\n    else:\n        return None\n</code></pre>\n\n<p>You'll need a <code>assertFalsey</code> function that raises an exception when one of the called function arguments evaluates as truthy: </p>\n\n<pre><code>def assertFalsey(*funcs):\n    for f in funcs:\n        o = f()\n        if o:\n            raise TruthyException(o)\n</code></pre>\n\n<p>The above could be modified so as to also provide arguments for the functions to be evaluated. </p>\n\n<p>And of course you'll need the <code>TruthyException</code> itself. This exception provides the <code>object</code> that triggered the exception: </p>\n\n<pre><code>class TruthyException(Exception):\n    def __init__(self, obj, *args):\n        super().__init__(*args)\n        self.trigger = obj\n</code></pre>\n\n<p>You can turn the original function into something more general, of course: </p>\n\n<pre><code>def get_truthy_condition(*conditions):\n    try:\n        assertFalsey(*conditions)\n    except TruthyException as e:\n        return e.trigger\n    else:\n        return None\n\nresult = get_truthy_condition(check_size, check_color, check_tone, check_flavor)\n</code></pre>\n\n<p>This might be a bit slower because you are using both an <code>if</code> statement and handling an exception. However, the exception is only handled a maximum of one time, so the hit to performance should be minor unless you expect to run the check and get a <code>True</code> value many many thousands of times. </p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36182959, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>The pythonic way is either using reduce (as someone already mentioned) or itertools (as shown below), but <strong>it seems to me that simply using short circuiting of the <code>or</code> operator produces clearer code</strong></p>\n\n<pre><code>from itertools import imap, dropwhile\n\ndef check_all_conditions():\n    conditions = (check_size,\\\n        check_color,\\\n        check_tone,\\\n        check_flavor)\n    results_gen = dropwhile(lambda x:not x, imap(lambda check:check(), conditions))\n    try:\n        return results_gen.next()\n    except StopIteration:\n        return None\n</code></pre>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36254009, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>For me, the best answer is that from @phil-frost, followed by @wayne-werner's.</p>\n\n<p>What I find interesting is that no one has said anything about the fact that a function will be returning many different data types, which will make then mandatory to do checks on the type of x itself to do any further work.</p>\n\n<p>So I would mix @PhilFrost's response with the idea of keeping a single type:</p>\n\n<pre><code>def all_conditions(x):\n    yield check_size(x)\n    yield check_color(x)\n    yield check_tone(x)\n    yield check_flavor(x)\n\ndef assessed_x(x,func=all_conditions):\n    for condition in func(x):\n        if condition:\n            return x\n    return None\n</code></pre>\n\n<p>Notice that <code>x</code> is passed as an argument, but also <code>all_conditions</code> is used as a passed generator of checking functions where all of them get an <code>x</code> to be checked, and return <code>True</code> or <code>False</code>. By using <code>func</code> with <code>all_conditions</code> as default value, you can use <code>assessed_x(x)</code>, or you can pass a further personalised generator via <code>func</code>.</p>\n\n<p>That way, you get <code>x</code> as soon as one check passes, but it will always be the same type.</p>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36279970, "question_id": 36117583, "tags": ["python"]}, {"body": "<p>I'm going to jump in here and have never written a single line of Python, but I assume <code>if x = check_something(): return x</code> is valid?</p>\n\n<p>if so:</p>\n\n<pre><code>def check_all_conditions():\n\n    if x = check_size(): return x\n\n    if x = check_color(): return x\n\n    if x = check_tone(): return x\n\n    if x = check_flavor(): return x\n\n    return None\n</code></pre>\n", "title": "Pythonic way to avoid &quot;if x: return x&quot; statements", "answer_id": 36579695, "question_id": 36117583, "tags": ["python"]}], "question_id": 36117583}, {"body": "<p>In CSS, <code>*</code> will match any element.</p>\n\n<p>Frequently, <code>*|*</code> is used instead of <code>*</code> to match all elements. This is generally used for testing purposes.</p>\n\n<p>What is the difference between <code>*</code> and <code>*|*</code> in CSS?</p>\n", "title": "What is the difference between * and *|* in CSS?", "tags": ["css", "css-selectors"], "answer_count": 3, "link": "http://stackoverflow.com/questions/34987370/what-is-the-difference-between-and-in-css", "answers": [{"body": "<p><code>*|*</code> represents the selector of \"all elements in any namespace\". According <a href=\"https://www.w3.org/TR/selectors/#typenmsp\">to W3C</a>, the selector is divided into:</p>\n\n<p><strong>ns|E</strong></p>\n\n<p>Where ns is the <a href=\"https://developer.mozilla.org/en-US/docs/Web/CSS/@namespace\">namespace</a> and E is the element. By default, no namespaces are declared. So unless a namespace is declared explicitly, <code>*|*</code> and <code>*</code> will select the same elements.</p>\n", "title": "What is the difference between * and *|* in CSS?", "answer_id": 34987465, "question_id": 34987370, "tags": ["css", "css-selectors"]}, {"body": "<p>As per <a href=\"http://www.w3.org/TR/css3-selectors/#univnmsp\" rel=\"nofollow\">W3C Selector Spec</a>:</p>\n\n<blockquote>\n  <p>The universal selector allows an optional namespace component. It is used as follows:</p>\n  \n  <p><code>ns|*</code><br>\n  all elements in namespace ns</p>\n  \n  <p><code>*|*</code><br>\n  all elements</p>\n  \n  <p><code>|*</code><br>\n  all elements without a namespace</p>\n  \n  <p><code>*</code><br>\n  if no default namespace has been specified, this is equivalent to *|*. Otherwise it is equivalent to ns|* where ns is the default namespace.</p>\n</blockquote>\n\n<p>So, no <code>*</code> and <code>*|*</code> are not always the same. If a default name space is provided then <code>*</code> selects only elements that are part of that namespace.</p>\n\n<hr>\n\n<p>You can visually see the differences using the below two snippets. In the first, a default namespace is defined and so the <code>*</code> selector applies the beige colored background only to the element which is part of that namsepace whereas the <code>*|*</code> applies the border to all elements.\n<div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"false\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>@namespace \"http://www.w3.org/2000/svg\";\r\n\r\n* {\r\n  background: beige;\r\n}\r\n*|* {\r\n  border: 1px solid;\r\n}</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;a href=\"#\"&gt;This is some link&lt;/a&gt;\r\n\r\n&lt;svg xmlns=\"http://www.w3.org/2000/svg\"&gt;\r\n  &lt;a xlink:href=\"#\"&gt;\r\n    &lt;text x=\"20\" y=\"20\"&gt;This is some link&lt;/text&gt;\r\n  &lt;/a&gt;\r\n&lt;/svg&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>In the below snippet no default namespace is defined and so both <code>*</code> and <code>*|*</code> applies to all elements and so all of them get both the beige background and the black border. In other words, they work the same way when no default namespace is specified.</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"false\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>* {\r\n  background: beige;\r\n}\r\n*|* {\r\n  border: 1px solid;\r\n}</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;a href=\"#\"&gt;This is some link&lt;/a&gt;\r\n\r\n&lt;svg xmlns=\"http://www.w3.org/2000/svg\"&gt;\r\n  &lt;a xlink:href=\"#\"&gt;\r\n    &lt;text x=\"20\" y=\"20\"&gt;This is some link&lt;/text&gt;\r\n  &lt;/a&gt;\r\n&lt;/svg&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<hr>\n\n<p>As BoltClock points out in comments (<a href=\"http://stackoverflow.com/questions/34987370/what-is-the-difference-between-and-in-css/34987469#comment57763935_34987469\">1</a>,<a href=\"http://stackoverflow.com/questions/34987370/what-is-the-difference-between-and-in-css/34987469#comment57827474_34987469\">2</a>), initially namespaces applied only to XML based languages such as XHTML, SVG etc but as per latest specs, all HTML elements (that is, elements in the HTML namespace) are namespaced to <code>http://www.w3.org/1999/xhtml</code>. Firefox follows this behavior and it is consistent across all HTML5 user agents. You can find more information in <a href=\"http://stackoverflow.com/questions/3608819/what-does-this-mean-in-css/12979656#12979656\">this answer</a>.</p>\n", "title": "What is the difference between * and *|* in CSS?", "answer_id": 34987469, "question_id": 34987370, "tags": ["css", "css-selectors"]}, {"body": "<p>In CSS, <strong>* will match any element.</strong></p>\n\n<p><em>|</em> is used to match select <strong>specific elements</strong>. Both are selector used for our testing purpose</p>\n", "title": "What is the difference between * and *|* in CSS?", "answer_id": 35041239, "question_id": 34987370, "tags": ["css", "css-selectors"]}], "question_id": 34987370}, {"body": "<p>I have an action that updates notification state of my application. Usually this notification will be an error or info of some sort. I need to then dispatch another action after 5 seconds that will return the notification state to initially one, so no notification. Main reason behind this is to provide functionality where notifications disappear automatically after 5 seconds.</p>\n\n<p>I had no luck with using <code>setTimeout</code> and returning another action and can't find how this is done online. So any advice is welcome.</p>\n", "title": "How to dispatch a Redux action with a timeout?", "tags": ["javascript", "redux"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35411423/how-to-dispatch-a-redux-action-with-a-timeout", "answers": [{"body": "<p>You could do this with <a href=\"https://github.com/gaearon/redux-thunk\">redux-thunk</a>. There is a <a href=\"http://redux.js.org/docs/advanced/AsyncActions.html\">guide in redux document</a> for async actions like setTimeout.</p>\n", "title": "How to dispatch a Redux action with a timeout?", "answer_id": 35411701, "question_id": 35411423, "tags": ["javascript", "redux"]}, {"body": "<p>Don\u2019t fall into the <a href=\"http://amasad.me/2016/01/03/overcoming-intuition-in-programming/\">trap of thinking a library should prescribe how to do everything</a>. If you want to do something with a timeout in JavaScript, you need to use <code>setTimeout</code>. There is no reason why Redux actions should be any different.</p>\n\n<p>Redux <em>does</em> offer some alternative ways of dealing with asynchronous stuff, but you should only use those when you realize you are repeating too much code. Unless you have this problem, use what the language offers and go for the simplest solution.</p>\n\n<h2>Writing Async Code Inline</h2>\n\n<p>This is by far the simplest way. And there\u2019s nothing specific to Redux here.</p>\n\n<pre><code>store.dispatch({ type: 'SHOW_NOTIFICATION', text: 'You logged in.' })\nsetTimeout(() =&gt; {\n  store.dispatch({ type: 'HIDE_NOTIFICATION' })\n}, 5000)\n</code></pre>\n\n<p>Similarly, from inside a connected component:</p>\n\n<pre><code>this.props.dispatch({ type: 'SHOW_NOTIFICATION', text: 'You logged in.' })\nsetTimeout(() =&gt; {\n  this.props.dispatch({ type: 'HIDE_NOTIFICATION' })\n}, 5000)\n</code></pre>\n\n<p>The only difference is that in a connected component you usually don\u2019t have access to the store itself, but get either <code>dispatch()</code> or specific action creators injected as props. However this doesn\u2019t make any difference for us.</p>\n\n<p>If you don\u2019t like making typos when dispatching the same actions from different components, you might want to extract action creators instead of dispatching action objects inline:</p>\n\n<pre><code>// actions.js\nexport function showNotification(text) {\n  return { type: 'SHOW_NOTIFICATION', text }\n}\nexport function hideNotification() {\n  return { type: 'HIDE_NOTIFICATION' }\n}\n\n// component.js\nimport { showNotification, hideNotification } from '../actions'\n\nthis.props.dispatch(showNotification('You just logged in.'))\nsetTimeout(() =&gt; {\n  this.props.dispatch(hideNotification())\n}, 5000)\n</code></pre>\n\n<p>Or, if you have previously bound them with <code>connect()</code>:</p>\n\n<pre><code>this.props.showNotification('You just logged in.')\nsetTimeout(() =&gt; {\n  this.props.hideNotification()\n}, 5000)\n</code></pre>\n\n<p>So far we have not used any middleware or other advanced concept. </p>\n\n<h2>Extracting Async Action Creator</h2>\n\n<p>The approach above works fine in simple cases but you might find that it has a few problems:</p>\n\n<ul>\n<li>It forces you to duplicate this logic anywhere you want to show a notification.</li>\n<li>The notifications have no IDs so you\u2019ll have a race condition if you show two notifications fast enough. When the first timeout finishes, it will dispatch <code>HIDE_NOTIIFICATION</code>, erroneously hiding the second notification sooner than after the timeout.</li>\n</ul>\n\n<p>To solve these problems, you would need to extract a function that centralizes the timeout logic and dispatches those two actions. It might look like this:</p>\n\n<pre><code>// actions.js\nfunction showNotification(id, text) {\n  return { type: 'SHOW_NOTIFICATION', id, text }\n}\nfunction hideNotification(id) {\n  return { type: 'HIDE_NOTIFICATION', id }\n}\n\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(dispatch, text) {\n  // Assigning IDs to notifications lets reducer ignore HIDE_NOTIFICATION\n  // for the notification that is not currently visible.\n  // Alternatively, we could store the interval ID and call\n  // clearInterval(), but we\u2019d still want to do it in a single place.\n  const id = nextNotificationId++\n  dispatch(showNotification(id, text))\n\n  setTimeout(() =&gt; {\n    dispatch(hideNotification(id))\n  }, 5000)\n}\n</code></pre>\n\n<p>Now components can use <code>showNotificationWithTimeout</code> without duplicating this logic or having race conditions with different notifications:</p>\n\n<pre><code>// component.js\nshowNotificationWithTimeout(this.props.dispatch, 'You just logged in.')\n\n// otherComponent.js\nshowNotificationWithTimeout(this.props.dispatch, 'You just logged out.')    \n</code></pre>\n\n<p>Why does <code>showNotificationWithTimeout()</code> accept <code>dispatch</code> as the first argument? Because it needs to dispatch actions to the store. Normally a component has access to <code>dispatch</code> but since we want an external function to take control over dispatching, we need to give it control over dispatching.</p>\n\n<p>If you had a singleton store exported from some module, you could just import it and <code>dispatch</code> directly on it instead:</p>\n\n<pre><code>// store.js\nexport default createStore(reducer)\n\n// actions.js\nimport store from './store'\n\n// ...\n\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  const id = nextNotificationId++\n  store.dispatch(showNotification(id, text))\n\n  setTimeout(() =&gt; {\n    store.dispatch(hideNotification(id))\n  }, 5000)\n}\n\n// component.js\nshowNotificationWithTimeout('You just logged in.')\n\n// otherComponent.js\nshowNotificationWithTimeout('You just logged out.')    \n</code></pre>\n\n<p>This looks simpler but <strong>we don\u2019t recommend this approach</strong>. The main reason we dislike it is because <strong>it forces store to be a singleton</strong>. This makes it very hard to implement <a href=\"http://redux.js.org/docs/recipes/ServerRendering.html\">server rendering</a>. On the server, you will want each request to have its own store, so that different users get different preloaded data.</p>\n\n<p>A singleton store also makes testing harder. You can no longer mock a store when testing action creators because they reference a specific real store exported from a specific module. You can\u2019t even reset its state from outside.</p>\n\n<p>So while you technically can export a singleton store from a module, we discourage it. Don\u2019t do this unless you are sure that your app will never add server rendering.</p>\n\n<p>Getting back to the previous version:</p>\n\n<pre><code>// actions.js\n\n// ...\n\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(dispatch, text) {\n  const id = nextNotificationId++\n  dispatch(showNotification(id, text))\n\n  setTimeout(() =&gt; {\n    dispatch(hideNotification(id))\n  }, 5000)\n}\n\n// component.js\nshowNotificationWithTimeout(this.props.dispatch, 'You just logged in.')\n\n// otherComponent.js\nshowNotificationWithTimeout(this.props.dispatch, 'You just logged out.')    \n</code></pre>\n\n<p>This solves the problems with duplication of logic and saves us from race conditions.</p>\n\n<h2>Thunk Middleware</h2>\n\n<p>For simple apps, the approach should suffice. Don\u2019t worry about middleware if you\u2019re happy with it.</p>\n\n<p>In larger apps, however, you might find certain inconveniences around it.</p>\n\n<p>For example, it seems unfortunate that we have to pass <code>dispatch</code> around. This makes it trickier to <a href=\"https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0\">separate container and presentational components</a> because any component that dispatches Redux actions asynchronously in the manner above has to accept <code>dispatch</code> as a prop so it can pass it further. You can\u2019t just bind action creators with <code>connect()</code> anymore because <code>showNotificationWithTimeout()</code> is not really an action creator. It does not return a Redux action.</p>\n\n<p>In addition, it can be awkward to remember which functions are synchronous action creators like <code>showNotification()</code> and which are asynchronous helpers like <code>showNotificationWithTimeout()</code>. You have to use them differently and be careful not to mistake them with each other.</p>\n\n<p>This was the motivation for <strong>finding a way to \u201clegitimize\u201d this pattern of providing <code>dispatch</code> to a helper function, and help Redux \u201csee\u201d such asynchronous action creators as a special case of normal action creators</strong> rather than totally different functions.</p>\n\n<p>If you\u2019re still with us and you also recognize as a problem in your app, you are welcome to use the <a href=\"http://github.com/gaearon/redux-thunk\">Redux Thunk</a> middleware.</p>\n\n<p>In a gist, Redux Thunk teaches Redux to recognize special kinds of actions that are in fact functions:</p>\n\n<pre><code>import { createStore, applyMiddleware } from 'redux'\nimport thunk from 'redux-thunk'\n\nconst store = createStore(\n  reducer,\n  applyMiddleware(thunk)\n)\n\n// It still recognizes plain object actions\nstore.dispatch({ type: 'INCREMENT' })\n\n// But with thunk middleware, it also recognizes functions\nstore.dispatch(function (dispatch) {\n  // ... which themselves may dispatch many times\n  dispatch({ type: 'INCREMENT' })\n  dispatch({ type: 'INCREMENT' })\n  dispatch({ type: 'INCREMENT' })\n\n  setTimeout(() =&gt; {\n    // ... even asynchronously!\n    dispatch({ type: 'DECREMENT' })\n  }, 1000)\n})\n</code></pre>\n\n<p>When this middleware is enabled, <strong>if you dispatch a function</strong>, Redux Thunk middleware will give it <code>dispatch</code> as an argument. It will also \u201cswallow\u201d such actions so don\u2019t worry about your reducers receiving weird function arguments. Your reducers will only receive plain object actions\u2014either emitted directly, or emitted by the functions as we just described.</p>\n\n<p>This does not look very useful, does it? Not in this particular situation. However it lets us declare <code>showNotificationWithTimeout()</code> as a regular Redux action creator:</p>\n\n<pre><code>// actions.js\nfunction showNotification(id, text) {\n  return { type: 'SHOW_NOTIFICATION', id, text }\n}\nfunction hideNotification(id) {\n  return { type: 'HIDE_NOTIFICATION', id }\n}\n\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch) {\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() =&gt; {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n</code></pre>\n\n<p>Note how the function is almost identical to the one we wrote in the previous section. However it doesn\u2019t accept <code>dispatch</code> as the first argument. Instead it <em>returns</em> a function that accepts <code>dispatch</code> as the first argument.</p>\n\n<p>How would we use it in our component? Definitely, we could write this:</p>\n\n<pre><code>// component.js\nshowNotificationWithTimeout('You just logged in.')(this.props.dispatch)\n</code></pre>\n\n<p>We are calling the async action creator to get the inner function that wants just <code>dispatch</code>, and then we pass <code>dispatch</code>.</p>\n\n<p>However this is even more awkward than the original version! Why did we even go that way?</p>\n\n<p>Because of what I told you before. <strong>If Redux Thunk middleware is enabled, any time you attempt to dispatch a function instead of an action object, the middleware will call that function with <code>dispatch</code> method itself as the first argument</strong>.</p>\n\n<p>So we can do this instead:</p>\n\n<pre><code>// component.js\nthis.props.dispatch(showNotificationWithTimeout('You just logged in.'))\n</code></pre>\n\n<p>Finally, dispatching an asynchronous action (really, a series of actions) looks no different than dispatching a single action synchronously to the component. Which is good because components shouldn\u2019t care whether something happens synchronously or asynchronously. We just abstracted that away.</p>\n\n<p>Notice that since we \u201ctaught\u201d Redux to recognize such \u201cspecial\u201d action creators (we call them <a href=\"https://en.wikipedia.org/wiki/Thunk\">thunk</a> action creators), we can now use them in any place where we would use regular action creators. For example, we can use them with <code>connect()</code>:</p>\n\n<pre><code>// actions.js\n\nfunction showNotification(id, text) {\n  return { type: 'SHOW_NOTIFICATION', id, text }\n}\nfunction hideNotification(id) {\n  return { type: 'HIDE_NOTIFICATION', id }\n}\n\nlet nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch) {\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() =&gt; {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n\n// component.js\n\nimport { connect } from 'react-redux'\n\n// ...\n\nthis.props.showNotificationWithTimeout('You just logged in.')\n\n// ...\n\nexport default connect(\n  mapStateToProps,\n  { showNotificationWithTimeout }\n)(MyComponent)\n</code></pre>\n\n<h2>Reading State in Thunks</h2>\n\n<p>Usually your reducers contain the business logic for determining the next state. However, reducers only kick in after the actions are dispatched. What if you have a side effect (such as calling an API) in a thunk action creator, and you want to prevent it under some condition?</p>\n\n<p>Without using the thunk middleware, you\u2019d just do this check inside the component:</p>\n\n<pre><code>// component.js\nif (this.props.areNotificationsEnabled) {\n  showNotificationWithTimeout(this.props.dispatch, 'You just logged in.')\n}\n</code></pre>\n\n<p>However, the point of extracting an action creator was to centralize this repetitive logic across many components. Fortunately, Redux Thunk offers you a way to <em>read</em> the current state of the Redux store. In addition to <code>dispatch</code>, it also passes <code>getState</code> as the second argument to the function you return from your thunk action creator. This lets the thunk read the current state of the store.</p>\n\n<pre><code>let nextNotificationId = 0\nexport function showNotificationWithTimeout(text) {\n  return function (dispatch, getState) {\n    // Unlike in a regular action creator, we can exit early in a thunk\n    // Redux doesn\u2019t care about its return value (or lack of it)\n    if (!getState().areNotificationsEnabled) {\n      return\n    }\n\n    const id = nextNotificationId++\n    dispatch(showNotification(id, text))\n\n    setTimeout(() =&gt; {\n      dispatch(hideNotification(id))\n    }, 5000)\n  }\n}\n</code></pre>\n\n<p>Don\u2019t abuse this pattern. It is good for bailing out of API calls when there is cached data available, but it is not a very good foundation to build your business logic upon. If you use <code>getState()</code> only to conditionally dispatch different actions, consider putting the business logic into the reducers instead.</p>\n\n<h2>Next Steps</h2>\n\n<p>Now that you have a basic intuition about how thunks work, check out Redux <a href=\"http://redux.js.org/docs/introduction/Examples.html#async\">async example</a> which uses them.</p>\n\n<p>You may find many examples in which thunks return Promises. This is not required but can be very convenient. Redux doesn\u2019t care what you return from a thunk, but it gives you its return value from <code>dispatch()</code>. This is why you can return a Promise from a thunk and wait for it to complete by calling <code>dispatch(someThunkReturningPromise()).then(...)</code>.</p>\n\n<p>You may also split complex thunk action creators into several smaller thunk action creators. The <code>dispatch</code> method provided by thunks can accept thunks itself, so you can apply the pattern recursively. Again, this works best with Promises because you can implement asynchronous control flow on top of that.</p>\n\n<p>For some apps, you may find yourself in a situation where your asynchronous control flow requirements are too complex to be expressed with thunks. For example, retrying failed requests, reauthorization flow with tokens, or a step-by-step onboarding can be too verbose and error-prone when written this way. In this case, you might want to look at more advanced asynchronous control flow solutions such as <a href=\"https://github.com/yelouafi/redux-saga\">Redux Saga</a> or <a href=\"https://github.com/raisemarketplace/redux-loop\">Redux Loop</a>. Evaluate them, compare the examples relevant to your needs, and pick the one you like the most.</p>\n\n<p>Finally, don\u2019t use anything (including thunks) if you don\u2019t have the genuine need for them. Remember that, depending on the requirements, your solution might look as simple as</p>\n\n<pre><code>store.dispatch({ type: 'SHOW_NOTIFICATION', text: 'You logged in.' })\nsetTimeout(() =&gt; {\n  store.dispatch({ type: 'HIDE_NOTIFICATION' })\n}, 5000)\n</code></pre>\n\n<p>Don\u2019t sweat it unless you know why you\u2019re doing this.</p>\n", "title": "How to dispatch a Redux action with a timeout?", "answer_id": 35415559, "question_id": 35411423, "tags": ["javascript", "redux"]}, {"body": "<p>I would recommend also taking a look at the <a href=\"http://www.infoq.com/articles/no-more-mvc-frameworks\" rel=\"nofollow\">SAM pattern</a>. </p>\n\n<p>The SAM pattern advocates for including a \"next-action-predicate\" where (automatic) actions such as \"notifications disappear automatically after 5 seconds\" are triggered once the model has been updated (SAM model ~ reducer state + store). </p>\n\n<p>The pattern advocates for sequencing actions and model mutations one at a time, because the \"control state\" of the model \"controls\" which actions are enabled and/or automatically executed by the next-action predicate. You simply cannot predict (in general) what state the system will be prior to processing an action and hence whether your next expected action will be allowed/possible. </p>\n\n<p>So for instance the code, </p>\n\n<pre><code>export function showNotificationWithTimeout(dispatch, text) {\n  const id = nextNotificationId++\n  dispatch(showNotification(id, text))\n\n  setTimeout(() =&gt; {\n    dispatch(hideNotification(id))\n  }, 5000)\n}\n</code></pre>\n\n<p>would not be allowed with SAM, because the fact that a hideNotification action can be dispatched is dependent on the model successfully accepting the value \"showNotication: true\". There could be other parts of the model that prevents it from accepting it and therefore, there would be no reason to trigger the hideNotification action. </p>\n\n<p>I would highly recommend that implement a proper next-action predicate after the store updates and the new control state of the model can be known. That's the safest way to implement the behavior you are looking for.</p>\n\n<p>You can join us on Gitter if you'd like. There is also a <a href=\"http://jdubray.github.io/sam/#\" rel=\"nofollow\">SAM getting started guide available here</a>.</p>\n", "title": "How to dispatch a Redux action with a timeout?", "answer_id": 35592180, "question_id": 35411423, "tags": ["javascript", "redux"]}, {"body": "<h1>Using Redux-saga</h1>\n\n<p>As Dan Abramov said, if you want more advanced control over your async code, you might take a look at <a href=\"https://github.com/yelouafi/redux-saga\">redux-saga</a>. I've tried to explain how redux-saga is better (but also more complex) than redux-thunk <a href=\"http://stackoverflow.com/a/34623840/82609\">here</a>.</p>\n\n<p>I'll try here to describe here the notification system I built on top of redux-saga.</p>\n\n<h1>Advanced notification system specification</h1>\n\n<ul>\n<li>You can request a notification to be displayed</li>\n<li>You can request a notification to hide</li>\n<li>A notification should not be displayed more than 4 seconds</li>\n<li>Multiple notifications can be displayed at the same time</li>\n<li>No more than 3 notifications can be displayed at the same time</li>\n<li>If a notification is requested while there are already 3 displayed notifications, then queue/postpone it.</li>\n</ul>\n\n<h1>Result</h1>\n\n<p>Screenshot of my production app <a href=\"http://Stample.co\">Stample.co</a></p>\n\n<p><a href=\"http://i.stack.imgur.com/L80nq.png\"><img src=\"http://i.stack.imgur.com/L80nq.png\" alt=\"toasts\"></a></p>\n\n<h1>Code</h1>\n\n<p>Code from production app <a href=\"http://Stample.co\">Stample.co</a>. </p>\n\n<p>Here I named the notification a <code>toast</code> but this is a naming detail.</p>\n\n<pre><code>function* toastSaga() {\n\n    const MaxToasts = 3;\n    const ToastDisplayTime = 4000;\n\n    let pendingToasts = [];\n    let activeToasts = [];\n\n    function* displayToast(toast) {\n        if ( activeToasts &gt;= MaxToasts ) {\n            throw new Error(\"can't display more than \" + MaxToasts + \" at the same time\");\n        }\n        activeToasts = [...activeToasts,toast];\n        yield put(events.toastDisplayed(toast));\n        yield call(delay,ToastDisplayTime);\n        yield put(events.toastHidden(toast));\n        activeToasts = _.without(activeToasts,toast);\n    }\n\n    function* toastRequestsWatcher() {\n        while ( true ) {\n            const event = yield take(Names.TOAST_DISPLAY_REQUESTED);\n            const newToast = event.data.toastData;\n            pendingToasts = [...pendingToasts,newToast];\n        }\n    }\n\n    function* toastScheduler() {\n        while ( true ) {\n            if ( activeToasts.length &lt; MaxToasts &amp;&amp; pendingToasts.length &gt; 0 ) {\n                const [firstToast,...remainingToasts] = pendingToasts;\n                pendingToasts = remainingToasts;\n                yield fork(displayToast,firstToast);\n                // Add little delay so that 2 concurrent 2 toast requests aren't display at the same time\n                yield call(delay,300);\n            }\n            else {\n                yield call(delay,50);\n            }\n        }\n    }\n\n    yield [\n        call(toastRequestsWatcher),\n        call(toastScheduler)\n    ]\n}\n</code></pre>\n\n<h1>Usage</h1>\n\n<p>You can simply dispatch <code>TOAST_DISPLAY_REQUESTED</code> events. If you dispatch 4 requests, only 3 notifications will be displayed, and the 4th one will appear a bit later once the 1st notification disappears.</p>\n\n<p>Note that I don't specifically recommend dispatching <code>TOAST_DISPLAY_REQUESTED</code> from JSX. You'd rather add another saga that listens to your already-existing app events, and then dispatch the <code>TOAST_DISPLAY_REQUESTED</code>: your component that triggers the notification, does not have to be tightly coupled to the notification system.</p>\n\n<h1>Conclusion</h1>\n\n<p>My code is not perfect but runs in production with 0 bugs for months. Redux-saga and generators are a bit hard initially but once you understand them this kind of system is pretty easy to build.</p>\n\n<p>It's even quite easy to implement more complex rules, like:</p>\n\n<ul>\n<li>when too many notifications are \"queued\", give less display-time for each notification so that the queue size can decrease faster. </li>\n<li>detect window size changes, and change the maximum number of displayed notifications accordingly (for example, desktop=3, phone portrait = 2, phone landscape = 1)</li>\n</ul>\n\n<p>Honnestly, good luck implementing this kind of stuff properly with thunks.</p>\n", "title": "How to dispatch a Redux action with a timeout?", "answer_id": 38574266, "question_id": 35411423, "tags": ["javascript", "redux"]}, {"body": "<p>After trying the various popular approaches (action creators, thunks, sagas, epics, effects, custom middleware), I still felt that maybe there was room for improvement so I documented my journey in this blog article, <a href=\"http://codewinds.com/blog/2016-08-16-business-logic-redux.html\" rel=\"nofollow\">Where do I put my business logic in a React/Redux application?</a> </p>\n\n<p>Much like the discussions here, I tried to contrast and compare the various approaches. Eventually it led me to introducing a new library <a href=\"https://github.com/jeffbski/redux-logic\" rel=\"nofollow\">redux-logic</a> which takes inspiration from epics, sagas, custom middleware. </p>\n\n<p>It allows you to intercept actions to validate, verify, authorize, as well as providing a way to perform async IO. </p>\n\n<p>Some common functionality can simply be declared like debouncing, throttling, cancellation, and only using the response from the latest request (takeLatest). redux-logic wraps your code providing this functionality for you.</p>\n\n<p>That frees you to implement your core business logic however you like. You don't have to use observables or generators unless you want to. Use functions and callbacks, promises, async functions (async/await), etc. </p>\n\n<p>The code for doing a simple 5s notification would be something like:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>const notificationHide = createLogic({\r\n  // the action type that will trigger this logic\r\n  type: 'NOTIFICATION_DISPLAY',\r\n  \r\n  // your business logic can be applied in several\r\n  // execution hooks: validate, transform, process\r\n  // We are defining our code in the process hook below\r\n  // so it runs after the action hit reducers, hide 5s later\r\n  process({ getState, action }, dispatch) {\r\n    setTimeout(() =&gt; {\r\n      dispatch({ type: 'NOTIFICATION_CLEAR' });\r\n    }, 5000);\r\n  }\r\n});\r\n    </code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>I have a more advanced notification example in my repo that works similar to what Sebastian Lorber described where you could limit the display to N items and rotate through any that queued up. <a href=\"https://github.com/jeffbski/redux-logic/tree/master/examples/notification\" rel=\"nofollow\">redux-logic notification example</a> </p>\n\n<p>I have a variety of <a href=\"https://github.com/jeffbski/redux-logic#examples\" rel=\"nofollow\">redux-logic jsfiddle live examples as well as full examples</a>. I'm continuing to work on docs and examples. </p>\n\n<p>I'd love to hear your feedback. </p>\n", "title": "How to dispatch a Redux action with a timeout?", "answer_id": 39135262, "question_id": 35411423, "tags": ["javascript", "redux"]}], "question_id": 35411423}, {"body": "<p>As I've been using </p>\n\n<pre><code>classpath 'com.android.tools.build:gradle:+'\n</code></pre>\n\n<p>In the <strong>build.gradle</strong> file, I got the following error since <strong>gradle version 2.10</strong> has been released.\nThe error is :</p>\n\n<blockquote>\n  <p>Warning:Gradle version 2.10 is required. Current version is 2.8. If\n  using the gradle wrapper, try editing the distributionUrl in\n  C:\\Users\\blahblah\\myproject\\gradle\\wrapper\\gradle-wrapper.properties\n  to gradle-2.10-all.zip</p>\n</blockquote>\n\n<p>at first it looks easy to solve by the guide, but when I'd done the change, I got the error again. </p>\n\n<p>I'm using <strong>Android Studio 2.0</strong> with the latest <strong>Android SDK Tools 24.4.1</strong> and <strong>Android SDK Build Tools 23.0.2</strong></p>\n\n<p>I've even tried to download <strong>Gradle 2.10</strong> and put it manually on android-studio\\gradle\\ folder, but no luck.</p>\n\n<p>Any help would be appreciated.</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"], "answer_count": 17, "link": "http://stackoverflow.com/questions/34814368/gradle-version-2-10-is-required-error", "answers": [{"body": "<p>You need to change <strong>File > Settings > Builds,Execution,Deployment > Build Tools > Gradle >Gradle home</strong> path</p>\n\n<p>On Mac OS, change the path in <strong>Android Studio > Preferences > Builds,Execution,Deployment > Build Tools > Gradle >Gradle home</strong></p>\n\n<p>Or set <strong>Use default gradle wrapper</strong> and edit <code>Project\\gradle\\wrapper\\gradle-wrapper.properties</code> files field <code>distributionUrl</code>  like this</p>\n\n<pre><code>distributionUrl=https\\://services.gradle.org/distributions/gradle-2.10-all.zip\n</code></pre>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 34814536, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>An easier way to accomplish this is to go to:\n<strong>Open Module Settings (your project) > Project > Gradle version</strong> and enter 2.10</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 34927005, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>Download the latest gradle-3.0-all.zip from</p>\n\n<p><a href=\"http://gradle.org/gradle-download/\" rel=\"nofollow\">http://gradle.org/gradle-download/</a></p>\n\n<p>download from Complete Distribution link</p>\n\n<p>open in android studio file ->settings ->gradle</p>\n\n<p>open the path and paste the downloaded zip folder gradle-3.0 in that folder</p>\n\n<p>change your gradle 2.8 to gradle 3.0 in file ->settings ->gradle</p>\n\n<p>Or you can change your gradle wrapper in the project</p>\n\n<p>edit YourProject\\gradle\\wrapper\\gradle-wrapper.properties file and edit the  field distributionUrl in to</p>\n\n<p>distributionUrl=   <a href=\"https://services.gradle.org/distributions/gradle-3.0-all.zip\" rel=\"nofollow\">https://services.gradle.org/distributions/gradle-3.0-all.zip</a></p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 34939183, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>Open Preference and search for Gradle or Navigate to <strong>Builds, Execution, Deployment > Build Tools > Gradle</strong> </p>\n\n<p>Then change Project-Level setting to <code>Use default gradle wrapper (recommended)</code> </p>\n\n<p>Or keep local gradle distribution option and set <em>Gradle home to /.../gradle-2.10</em></p>\n\n<p><a href=\"http://i.stack.imgur.com/V42LK.png\"><img src=\"http://i.stack.imgur.com/V42LK.png\" alt=\"Preference screen - Gradle\"></a></p>\n\n<blockquote>\n  <p>Make Sure that Gradle version is already setup to 2.10 in Module Settings</p>\n</blockquote>\n\n<p>On the Project Window, right click on your project then select <code>Open Module Settings</code> (<kbd>\u2318</kbd>+<kbd>\u2193</kbd>)</p>\n\n<p><a href=\"http://i.stack.imgur.com/pfJBH.png\"><img src=\"http://i.stack.imgur.com/pfJBH.png\" alt=\"Open Module Settings Project &gt; Gradle version and enter 2.10\"></a></p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 35043898, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>The Android build system uses the Android Plugin for Gradle to support building Android applications with the Gradle build toolkit. The plugin runs independent of Android Studio so the plugin and the Gradle build system can be updated independently of Android Studio.</p>\n\n<blockquote>\n  <p>Use gradle wrapper (recommended) - select this option to use Gradle\n  wrapper. Using Gradle wrapper lets you get automatic Gradle download\n  for the build. It also lets you build with the precise Gradle version.</p>\n  \n  <p>Newer versions of the Gradle plugin may require newer versions of\n  Studio to enable the new features in the IDE, but the project should\n  open in older versions of Studio, and build from there (since Gradle\n  does the build). We will be very careful about this.</p>\n</blockquote>\n\n<pre><code>Click on Settings -&gt; Build -&gt; Execution -&gt; Deployment -&gt;\nGradle -&gt; Select  default gradle wrapper\n</code></pre>\n\n<p><strong>You can use</strong></p>\n\n<pre><code>classpath 'com.android.tools.build:gradle:2.0.0-alpha3'  // or alpha2\nOr //classpath 'com.android.tools.build:gradle:2.1.3'\n</code></pre>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 35147471, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>Go to File -> Settings -> Build, Execution, Deployment -> Gradle -> choose <code>Use default gradle wrapper</code></p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 35189226, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>for me all previous solutions failed, so i just edited</p>\n\n<pre><code>Project\\gradle\\wrapper\\gradle-wrapper.properties -&gt; field distributionUrl \ndistributionUrl=https\\://services.gradle.org/distributions/gradle-2.8-all.zip\n</code></pre>\n\n<p>to</p>\n\n<pre><code>distributionUrl=https://services.gradle.org/distributions/gradle-2.10-all.zip\n</code></pre>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 35810598, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>First Check your  <strong>Project-level settings</strong> at </p>\n\n<blockquote>\n  <p>File > Settings > Build, Executions, Deployment > Build Tools > Gradle</p>\n</blockquote>\n\n<p>and Select the Option :</p>\n\n<ul>\n<li>Use default gradle wrapper (recommended) </li>\n</ul>\n\n<p>then go to Project's </p>\n\n<blockquote>\n  <p>Gradle > wrapper > gradle-wrapper.properties </p>\n</blockquote>\n\n<p>and edit version of the distributionUrl</p>\n\n<blockquote>\n  <p>distributionUrl=https://services.gradle.org/distributions/gradle-2.10-all.zip</p>\n</blockquote>\n\n<p>And you are done :)</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 35934135, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>You need to <code>change Prefrences &gt; Builds,Execution,Deployment &gt; Build Tools &gt; Gradle &gt;Gradle home path</code></p>\n\n<p>Or set Use default gradle wrapper and edit Project\\gradle\\wrapper\\gradle-wrapper.properties files field distributionUrl like this</p>\n\n<p>distributionUrl=https://services.gradle.org/distributions/gradle-2.10-all.zip</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 36504325, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>My problem maybe is different,so I just wanna tell somebody who may have the same problem as me.</p>\n\n<p>I solved this problem by change Terminal path.\nwhen I run <code>gradle clean</code>, the error come cross \"<code>Gradle version 2.10 is required. Current version is 2.2.1</code>\".</p>\n\n<p>I checked .gradle directory in my project, there are two versions in it: 2.2.1, 2.10.</p>\n\n<p>when I run <code>gradle -v</code>, it shows my current version is 2.2.1.\nI change my system environment <code>GRADLE_HOME</code> to <code>2.10</code> root directory, then I restart terminal in AS and run 'gradle -v', it still shows the current verision is 2.2.1.</p>\n\n<p>open setting -> terminal change the cmd.exe path to the System32/cmd.exe.\nthen restart terminal, run <code>gradle clean</code>,everything is fine.</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 36690989, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>In Terminal type gradlew clean. it will automatically download and install gradle version 2.10(ie latest gradle verson available)</p>\n\n<p>Eg : C:\\android\\workspace\\projectname>gradlew clean</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 36801281, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>For Android studio v2.1</p>\n\n<p>Follow these easy steps from images.</p>\n\n<ol>\n<li><p>Go \"File\" and click \"Project structure\".<a href=\"http://i.stack.imgur.com/3gGab.jpg\"><img src=\"http://i.stack.imgur.com/3gGab.jpg\" alt=\"Opening project structure settings for the project\"></a></p></li>\n<li><p>Then select \"Project\" from left menu and then change \"Gradle version\" to the version your sdk manager has installed. In my case it is 2.10 so i change version to 2.10 and then click on \"Ok\". And then android studio automatically do gradle sync again and error was fixed.<a href=\"http://i.stack.imgur.com/iLCPe.jpg\"><img src=\"http://i.stack.imgur.com/iLCPe.jpg\" alt=\"Changing gradle version to the latest installed version for the project\"></a></p></li>\n</ol>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 36954091, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>I had Android Studio version 1.5.1 installed and was running into this error. I have the following buildscript (which was working on Ubuntu but not Windows):</p>\n\n<pre><code>buildscript {\n    dependencies {\n        classpath 'com.android.tools.build:gradle:2.1.0'\n    }\n}\n</code></pre>\n\n<p>I tried:</p>\n\n<ol>\n<li><code>gradlew clean</code></li>\n<li>Use default gradle wrapper (recommended)</li>\n<li>Use local distribution and linked to downloaded gradle 2.2.1 binary</li>\n</ol>\n\n<p>None of these solutions worked unfortunately. It should be worth noting that upgrading Android Studio from 1.5.1 to 2.1.1 also failed with errors about 2.10 missing so could not be performed.</p>\n\n<p><strong>Solution:</strong> What I ended up doing was simply downloaded the latest stable version of Android Studio and installing that instead (2.1.1 at the time).  After doing that gradle synced succesfully</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 37464976, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>What worked for me on Mint 17.03 which is base off Ubuntu 14.04</p>\n\n<p>sudo add-apt-repository ppa:cwchien/gradle\nsudo apt-get update\nsudo apt-get install gradle</p>\n\n<p>Reopen Android Studio. Go to Run/Debug Configuration and there should be  an icon saying you need to Update gradle. Just click it.</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 38283856, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>For people who are using Ionic/Cordova framework, we need to change the <code>distributionUrl</code> in <code>GradleBuilder.js</code> file to <code>var distributionUrl = process.env['CORDOVA_ANDROID_GRADLE_DISTRIBUTION_URL'] || 'http\\\\://services.gradle.org/distributions/gradle-2.10-all.zip';</code></p>\n\n<p>The <code>GradleBuilder.js</code> locates at <code>project/platforms/android/cordova/lib/builders/GradleBuilder.js</code></p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 38361573, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>Easiest way for me to fix this issue:</p>\n\n<ol>\n<li>close IDE.</li>\n<li>delete \"gradle\" folder </li>\n<li>re-open project.</li>\n</ol>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 38668946, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}, {"body": "<p>File - setting - Build Execution Deployment - Gradle - Select 'Use default gradle wrapper(recommended)'</p>\n", "title": "&quot;Gradle Version 2.10 is required.&quot; Error", "answer_id": 38916761, "question_id": 34814368, "tags": ["android", "android-studio", "plugins", "gradle", "android-gradle"]}], "question_id": 34814368}, {"body": "<p>I was told today that it's possible to invoke a function without parentheses. The only ways I could think of was using functions like <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/apply\"><code>apply</code></a> or <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Function/call\"><code>call</code></a>.</p>\n\n<pre><code>f.apply(this);\nf.call(this);\n</code></pre>\n\n<p>But these require parentheses on <code>apply</code> and <code>call</code> leaving us at square one. I also considered the idea of passing the function to some sort of event handler such as <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/WindowTimers/setTimeout\"><code>setTimeout</code></a>:</p>\n\n<pre><code>setTimeout(f, 500);\n</code></pre>\n\n<p>But then the question becomes \"how do you invoke <code>setTimeout</code> without parentheses?\"</p>\n\n<p>So what's the solution to this riddle? How can you invoke a function in Javascript without using parentheses?</p>\n", "title": "Invoking a function without parentheses", "tags": ["javascript"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35949554/invoking-a-function-without-parentheses", "answers": [{"body": "<p>Here's an <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/GlobalEventHandlers/onload\">example</a> for a particular situation:</p>\n\n<pre><code>window.onload = funcRef;\n</code></pre>\n\n<p>Although that statement is not actually <strong>invoking</strong> but will lead to a <strong>future invocation</strong>.</p>\n\n<p>But, I figure grey-areas might be ok for riddles like this :)</p>\n", "title": "Invoking a function without parentheses", "answer_id": 35949597, "question_id": 35949554, "tags": ["javascript"]}, {"body": "<p>There are several different ways to call a function without parentheses. </p>\n\n<p>Let's assume you have this function defined:</p>\n\n<pre><code>function greet() {\n    console.log('hello');\n}\n</code></pre>\n\n<p>Then here follow some ways to call <code>greet</code> without parentheses:</p>\n\n<h2>1. As Constructor</h2>\n\n<p>With <code>new</code> you can invoke a function without parentheses:</p>\n\n<pre><code>new greet; // parentheses are optional in this construct.\n</code></pre>\n\n<p>From <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Operators/new\" rel=\"nofollow\">MDN on the <code>new</code> oprator</a>:</p>\n\n<blockquote>\n  <p><strong>Syntax</strong></p>\n\n<pre><code>new constructor[([arguments])]\n</code></pre>\n</blockquote>\n\n<h2>2. As <code>toString</code> or <code>valueOf</code> Implementation</h2>\n\n<p><a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Object/toString\" rel=\"nofollow\"><code>toString</code></a> and <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Object/valueOf\" rel=\"nofollow\"><code>valueOf</code></a> are special methods: they get called implicitly when a conversion is necessary:</p>\n\n<pre><code>var obj = {\n    toString: function() {\n         return 'hello';\n    }\n}\n\n'' + obj; // concatenation forces cast to string and call to toString.\n</code></pre>\n\n<p>You could (ab)use this pattern to call <code>greet</code> without parentheses:</p>\n\n<pre><code>'' + { toString: greet };\n</code></pre>\n\n<p>Or with <code>valueOf</code>:</p>\n\n<pre><code>+{ valueOf: greet };\n</code></pre>\n\n<h3>2.b Overriding <code>valueOf</code> in Function Prototype</h3>\n\n<p>You could take the previous idea to override the <code>valueOf</code> method on the <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Function/prototype\" rel=\"nofollow\"><code>Function</code> prototype</a>: </p>\n\n<pre><code>Function.prototype.valueOf = function() {\n    this.call(this);\n    // Optional improvement: avoid `NaN` issues when used in expressions.\n    return 0; \n};\n</code></pre>\n\n<p>Once you have done that, you can write:</p>\n\n<pre><code>+greet;\n</code></pre>\n\n<p>And although there are parentheses involved down the line, the actual triggering invocation has no parentheses. See more about this in the blog <a href=\"http://www.tapper-ware.net/blog/?p=87\" rel=\"nofollow\">\"Calling methods in Javascript, without really calling them\"</a></p>\n\n<h2>3. As Generator</h2>\n\n<p>You could define a <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Statements/function*\" rel=\"nofollow\">generator function</a> (with <code>*</code>), which returns an <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Iteration_protocols#iterator\" rel=\"nofollow\">iterator</a>. You can call it with the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Spread_operator\" rel=\"nofollow\">spread operator</a> or <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Statements/for...of\" rel=\"nofollow\"><code>for...of</code></a> syntax. </p>\n\n<p>First we need a generator variant of the original <code>greet</code> function:</p>\n\n<pre><code>function* greet_gen() {\n    console.log('hello');\n}\n</code></pre>\n\n<p>And then we call it without parentheses: </p>\n\n<pre><code>[...{ [Symbol.iterator]: greet_gen }];\n</code></pre>\n\n<p>Normally generators would have a <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Operators/yield\" rel=\"nofollow\"><code>yield</code></a> keyword somewhere, but it is not needed for the function to get called.</p>\n\n<p>The last statement invokes the function, but that could also be done with <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment\" rel=\"nofollow\">destructuring</a>:</p>\n\n<pre><code>[,] = { [Symbol.iterator]: greet_gen };\n</code></pre>\n\n<p>or a <code>for ... of</code> construct:</p>\n\n<pre><code>for ({} of { [Symbol.iterator]: greet_gen });\n</code></pre>\n\n<p>Note that you <em>can</em> do the above with the original <code>greet</code> function as well, but it will trigger an exception in the process, <em>after</em> <code>greet</code> has been executed (tested on FF and Chrome). You could manage the exception with a <code>try...catch</code> block.</p>\n\n<h2>4. As Getter</h2>\n\n<p>@jehna1 has a full answer on this, so give him credit. Here is a way to call a function <em>parentheses-less</em> on the global scope, avoiding the <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Object/__defineGetter__\" rel=\"nofollow\">deprecated <code>__defineGetter__</code></a> method. It uses <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Object/defineProperty\" rel=\"nofollow\"><code>Object.defineProperty</code></a> instead.</p>\n\n<p>We need to create a variant of the original <code>greet</code> function for this:</p>\n\n<pre><code>Object.defineProperty(window, 'greet_get', { get: greet });\n</code></pre>\n\n<p>And then:</p>\n\n<pre><code>greet_get;\n</code></pre>\n\n<p>Replace <code>window</code> with whatever your global object is.</p>\n\n<p>You could call the original <code>greet</code> function without leaving a trace on the global object like this:</p>\n\n<pre><code>Object.defineProperty({}, 'greet', { get: greet }).greet;\n</code></pre>\n\n<p>But one could argue we do have parentheses here (although they are not involved in the actual invocation).</p>\n\n<h2>5. As Tag Function</h2>\n\n<p>With ES6 you can call a function passing it a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals\" rel=\"nofollow\">template literal</a> with this syntax:</p>\n\n<pre><code>greet``;\n</code></pre>\n\n<p>See <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#Tagged_template_literals\" rel=\"nofollow\">\"Tagged Template Literals\"</a>. Normally you would not pass an empty literal like here, but as example of a <em>parentheses-less</em> call, it will do.</p>\n\n<h2>6. As Proxy Handler</h2>\n\n<p>In ES6, you can define a <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Proxy\" rel=\"nofollow\">proxy</a>:</p>\n\n<pre><code>var proxy = new Proxy({}, { get: greet } );\n</code></pre>\n\n<p>And then reading any property value will invoke <code>greet</code>:</p>\n\n<pre><code>proxy._; // even if property not defined, it still triggers greet\n</code></pre>\n\n<p>There are many variations of this. One more example:</p>\n\n<pre><code>var proxy = new Proxy({}, { has: greet } );\n\n1 in proxy; // triggers greet\n</code></pre>\n", "title": "Invoking a function without parentheses", "answer_id": 35949617, "question_id": 35949554, "tags": ["javascript"]}, {"body": "<p>The easiest way to do that is with the <code>new</code> operator:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>function f() {\r\n  alert('hello');\r\n}\r\n\r\nnew f;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>While that's unorthodox and unnatural, it works and is perfectly legal.</p>\n\n<p>The <code>new</code> operator doesn't require parentheses if no parameters are used.</p>\n", "title": "Invoking a function without parentheses", "answer_id": 35949619, "question_id": 35949554, "tags": ["javascript"]}, {"body": "<p>You can use getters and setters.</p>\n\n<pre><code>var h = {\n  get ello () {\n    alert(\"World\");\n  }\n}\n</code></pre>\n\n<p>Run this script just with:</p>\n\n<pre><code>h.ello  // Fires up alert \"world\"\n</code></pre>\n\n<p><strong>Edit:</strong></p>\n\n<p>We can even do arguments!</p>\n\n<pre><code>var h = {\n  set ello (what) {\n    alert(\"Hello \" + what);\n  }\n}\n\nh.ello = \"world\" // Fires up alert \"Hello world\"\n</code></pre>\n\n<p><strong>Edit 2:</strong></p>\n\n<p>You can also define global functions that can be run without parenthesis:</p>\n\n<pre><code>window.__defineGetter__(\"hello\", function() { alert(\"world\"); });\nhello;  // Fires up alert \"world\"\n</code></pre>\n\n<p>And with arguments:</p>\n\n<pre><code>window.__defineSetter__(\"hello\", function(what) { alert(\"Hello \" + what); });\nhello = \"world\";  // Fires up alert \"Hello world\"\n</code></pre>\n\n<p><strong>Disclaimer:</strong></p>\n\n<p>As @MonkeyZeus stated: Never ever shall you use this piece of code in production, no matter how good your intentions.</p>\n", "title": "Invoking a function without parentheses", "answer_id": 35949683, "question_id": 35949554, "tags": ["javascript"]}, {"body": "<p>If we accept a lateral thinking approach, in a browser there are several API's we can abuse to execute arbitrary JavaScript, including calling a function, without any actual parenthesis characters.</p>\n\n<h3>1. <code>location</code> and <code>javascript:</code> protocol:</h3>\n\n<p>One such technique is to abuse the <code>javascript:</code> protocol on <code>location</code> assignment.</p>\n\n<p><strong>Working Example:</strong></p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>location='javascript:alert\\x281\\x29'</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>Although <em>technically</em> <code>\\x28</code> and <code>\\x29</code> are still parenthesis once the code is evaluated, the actual <code>(</code> and <code>)</code> character does not appear. The parentheses are escaped in a string of JavaScript which gets evaluated on assignment.</p>\n\n<hr>\n\n<h3>2. <code>onerror</code> and <code>eval</code>:</h3>\n\n<p>Similarly, depending on the browser we can abuse the global <code>onerror</code>, by setting it to <code>eval</code>, and throwing something that will stringify to valid JavaScript. This one is trickier, because browsers are inconsistent in this behavior, but here's an example for Chrome.</p>\n\n<p><strong>Working example for Chrome (not Firefox, others untested):</strong></p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>window.onerror=eval;Uncaught=0;throw';alert\\x281\\x29';</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>This works in Chrome because <code>throw'test'</code> will pass <code>'Uncaught test'</code> as the first argument to <code>onerror</code>, which is almost valid JavaScript. If we instead do <code>throw';test'</code> it will pass <code>'Uncaught ;test'</code>. Now we have valid JavaScript! Just define <code>Uncaught</code>, and replace test with the payload.</p>\n\n<hr>\n\n<h2>In conclusion:</h2>\n\n<p>Such code is <strong>truly awful</strong>, and <strong>should never be used</strong>, but is sometimes used in XSS attacks, so the moral of the story is don't rely on filtering parenthesis to prevent XSS. Using a <a href=\"https://developer.mozilla.org/en-US/docs/Web/Security/CSP\">CSP</a> to prevent such code would also be a good idea.</p>\n", "title": "Invoking a function without parentheses", "answer_id": 35982227, "question_id": 35949554, "tags": ["javascript"]}], "question_id": 35949554}, {"body": "<p>I'm reading <a href=\"http://jibbering.com/faq/notes/type-conversion/\">this</a> but I'm confused by what is written in the <strong>parseInt with a radix argument</strong> chapter</p>\n\n<p><a href=\"http://i.stack.imgur.com/b8ABb.png\"><img src=\"http://i.stack.imgur.com/b8ABb.png\" alt=\"table of parseInt(_, 3) outcomes\"></a></p>\n\n<p>Why is it that <code>parseInt(8, 3)</code> \u2192 <code>NaN</code> and <code>parseInt(16, 3)</code> \u2192 <code>1</code>?</p>\n\n<p>AFAIK 8 and 16 are not base-3 numbers, so <code>parseInt(16, 3)</code> should return <code>NaN</code> too</p>\n\n<p><a href=\"http://i.stack.imgur.com/RzkKJ.png\"><img src=\"http://i.stack.imgur.com/RzkKJ.png\" alt=\"the first ten base-3 natural numbers\"></a></p>\n", "title": "Why is it that parseInt(8,3) == NaN and parseInt(16,3) == 1?", "tags": ["javascript", "numbers", "parseint", "radix"], "answer_count": 2, "link": "http://stackoverflow.com/questions/39147108/why-is-it-that-parseint8-3-nan-and-parseint16-3-1", "answers": [{"body": "<p>This is something people trip over all the time, even when they know about it. :-) You're seeing this for the same reason <code>parseInt(\"1abc\")</code> returns 1: <code>parseInt</code> stops at the first invalid character and returns whatever it has at that point. If there are no valid characters to parse, it returns <code>NaN</code>.</p>\n\n<p><code>parseInt(8, 3)</code> means \"parse <code>\"8\"</code> in base 3\" (note that it converts the number <code>8</code> to a string; <a href=\"http://www.ecma-international.org/ecma-262/7.0/index.html#sec-parseint-string-radix\">details in the spec</a>). But in base 3, the single-digit numbers are just <code>0</code>, <code>1</code>, and <code>2</code>. It's like asking it to parse <code>\"9\"</code> in octal. Since there were <strong>no</strong> valid characters, you got <code>NaN</code>.</p>\n\n<p><code>parseInt(16, 3)</code> is asking it to parse <code>\"16\"</code> in base 3. Since it can parse the <code>1</code>, it does, and then it stops at the <code>6</code> because it can't parse it. So it returns <code>1</code>.</p>\n\n<hr>\n\n<p>Since this question is getting a lot of attention and might rank highly in search results, here's a rundown of options for converting strings to numbers in JavaScript, with their various idiosyncracies and applications (lifted from another answer of mine here on SO):</p>\n\n<ul>\n<li><p><code>parseInt(str[, radix])</code> - Converts as much of the beginning of the string as it can into a whole (integer) number, ignoring extra characters at the end. So <code>parseInt(\"10x\")</code> is <code>10</code>; the <code>x</code> is ignored. Supports an optional radix (number base) argument, so <code>parseInt(\"15\", 16)</code> is <code>21</code> (<code>15</code> in hex). If there's no radix, assumes decimal unless the string starts with <code>0x</code> (or <code>0X</code>), in which case it skips those and assumes hex. <em>(Some browsers used to treat strings starting with <code>0</code> as octal; that behavior was never specified, and was <a href=\"http://ecma-international.org/ecma-262/5.1/#sec-E\">specifically disallowed</a> in the ES5 specification.)</em> Returns <code>NaN</code> if no parseable digits are found.</p></li>\n<li><p><code>parseFloat(str)</code> - Like <code>parseInt</code>, but does floating-point numbers and only supports decimal. Again extra characters on the string are ignored, so <code>parseFloat(\"10.5x\")</code> is <code>10.5</code> (the <code>x</code> is ignored). As only decimal is supported, <code>parseFloat(\"0x15\")</code> is <code>0</code> (because parsing ends at the <code>x</code>). Returns <code>NaN</code> if no parseable digits are found.</p></li>\n<li><p>Unary <code>+</code>, e.g. <code>+str</code> - <em>(E.g., implicit conversion)</em> Converts the <em>entire</em> string to a number using floating point and JavaScript's standard number notation (just digits and a decimal point = decimal; <code>0x</code> prefix = hex; <code>0o</code> prefix = octal [ES2015+]; <strong>some</strong> implementations extend it to treat a leading <code>0</code> as octal, but not in strict mode). <code>+\"10x\"</code> is <code>NaN</code> because the <code>x</code> is <strong>not</strong> ignored. <code>+\"10\"</code> is <code>10</code>, <code>+\"10.5\"</code> is <code>10.5</code>, <code>+\"0x15\"</code> is <code>21</code>, <code>+\"0o10\"</code> is <code>8</code> [ES2015+]. Has a gotcha: <code>+\"\"</code> is <code>0</code>, not <code>NaN</code> as you might expect.</p></li>\n<li><p><code>Number(str)</code> - Exactly like implicit conversion (e.g., like the unary <code>+</code> above), but slower on some implementations. <em>(Not that it's likely to matter.)</em></p></li>\n</ul>\n", "title": "Why is it that parseInt(8,3) == NaN and parseInt(16,3) == 1?", "answer_id": 39147168, "question_id": 39147108, "tags": ["javascript", "numbers", "parseint", "radix"]}, {"body": "<p>For the same reason that</p>\n\n<pre><code>&gt;&gt; parseInt('1foobar',3)\n&lt;- 1\n</code></pre>\n\n<p>In <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/parseInt\">the doc</a>, <code>parseInt</code> takes a string. And</p>\n\n<blockquote>\n  <p>If <em>string</em> is not a string, then it is converted to a string</p>\n</blockquote>\n\n<p>So <code>16</code>, <code>8</code>, or <code>'1foobar'</code> is first converted to string.</p>\n\n<p>Then</p>\n\n<blockquote>\n  <p>If <code>parseInt</code> encounters a character that is not a numeral in the specified radix, it ignores it and all succeeding characters</p>\n</blockquote>\n\n<p>Meaning it converts up to where it can. The <code>6</code>, <code>8</code>, and <code>foobar</code> are ignored, and only what is before is converted. If there is nothing, <code>NaN</code> is returned.</p>\n", "title": "Why is it that parseInt(8,3) == NaN and parseInt(16,3) == 1?", "answer_id": 39147209, "question_id": 39147108, "tags": ["javascript", "numbers", "parseint", "radix"]}], "question_id": 39147108}, {"body": "<p>Many Python programmers are probably unaware that the syntax of <code>while</code> loops and <code>for</code> loops includes an optional <code>else:</code> clause:</p>\n\n<pre><code>for val in iterable:\n    do_something(val)\nelse:\n    clean_up()\n</code></pre>\n\n<p>The body of the <code>else</code> clause is a good place for certain kinds of clean-up actions, and is executed on normal termination of the loop: I.e., exiting the loop with <code>return</code> or <code>break</code> skips the <code>else</code> clause; exiting after a <code>continue</code> executes it. I know this only because I just <a href=\"https://docs.python.org/3/reference/compound_stmts.html#the-for-statement\">looked it up</a> (yet again), because  I can never remember <em>when</em> the <code>else</code> clause is executed.</p>\n\n<p>Always? On \"failure\" of the loop, as the name suggests? On regular termination? Even if the loop is exited with <code>return</code>? I can never be entirely sure without looking it up.</p>\n\n<p>I blame my persisting uncertainty on the choice of keyword: I find <code>else</code> incredibly unmnemonic for this semantics. My question is not \"why is this keyword used for this purpose\" (which I would probably vote to close, though only after reading the answers and comments), but <strong>how can I think about the <code>else</code> keyword so that its semantics make sense, and I can therefore remember it?</strong></p>\n\n<p>I'm sure there was a fair amount of discussion about this, and I can imagine that the choice was made for consistency with the <code>try</code> statement's <code>else:</code> clause (which I also have to look up), and with the goal of not adding to the list of Python's reserved words. Perhaps the reasons for choosing <code>else</code> will clarify its function and make it more memorable, but I'm after connecting name to function, not after historical explanation per se.</p>\n\n<p>The answers to <a href=\"http://stackoverflow.com/q/9979970/699305\">this question</a>, which my question was briefly closed as a duplicate of, contain a lot of interesting back story. My question has a different focus (how to connect the specific semantics of <code>else</code> with the keyword choice), but I feel there should be a link to this question somewhere.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "tags": ["python", "loops", "for-loop", "while-loop"], "answer_count": 13, "link": "http://stackoverflow.com/questions/37642573/how-can-i-make-sense-of-the-else-statement-in-python-loops", "answers": [{"body": "<p>When does an <code>if</code> execute an <code>else</code>? When its condition is false. It is exactly the same for the <code>while</code>/<code>else</code>.  So you can think of <code>while</code>/<code>else</code> as just an <code>if</code> that keeps running its true condition until it evaluates false.  A <code>break</code> doesn't change that.  It just jumps of of the containing loop with no evaluation.  The <code>else</code> is only executed if <em>evaluating</em> the <code>if</code>/<code>while</code> condition is false.</p>\n\n<p>The <code>for</code> is similar, except its false condition is exhausting its iterator. </p>\n\n<p><code>continue</code> and <code>break</code> don't execute <code>else</code>.  That isn't their function.  The <code>break</code> exits the containing loop.  The <code>continue</code> goes back to the top of the containing loop, where the loop condition is evaluated.  It is the act of evaluating <code>if</code>/<code>while</code> to false (or <code>for</code> has no more items) that executes <code>else</code> and no other way.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37643193, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>This is what it essentially means:</p>\n\n<pre><code>for/while ...:\n    if ...:\n        break\nif there was a break:\n    pass\nelse:\n    ...\n</code></pre>\n\n<p>It's a nicer way of writing of this common pattern:</p>\n\n<pre><code>found = False\nfor/while ...:\n    if ...:\n        found = True\n        break\nif not found:\n    ...\n</code></pre>\n\n<p>The <code>else</code> clause will not be executed if there is a <code>return</code> because <code>return</code> leaves the function, as it is meant to. The only exception to that which you may be thinking of is <code>finally</code>, whose purpose is to be sure that it is always executed.</p>\n\n<p><code>continue</code> has nothing special to do with this matter. It causes the current iteration of the loop to end which may happen to end the entire loop, and clearly in that case the loop wasn't ended by a <code>break</code>.</p>\n\n<p><code>try/else</code> is similar:</p>\n\n<pre><code>try:\n    ...\nexcept:\n    ...\nif there was an exception:\n    pass\nelse:\n    ...\n</code></pre>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37643265, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>If you think of your loops as a structure similar to this (somewhat pseudo-code):</p>\n\n<pre><code>loop:\nif condition then\n\n   ... //execute body\n   goto loop\nelse\n   ...\n</code></pre>\n\n<p>it might make a little bit more sense. A loop is essentially just an <code>if</code> statement that is repeated until the condition is <code>false</code>. And this is the important point. The loop checks its condition and sees that it's <code>false</code>, thus executes the <code>else</code> (just like a normal <code>if/else</code>) and then the loop is done.</p>\n\n<p>So notice that the <code>else</code> <strong>only get's executed when the condition is checked</strong>. That means that if you exit the body of the loop in the middle of execution with for example a <code>return</code> or a <code>break</code>, since the condition is not checked again, the <code>else</code> case won't be executed.</p>\n\n<p>A <code>continue</code> on the other hand stops the current execution and then jumps back to check the condition of the loop again, which is why the <code>else</code> can be reached in this scenario.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37643296, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>Better to think of it this way: The <code>else</code> block will <strong>always</strong> be executed if everything goes <em>right</em> in the preceding <code>for</code> block such that it reaches exhaustion. </p>\n\n<p><em>Right</em> in this context will mean no <code>exception</code>, no <code>break</code>, no <code>return</code>. Any statement that hijacks control from <code>for</code> will cause the <code>else</code> block to be bypassed.</p>\n\n<hr>\n\n<p>A common use case is found when searching for an item in an <code>iterable</code>, for which the search is either called off when the item is found or a <code>\"not found\"</code> flag is raised/printed via the following <code>else</code> block:</p>\n\n<pre><code>for items in basket:\n    if isinstance(item, Egg):\n        break\nelse:\n    print(\"No eggs in basket\")  \n</code></pre>\n\n<p>A <code>continue</code> does not hijack control from <code>for</code>, so control will proceed to the <code>else</code> after the <code>for</code> is exhausted.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37643358, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>Usually I tend to think of a loop structure like this:</p>\n\n<pre><code>for item in my_sequence:\n    if logic(item):\n        do_something(item)\n        break\n</code></pre>\n\n<p>To be a lot like a variable number of <code>if/elif</code> statements:</p>\n\n<pre><code>if logic(my_seq[0]):\n    do_something(my_seq[0])\nelif logic(my_seq[1]):\n    do_something(my_seq[1])\nelif logic(my_seq[2]):\n    do_something(my_seq[2])\n....\nelif logic(my_seq[-1]):\n    do_something(my_seq[-1])\n</code></pre>\n\n<p>In this case the <code>else</code> statement on the for loop works exactly like the <code>else</code> statement on the chain of <code>elif</code>s, it only executes if none of the conditions before it evaluate to True. (or break execution with <code>return</code> or an exception)  If my loop does not fit this specification usually I choose to opt out of using <code>for: else</code> for the exact reason you posted this question: it is non-intuitive.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37643965, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>(This is inspired by @Mark Tolonen's answer.)</p>\n\n<p>An <code>if</code> statement runs its <code>else</code> clause if its condition evaluates to false.\nIdentically, a <code>while</code> loop runs the else clause if its condition evaluates to false.</p>\n\n<p>This rule matches the behavior you described:</p>\n\n<ul>\n<li>In normal execution, the while loop repeatedly runs until the condition evaluates to false, and therefore naturally exiting the loop runs the else clause.</li>\n<li>When you execute a <code>break</code> statement, you exit out of the loop without evaluating the condition, so the condition cannot evaluate to false and you never run the else clause.</li>\n<li>When you execute a <code>continue</code> statement, you evaluate the condition again, and do exactly what you normally would at the beginning of a loop iteration.\nSo, if the condition is true, you keep looping, but if it is false you run the else clause.</li>\n<li>Other methods of exiting the loop, such as <code>return</code>, do not evaluate the condition and therefore do not run the else clause.</li>\n</ul>\n\n<p><code>for</code> loops behave the same way. Just consider the condition as true if the iterator has more elements, or false otherwise.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37644460, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>Think of the <code>else</code> clause as being part of the loop construct; <code>break</code> breaks out of the loop construct entirely, and thus skips the <code>else</code> clause.</p>\n\n<p>But really, my mental mapping is simply that it's the 'structured' version of the pattern C/C++ pattern:</p>\n\n<pre><code>  for (...) {\n    ...\n    if (test) { goto done; }\n    ...\n  }\n  ...\ndone:\n  ...\n</code></pre>\n\n<p>So when I encounter <code>for...else</code> or write it myself, rather than understand it <em>directly</em>, I mentally translate it into the above understanding of the pattern and then work out which parts of the python syntax map to which parts of the pattern.</p>\n\n<p>(I put 'structured' in scare quotes because the difference is not whether the code is structured or unstructured, but merely whether there are keywords and grammar dedicated to the particular structure)</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37648033, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>Others have already explained the mechanics of <code>while/for...else</code>, and the <a href=\"https://docs.python.org/3/reference/\">Python 3 language reference</a> has the authoritative definition (see <a href=\"https://docs.python.org/3/reference/compound_stmts.html#the-while-statement\">while</a> and <a href=\"https://docs.python.org/3/reference/compound_stmts.html#the-for-statement\">for</a>), but here is my personal mnemonic, FWIW. I guess the key for me has been to break this down into two parts: one for understanding the meaning of the <code>else</code> in relation to the loop conditional, and one for understanding loop control.</p>\n\n<p>I find it's easiest to start by understanding <code>while...else</code>:</p>\n\n<blockquote>\n  <p><em><code>while</code> you have more items, do stuff, <code>else</code> if you run out, do this</em></p>\n</blockquote>\n\n<p>The <code>for...else</code> mnemonic is basically the same:</p>\n\n<blockquote>\n  <p><em><code>for</code> every item, do stuff, but <code>else</code> if you run out, do this</em></p>\n</blockquote>\n\n<p>In both cases, the <code>else</code> part is only reached once there are no more items to process, and the last item has been processed in a regular manner (i.e. no <code>break</code> or <code>return</code>). A <code>continue</code> just goes back and sees if there are any more items. My mnemonic for these rules applies to both <code>while</code> and <code>for</code>:</p>\n\n<blockquote>\n  <p><em>when <code>break</code>ing or <code>return</code>ing, there's nothing <code>else</code> to do,<br>\n  and when I say <code>continue</code>, that's \"loop back to start\" for you</em></p>\n</blockquote>\n\n<p>&ndash; with \"loop back to start\" meaning, obviously, the start of the loop where we check whether there are any more items in the iterable, so as far as the <code>else</code> is concerned, <code>continue</code> really plays no role at all.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37650949, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>My gotcha moment with the loop's <code>else</code> clause was when I was watching a talk by <a href=\"http://stackoverflow.com/users/1001643/raymond-hettinger\">Raymond Hettinger</a>, who told a story about how he thought it should have been called <code>nobreak</code>. Take a look at the following code, what do you think it would do?</p>\n\n<pre><code>for i in range(10):\n    if test(i):\n        break\n    # ... work with i\nnobreak:\n    print('Loop completed')\n</code></pre>\n\n<p>What would you guess it does? Well, the part that says <code>nobreak</code> would only be executed if a <code>break</code> statement wasn't hit in the loop.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37658079, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>In <a href=\"http://en.wikipedia.org/wiki/Test-driven_development\" rel=\"nofollow\">Test-driven development</a> (TDD), when using the <a href=\"https://en.wikipedia.org/wiki/Transformation_Priority_Premise\" rel=\"nofollow\">Transformation Priority Premise</a> paradigm, you treat loops as a generalization of conditional statements.</p>\n\n<p>This approach combines well with this syntax, if you consider only simple <code>if/else</code> (no <code>elif</code>) statements:</p>\n\n<pre><code>if cond:\n    # 1\nelse:\n    # 2\n</code></pre>\n\n<p>generalizes to:</p>\n\n<pre><code>while cond:  # &lt;-- generalization\n    # 1\nelse:\n    # 2\n</code></pre>\n\n<p>nicely.</p>\n\n<p>In other languages, TDD steps from a single case to cases with collections require more refactoring.</p>\n\n<hr>\n\n<p>Here is an example from <a href=\"https://blog.8thlight.com/uncle-bob/2013/05/27/TheTransformationPriorityPremise.html\" rel=\"nofollow\">8thlight blog</a>:</p>\n\n<p>In the linked article at 8thlight blog, the Word Wrap kata is considered: adding line breaks to strings (the <code>s</code> variable in the snippets below) to make them fit a given width (the <code>length</code> variable in the snippets below). At one point the implementation looks as follows (Java):</p>\n\n\n\n<pre><code>String result = \"\";\nif (s.length() &gt; length) {\n    result = s.substring(0, length) + \"\\n\" + s.substring(length);\n} else {\n    result = s;\n}\nreturn result;\n</code></pre>\n\n<p>and the next test, that currently fails is:</p>\n\n\n\n<pre><code>@Test\npublic void WordLongerThanTwiceLengthShouldBreakTwice() throws Exception {\n    assertThat(wrap(\"verylongword\", 4), is(\"very\\nlong\\nword\"));\n    }\n</code></pre>\n\n<p>So we have code that works conditionally: when a particular condition is met, a line break is added. We want to improve the code to handle multiple line breaks. The solution presented in the article proposes to apply the <em>(if->while)</em> transformation, however the author makes a comment that:</p>\n\n<blockquote>\n  <p>While loops can\u2019t have <code>else</code> clauses, so we need to eliminate the <code>else</code> path by doing less in the <code>if</code> path. Again, this is a refactoring.</p>\n</blockquote>\n\n<p>which forces to do more changes to the code in the context of one failing test:</p>\n\n\n\n<pre><code>String result = \"\";\nwhile (s.length() &gt; length) {\n    result += s.substring(0, length) + \"\\n\";\n    s = s.substring(length);\n}\nresult += s;\n</code></pre>\n\n<p>In TDD we want to write as less code as possible to make tests pass. Thanks to Python's syntax the following transformation is possible:</p>\n\n<p>from:</p>\n\n<pre><code>result = \"\"\nif len(s) &gt; length:\n    result = s[0:length] + \"\\n\"\n    s = s[length:]\nelse:\n    result += s\n</code></pre>\n\n<p>to:</p>\n\n<pre><code>result = \"\"\nwhile len(s) &gt; length:\n    result += s[0:length] + \"\\n\"\n    s = s[length:]\nelse:\n    result += s\n</code></pre>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37662589, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>Decision structures evaluate multiple expressions which produce TRUE or FALSE as outcome. You need to determine which action to take and which statements to execute if outcome is TRUE or FALSE otherwise.</p>\n\n<blockquote>\n  <p>Basically, without if Statement ,else Statement Does not mean.!</p>\n</blockquote>\n\n<pre><code>n=7 \nfor i in n:\n          if i==6:\n               break\n           else:\n                print \"welcome\"\n</code></pre>\n\n<p>In the above code, it get breaks when i equal to 6..otherwise it print welcome for 5 times.!</p>\n\n<p>Thanks,\n<strong>Ajay</strong></p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37664133, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>The way I see it, <code>else:</code> fires when you iterate past the end of the loop.</p>\n\n<p>If you <code>break</code> or <code>return</code> or <code>raise</code> you don't iterate past the end of loop, you stop immeadiately, and thus the <code>else:</code> block won't run. If you <code>continue</code> you still iterate past the end of loop, since continue just skips to the next iteration. It doesn't stop the loop. </p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37670045, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}, {"body": "<p>The way I think about it, the key is to consider the meaning of <code>continue</code> rather than <code>else</code>.</p>\n\n<p>The other keywords you mention break out of the loop (exit abnormally) whilst <code>continue</code> does not, it just skips the remainder of the code block inside the loop. The fact that it can precede loop termination is incidental: the termination is actually done in the normal way by evaluation of the loop conditional expression.</p>\n\n<p>Then you just need to remember that the <code>else</code> clause is executed after normal loop termination.</p>\n", "title": "How can I make sense of the `else` statement in Python loops?", "answer_id": 37704287, "question_id": 37642573, "tags": ["python", "loops", "for-loop", "while-loop"]}], "question_id": 37642573}, {"body": "<p>I am trying to upload an Apple TV App to the App Store for testing purposes, but I got the issue <code>This Action could not be completed. Try Again (-22421)</code> as in the below image.</p>\n\n<p>So what can I do?</p>\n\n<p><a href=\"http://i.stack.imgur.com/eV1e7.png\"><img src=\"http://i.stack.imgur.com/eV1e7.png\" alt=\"enter image description here\"></a></p>\n", "title": "This action could not be completed. Try Again (-22421)", "tags": ["ios", "apple-tv"], "answer_count": 32, "link": "http://stackoverflow.com/questions/35429801/this-action-could-not-be-completed-try-again-22421", "answers": [{"body": "<p>Hey i have same issue when i am trying to upload IPA on Appstore. I solved it when i checked that my Distribution provisioning profile was invalid. So just update provisioning profile,download then install and restart Xcode. Now archive and upload it on Appstore it will be done. </p>\n\n<p>OR</p>\n\n<p>Create IPA and upload via Application Loader.</p>\n\n<p>Good luck.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35477776, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Just try exporting the iPA file and then upload that exported iPA file with application loader. It will solve your problem.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35481295, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>As with all thing Xcode related. A simple reboot fixed the problem for me.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35514295, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I've tried to load archive nearly 10 times (with all kinds of cleans and reloads, with different versions and provisioning files), still received the same error. Then suddenly on my next attempt it showed me: \"cannot proceed with delivery an existing transporter instance is currently uploading this package\", I did the following and it helped:</p>\n\n<p>Basically, you need to clear out the transport tokens. This can happen if you were to close out of Xcode while in the middle of submitting an app to iTunes Connect.</p>\n\n<pre><code>/Users/&lt;username&gt;/.itmstransporter/UploadTokens/\n</code></pre>\n\n<p>Delete any .token files in this directory.\nIf you are unable to find the .token files, this is because they are hidden in Finder. To hide/show hidden files in Finder, use the following Terminal command (TRUE = UNHIDE, FALSE = HIDE):</p>\n\n<p>defaults write com.apple.finder AppleShowAllFiles TRUE;killall Finder</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35582849, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Just In case it is same my mistake.</p>\n\n<p>Did you check <strong>both</strong> provisioning profile settings in \"your Project\" and \"your Targets\"?</p>\n\n<p><a href=\"http://i.stack.imgur.com/d2IRv.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/d2IRv.png\" alt=\"enter image description here\"></a></p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35601072, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>This happens because Apple's servers may not working correctly. Just wait and try later or next time. It will work for sure finally.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35663309, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I had the same issue but its solved by below steps:</p>\n\n<ol>\n<li>Delete all profiles from: ~/Library/MobileDevice/Provisioning Profiles/</li>\n<li>Delete developer account from Xcode >> Preference >> Account >> Delete your Apple Id</li>\n<li>Delete derived data</li>\n<li>Restart machine</li>\n<li>Download your provisioning profiles which you require to submit app</li>\n<li>Open your project and clean it</li>\n<li>And try again for submit app.</li>\n</ol>\n\n<p>I hope this will work for all.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35667831, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I was experiencing the exact same error hence being here (failed about 6 times over an hour).</p>\n\n<p>I just tried again now (without changing anything), it worked by itself. </p>\n\n<p>I read on the Apple-forums that it's an issue with Apple servers, nothing to do with your Xcode.</p>\n\n<p>I also read that others trying the upload via ApplicationLoader (Menu -> Xcode -> Open Developer Tool -> Application Loader), this will ask where your code is, you would then have to select your Archive (not the source code), you can upload your code this way if you need. I personally prefer the built-in Xcode instead.</p>\n\n<p>Works either way.</p>\n\n<p>For the record, I am copying the thread below from Apple Forums:</p>\n\n<p><a href=\"https://forums.developer.apple.com/thread/35101\">https://forums.developer.apple.com/thread/35101</a></p>\n\n<p>Cheers\nHeider </p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35700386, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>The best solution is indeed Application Loader.\nI can't comment on Naresh's note on this (I'm too much of a newbie to StackOverflow), but am warning you that if you are using Swift, don't the use Xcode ipa generation system. I prefer manual creation of the IPA by:</p>\n\n<pre><code>1) zipping 2 folders : a Payload folder that contain your .app and a SwiftSupport folder that you can copy from your Xarchive. \n2) rename the zip to .ipa.\n</code></pre>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35870653, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>This occurred to me after I enabled two-factor authentication in my Apple ID. After I disable two-factor authentication, everything works as expected.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 35930193, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>In my case it was caused by lack of some of the app icons in the Images.xcassets</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 36234652, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>For me VPN caused that problem. After I disconnected from it upload was successful.  </p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 36598977, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<pre><code>Tested in Xcode 7.3, 04142016\n</code></pre>\n\n<p>Instead of clicking the blue <strong>Upload to App Store\u2026</strong> button from the Organizer or while Archiving, Xcode offers a better tool to do this, but it is a little hidden and the process is not clear how to use it.</p>\n\n<p>Here is what I documented while submitting our latest iOS application to the App Store:</p>\n\n<p>First, do NOT use the blue <strong>Upload to App Store\u2026</strong> button.</p>\n\n<p>From Xcode's <strong>Xcode: Developer Tools:</strong> menu, select <strong>Application Loader</strong>.</p>\n\n<p>Sign in with appropriate credentials that allow you to upload to the App Store.</p>\n\n<p>When Application Loader shows you its main screen, select the <strong>Choose</strong> button on the lower right of the main window and open your IPA that you exported for the App Store.</p>\n\n<p>Proceed with the submission.</p>\n\n<p><strong>Why you want to do this:</strong></p>\n\n<ol>\n<li>Detailed app submission error reporting. Exact explanations of submission errors. It tells you what's incorrect in your app that you are trying to submit.</li>\n<li>You don't have to deal with the insipid error that prompted this question in the first place.</li>\n<li>It works consistently in my tests where Xcode's <strong>Submit App to Store\u2026</strong> from the <strong>Archive</strong> menu does not.</li>\n</ol>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 36627858, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Validating the same archive that failed to upload worked for me. Or just Apple's servers came back up.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 36792482, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Uncheck \"Include bitcode\" box, worked for me.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 36836771, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I think my issue was that I had previously attempted to upload the same version number of my application and it either failed for another reason, or I cancelled it myself.</p>\n\n<p>I changed the version of my app to a version which had not been used before. Cleaned my project. Re-archived it, then uploaded with no issues.</p>\n\n<p>......\nYou need to INCREASE your version number, you can't make it smaller...</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37215387, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I had this problem too. My issue was that I had previously uploaded a build with the same build number.I just increased the build number in Unity and the problem was resolved. As a matter of fact I didn't realise this was the cause until I used Application Loader, which gave a much more informative error message.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37232544, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>When you receive error \"<strong>Action could not be completed. Try Again (-22421)</strong>\",\nJust try to resubmit same app on iTunes store using XCode again, and you will get the actual error messages next time. </p>\n\n<p>Detailed error message which your received next time will help you what to do actually!!!</p>\n\n<p>If you are receiving same error again and again, you can submit app using <strong>\"App Loader\"</strong> and it may success to submit.</p>\n\n<p><strong>[ Take care Point: ]</strong> </p>\n\n<p>Please check your <strong>app version</strong> / <strong>build version</strong> number is greater than previously submitted version. \nYou can verify it in Xcode -> Project setting -> General section AND <a href=\"http://tunesconnect.apple.com/\" rel=\"nofollow\">iTunes-Connect-Account</a> -> App Package -> Activity tab</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37544196, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>In my case the space on the hard drive was not enough. Using the Application Loader showed the actual error. </p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37586486, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Just close your Xcode and restart your mac.\nThen try upload your app again.\nIt result from the networking problem,not your operation.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37671336, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p><a href=\"http://i.stack.imgur.com/5Ront.png\" rel=\"nofollow\">This is the successful submission when I try after some time. </a></p>\n\n<p><em>yes this is an error by server do nothing with your code .</em></p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37723433, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I also ran into this with my xCode 7.3.1, I did below and looks like 22421 is generic error code if Enable Bitcode is checked/selected:</p>\n\n<ul>\n<li>Removed and regenerated fresh certificates &amp; profiles</li>\n<li>Cleaned project and restarted machine</li>\n<li>Made sure that I have all icons for my watch app and iOS app</li>\n<li>watch image asset/bundle is &lt;5MB</li>\n<li>followed <a href=\"http://stackoverflow.com/questions/32640149/disable-bitcode-for-project-and-cocoapods-dependencies-with-xcode7\">this</a> to enable or disable Bitcode flag</li>\n</ul>\n\n<p>And achieved.\n- Still build gets uploaded only if I uncheck Enable Bitcode (which I don't understand why)</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37880139, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I increased build number but the problem remained.</p>\n\n<p>Then,</p>\n\n<ol>\n<li>I approved renewed licence agreement at <a href=\"http://itunesconnect.apple.com\" rel=\"nofollow\">itunesconnect</a>.</li>\n<li>I opened App Store application on my MAC, and clicked update button for \"Command Line Tools (OS X 10.11) for XCode, Version 7.3\"</li>\n<li>I opened XCode. Clicked \"Product\" button on top bar. While tab content is open, I clicked \"alt\" key of keyboard to make \"Clean Build Folder\" option visible. I clicked this option.</li>\n<li>I rebuild my application for the same version that I received below error;</li>\n</ol>\n\n<blockquote>\n  <p>This action could not be completed. Try Again (-22421)</p>\n</blockquote>\n\n<ol start=\"5\">\n<li>This time I received below error when I want to upload new version;</li>\n</ol>\n\n<blockquote>\n  <p>This action could not be completed. Try again.</p>\n</blockquote>\n\n<ol start=\"6\">\n<li>I clicked Done and clicked blue \"Upload to App Store\" button again, and upload completed successfully.</li>\n</ol>\n\n<p>I hope this helps.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37884524, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I had this problem over and over again. No descriptive description, just the error message that OP posted.</p>\n\n<p>In fact, it was a problem with the build and so had nothing to do with Apple's servers at all, as you might think.</p>\n\n<p>The solution was to go to <code>Organizer</code> and <code>Export</code> the build, then upload it via <code>Application Loader</code>. Here I got the correct error messages. In my case, it was a problem with CocoaPods breaking app extension validations. I solved this and then I could upload my build.</p>\n\n<p>Hope this helps someone like me.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37952450, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>If you connect a device while the app is building/archiving, your generic device will change to your connected device automatically, and when you are trying to upload to the app store you will have that weird crash. </p>\n\n<p>This:\n<a href=\"http://i.stack.imgur.com/qKblr.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/qKblr.png\" alt=\"enter image description here\"></a> </p>\n\n<p>will change to this automatically:</p>\n\n<p><a href=\"http://i.stack.imgur.com/UGhtJ.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/UGhtJ.png\" alt=\"enter image description here\"></a></p>\n\n<p>So to answer your question, I don't know if your issue have been caused because you plugged your iOs device while archiving, but you will be able to reproduce that issue if you plug your iOS device while archiving your app. Hope this helps somebody</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37963812, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I had the same issue. It was related to the Build Settings -> Code Signing Identity -> Release which was set to iOS Developer. If has to be set to iOS Distribution in order for Xcode upload to work.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 37987940, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>It is probably everything all right with your code/provisioning profiles/Xcode.</p>\n\n<p>I did try once more, and it worked like a charm. I did not change anything.</p>\n\n<p>Please take note that some actions of people replying earlier did take, could not have any effect on this problem but still, it might look like that actions did help. It would work with or without it anyway.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 38048836, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Close your xCode and Application Loader, </p>\n\n<p><strong>Reboot your Mac</strong>.</p>\n\n<p>Now, launch Application Loader and send it.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 38075406, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>My problem was with the provisional profiles.</p>\n\n<p>So for me it worked:</p>\n\n<p>Go to Xcode preferences -> Accounts -> In my ID Apple Account -> Select \"<strong>Download All</strong>\"</p>\n\n<p>I checked too, in member center and delete all invalid provisionals profiles and:</p>\n\n<p>1 - Close Xcode</p>\n\n<p>2 - Open Xcode, do clean and build of my project.</p>\n\n<p>3 - Archive my code again.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 38194744, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>Check this list <strong>or just keep trying upload!</strong></p>\n\n<ul>\n<li>Got to member center</li>\n<li>Check if there are any invalid provisioning profiles (for some reason, my app store distribution profile has become invalid)</li>\n<li>Delete invalid profiles</li>\n<li>Create new ones</li>\n<li>Download and drag on XCode</li>\n<li>Reboot Mac</li>\n<li>Archive => Upload to App Store</li>\n<li>Everything should be fine again</li>\n</ul>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 38229045, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>I was too having this issues, I had uploaded my application and also updated it once but had no issues, but as i was uploading it for the 3rd time, i came across this issue, tried hell lot of things, scratched my head, googled it for hours, kept on trying the whole day, but nothing happened, but next day( a new light came) and my app uploaded on the first attempt.</p>\n\n<p>Feel like there was some issue from apple side(be it there server, there software xCode or there mac).</p>\n\n<p>So don not get dishearten if facing this problem, you can explore diff things with this problem but its just simple thing to correct this issue is to wait for some time take rest and try again.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 38239194, "question_id": 35429801, "tags": ["ios", "apple-tv"]}, {"body": "<p>What worked for me was removing expired provisioning profiles from the developer console. I only left already active ones and I was able to submit.</p>\n", "title": "This action could not be completed. Try Again (-22421)", "answer_id": 38800090, "question_id": 35429801, "tags": ["ios", "apple-tv"]}], "question_id": 35429801}, {"body": "<p>Why does the reverse function for the <code>std::list</code> class in the C++ standard library have linear runtime? I would think that for doubly-linked lists the reverse function should have been O(1).  </p>\n\n<p>Reversing a doubly-linked list should just involve switching the head and the tail pointers.</p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "tags": ["c++", "c++11", "stl", "linked-list"], "answer_count": 7, "link": "http://stackoverflow.com/questions/35612220/why-does-stdlistreverse-have-on-complexity", "answers": [{"body": "<p>Because it has to traverse every node (<code>n</code> total) and update their data (the update step is indeed <code>O(1)</code>). This makes the whole operation <code>O(n*1) = O(n)</code>.</p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35612278, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}, {"body": "<p>Hypothetically, <code>reverse</code> could have been <em>O(1)</em>. There (again hypothetically) could have been a boolean list member indicating whether the direction of the linked list is currently the same or opposite as the original one where the list was created.</p>\n\n<p>Unfortunately, that would reduce the performance of basically any other operation (albeit without changing the asymptotic runtime). In each operation, a boolean would need to be consulted to consider whether to follow a \"next\" or \"prev\" pointer of a link.</p>\n\n<p>Since this was presumably considered a relatively infrequent operation, the standard (which does not dictate implementations, only complexity), specified that the complexity could be linear. This allows \"next\" pointers to always mean the same direction unambiguously, speeding up common-case operations.</p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35612350, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}, {"body": "<p>It <em>could</em> be <i>O</i>(1) if the list would store a flag that allows swapping the meaning of the \u201c<code>prev</code>\u201d and \u201c<code>next</code>\u201d pointers each node has. If reversing the list would be a frequent operation, such an addition might be in fact useful and I don't know of any reason why implementing it would be <em>prohibited</em> by the current standard. However, having such a flag would make ordinary <em>traversal</em> of the list more expensive (if only by a constant factor) because instead of</p>\n\n<pre><code>current = current-&gt;next;\n</code></pre>\n\n<p>in the <code>operator++</code> of the list iterator, you would get</p>\n\n<pre><code>if (reversed)\n  current = current-&gt;prev;\nelse\n  current = current-&gt;next;\n</code></pre>\n\n<p>which is not something you'd decide to add easily. Given that lists are usually traversed much more often than they are reversed, it would be very unwise for the standard to <em>mandate</em> this technique. Therefore, the reverse operation is allowed to have linear complexity. Do note, however, that <i>t</i> &isin; <i>O</i>(1) &rArr; <i>t</i> &isin; <i>O</i>(<i>n</i>) so, as mentioned earlier, implementing your \u201coptimization\u201d technically would be permitted.</p>\n\n<p>If you come from a Java or similar background, you might wonder why the iterator has to check the flag each time. Couldn't we instead have two distinct iterator types, both derived from a common base type, and have <code>std::list::begin</code> and <code>std::list::rbegin</code> polymorphically return the appropriate iterator? While possible, this would make the whole thing even worse because advancing the iterator would be an indirect (hard to inline) function call now. In Java, you're paying this price routinely anyway, but then again, this is one of the reasons many people reach for C++ when performance is critical.</p>\n\n<p>As pointed out by <a href=\"https://stackoverflow.com/users/440119/benjamin-lindley\">Benjamin Lindley</a> in the comments, since <code>reverse</code> is not allowed to invalidate iterators, the only approach permitted by the standard seems to be to store a pointer back to the list inside the iterator which causes a double-indirect memory access.</p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35612583, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}, {"body": "<p>Surely since all containers that support bidirectional iterators have the concept of rbegin() and rend(), this question is moot? </p>\n\n<p>It's trivial to build a proxy that reverses the iterators and access the container through that. </p>\n\n<p>This non-operation is indeed O(1).</p>\n\n<p>such as:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;list&gt;\n#include &lt;string&gt;\n#include &lt;iterator&gt;\n\ntemplate&lt;class Container&gt;\nstruct reverse_proxy\n{\n    reverse_proxy(Container&amp; c)\n    : _c(c)\n    {}\n\n    auto begin() { return std::make_reverse_iterator(std::end(_c)); }\n    auto end() { return std::make_reverse_iterator(std::begin(_c)); }\n\n    auto begin() const { return std::make_reverse_iterator(std::end(_c)); }\n    auto end() const { return std::make_reverse_iterator(std::begin(_c)); }\n\n    Container&amp; _c;\n};\n\ntemplate&lt;class Container&gt;\nauto reversed(Container&amp; c)\n{\n    return reverse_proxy&lt;Container&gt;(c);\n}\n\nint main()\n{\n    using namespace std;\n    list&lt;string&gt; l { \"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\" };\n\n    auto r = reversed(l);\n    copy(begin(r), end(r), ostream_iterator&lt;string&gt;(cout, \"\\n\"));\n\n    return 0;\n}\n</code></pre>\n\n<p>expected output:</p>\n\n<pre><code>mat\nthe\non\nsat\ncat\nthe\n</code></pre>\n\n<p>Given this, it seems to me that the standards committee have not taken time to mandate O(1) reverse-ordering of the container because it's not necessary, and the standard library is largely built on the principle of mandating only what is strictly necessary while avoiding duplication.</p>\n\n<p>Just my 2c.</p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35612668, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}, {"body": "<p>Only an algorithm explanation. \nImagine you have an array with elements, then you need to inverted it.\nThe basic idea is to iterate on each element changing the element on the\nfirst position to the last position, the element on second position to penultimate position, and so on. When you reach at the middle of the array you'll have all elements changed, thus in (n/2) iterations, which is considered O(n).</p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35650903, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}, {"body": "<p>It is O(n) simply because it needs to copy the list in reverse order.  Each individual item operation is O(1) but there are n of them in the entire list.  </p>\n\n<p>Of course there are some constant-time operations involved in setting up the space for the new list, and changing pointers afterwards, etc.  The O notation doesn't consider individual constants once you include a first-order n factor.  </p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35731915, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}, {"body": "<p>It also swaps previous and next pointer for every node. Thats why it takes Linear. Although it can be done in O(1) if the function using this LL also takes information about LL as input like whether it is accessing normally or reverse.  </p>\n", "title": "Why does std::list::reverse have O(n) complexity?", "answer_id": 35738691, "question_id": 35612220, "tags": ["c++", "c++11", "stl", "linked-list"]}], "question_id": 35612220}, {"body": "<p><em>This is a real-world example from a 3rd party library API, but simplified.</em></p>\n\n<p><em>Compiled with Oracle JDK 8u72</em></p>\n\n<p>Consider these two methods:</p>\n\n<pre><code>&lt;X extends CharSequence&gt; X getCharSequence() {\n    return (X) \"hello\";\n}\n\n&lt;X extends String&gt; X getString() {\n    return (X) \"hello\";\n}\n</code></pre>\n\n<p>Both report an \"unchecked cast\" warning - I get why. The thing that baffles me is why can I call</p>\n\n<pre><code>Integer x = getCharSequence();\n</code></pre>\n\n<p>and it compiles? The compiler should know that <code>Integer</code> does not implement <code>CharSequence</code>. The call to</p>\n\n<pre><code>Integer y = getString();\n</code></pre>\n\n<p>gives an error (as expected)</p>\n\n<blockquote>\n<pre><code>incompatible types: inference variable X has incompatible upper bounds java.lang.Integer,java.lang.String\n</code></pre>\n</blockquote>\n\n<p>Can someone explain why would this behaviour be considered valid? How would it be useful?</p>\n\n<p>The client does not know that this call is unsafe - the client's code compiles without warning. Why wouldn't the compile warn about that / issue an error?</p>\n\n<p>Also, how is it different from this example:</p>\n\n<pre><code>&lt;X extends CharSequence&gt; void doCharSequence(List&lt;X&gt; l) {\n}\n\nList&lt;CharSequence&gt; chsL = new ArrayList&lt;&gt;();\ndoCharSequence(chsL); // compiles\n\nList&lt;Integer&gt; intL = new ArrayList&lt;&gt;();\ndoCharSequence(intL); // error\n</code></pre>\n\n<p>Trying to pass <code>List&lt;Integer&gt;</code> gives an error, as expected:</p>\n\n<blockquote>\n<pre><code>method doCharSequence in class generic.GenericTest cannot be applied to given types;\n  required: java.util.List&lt;X&gt;\n  found: java.util.List&lt;java.lang.Integer&gt;\n  reason: inference variable X has incompatible bounds\n    equality constraints: java.lang.Integer\n    upper bounds: java.lang.CharSequence\n</code></pre>\n</blockquote>\n\n<p>If that is reported as an error, why <code>Integer x = getCharSequence();</code> isn't?</p>\n", "title": "Generic return type upper bound - interface vs. class - surprisingly valid code", "tags": ["java", "generics", "java-8"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36402646/generic-return-type-upper-bound-interface-vs-class-surprisingly-valid-code", "answers": [{"body": "<p><code>CharSequence</code> is an <code>interface</code>. Therefore even if <code>SomeClass</code> does not implement <code>CharSequence</code> it would be perfectly possible to create a class</p>\n\n<pre><code>class SubClass extends SomeClass implements CharSequence\n</code></pre>\n\n<p>Therefore you can write</p>\n\n<pre><code>SomeClass c = getCharSequence();\n</code></pre>\n\n<p>because the inferred type <code>X</code> is the intersection type <code>SomeClass &amp; CharSequence</code>.</p>\n\n<p>This is a bit odd in the case of <code>Integer</code> because <code>Integer</code> is final, but <code>final</code> doesn't play any role in these rules. For example you can write</p>\n\n<pre><code>&lt;T extends Integer &amp; CharSequence&gt;\n</code></pre>\n\n<p>On the other hand, <code>String</code> is not an <code>interface</code>, so it would be impossible to extend <code>SomeClass</code> to get a subtype of <code>String</code>, because java does not support multiple-inheritance for classes.</p>\n\n<p>With the <code>List</code> example, you need to remember that generics are neither covariant nor contravariant. This means that if <code>X</code> is a subtype of <code>Y</code>, <code>List&lt;X&gt;</code> is neither a subtype nor a supertype of <code>List&lt;Y&gt;</code>. Since <code>Integer</code> does not implement <code>CharSequence</code>, you cannot use <code>List&lt;Integer&gt;</code> in your <code>doCharSequence</code> method.</p>\n\n<p>You can, however get this to compile</p>\n\n<pre><code>&lt;T extends Integer &amp; CharSequence&gt; void foo(List&lt;T&gt; list) {\n    doCharSequence(list);\n}  \n</code></pre>\n\n<p>If you have a method that <em>returns</em> a <code>List&lt;T&gt;</code> like this:</p>\n\n<pre><code>static &lt;T extends CharSequence&gt; List&lt;T&gt; foo() \n</code></pre>\n\n<p>you can do</p>\n\n<pre><code>List&lt;? extends Integer&gt; list = foo();\n</code></pre>\n\n<p>Again, this is because the inferred type is <code>Integer &amp; CharSequence</code> and this is a subtype of <code>Integer</code>.</p>\n\n<p>Intersection types occur implicitly when you specify multiple bounds (e.g. <code>&lt;T extends SomeClass &amp; CharSequence&gt;</code>).</p>\n\n<p>For further information, <a href=\"https://docs.oracle.com/javase/specs/jls/se8/html/jls-4.html#jls-TypeBound\">here</a> is the part of the JLS where it explains how type bounds work. You can include multiple interfaces, e.g.</p>\n\n<pre><code>&lt;T extends String &amp; CharSequence &amp; List &amp; Comparator&gt;\n</code></pre>\n\n<p>but only the first bound may be a non-interface.</p>\n", "title": "Generic return type upper bound - interface vs. class - surprisingly valid code", "answer_id": 36403072, "question_id": 36402646, "tags": ["java", "generics", "java-8"]}, {"body": "<p>The type that is inferred by your compiler prior to the assignment for <code>X</code> is <code>Integer &amp; CharSequence</code>. This type <em>feels</em> weird, because <code>Integer</code> is final, but it's a perfectly valid type in Java. It is then cast to <code>Integer</code>, which is perfectly OK.</p>\n\n<p>There is exactly one possible value for the <code>Integer &amp; CharSequence</code> type: <code>null</code>. With the following implementation:</p>\n\n<pre><code>&lt;X extends CharSequence&gt; X getCharSequence() {\n    return null;\n}\n</code></pre>\n\n<p>The following assignment will work:</p>\n\n<pre><code>Integer x = getCharSequence();\n</code></pre>\n\n<p>Because of this possible value, there's no reason why the assignment should be wrong, even if it is obviously useless. A warning would be useful.</p>\n\n<h3>The real problem is the API, not the call site</h3>\n\n<p>In fact, I've recently blogged about this <a href=\"http://blog.jooq.org/2015/04/30/this-common-api-technique-is-actually-an-anti-pattern/\">API design anti pattern</a>. You should (almost) never design a generic method to return arbitrary types because you can (almost) never guarantee that the inferred type will be delivered. An exception are methods like <code>Collections.emptyList()</code>, in case of which the emptiness of the list (and generic type erasure) is the reason why any inference for <code>&lt;T&gt;</code> will work:</p>\n\n<pre><code>public static final &lt;T&gt; List&lt;T&gt; emptyList() {\n    return (List&lt;T&gt;) EMPTY_LIST;\n}\n</code></pre>\n", "title": "Generic return type upper bound - interface vs. class - surprisingly valid code", "answer_id": 36430085, "question_id": 36402646, "tags": ["java", "generics", "java-8"]}], "question_id": 36402646}, {"body": "<p>I see patterns like</p>\n\n<pre><code>def __init__(self, x, y, z):\n    ...\n    self.x = x\n    self.y = y\n    self.z = z\n    ...\n</code></pre>\n\n<p>quite frequently, often with a lot more parameters. Is there a good way to avoid this type of tedious repetitiveness? Should I inherit from <code>namedtuple</code>?</p>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "tags": ["python", "python-decorators", "namedtuple"], "answer_count": 9, "link": "http://stackoverflow.com/questions/35190877/how-do-i-avoid-the-self-x-x-self-y-y-self-z-z-pattern-in-init", "answers": [{"body": "<p>explicit is better than implicit ... \nso sure you could make it more concise:</p>\n\n<pre><code>def __init__(self,a,b,c):\n    for k,v in locals().items():\n        if k != \"self\":\n             setattr(self,k,v)\n</code></pre>\n\n<p>The better question is should you?</p>\n\n<p>... that said if you want a  named tuple I would recommend using a namedtuple (remember tuples have certain conditions attached to them) ... perhaps you want an ordereddict or even just a dict ...</p>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35190922, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p><strong>EDIT</strong></p>\n\n<p>It seems that several people are concerned about presenting this solution, so I will provide a very clear disclaimer. You should not use this solution. I only provide it as information, so you know that the language is capable of this. The rest of the answer is just showing language capabilities, not endorsing using them in this way.</p>\n\n<p><strong>ORIGINAL ANSWER</strong></p>\n\n<p>There isn't really anything wrong with explicitly copying parameters into attributes. If you have too many parameters in the ctor, it is sometimes considered a code smell and maybe you should group these params into a fewer objects. Other times, it is necessary and there is nothing wrong with it. <strong>Anyway, doing it explicitly is the way to go.</strong></p>\n\n<p>However, since you are asking HOW it can be done (and not whether it should be done), then one solution is this:</p>\n\n<pre><code>class A:\n    def __init__(self, **kwargs):\n        for key in kwargs:\n          setattr(self, key, kwargs[key])\n\na = A(l=1, d=2)\na.l # will return 1\na.d # will return 2\n</code></pre>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35190941, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>You could also do:</p>\n\n<pre><code>locs = locals()\nfor arg in inspect.getargspec(self.__init__)[0][1:]:\n    setattr(self, arg, locs[arg])\n</code></pre>\n\n<p>Of course, you would have to import the <code>inspect</code> module.</p>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35190996, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>As others have mentioned, the repetition isn't bad, but in some cases a namedtuple can be a great fit for this type of issue. This avoids using locals() or kwargs, which are usually a bad idea. </p>\n\n<pre><code>from collections import namedtuple\n# declare a new object type with three properties; x y z\n# the first arg of namedtuple is a typename\n# the second arg is comma-separated or space-separated property names\nXYZ = namedtuple(\"XYZ\", \"x, y, z\")\n\n# create an object of type XYZ. properties are in order\nabc = XYZ(\"one\", \"two\", 3)\nprint abc.x\nprint abc.y\nprint abc.z\n</code></pre>\n\n<p>I've found limited use for it, but you can inherit a namedtuple as with any other object (example continued):</p>\n\n<pre><code>class MySuperXYZ(XYZ):\n    \"\"\" I add a helper function which returns the original properties \"\"\"\n    def properties(self):\n        return self.x, self.y, self.z\n\nabc2 = MySuperXYZ(4, \"five\", \"six\")\nprint abc2.x\nprint abc2.y\nprint abc2.z\nprint abc2.properties()\n</code></pre>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35191590, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>A decorator solution that keeps the signature:</p>\n\n<pre><code>import decorator\nimport inspect\nimport sys\n\n\n@decorator.decorator\ndef simple_init(func, self, *args, **kws):\n    \"\"\"\n    @simple_init\n    def __init__(self,a,b,...,z)\n        dosomething()\n\n    behaves like\n\n    def __init__(self,a,b,...,z)\n        self.a = a\n        self.b = b\n        ...\n        self.z = z\n        dosomething()\n    \"\"\"\n\n    #init_argumentnames_without_self = ['a','b',...,'z']\n    if sys.version_info.major == 2:\n        init_argumentnames_without_self = inspect.getargspec(func).args[1:]\n    else:\n        init_argumentnames_without_self = tuple(inspect.signature(func).parameters.keys())[1:]\n\n    positional_values = args\n    keyword_values_in_correct_order = tuple(kws[key] for key in init_argumentnames_without_self if key in kws)\n    attribute_values = positional_values + keyword_values_in_correct_order\n\n    for attribute_name,attribute_value in zip(init_argumentnames_without_self,attribute_values):\n        setattr(self,attribute_name,attribute_value)\n\n    # call the original __init__\n    func(self, *args, **kws)\n\n\nclass Test():\n    @simple_init\n    def __init__(self,a,b,c,d=4):\n        print(self.a,self.b,self.c,self.d)\n\n#prints 1 3 2 4\nt = Test(1,c=2,b=3)\n#keeps signature\n#prints ['self', 'a', 'b', 'c', 'd']\nif sys.version_info.major == 2:\n    print(inspect.getargspec(Test.__init__).args)\nelse:\n    print(inspect.signature(Test.__init__))\n</code></pre>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35194904, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>To expand on <code>gruszczy</code>s answer, I have used a pattern like:</p>\n\n<pre><code>class X:\n    x = None\n    y = None\n    z = None\n    def __init__(self, **kwargs):\n        for (k, v) in kwargs.items():\n            if hasattr(self, k):\n                setattr(self, k, v)\n            else:\n                raise TypeError('Unknown keyword argument: {:s}'.format(k))\n</code></pre>\n\n<p>I like this method because it:</p>\n\n<ul>\n<li>avoids repetition</li>\n<li>is resistant against typos when constructing an object</li>\n<li>works well with subclassing (can just <code>super().__init(...)</code>)</li>\n<li>allows for documentation of the attributes on a class-level (where they belong) rather than in <code>X.__init__</code></li>\n</ul>\n\n<p>It could probably be improved upon a bit, but I'm the only user of my own code so I am not worried about any form of input sanitation.</p>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35196024, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>This is a solution without any additional imports.</p>\n\n<h2>Helper function</h2>\n\n<p>A small helper function makes it more convenient and re-usable:</p>\n\n<pre><code>def auto_init(local_name_space):\n    \"\"\"Set instance attributes from arguments.\n    \"\"\"\n    self = local_name_space.pop('self')\n    for name, value in local_name_space.items():\n        setattr(self, name, value)\n</code></pre>\n\n<h2>Application</h2>\n\n<p>You need to call it with <code>locals()</code>:</p>\n\n<pre><code>class A:\n    def __init__(self, x, y, z):\n        auto_init(locals())\n</code></pre>\n\n<h2>Test</h2>\n\n<pre><code>a = A(1, 2, 3)\nprint(a.__dict__)\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>{'y': 2, 'z': 3, 'x': 1}\n</code></pre>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35239676, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>My 0.02$. It is very close to Joran Beasley answer, but more elegant:</p>\n\n<pre><code>def __init__(self, a, b, c, d, e, f):\n    vars(self).update((k, v) for k, v in locals().items() if v is not self)\n</code></pre>\n\n<p>Additionally, Mike M\u00fcller's answer (the best one to my taste) can be reduced with this technique:</p>\n\n<pre><code>def auto_init(ns):\n    self = ns.pop('self')\n    vars(self).update(ns)\n</code></pre>\n\n<p>And the just call <code>auto_init(locals())</code> from your <code>__init__</code></p>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35311188, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}, {"body": "<p>It's a natural way to do things in Python. Don't try to invent something more clever, it will lead to overly clever code that no one on your team will understand. If you want to be a team player and then keep writing it this way.</p>\n", "title": "How do I avoid the &quot;self.x = x; self.y = y; self.z = z&quot; pattern in __init__?", "answer_id": 35531267, "question_id": 35190877, "tags": ["python", "python-decorators", "namedtuple"]}], "question_id": 35190877}, {"body": "<p>For a school project, I've to code the C function printf. Things are going pretty well, but there is one question I can't find a good answer to, so here I am.</p>\n\n<pre><code>printf(\"PRINTF(d) \\t: %d\\n\", -2147483648);\n</code></pre>\n\n<p>tells me (<code>gcc -Werror -Wextra -Wall</code>):</p>\n\n<pre><code>   error: format specifies type 'int' but the argument has type 'long'\n      [-Werror,-Wformat]\n        printf(\"PRINTF(d) \\t: %d\\n\", -2147483648);\n                              ~~     ^~~~~~~~~~~\n                              %ld\n</code></pre>\n\n<p>But if I use an int variable, everything is going well:</p>\n\n<pre><code>int i;\n\ni = -2147483648;\nprintf(\"%d\", i);\n</code></pre>\n\n<p>Why?</p>\n\n<h2>EDIT :</h2>\n\n<p>I understood many points and were very interesting. Anyway, I guess <code>printf</code> is using the <code>&lt;stdarg.h&gt;</code> librairy and so, <code>va_arg(va_list ap, type)</code> should also return the right type. For <code>%d</code> and <code>%i</code>, obviously the type returned is an <code>int</code>. Does it change anything ?</p>\n", "title": "Why does the smallest int, \u22122147483648, have type &#39;long&#39;?", "tags": ["c", "types"], "answer_count": 2, "link": "http://stackoverflow.com/questions/34724320/why-does-the-smallest-int-%e2%88%922147483648-have-type-long", "answers": [{"body": "<p>In C, <code>-2147483648</code> is not an integer constant. <code>2147483648</code> is an integer constant, and <code>-</code> is just a unary operator applied to it, yielding a constant expression. The value of <code>2147483648</code> does not fit in an <code>int</code> (it's one too large, <code>2147483647</code> is typically the largest integer) and thus the integer constant has type <code>long</code>, which causes the problem you observe. If you want to mention the lower limit for an <code>int</code>, either use the macro <code>INT_MIN</code> from <code>&lt;limits.h&gt;</code> (the portable approach) or carefully avoid mentioning <code>2147483648</code>:</p>\n\n<pre><code>printf(\"PRINTF(d) \\t: %d\\n\", -1 - 2147483647);\n</code></pre>\n", "title": "Why does the smallest int, \u22122147483648, have type &#39;long&#39;?", "answer_id": 34724513, "question_id": 34724320, "tags": ["c", "types"]}, {"body": "<p>The problem is that <code>-2147483648</code> is not an integer literal. It's an expression consisting of the unary negation operator <code>-</code> and the integer <code>2147483648</code>, which is too big to be an <code>int</code> if <code>int</code>s are 32 bits. Since the compiler will choose an appropriately-sized signed integer to represent <code>2147483648</code> before applying the negation operator, the type of the result will be larger than an <code>int</code>.</p>\n\n<p>If you know that your <code>int</code>s are 32 bits, and want to avoid the warning without mutilating readability, use an explicit cast:</p>\n\n<pre><code>printf(\"PRINTF(d) \\t: %d\\n\", (int)(-2147483648));\n</code></pre>\n\n<p>That's defined behaviour on a 2's complement machine with 32-bit <code>int</code>s.</p>\n\n<p>For increased theoretical portability, use <code>INT_MIN</code> instead of the number, and let us know where you found a non-2's-complement machine to test it on. </p>\n\n<hr>\n\n<p>To be clear, that last paragraph was partly a joke. <code>INT_MIN</code> is definitely the way to go if you mean \"the smallest <code>int</code>\", because <code>int</code> varies in size. There are still lots of 16-bit implementations, for example. Writing out -2<sup>31</sup> is only useful if you definitely always mean precisely that value, in which case you would probably use a fixed-sized type like <code>int32_t</code> instead of <code>int</code>.</p>\n\n<p>You might want some alternative to writing out the number in decimal to make it clearer for those who might not notice the difference between <code>2147483648</code> and <code>2174483648</code>, but you need to be careful.</p>\n\n<p>As mentioned above, on a 32-bit 2's-complement machine, <code>(int)(-2147483648)</code> will not overflow and is therefore well-defined, because <code>-2147483648</code> will be treate as a wider signed type. However, the same is not true for <code>(int)(-0x80000000)</code>. <code>0x80000000</code> will be treated as an <code>unsigned int</code> (since it fits into the unsigned representation); <code>-0x80000000</code> is well-defined (but the <code>-</code> has no effect if <code>int</code> is 32 bits), and the conversion of the resulting <code>unsigned int</code> <code>0x80000000</code> to <code>int</code> involves an overflow. To avoid the overflow, you would need to cast the hex constant to a signed type: <code>(int)(-(long long)(0x80000000))</code>. </p>\n\n<p>Similarly, you need to take care if you want to use the left shift operator. <code>1&lt;&lt;31</code> is undefined behaviour on 32-bit machines with 32-bit (or smaller) <code>int</code>s; it will only evaluate to 2<sup>31</sup> if <code>int</code> is at least 33 bits, because left shift by <code>k</code> bits is only well-defined if <code>k</code> is strictly less than the number of non-sign bits of the integer type of the left-hand argument.</p>\n\n<p><code>1LL&lt;&lt;31</code> is safe, since <code>long long int</code> is required to be able to represent 2<sup>63</sup>-1, so its bit size must be greater than 32. So the form</p>\n\n<pre><code>(int)(-(1LL&lt;&lt;31))\n</code></pre>\n\n<p>is possibly the most readable. YMMV.</p>\n\n<hr>\n\n<p>For any passing pedants, this question is tagged C, and the latest C draft (n1570.pdf) says, with respect to <code>E1 &lt;&lt; E2</code>, where <code>E1</code> has a signed type, that the value is defined only if <code>E1</code> is nonnegative and <code>E1 &times; 2<sup>E2</sup></code> \"is representable in the result type\". (&sect;6.5.7 para 4).</p>\n\n<p>That's different from C++, in which the application of the left-shift operator is defined if <code>E1</code> is nonnegative and <code>E1 &times; 2<sup>E2</sup></code> \"is representable\n<em>in the corresponding unsigned type</em> of the result type\" (&sect;5.8 para. 2, emphasis added).</p>\n\n<p>In C++, according to the most recent draft standard, the conversion of an integer value to a signed integer type is <em>implementation-defined</em> if the value cannot be represented in the destination type (&sect;4.7 para. 3). The corresponding paragraph of the C standard -- &sect;6.3.1.3 para. 3 -- says that \"either the result is implementation-defined or an implementation-defined signal is raised\".)</p>\n", "title": "Why does the smallest int, \u22122147483648, have type &#39;long&#39;?", "answer_id": 34725116, "question_id": 34724320, "tags": ["c", "types"]}], "question_id": 34724320}, {"body": "<p>I am researching Coffee-script on the website <a href=\"http://coffeescript.org/\">http://coffeescript.org/</a> it has the text</p>\n\n<blockquote>\n  <p>The CoffeeScript compiler is itself written in CoffeeScript</p>\n</blockquote>\n\n<p>How can a compiler compile itself, or what does this statement mean?</p>\n", "title": "How can a compiler compile itself?", "tags": ["compilation"], "answer_count": 9, "link": "http://stackoverflow.com/questions/38005656/how-can-a-compiler-compile-itself", "answers": [{"body": "<p>The first edition of a compiler can't be machine-generated from a programming language specific to it; your confusion is understandable. A later version of the compiler with more language features (with source rewritten in the first version of the new language) could be built by the first compiler. That version could then compile the next compiler, and so on. Here's an example:</p>\n\n<ol>\n<li>The first CoffeeScript compiler is written in Ruby, producing version 1 of CoffeeScript</li>\n<li>The source code of the CS compiler is rewritten in CoffeeScript 1</li>\n<li>The original CS compiler compiles the new code (written in CS 1) into version 2 of the compiler</li>\n<li>Changes are made to the compiler source code to add new language features</li>\n<li>The second CS compiler (the first one written in CS) compiles the revised new source code into version 3 of the compiler</li>\n<li>Repeat steps 4 and 5 for each iteration</li>\n</ol>\n\n<p>Note: I'm not sure exactly how CoffeeScript versions are numbered, that was just an example. Thanks to <a href=\"http://superuser.com/questions/1090890/how-can-a-compiler-compile-itself/1090894?noredirect=1#comment1545209_1090894\">J\u00f6rg W Mittag's comment</a> for the original language correction (it's Ruby, not JavaScript).</p>\n\n<p>As mentioned by <a href=\"http://superuser.com/users/358758/marc-2377\">Marc.2377</a> in a comment, this process is called <a href=\"https://en.wikipedia.org/wiki/Bootstrapping_(compilers)\">bootstrapping</a>. Another example of a bootstrapping compiler is <code>rustc</code>, the compiler for the <a href=\"https://en.wikipedia.org/wiki/Rust_(programming_language)\">Rust language</a>.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005657, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<p>You have already gotten a very good answer, however I want to offer you a different perspective, that will hopefully be enlightening to you. Let's first establish two facts that we can both agree on:</p>\n\n<ol>\n<li>The CoffeeScript compiler is a program which can compile programs written in CoffeeScript.</li>\n<li>The CoffeeScript compiler is a program written in CoffeeScript.</li>\n</ol>\n\n<p>I'm sure you can agree that both #1 and #2 are true. Now, look at the two statements. Do you see now that it is completely normal for the CoffeeScript compiler to be able to compile the CoffeeScript compiler?</p>\n\n<p>The compiler doesn't care <em>what</em> it compiles. As long as it's a program written in CoffeeScript, it can compile it. And the CoffeeScript compiler itself just happens to be such a program. The CoffeeScript compiler doesn't care that it's the CoffeeScript compiler itself it is compiling. All it sees is some CoffeeScript code. Period.</p>\n\n<blockquote>\n  <p>How can a compiler compile itself, or what does this statement mean?</p>\n</blockquote>\n\n<p>Yes, that's exactly what that statement means, and I hope you can see now how that statement is true.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005659, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<p>A small clarification: Here the term 'the compiler' glosses over the fact that there are TWO files involved.  One is an executable which takes as input files written in CoffeScript and produces as its output file another executable, a linkable object file, or a shared library.  The other is a CoffeeScript source file which just happens to describe the procedure for compiling CoffeeScript.  You apply the first file to the second, producing a third which is capable of performing the same act of compilation as the first (possibly more, if the second file defines features not implemented by the first), and so may replace the first if you so desire.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005660, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<p>In the paper <a href=\"http://dl.acm.org/citation.cfm?id=358210\"><em>Reflections on Trusting Trust</em></a>, Ken Thompson, one of the originators of Unix, writes a fascinating (and easily readable) overview of how the C compiler compiles itself.  Similar concepts can be applied to CoffeeScript or any other language.</p>\n\n<p>The idea of a compiler that compiles its own code is vaguely similar to a <a href=\"https://en.wikipedia.org/wiki/Quine_(computing)\">quine</a>: source code that, when executed, produces as output the original source code.  <a href=\"https://github.com/namuol/quines/blob/3b55b50badfc01b3ae012f2c960dc0d8aeab3d30/coffeescript/quine.coffee\">Here is one example</a> of a CoffeeScript quine.  Thompson gave this example of a C quine:</p>\n\n<pre><code>char s[] = {\n    '\\t',\n    '0',\n    '\\n',\n    '}',\n    ';',\n    '\\n',\n    '\\n',\n    '/',\n    '*',\n    '\\n',\n    \u2026 213 lines omitted \u2026\n    0\n};\n\n/*\n * The string s is a representation of the body\n * of this program from '0'\n * to the end.\n */\n\nmain()\n{\n    int i;\n\n    printf(\"char\\ts[] = {\\n\");\n    for(i = 0; s[i]; i++)\n        printf(\"\\t%d,\\n\", s[i]);\n    printf(\"%s\", s);\n}\n</code></pre>\n\n<p>Next, you might wonder how the compiler is taught that an escape sequence like <code>'\\n'</code> represents ASCII code 10.  The answer is that somewhere in the C compiler, there is a routine that interprets character literals, containing some conditions like this to recognize backslash sequences:</p>\n\n<pre><code>\u2026\nc = next();\nif (c != '\\\\') return c;        /* A normal character */\nc = next();\nif (c == '\\\\') return '\\\\';     /* Two backslashes in the code means one backslash */\nif (c == 'r')  return '\\r';     /* '\\r' is a carriage return */\n\u2026\n</code></pre>\n\n<p>So, we can add one condition to the code above\u2026</p>\n\n<pre><code>if (c == 'n')  return 10;       /* '\\n' is a newline */\n</code></pre>\n\n<p>\u2026 to produce a compiler that knows that <code>'\\n'</code> represents ASCII 10.  Interestingly, that compiler, <em>and all subsequent compilers compiled by it</em>, \"know\" that mapping, so in the next generation of the source code, you can change that last line into</p>\n\n<pre><code>if (c == 'n')  return '\\n';\n</code></pre>\n\n<p>\u2026 and it will do the right thing!  The <code>10</code> comes from the compiler, and no longer needs to be explicitly defined in the compiler's source code.<sup>1</sup></p>\n\n<p>That is one example of a C language feature that was implemented in C code.  Now, repeat that process for every single language feature, and you have a \"self-hosting\" compiler: a C compiler that is written in C.</p>\n\n<hr>\n\n<p><sup>1</sup> The plot twist described in the paper is that since the compiler can be \"taught\" facts like this, it can also be mis-taught to generate trojaned executables in a way that is difficult to detect, and such an act of sabotage can persist in all compilers produced by the tainted compiler.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005661, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<blockquote>\n  <p>How can a compiler compile itself, or what does this statement mean?</p>\n</blockquote>\n\n<p>It means exactly that. First of all, some things to consider. There are four objects we need to look at:</p>\n\n<ul>\n<li>The source code of any arbitrary CoffeScript program</li>\n<li>The (generated) assembly of any arbitrary CoffeScript program</li>\n<li>The source code of the CoffeScript compiler</li>\n<li>The (generated) assembly of the CoffeScript compiler</li>\n</ul>\n\n<p>Now, it should be obvious that you can use the generated assembly - the executable - of the CoffeScript compiler to compile any arbitrary CoffeScript program, and generate the assembly for that program.</p>\n\n<p>Now, the CoffeScript compiler itself is just an arbitrary CoffeScript program, and thus, it can be compiled by the CoffeScript compiler.</p>\n\n<p>It seems that your confusion stems from the fact that when you create your own new language, you don't <em>have</em> a compiler yet you can use to compile your compiler. This surely looks like an <strong>chicken-egg problem</strong>, right?</p>\n\n<p>Introduce the process called <a href=\"https://en.wikipedia.org/wiki/Bootstrapping_(compilers)\"><strong>bootstrapping</strong></a>. </p>\n\n<ol>\n<li>You write a compiler in an already existing language (in case of CoffeScript, the original compiler was written in Ruby) that can compile a subset of the new language</li>\n<li>You write a compiler that can compile a subset of the new language in the new language itself. You can only use language features the compiler from the step above can compile.</li>\n<li>You use the compiler from step 1 to compile the compiler from step 2. This leaves you with an assembly that was originally written in a subset of the new language, and that is able to compile a subset of the new language.</li>\n</ol>\n\n<p>Now you need to add new features. Say you have only implemented <code>while</code>-loops, but also want <code>for</code>-loops. This isn't a problem, since you can rewrite any <code>for</code>-loop in such a way that it is a <code>while</code>-loop. This means you can only use <code>while</code>-loops in the source code of your compiler, since the assembly you have at hand can only compile those. But you can create functions inside your compiler that can pase and compile <code>for</code>-loops with it. Then you use the assembly you already have, and compile the new compiler version. And now you have an assembly of an compiler that can also parse and compile <code>for</code>-loops! You can now go back to the source file of your compiler, and rewrite any <code>while</code>-loops you don't want into <code>for</code>-loops.</p>\n\n<p>Rinse and repeat until all language features that are desired can be compiled with the compiler.</p>\n\n<p><code>while</code> and <code>for</code> obviously were only examples, but this works for any new language feature you want. And then you are in the situation CoffeScript is in now: The compiler compiles itself.</p>\n\n<p>There is much literature out there. <a href=\"http://delivery.acm.org/10.1145/360000/358210/reflections.pdf?ip=93.197.235.200&amp;id=358210&amp;acc=OPEN&amp;key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&amp;CFID=633144075&amp;CFTOKEN=28514372&amp;__acm__=1466325779_14230b9a3b34d97798dec9f6990c1abe\">Reflections on Trusting Trust</a> is a classic everyone interested in that topic should read at least once.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005662, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<p>It's not a matter of compilers here, but a matter of expressiveness of the language, since a compiler is just a program written in some language.</p>\n\n<p>When we say that \"a language is written/implemented\" we actually mean that a compiler or interpreter for that language is implemented. There are programming languages in which you can write programs that implement the language (are compilers/interpreters for the same language). These languages are called <a href=\"https://en.wikipedia.org/wiki/Universal_Turing_machine\" rel=\"nofollow\">universal languages</a>.</p>\n\n<p>In order to be able to understand this, think about a metal lathe. It is a tool used to shape metal. It is possible, using just that tool, to create another, identical tool, by creating its parts. Thus, that tool is a universal machine. Of course, the first one was created using other means (other tools), and was probably of lower quality. But the first one was used to build new ones with higher precision.</p>\n\n<p>A 3D printer is almost a universal machine. You can print the whole 3D printer using a 3D printer (you can't build the tip that melts the plastic).</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005663, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<h1>Proof by induction</h1>\n\n<h2>Inductive step</h2>\n\n<p>The n+1th version of the compiler is written in X.</p>\n\n<p>Thus it can be compiled by the nth version of the compiler (also written in X).</p>\n\n<h2>Base case</h2>\n\n<p>But the first version of the compiler written in X must be compiled\nby a compiler for X that is written in a language other than X. This step is called bootstrapping the compiler.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005664, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<p>Compilers take a high-level specification and turn it into a low-level implementation, such as can be executed on hardware. Therefore there is no relationship between the format of the specification and the actual execution besides the semantics of the language being targeted. Cross-compilers move from one system to another system, cross-language compilers compile one language specification into another language specification. Basically compiling is just translation and the level is usually higher-level of language to lower-level of language but there are many variants. Bootstrapping compilers are the most confusing of course because they compile the language they are written in. Don't forget the initial step in bootstrapping which requires at least a minimal existing version that is executable. Many bootstrapped compilers work on the minimal features of a programming language first and add additional complex language features going forward as long as the new feature can be expressed using the previous features. If that were not the case it would require to have that part of the \"compiler\" be developed in another language beforehand.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38005665, "question_id": 38005656, "tags": ["compilation"]}, {"body": "<ol>\n<li>The CoffeeScript compiler was first written in Ruby.  </li>\n<li>The CoffeeScript compiler was then re-written in CoffeeScript.  </li>\n</ol>\n\n<p>Since the Ruby version of the CoffeeScript compiler already existed,<br>\nit was used to create the CoffeeScript version of the CoffeeScript compiler.  </p>\n\n<p><a href=\"http://i.stack.imgur.com/6Jx8T.png\"><img src=\"http://i.stack.imgur.com/6Jx8T.png\" alt=\"enter image description here\"></a>\nThis is known as a <a href=\"https://en.wikipedia.org/wiki/Bootstrapping_(compilers)\">self-hosting compiler</a>. </p>\n\n<p>It's extremely common, and usually results from an author's desire to use their own language to maintain that language's growth.</p>\n", "title": "How can a compiler compile itself?", "answer_id": 38235631, "question_id": 38005656, "tags": ["compilation"]}], "question_id": 38005656}, {"body": "<p>I've seen programmers use the formula</p>\n\n<pre><code>mid = start + (end - start) / 2\n</code></pre>\n\n<p>instead of using the simpler formula</p>\n\n<pre><code>mid = (start + end) / 2\n</code></pre>\n\n<p>for finding the middle element in the array or list.</p>\n\n<p>Why do they use the former one?</p>\n", "title": "Why prefer start + (end - start) / 2 over (start + end) / 2 when calculating the middle of an array?", "tags": ["c", "algorithm"], "answer_count": 3, "link": "http://stackoverflow.com/questions/38688028/why-prefer-start-end-start-2-over-start-end-2-when-calculating-the", "answers": [{"body": "<p>There are three reasons.</p>\n\n<p>First of all, <code>start + (end - start) / 2</code> works even if you are using pointers, as long as <code>end - start</code> doesn't overflow<sup>1</sup>.</p>\n\n<pre><code>int *start = ..., *end = ...;\nint *mid = start + (end - start) / 2; // works as expected\nint *mid = (start + end) / 2;         // type error, won't compile\n</code></pre>\n\n<p>Second of all, <code>start + (end - start) / 2</code> won't overflow if <code>start</code> and <code>end</code> are large positive numbers.  With signed operands, overflow is undefined:</p>\n\n<pre><code>int start = 0x7ffffffe, end = 0x7fffffff;\nint mid = start + (end - start) / 2; // works as expected\nint mid = (start + end) / 2;         // overflow... undefined\n</code></pre>\n\n<p>(Note that <code>end - start</code> may overflow, but only if <code>start &lt; 0</code> or <code>end &lt; 0</code>.)</p>\n\n<p>Or with unsigned arithmetic, overflow is defined but gives you the wrong answer.  However, for unsigned operands, <code>start + (end - start) / 2</code> will never overflow as long as <code>end &gt;= start</code>.</p>\n\n<pre><code>unsigned start = 0xfffffffeu, end = 0xffffffffu;\nunsigned mid = start + (end - start) / 2; // works as expected\nunsigned mid = (start + end) / 2;         // mid = 0x7ffffffe\n</code></pre>\n\n<p>Finally, you often want to round towards the <code>start</code> element.</p>\n\n<pre><code>int start = -3, end = 0;\nint mid = start + (end - start) / 2; // -2, closer to start\nint mid = (start + end) / 2;         // -1, surprise!\n</code></pre>\n\n<h3>Footnotes</h3>\n\n<p><sup>1</sup> According to the C standard, if the result of pointer subtraction is not representable as a <code>ptrdiff_t</code>, then the behavior is undefined.  However, in practice, this requires allocating a <code>char</code> array using at least half the entire address space.</p>\n", "title": "Why prefer start + (end - start) / 2 over (start + end) / 2 when calculating the middle of an array?", "answer_id": 38688126, "question_id": 38688028, "tags": ["c", "algorithm"]}, {"body": "<p>We can take a simple example to demonstrate this fact. Suppose in a certain <strong>large</strong> array, we are trying to find the midpoint of the range <code>[1000, INT_MAX]</code>. Now, <code>INT_MAX</code> is the largest value the <code>int</code> data type can store. Even if <code>1</code> is added to this, the final value will become negative.</p>\n\n<p>Also, <code>start = 1000</code> and <code>end = INT_MAX</code>.</p>\n\n<p>Using the formula: <code>(start + end)/2</code>, </p>\n\n<p>the mid-point will be </p>\n\n<blockquote>\n  <p><code>(1000 + INT_MAX)/2</code> = <code>-(INT_MAX+999)/2</code>, which is <strong>negative</strong> and <strong>may give segmentation fault</strong> if we try to index using this value.</p>\n</blockquote>\n\n<p>But, using the formula, <code>(start + (end-start)/2)</code>, we get:</p>\n\n<blockquote>\n  <p><code>(1000 + (INT_MAX-1000)/2)</code> = <code>(1000 + INT_MAX/2 - 500)</code> = <code>(INT_MAX/2 + 500)</code> <strong>which will not overflow</strong>.</p>\n</blockquote>\n", "title": "Why prefer start + (end - start) / 2 over (start + end) / 2 when calculating the middle of an array?", "answer_id": 38688150, "question_id": 38688028, "tags": ["c", "algorithm"]}, {"body": "<p>To add to what others have already said, the first one explains its meaning clearer to those less mathematically minded:</p>\n\n<pre><code>mid = start + (end - start) / 2\n</code></pre>\n\n<p>reads as:</p>\n\n<blockquote>\n  <p>mid equals start plus half of the length.</p>\n</blockquote>\n\n<p>whereas:</p>\n\n<pre><code>mid = (start + end) / 2\n</code></pre>\n\n<p>reads as:</p>\n\n<blockquote>\n  <p>mid equals half of start plus end</p>\n</blockquote>\n\n<p>Which does not seem as clear as the first, at least when expressed like that.</p>\n\n<p>as Kos pointed out it can also read:</p>\n\n<blockquote>\n  <p>mid equals the average of start and end</p>\n</blockquote>\n\n<p>Which is clearer but still not, at least in my opinion, as clear as the first.</p>\n", "title": "Why prefer start + (end - start) / 2 over (start + end) / 2 when calculating the middle of an array?", "answer_id": 38697065, "question_id": 38688028, "tags": ["c", "algorithm"]}], "question_id": 38688028}, {"body": "<p>Consider:</p>\n\n<pre><code>using System;\n\npublic class Test\n{\n    enum State : sbyte { OK = 0, BUG = -1 }\n\n    static void Main(string[] args)\n    {\n        var s = new State[1, 1];\n        s[0, 0] = State.BUG;\n        State a = s[0, 0];\n        Console.WriteLine(a == s[0, 0]); // False\n    }\n}\n</code></pre>\n\n<p>How can this be explained? It occurs in debug builds in Visual Studio 2015 when running in the x86 JIT. A release build or running in the x64 JIT prints True as expected.</p>\n\n<p>To reproduce from the command line:</p>\n\n<pre><code>csc Test.cs /platform:x86 /debug\n</code></pre>\n\n<p>(<code>/debug:pdbonly</code>, <code>/debug:portable</code> and <code>/debug:full</code> also reproduce.)</p>\n", "title": "Why is the enumeration value from a multi dimensional array not equal to itself?", "tags": ["c#", "multidimensional-array", "enumeration"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36588365/why-is-the-enumeration-value-from-a-multi-dimensional-array-not-equal-to-itself", "answers": [{"body": "<p>Let's consider OP's declaration:</p>\n\n<pre><code>enum State : sbyte { OK = 0, BUG = -1 }\n</code></pre>\n\n<p>Since the bug only occurs when <code>BUG</code> is negative (from -128 to -1) and State is an enum of <em>signed byte</em> I started to suppose that there were a cast issue somewhere.</p>\n\n<p>If you run this:</p>\n\n<pre><code>Console.WriteLine((sbyte)s[0, 0]);\nConsole.WriteLine((sbyte)State.BUG);\nConsole.WriteLine(s[0, 0]);\nunchecked\n{\n    Console.WriteLine((byte) State.BUG);\n}\n</code></pre>\n\n<p>it will output :</p>\n\n<blockquote>\n  <p>255</p>\n  \n  <p>-1</p>\n  \n  <p>BUG</p>\n  \n  <p>255</p>\n</blockquote>\n\n<p>For a reason that I ignore<sup>(as of now)</sup> <code>s[0, 0]</code> is cast to a byte before evaluation and that's why it claims that <code>a == s[0,0]</code> is false.</p>\n", "title": "Why is the enumeration value from a multi dimensional array not equal to itself?", "answer_id": 36593262, "question_id": 36588365, "tags": ["c#", "multidimensional-array", "enumeration"]}, {"body": "<p>You found a code generation bug in the .NET 4 x86 jitter.  It is a very unusual one, it only fails when the code is not optimized.  The machine code looks like this:</p>\n\n<pre><code>        State a = s[0, 0];\n013F04A9  push        0                            ; index 2 = 0\n013F04AB  mov         ecx,dword ptr [ebp-40h]      ; s[] reference\n013F04AE  xor         edx,edx                      ; index 1 = 0\n013F04B0  call        013F0058                     ; eax = s[0, 0]\n013F04B5  mov         dword ptr [ebp-4Ch],eax      ; $temp1 = eax \n013F04B8  movsx       eax,byte ptr [ebp-4Ch]       ; convert sbyte to int\n013F04BC  mov         dword ptr [ebp-44h],eax      ; a = s[0, 0]\n        Console.WriteLine(a == s[0, 0]); // False\n013F04BF  mov         eax,dword ptr [ebp-44h]      ; a\n013F04C2  mov         dword ptr [ebp-50h],eax      ; $temp2 = a\n013F04C5  push        0                            ; index 2 = 0\n013F04C7  mov         ecx,dword ptr [ebp-40h]      ; s[] reference \n013F04CA  xor         edx,edx                      ; index 1 = 0\n013F04CC  call        013F0058                     ; eax = s[0, 0]\n013F04D1  mov         dword ptr [ebp-54h],eax      ; $temp3 = eax \n                                               ; &lt;=== Bug here!\n013F04D4  mov         eax,dword ptr [ebp-50h]      ; a == s[0, 0] \n013F04D7  cmp         eax,dword ptr [ebp-54h]  \n013F04DA  sete        cl  \n013F04DD  movzx       ecx,cl  \n013F04E0  call        731C28F4  \n</code></pre>\n\n<p>A plodding affair with lots of temporaries and code duplication, that's normal for unoptimized code.  The instruction at 013F04B8 is notable, that is where the necessary conversion from sbyte to a 32-bit integer occurs.  The array getter helper function returned 0x0000000FF, equal to State.BUG, and that needs to be converted to -1 (0xFFFFFFFF) before the value can be compared.  The MOVSX instruction is a Sign eXtension instruction.</p>\n\n<p>Same thing happens again at 013F04CC, but this time there is <strong>no</strong> MOVSX instruction to make the same conversion.  That's where the chips fall down, the CMP instruction compares 0xFFFFFFFF with 0x000000FF and that is false.  So this is an error of omission, the code generator failed to emit MOVSX again to perform the same sbyte to int conversion.</p>\n\n<p>What is particularly unusual about this bug is that this works correctly when you enable the optimizer, it now knows to use MOVSX in both cases.</p>\n\n<p>The probable reason that this bug went undetected for so long is the usage of sbyte as the base type of the enum.  Quite rare to do.  Using a multi-dimensional array is instrumental as well, the combination is fatal.</p>\n\n<p>Otherwise a pretty critical bug I'd say.  How widespread it might be is hard to guess, I only have the 4.6.1 x86 jitter to test.  The x64 and the 3.5 x86 jitter generate very different code and avoid this bug.  The temporary workaround to keep going is to remove sbyte as the enum base type and let it be the default, <em>int</em>, so no sign extension is necessary.</p>\n\n<p>You can file the bug at connect.microsoft.com, linking to this Q+A should be enough to tell them everything they need to know.  Let me know if you don't want to take the time and I'll take care of it.</p>\n", "title": "Why is the enumeration value from a multi dimensional array not equal to itself?", "answer_id": 36593473, "question_id": 36588365, "tags": ["c#", "multidimensional-array", "enumeration"]}], "question_id": 36588365}, {"body": "<p>We have a very simple program invoking the <code>Type.GetType</code> static method. Both examples should return a valid type instance. Only the second one actually is. Looks like something odd is happening with the stack crawl used by <code>GetType</code>, but what exactly is the issue here? Is it bug or some obscure feature?</p>\n\n<pre><code>public class TestClass { }\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        var fullName = typeof(TestClass).FullName;\n        Console.WriteLine(\"Full name: {0}\", fullName);\n\n        new[] { fullName }.Select(Type.GetType).ToList().ForEach(t =&gt; Console.WriteLine(\"Method group: '{0}'\", t));\n        new[] { fullName }.Select(t =&gt; Type.GetType(t)).ToList().ForEach(t =&gt; Console.WriteLine(\"Closure: '{0}'\", t));\n    }\n}\n</code></pre>\n\n<p>Running:</p>\n\n<pre><code>Full name: GetTypeBeingWeird.TestClass\nMethod group: ''\nClosure: 'GetTypeBeingWeird.TestClass'\n</code></pre>\n", "title": "Why can&#39;t GetType() find types when invoked through a method group delegate?", "tags": ["c#", "reflection"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35701953/why-cant-gettype-find-types-when-invoked-through-a-method-group-delegate", "answers": [{"body": "<p>This is really interesting. It's a mixture of the behaviour of <code>Type.GetType(string)</code> in terms of the calling assembly, and how method group conversions work.</p>\n\n<p>First, the <a href=\"https://msdn.microsoft.com/en-us/library/w3f99sx1(v=vs.110).aspx\"><code>Type.GetType</code></a> documentation includes this:</p>\n\n<blockquote>\n  <p>If <em>typeName</em> includes the namespace but not the assembly name, this method searches only the calling object's assembly and Mscorlib.dll, in that order.</p>\n</blockquote>\n\n<p>In your first call, you're passing in a delegate which calls <code>Type.GetType</code>... but it isn't particularly called from your assembly. It's effectively called <em>directly</em> from the <code>Select</code> method in LINQ... if you looked at the stack trace from within <code>Type.GetType</code>, you'd see <code>Select</code> as the direct caller, I believe.</p>\n\n<p>In your second call, you're passing in a closure which calls <code>Type.GetType</code>, and that call is within your assembly.</p>\n\n<p>That's why it finds the type in the second case but not the first. This is further validated by specifying a type which is in the LINQ assembly:</p>\n\n<pre><code>var fullName = typeof(Enumerable).FullName;\n</code></pre>\n\n<p>Then the results are the opposite way round:</p>\n\n<pre><code>Full name: System.Linq.Enumerable\nMethod group: 'System.Linq.Enumerable'\nClosure: ''\n</code></pre>\n\n<p>If you specify something in mscorlib (e.g. <code>typeof(string).FullName</code>) then both approaches work:</p>\n\n<pre><code>Full name: System.String\nMethod group: 'System.String'\nClosure: 'System.String'\n</code></pre>\n\n<p>The way to get around this oddity when looking for your class, still using a method group, is simply to supply the assembly-qualified name instead:</p>\n\n<pre><code>var fullName = typeof(TestClass).AssemblyQualifiedName;\n</code></pre>\n", "title": "Why can&#39;t GetType() find types when invoked through a method group delegate?", "answer_id": 35702245, "question_id": 35701953, "tags": ["c#", "reflection"]}], "question_id": 35701953}, {"body": "<p>I encountered a problem when creating a project in Android Studio. (Version 1.5.1)</p>\n\n<p>I describe my actions step-by-step:</p>\n\n<ul>\n<li>Open Android Studio.</li>\n<li>Create a new project without any activity.</li>\n<li>Make a blank activity.</li>\n</ul>\n\n<p>And there might be a problem in preview mode:</p>\n\n<p>I get this message:</p>\n\n<p><code>Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback.getXmlFileParser(Ljava/lang/String;)Lorg/xmlpull/v1/XmlPullParser;</code></p>\n\n<p>In stack trace:</p>\n\n<pre><code>java.lang.NoSuchMethodError: com.android.ide.common.rendering.api.LayoutlibCallback.getXmlFileParser(Ljava/lang/String;)Lorg/xmlpull/v1/XmlPullParser;\n    at com.android.layoutlib.bridge.impl.ResourceHelper.getInternalComplexColor(ResourceHelper.java:146)\n    at com.android.layoutlib.bridge.impl.ResourceHelper.getColorStateList(ResourceHelper.java:231)\n    at android.content.res.BridgeTypedArray.getColorStateList(BridgeTypedArray.java:308)\n    at android.widget.TextView.&lt;init&gt;(TextView.java:776)\n    at android.widget.TextView.&lt;init&gt;(TextView.java:705)\n    at android.widget.TextView.&lt;init&gt;(TextView.java:701)\n    at com.android.layoutlib.bridge.MockView.&lt;init&gt;(MockView.java:50)\n    at com.android.layoutlib.bridge.MockView.&lt;init&gt;(MockView.java:45)\n    at com.android.layoutlib.bridge.MockView.&lt;init&gt;(MockView.java:41)\n    at android.view.BridgeInflater.createViewFromTag(BridgeInflater.java:163)\n    at android.view.LayoutInflater.createViewFromTag(LayoutInflater.java:727)\n    at android.view.LayoutInflater.rInflate_Original(LayoutInflater.java:858)\n    at android.view.LayoutInflater_Delegate.rInflate(LayoutInflater_Delegate.java:70)\n    at android.view.LayoutInflater.rInflate(LayoutInflater.java:834)\n    at android.view.LayoutInflater.inflate(LayoutInflater.java:492)\n    at com.android.layoutlib.bridge.bars.CustomBar.&lt;init&gt;(CustomBar.java:95)\n    at com.android.layoutlib.bridge.bars.StatusBar.&lt;init&gt;(StatusBar.java:67)\n    at com.android.layoutlib.bridge.impl.Layout.createStatusBar(Layout.java:222)\n    at com.android.layoutlib.bridge.impl.Layout.&lt;init&gt;(Layout.java:144)\n    at com.android.layoutlib.bridge.impl.RenderSessionImpl.inflate(RenderSessionImpl.java:213)\n    at com.android.layoutlib.bridge.Bridge.createSession(Bridge.java:426)\n    at com.android.ide.common.rendering.LayoutLibrary.createSession(LayoutLibrary.java:350)\n    at com.android.tools.idea.rendering.RenderTask$2.compute(RenderTask.java:510)\n    at com.android.tools.idea.rendering.RenderTask$2.compute(RenderTask.java:498)\n    at com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:888)\n    at com.android.tools.idea.rendering.RenderTask.createRenderSession(RenderTask.java:498)\n    at com.android.tools.idea.rendering.RenderTask.access$600(RenderTask.java:72)\n    at com.android.tools.idea.rendering.RenderTask$3.call(RenderTask.java:610)\n    at com.android.tools.idea.rendering.RenderTask$3.call(RenderTask.java:607)\n    at com.android.tools.idea.rendering.RenderService.runRenderAction(RenderService.java:362)\n    at com.android.tools.idea.rendering.RenderTask.render(RenderTask.java:607)\n    at com.android.tools.idea.rendering.RenderTask.render(RenderTask.java:629)\n    at com.intellij.android.designer.designSurface.AndroidDesignerEditorPanel$6.run(AndroidDesignerEditorPanel.java:480)\n    at com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:320)\n    at com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:310)\n    at com.intellij.util.ui.update.MergingUpdateQueue$2.run(MergingUpdateQueue.java:254)\n    at com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:269)\n    at com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:227)\n    at com.intellij.util.ui.update.MergingUpdateQueue.run(MergingUpdateQueue.java:217)\n    at com.intellij.util.concurrency.QueueProcessor.runSafely(QueueProcessor.java:238)\n    at com.intellij.util.Alarm$Request$1.run(Alarm.java:351)\n    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n    at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n</code></pre>\n\n<p>What could be the problem?</p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "tags": ["android", "android-studio"], "answer_count": 7, "link": "http://stackoverflow.com/questions/35914565/rendering-problems-exception-raised-during-rendering-com-android-ide-common-ren", "answers": [{"body": "<p>I had the same problem after some updates minutes ago, what I did to solve the rendering issue was to change the android version used to render layouts from Android Studio back to \"API 23: Android 6.0\".</p>\n\n<p><a href=\"http://i.stack.imgur.com/ZkUhb.png\"><img src=\"http://i.stack.imgur.com/ZkUhb.png\" alt=\"Android studio\"></a></p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 35914788, "question_id": 35914565, "tags": ["android", "android-studio"]}, {"body": "<blockquote>\n  <p>java.lang.NoSuchMethodError:\n  com.android.ide.common.rendering.api.LayoutlibCallback.getXmlFileParser(Ljava/lang/String;)Lorg/xmlpull/v1/XmlPullParser;</p>\n</blockquote>\n\n<p>Thrown if an application tries to call a specified method of a class (either static or instance), and that class no longer has a definition of that method.Normally, this error is caught by the compiler; this error can only occur at run time if the definition of a class has incompatibly changed.</p>\n\n<p>Your app must target <strong>Android 6.0 (API level 23)</strong> to enable this behavior; you do not need to add any additional code.</p>\n\n<p>Just select  <strong>\"API 23: Android 6.0\"</strong> in Preview Section .\n<a href=\"http://i.stack.imgur.com/WuvLm.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/WuvLm.png\" alt=\"enter image description here\"></a></p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 36028378, "question_id": 35914565, "tags": ["android", "android-studio"]}, {"body": "<p>I had a similar problem with Android Studio 1.5.1 in Windows 10. The preview was not working no matter what version of Android was selected. The stacktrace starts with:</p>\n\n<blockquote>\n  <p>\"java.lang.NoSuchMethodError:\n  com.android.ide.common.rendering.api.LayoutlibCallback.getXmlFileParser(Ljava/lang/String;)Lorg/xmlpull/v1/XmlPullParser;\"</p>\n</blockquote>\n\n<p>When I remove the attribute <code>tools:showIn=\"@layout/activity_main\"</code>\nfrom the <code>RelativeLayout</code> the problem resolves for all Android versions except \"N\".\nI have Android 2.2, 2.3.3, 5.0.1 and 6.0 enabled</p>\n\n<p>I can't explain why the action resolves the problem though or why it does work for version N.</p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 36407015, "question_id": 35914565, "tags": ["android", "android-studio"]}, {"body": "<p>change the Android version.<br>\nClick the android robot icon at right on Android SDK to change version<a href=\"http://i.stack.imgur.com/NdU6C.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/NdU6C.png\" alt=\"enter image description here\"></a></p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 37510595, "question_id": 35914565, "tags": ["android", "android-studio"]}, {"body": "<p>This is no timely answer but for anyone looking for anyone else experiencing the same problem an update of the android studio does the trick. While all the suggestions above work you want get update(version 2.1.2).</p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 38331157, "question_id": 35914565, "tags": ["android", "android-studio"]}, {"body": "<p>There is bug in rendering XML layout in Android N,Change the API LEVEL Below 24(for example set it for Android - M or below),It will work.</p>\n\n<p><a href=\"http://i.stack.imgur.com/anYl5.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/anYl5.jpg\" alt=\"enter image description here\"></a></p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 38651780, "question_id": 35914565, "tags": ["android", "android-studio"]}, {"body": "<p>I had the same issue when I installed android studio 2.1 on ubuntu 14.04.Even the android robot icon which displays the android version used for rendering layout was only showing API Level 24 version and no other versions.</p>\n\n<p>I had to enable API 23 to be downloaded by navigating to the below path </p>\n\n<p><strong>Tools -> SDK Manager -> Android SDK -> SDK Platforms -> Name.</strong></p>\n\n<p>Check the Android 6.0(Marshwallow) option.This will be downloaded now.\nNow you will be able to see API Level 23 and by selecting which we can get around this issue.</p>\n", "title": "Rendering Problems Exception raised during rendering: com.android.ide.common.rendering.api.LayoutlibCallback", "answer_id": 39129894, "question_id": 35914565, "tags": ["android", "android-studio"]}], "question_id": 35914565}, {"body": "<p>A <code>script</code> element that got styled as <code>display:block</code> appears visible. Why is it possible and is there any real use case where it is desired?</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>td &gt; * {\r\n  display: block;\r\n}</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;table&gt;\r\n  &lt;tr&gt;\r\n    &lt;td&gt;\r\n      &lt;script type=\"text/javascript\"&gt;\r\n        var test = 1;\r\n      &lt;/script&gt;von 1\r\n    &lt;/td&gt;\r\n  &lt;/tr&gt;\r\n&lt;/table&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n", "title": "When should &lt;script&gt; tags be visible and why can they?", "tags": ["html", "css", "script-tag"], "answer_count": 5, "link": "http://stackoverflow.com/questions/38143580/when-should-script-tags-be-visible-and-why-can-they", "answers": [{"body": "<blockquote>\n  <p>Why can <code>&lt;script&gt;</code> Tags be visible?</p>\n</blockquote>\n\n<p>Because they are HTML elements like any other and there is no reason to write special case rules in the HTML specification (which would add complexity) to prevent CSS from applying to them.</p>\n\n<p>Any element can be styled. Take, for example:</p>\n\n<pre><code>head { display: block; }\ntitle { display: block; }\nmeta { display: block; }\nmeta[charset]:after { display: block; content: attr(charset); }\nmeta[content]:after { display: block; content: attr(content); }\n</code></pre>\n\n<blockquote>\n  <p>Is there any Usecase where it is wanted?</p>\n</blockquote>\n\n<p>Certainly no common ones, but general rules aren't designed to make sense for <em>everything</em> that you <em>can</em> apply them to. They are designed for the common cases.</p>\n", "title": "When should &lt;script&gt; tags be visible and why can they?", "answer_id": 38143675, "question_id": 38143580, "tags": ["html", "css", "script-tag"]}, {"body": "<p>Possible use case: for debugging purposes.</p>\n\n<p>You could apply a class at the document level, eg. <code>&lt;body class=\"debugscript\"&gt;</code>, then use some CSS:</p>\n\n<pre><code>body.debugscript script {\n    display: block;\n    background: #fcc;\n    border: 1px solid red;\n    padding: 2px;\n}\nbody.debugscript script:before {\n    content: 'Script:';\n    display: block;\n    font-weight: bold;\n}\nbody.debugscript script[src]:before {\n    content: 'Script: ' attr(src);\n}\n</code></pre>\n", "title": "When should &lt;script&gt; tags be visible and why can they?", "answer_id": 38143756, "question_id": 38143580, "tags": ["html", "css", "script-tag"]}, {"body": "<p>Another (not common) use case:</p>\n\n<p>I sometimes use <code>&lt;script&gt;</code> tags for brief HTML code examples in style guides. That way I don't have to escape HTML tags and special characters. And text editor tag autocomplete and syntax highlighting still work. But there's no easy way to add syntax highlighting in the browser.</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"false\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>script[type=\"text/example\"] {\r\n    background-color: #33373c;\r\n    border: 1px solid #ccc;\r\n    color: #aed9ef;\r\n    display: block;\r\n    font-family: monospace;\r\n    overflow: auto;\r\n    padding: 2px 10px 16px;\r\n    white-space: pre-wrap;\r\n    word-break: break-all;\r\n    word-wrap: break-word;\r\n}</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;p&gt;Here comes a code example:&lt;/p&gt;\r\n&lt;script type=\"text/example\"&gt;\r\n  &lt;div class=\"cool-component\"&gt;\r\n     Some code example\r\n  &lt;/div&gt;\r\n&lt;/script&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n", "title": "When should &lt;script&gt; tags be visible and why can they?", "answer_id": 38145647, "question_id": 38143580, "tags": ["html", "css", "script-tag"]}, {"body": "<p>The HTML5 specification <a href=\"https://www.w3.org/TR/2014/REC-html5-20141028/rendering.html#the-css-user-agent-style-sheet-and-presentational-hints\">defines a style sheet</a> that user agents (like browsers) are expected to use. <a href=\"https://www.w3.org/TR/2014/REC-html5-20141028/rendering.html#hidden-elements\">Section 10.3.1</a> lists the styles for \"Hidden elements\":</p>\n\n\n\n<blockquote>\n<pre class=\"lang-css prettyprint-override\"><code>@namespace url(http://www.w3.org/1999/xhtml);\n\n[hidden], area, base, basefont, datalist, head, link,\nmeta, noembed, noframes, param, rp, script, source, style, template, track, title {\n  display: none;\n}\n\nembed[hidden] { display: inline; height: 0; width: 0; }\n</code></pre>\n</blockquote>\n\n<p>As you can see, it applies <code>display: none;</code> to <code>script</code>.</p>\n\n<p><strong>This is the only \"barrier\" between your users and hidden <code>script</code> elements.</strong> It\u2019s perfectly fine and intended to be able to overwrite styles from user-agent style sheets within author style sheets (and of course also within user style sheets).</p>\n\n<p>Why someone might want to use it?  One use case is <a href=\"http://stackoverflow.com/a/16115085/1591669\">displaying content without having to escape characters like <code>&lt;</code>/<code>&gt;</code></a>, similar to the old <code>xmp</code> element. The <code>script</code> element can be used not only for scripts, but <a href=\"http://stackoverflow.com/a/14428316/1591669\">also for data blocks</a> (i.e., for anything with a MIME type).</p>\n", "title": "When should &lt;script&gt; tags be visible and why can they?", "answer_id": 38147398, "question_id": 38143580, "tags": ["html", "css", "script-tag"]}, {"body": "<p>Script tags are hidden by default by using <code>display:none;</code>. Unor<sup>1</sup> explains the underlying language specification. However, they are still part of the DOM and can be styled accordingly.</p>\n\n<p>That said, it is important to keep in mind exactly what a script tag is doing. While it used to be accompanied by types and languages, that is no longer required. It is now assumed that JavaScript is in there, and as a result browsers will interpret and execute the script as it is encountered (or loaded) from these tags. </p>\n\n<p><strong>Once the script has been executed, the content of the tag is only text (often hidden) on the page.</strong> This text can be revealed, but <em>it can also be removed</em> because it is just text. </p>\n\n<p>At the bottom of your page, right before the closing <code>&lt;/html&gt;</code> tag, you could very easily remove these tags along with their text and there would be no changes to the page.</p>\n\n<p>For example:</p>\n\n<pre><code>(function(){\n    var scripts = document.querySelectorAll(\"script\");\n    for(var i = 0; i &lt; scripts.length; i++){\n        scripts[i].parentNode.removeChild(scripts[i]);\n    }\n})()\n</code></pre>\n\n<p>This will not remove any functionality, as the state of the page has already been altered and is reflected in the current global execution context. For example, if the page had loaded a library such as jQuery, removing the tags will not mean that jQuery is no longer exposed because it has already been added to the page's runtime environment. It is essentially only making the DOM inspection tool not show script elements, but it does highlight that the script elements once executed really are only text.</p>\n\n<p><sub>1. unor, Thu Jul 07 2016, wutzebaer, \"When should  tags be visible and why can they?\", Jul 1 at 10:53, <a href=\"http://stackoverflow.com/a/38147398/1026459\">http://stackoverflow.com/a/38147398/1026459</a></sub></p>\n", "title": "When should &lt;script&gt; tags be visible and why can they?", "answer_id": 38256022, "question_id": 38143580, "tags": ["html", "css", "script-tag"]}], "question_id": 38143580}, {"body": "<p>I used spring boot to develop a shell project used to send email, e.g.</p>\n\n<pre><code>sendmail -from foo@bar.com -password  foobar -subject \"hello world\"  -to aaa@bbb.com\n</code></pre>\n\n<p>If the <code>from</code> and <code>password</code> arguments are missing, I use a default sender and password, e.g. <code>noreply@bar.com</code> and <code>123456</code>.</p>\n\n<p>So if the user passes the <code>from</code> argument they must also pass the <code>password</code> argument and vice versa. That is to say, either both are non-null, or both are null.</p>\n\n<p>How do I check this elegantly?</p>\n\n<p>Now my way is</p>\n\n<pre><code>if ((from != null &amp;&amp; password == null) || (from == null &amp;&amp; password != null)) {\n    throw new RuntimeException(\"from and password either both exist or both not exist\");\n}\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "tags": ["java"], "answer_count": 13, "link": "http://stackoverflow.com/questions/34586109/check-two-arguments-in-java-either-both-not-null-or-both-null-elegantly", "answers": [{"body": "<p>Well, it sounds like you're trying to check whether the \"nullity\" condition of the two is the same or not. You could use:</p>\n\n<pre><code>if ((from == null) != (password == null))\n{\n    ...\n}\n</code></pre>\n\n<p>Or make it more explicit with helper variables:</p>\n\n<pre><code>boolean gotFrom = from != null;\nboolean gotPassword = password != null;\nif (gotFrom != gotPassword)\n{\n    ...\n}\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34586132, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>There is a way using the <code>^</code> (<a href=\"https://en.wikipedia.org/wiki/Exclusive_or\" rel=\"nofollow\">XOR</a>) operator:</p>\n\n<pre><code>if (from == null ^ password == null) {\n    // Use RuntimeException if you need to\n    throw new IllegalArgumentException(\"message\");\n}\n</code></pre>\n\n<p>The <code>if</code> condition will be true if only one variable is null.</p>\n\n<p>But I think usually it's better to use two <code>if</code> conditions with different exception messages. You can't define what went wrong using a single condition.</p>\n\n<pre><code>if ((from == null) &amp;&amp; (password != null)) {\n    throw new IllegalArgumentException(\"If from is null, password must be null\");\n}\nif ((from != null) &amp;&amp; (password == null)) {\n    throw new IllegalArgumentException(\"If from is not null, password must not be null\");\n}\n</code></pre>\n\n<p>It is more readable and is much easier to understand, and it only takes a little extra typing.</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34586232, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>As I see your intentions, there is no need to always check both exclusive nullities but to check if <code>password</code> is null if and only if <code>from</code> is not null. You can ignore the given <code>password</code> argument and use your own default if <code>from</code> is null.</p>\n\n<p>Written in pseudo must be like this:</p>\n\n<pre><code>if (from == null) { // form is null, ignore given password here\n    // use your own defaults\n} else if (password == null) { // form is given but password is not\n    // throw exception\n} else { // both arguments are given\n    // use given arguments\n}\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34587405, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>Put that functionality in a 2 argument method with the signature:</p>\n\n<pre><code>void assertBothNullOrBothNotNull(Object a, Object b) throws RuntimeException\n</code></pre>\n\n<p>This saves space in the actual method you are interested in and makes it more readable. There is nothing wrong with slightly verbose method names and there is nothing wrong with very short methods.</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34588142, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>Personally, I prefer readable to elegant.</p>\n\n<pre><code>if (from != null &amp;&amp; password == null) {\n    throw new RuntimeException(\"-from given without -password\");\n}\nif (from == null &amp;&amp; password != null) {\n    throw new RuntimeException(\"-password given without -from\");\n}\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34590267, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>I think a correct way to handle this is to consider three situations: both 'from' and 'password' are provided, neither are provided, a mix of the two are provided.</p>\n\n<pre><code>if(from != null &amp;&amp; password != null){\n    //use the provided values\n} else if(from == null &amp;&amp; password == null){\n    //both values are null use the default values\n} else{\n   //throw an exception because the input is not correct.\n}\n</code></pre>\n\n<p>It sounds like the original question wants to break the flow if it is incorrect input, but then they will have to repeat some of the logic later. Perhaps a good throw statement might be:</p>\n\n<pre><code>throw new IllegalArgumentException(\"form of \" + form + \n    \" cannot be used with a \"\n    + (password==null?\"null\":\"not null\") +  \n    \" password. Either provide a value for both, or no value for both\"\n);\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34594734, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>I would like to suggest another alternative which is how I would actually write this piece of code:</p>\n\n<pre><code>if( from != null )\n{\n    if( password == null )\n        error( \"password required for \" + from );\n}\nelse\n{\n    if( password != null )\n        warn( \"the given password will not be used\" );\n}\n</code></pre>\n\n<p>To me this seems to be the most natural way to express this condition which makes it easy to understand for somebody who might have to read it in the future. It also allows you to give more helpful diagnostic messages and treat the unnecessary password as less serious and it makes it easy to modify which is rather likely for such a condition. I.e. you may find out that giving a password as a command line argument is not the best idea and may want allow reading the password from standard input optionally if the argument is missing. Or you may want to silently ignore the superfluous password argument. Changes like these would not require you to rewrite the whole thing.</p>\n\n<p>Besides that it executes only the minimum number of comparisons, so it's not more expensive than the more <em>\"elegant\"</em> alternatives. Although performance is very unlikely a problem here because starting a new process is already much more expensive than a extra null check.</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34622842, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>A Java 8 solution would be to use <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/Objects.html#isNull-java.lang.Object-\"><code>Objects.isNull(Object)</code></a>, assuming a static import:</p>\n\n<pre><code>if (isNull(from) != isNull(password)) {\n    throw ...;\n}\n</code></pre>\n\n<p>For Java &lt; 8 (or if you don't like using <code>Objects.isNull()</code>), you can easily write your own <code>isNull()</code> method.</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34623207, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>Here is a general solution for any number of null checks</p>\n\n<pre><code>public static int nulls(Object... objs)\n{\n    int n = 0;\n    for(Object obj : objs) if(obj == null) n++;\n    return n;\n}\n\npublic static void main (String[] args) throws java.lang.Exception\n{\n    String a = null;\n    String b = \"\";\n    String c = \"Test\";\n\n    System.out.println (\" \"+nulls(a,b,c));\n}\n</code></pre>\n\n<p>Uses</p>\n\n<pre><code>// equivalent to (a==null &amp; !(b==null|c==null) | .. | c==null &amp; !(a==null|b==null))\nif (nulls(a,b,c) == 1) { .. }\n\n// equivalent to (a==null | b==null | c==null)\nif (nulls(a,b,c) &gt;= 1) { .. }\n\n// equivalent to (a!=null | b!=null | c!=null)\nif (nulls(a,b,c) &lt; 3) { .. }\n\n// equivalent to (a==null &amp; b==null &amp; c==null)\nif (nulls(a,b,c) == 3) { .. }\n\n// equivalent to (a!=null &amp; b!=null &amp; c!=null)\nif (nulls(a,b,c) == 0) { .. }\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34630499, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>Here's a relatively straight-forward way that does not involve any Xor og lengthy ifs. It does however require you to be slightly more verbose, but on the upside, you can use the custom Exceptions I suggested to get a more meaningful error message.</p>\n\n<pre><code>private void validatePasswordExists(Parameters params) {\n   if (!params.hasKey(\"password\")){\n      throw new PasswordMissingException(\"Password missing\");\n   }\n}\n\nprivate void validateFromExists(Parameters params) {\n   if (!params.hasKey(\"from\")){\n      throw new FromEmailMissingException(\"From-email missing\");\n   }\n}\n\nprivate void validateParams(Parameters params) {\n\n  if (params.hasKey(\"from\") || params.hasKey(\"password\")){\n     validateFromExists(params);\n     validatePasswordExists(params);\n  }\n}\n</code></pre>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34633880, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>Since you want to do something special (use defaults) when both sender and password are absent, handle that first.<br>\nAfter that, you should have both a sender and a password to send an e-mail; throw an exception if either is missing.</p>\n\n<pre class=\"lang-java prettyprint-override\"><code>// use defaults if neither is provided\nif ((from == null) &amp;&amp; (password == null)) {\n    from = DEFAULT_SENDER;\n    password = DEFAULT_PASSWORD;\n}\n\n// we should have a sender and a password now\nif (from == null) {\n    throw new MissingSenderException();\n}\nif (password == null) {\n    throw new MissingPasswordException();\n}\n</code></pre>\n\n<p>An added benefit is that, should either of your defaults be null, that will be detected as well.</p>\n\n<hr>\n\n<p>Having said that, <em>in general</em> I think that use of XOR should be permissible when that is the operator you need. It <em>is</em> a part of the language, not just some trick that works because of an arcane compiler-bug.<br>\nI once had a cow-orker who found the ternary operator too confusing to use...</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34673278, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>Nobody seems to have mentioned the <a href=\"https://en.wikipedia.org/wiki/%3F:#Java\">ternary operator</a>:</p>\n\n<pre><code>if (a==null? b!=null:b==null)\n</code></pre>\n\n<p>Works nicely for checking this particular condition, but doesn't generalize well past two variables.</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34682336, "question_id": 34586109, "tags": ["java"]}, {"body": "<p>I'm surprised nobody mentioned the simple solution of making <code>from</code> and <code>password</code> fields of a class and passing a reference to an instance of that class:</p>\n\n<pre><code>class Account {\n    final String name, password;\n    Account(String name, String password) {\n        Objects.requireNonNull(name, () -&gt; \"name\");\n        Objects.requireNonNull(password, () -&gt; \"password\");\n        this.name = name;\n        this.password = password;\n    }\n}\n\n// the code that requires an account\nAccount from;\n// do stuff\n</code></pre>\n\n<p>Here <code>from</code> could be null or non-null and if it's non-null, both its fields have non-null values.</p>\n\n<p>One advantage of this approach is that the error of making one field but not the other field null gets triggered where the account is initially obtained, not when the code using the account runs. By the time the code using the account is executed, it's impossible for the data to be invalid.</p>\n\n<p>Another advantage to this approach is more readable as it provides more semantic information. Also, it's likely that you require the name and password together in other places so the cost of defining an additional class amortizes over multiple usages.</p>\n", "title": "Check two arguments in Java, either both not null or both null elegantly", "answer_id": 34959111, "question_id": 34586109, "tags": ["java"]}], "question_id": 34586109}, {"body": "<p>I noticed today that Chrome 49 no longer outputs <code>NaN</code> when you type <code>{}+{}</code> into the console. Instead it outputs the string <code>[object Object][object Object]</code>. </p>\n\n<p>Why is this? Did the language change?</p>\n", "title": "Why is {} + {} no longer NaN in Chrome console?", "tags": ["javascript", "google-chrome"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36438034/why-is-no-longer-nan-in-chrome-console", "answers": [{"body": "<p>If you hit the up arrow after checking this, you'll notice that instead of <code>{} + {}</code> it displays <code>({} + {})</code>, which results in <code>\"[object Object][object Object]\"</code>.</p>\n\n<p>In comparison, in Firefox, <code>{} + {}</code> still displays <code>NaN</code>, but if you do <code>({} + {})</code> it also displays <code>\"[object Object][object Object]\"</code>.</p>\n\n<p>So, it looks like Chrome is adding the surrounding parenthesis automatically when it sees this operation.</p>\n", "title": "Why is {} + {} no longer NaN in Chrome console?", "answer_id": 36438135, "question_id": 36438034, "tags": ["javascript", "google-chrome"]}, {"body": "<p>Chrome devtools now automatically wrap everything that begins with <code>{</code> and ends with <code>}</code> in an implicit pair of parentheses (<a href=\"https://chromium.googlesource.com/chromium/src.git/+/4fd348fdb9c0b3842829acdfb2b82c86dacd8e0a%5E!/#F2\">see code</a>), to force its evaluation as an expression. That way, <code>{}</code> creates an empty object now. You can see this if you go back through the history (<kbd>\u2191</kbd>), the previous line will be contained in <code>(\u2026)</code>.</p>\n\n<p>Why? <s>I don't know, but</s> I could guess it reduces confusion for newbies that don't know of the block-vs-object-literal thing, and it's also more helpful if you just want to evaluate an expression.</p>\n\n<p>And in fact that's the reasoning, as discussed in <a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=499864\">bug 499864</a>. Pure convenience. And because <a href=\"http://stackoverflow.com/q/17268468/1048572\">node REPL had it</a> as well (<a href=\"https://github.com/nodejs/node/blob/6d95f4ff923018bb9720888e4ea986cddf0ff5b5/lib/repl.js#L294-L299\">see code</a>).</p>\n", "title": "Why is {} + {} no longer NaN in Chrome console?", "answer_id": 36438289, "question_id": 36438034, "tags": ["javascript", "google-chrome"]}], "question_id": 36438034}, {"body": "<p>From something like this:</p>\n\n<pre><code>print(get_indentation_level())\n\n    print(get_indentation_level())\n\n        print(get_indentation_level())\n</code></pre>\n\n<p>I would like to get something like this:</p>\n\n<pre><code>1\n2\n3\n</code></pre>\n\n<p>Can the code read itself in this way?</p>\n\n<p>All I want is the output from the more nested parts of the code to be more nested. In the same way that this makes code easier to read, it would make the output easier to read. </p>\n\n<p>Of course I could implement this manually, using e.g. <code>.format()</code>, but what I had in mind was a custom print function which would <code>print(i*' ' + string)</code> where <code>i</code> is the indentation level. This would be a quick way to make readable output on my terminal. </p>\n\n<p>Is there a better way to do this which avoids painstaking manual formatting?</p>\n", "title": "Can a line of Python code know its indentation nesting level?", "tags": ["python", "reflection", "metaprogramming", "indentation", "tokenize"], "answer_count": 5, "link": "http://stackoverflow.com/questions/39172306/can-a-line-of-python-code-know-its-indentation-nesting-level", "answers": [{"body": "<pre><code>&gt;&gt;&gt; import inspect\n&gt;&gt;&gt; help(inspect.indentsize)\nHelp on function indentsize in module inspect:\n\nindentsize(line)\n    Return the indent size, in spaces, at the start of a line of text.\n</code></pre>\n", "title": "Can a line of Python code know its indentation nesting level?", "answer_id": 39172459, "question_id": 39172306, "tags": ["python", "reflection", "metaprogramming", "indentation", "tokenize"]}, {"body": "<p>You can use <code>sys.current_frame.f_lineno</code> in order to get the line number. Then in order to find the number of indentation level you need to find the previous line with zero indentation then be subtracting the current line number from that line's number you'll get the number of indentation:</p>\n\n<pre><code>import sys\ncurrent_frame = sys._getframe(0)\n\ndef get_ind_num():\n    with open(__file__) as f:\n        lines = f.readlines()\n    current_line_no = current_frame.f_lineno\n    to_current = lines[:current_line_no]\n    previous_zoro_ind = len(to_current) - next(i for i, line in enumerate(to_current[::-1]) if not line[0].isspace())\n    return current_line_no - previous_zoro_ind\n</code></pre>\n\n<p>Demo:</p>\n\n<pre><code>if True:\n    print get_ind_num()\n    if True:\n        print(get_ind_num())\n        if True:\n            print(get_ind_num())\n            if True: print(get_ind_num())\n# Output\n1\n3\n5\n6\n</code></pre>\n\n<p>If you want the number of the indentation level based on the previouse lines with <code>:</code> you can just do it with a little change:</p>\n\n<pre><code>def get_ind_num():\n    with open(__file__) as f:\n        lines = f.readlines()\n\n    current_line_no = current_frame.f_lineno\n    to_current = lines[:current_line_no]\n    previous_zoro_ind = len(to_current) - next(i for i, line in enumerate(to_current[::-1]) if not line[0].isspace())\n    return sum(1 for line in lines[previous_zoro_ind-1:current_line_no] if line.strip().endswith(':'))\n</code></pre>\n\n<p>Demo:</p>\n\n<pre><code>if True:\n    print get_ind_num()\n    if True:\n        print(get_ind_num())\n        if True:\n            print(get_ind_num())\n            if True: print(get_ind_num())\n# Output\n1\n2\n3\n3\n</code></pre>\n\n<p>And as an alternative answer here is a function for getting the number of indentation (whitespace):</p>\n\n<pre><code>import sys\nfrom itertools import takewhile\ncurrent_frame = sys._getframe(0)\n\ndef get_ind_num():\n    with open(__file__) as f:\n        lines = f.readlines()\n    return sum(1 for _ in takewhile(str.isspace, lines[current_frame.f_lineno - 1]))\n</code></pre>\n", "title": "Can a line of Python code know its indentation nesting level?", "answer_id": 39172520, "question_id": 39172306, "tags": ["python", "reflection", "metaprogramming", "indentation", "tokenize"]}, {"body": "<p>Yeah, that's definitely possible, here's a working example:</p>\n\n<pre><code>import inspect\n\ndef get_indentation_level():\n    callerframerecord = inspect.stack()[1]\n    frame = callerframerecord[0]\n    info = inspect.getframeinfo(frame)\n    cc = info.code_context[0]\n    return len(cc) - len(cc.lstrip())\n\nif 1:\n    print get_indentation_level()\n    if 1:\n        print get_indentation_level()\n        if 1:\n            print get_indentation_level()\n</code></pre>\n", "title": "Can a line of Python code know its indentation nesting level?", "answer_id": 39172552, "question_id": 39172306, "tags": ["python", "reflection", "metaprogramming", "indentation", "tokenize"]}, {"body": "<p>If you want indentation in terms of nesting level rather than spaces and tabs, things get tricky. For example, in the following code:</p>\n\n<pre><code>if True:\n    print(\nget_nesting_level())\n</code></pre>\n\n<p>the call to <code>get_nesting_level</code> is actually nested one level deep, despite the fact that there is no leading whitespace on the line of the <code>get_nesting_level</code> call. Meanwhile, in the following code:</p>\n\n<pre><code>print(1,\n      2,\n      get_nesting_level())\n</code></pre>\n\n<p>the call to <code>get_nesting_level</code> is nested zero levels deep, despite the presence of leading whitespace on its line.</p>\n\n<p>In the following code:</p>\n\n<pre><code>if True:\n  if True:\n    print(get_nesting_level())\n\nif True:\n    print(get_nesting_level())\n</code></pre>\n\n<p>the two calls to <code>get_nesting_level</code> are at different nesting levels, despite the fact that the leading whitespace is identical.</p>\n\n<p>In the following code:</p>\n\n<pre><code>if True: print(get_nesting_level())\n</code></pre>\n\n<p>is that nested zero levels, or one? In terms of <code>INDENT</code> and <code>DEDENT</code> tokens in the formal grammar, it's zero levels deep, but you might not feel the same way.</p>\n\n<hr>\n\n<p>If you want to do this, you're going to have to tokenize the whole file up to the point of the call and count <code>INDENT</code> and <code>DEDENT</code> tokens. The <a href=\"https://docs.python.org/2/library/tokenize.html\"><code>tokenize</code></a> module would be very useful for such a function:</p>\n\n<pre><code>import inspect\nimport tokenize\n\ndef get_nesting_level():\n    caller_frame = inspect.currentframe().f_back\n    filename, caller_lineno, _, _, _ = inspect.getframeinfo(caller_frame)\n    with open(filename) as f:\n        indentation_level = 0\n        for token_record in tokenize.generate_tokens(f.readline):\n            token_type, _, (token_lineno, _), _, _ = token_record\n            if token_lineno &gt; caller_lineno:\n                break\n            elif token_type == tokenize.INDENT:\n                indentation_level += 1\n            elif token_type == tokenize.DEDENT:\n                indentation_level -= 1\n        return indentation_level\n</code></pre>\n", "title": "Can a line of Python code know its indentation nesting level?", "answer_id": 39172845, "question_id": 39172306, "tags": ["python", "reflection", "metaprogramming", "indentation", "tokenize"]}, {"body": "<p>To solve the \u201dreal\u201d problem that lead to your question you could implement a contextmanager which keeps track of the indention level and make the <code>with</code> block structure in the code correspond to the indentation levels of the output. This way the code indentation still reflects the output indentation without coupling both too much. It is still possible to refactor the code into different functions and have other indentations based on code structure not messing with the output indentation.</p>\n\n<pre><code>#!/usr/bin/env python\n# coding: utf8\nfrom __future__ import absolute_import, division, print_function\n\n\nclass IndentedPrinter(object):\n\n    def __init__(self, level=0, indent_with='  '):\n        self.level = level\n        self.indent_with = indent_with\n\n    def __enter__(self):\n        self.level += 1\n        return self\n\n    def __exit__(self, *_args):\n        self.level -= 1\n\n    def print(self, arg='', *args, **kwargs):\n        print(self.indent_with * self.level + str(arg), *args, **kwargs)\n\n\ndef main():\n    indented = IndentedPrinter()\n    indented.print(indented.level)\n    with indented:\n        indented.print(indented.level)\n        with indented:\n            indented.print('Hallo', indented.level)\n            with indented:\n                indented.print(indented.level)\n            indented.print('and back one level', indented.level)\n\n\nif __name__ == '__main__':\n    main()\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>0\n  1\n    Hallo 2\n      3\n    and back one level 2\n</code></pre>\n", "title": "Can a line of Python code know its indentation nesting level?", "answer_id": 39191937, "question_id": 39172306, "tags": ["python", "reflection", "metaprogramming", "indentation", "tokenize"]}], "question_id": 39172306}, {"body": "<p>It seems that <code>auto</code> was a fairly significant feature to be added in C++11 that seems to follow a lot of the newer languages. As with a language like Python, I have not seen any explicit variable declaration (I am not sure if it is possible using Python standards).</p>\n\n<p>Is there a drawback to using <code>auto</code> to declare variables instead of explicitly declaring them?</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "tags": ["c++", "c++11", "type-inference", "auto"], "answer_count": 13, "link": "http://stackoverflow.com/questions/34758042/is-there-a-downside-to-declaring-variables-with-auto-in-c", "answers": [{"body": "<p>It makes your code a little harder, or tedious, to read.\nImagine something like that: </p>\n\n<pre><code>auto output = doSomethingWithData(variables);\n</code></pre>\n\n<p>Now, to figure out the type of output, you'd have to track down  signature of <code>doSomethingWithData</code> function.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34758113, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>Keyword <code>auto</code> simply deduce the type from the return value. Therefore, it is not equivalent with a Python object, e.g.</p>\n\n<pre><code># Python\na\na = 10       # OK\na = \"10\"     # OK\na = ClassA() # OK\n\n// C++\nauto a;      // Unable to deduce variable a\nauto a = 10; // OK\na = \"10\";    // Value of const char* can't be assigned to int\na = ClassA{} // Value of ClassA can't be assigned to int\na = 10.0;    // OK, implicit casting warning\n</code></pre>\n\n<p>Since <code>auto</code> is deduced during compilation, it won't have any drawback at runtime whatsoever.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34758156, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>You've only asked about drawbacks, so I'm highlighting some of those.   When used well, <code>auto</code> has several advantages as well.   The drawbacks result from ease of abuse, and from increased potential for code to behave in unintended ways.</p>\n\n<p>The main drawback is that, by using <code>auto</code>, you don't necessarily know the type of object being created.   There are also occasions where the programmer might expect the compiler to deduce one type, but the compiler adamantly deduces another.</p>\n\n<p>Given a declaration like</p>\n\n<pre><code>auto result = CallSomeFunction(x,y,z);\n</code></pre>\n\n<p>you don't necessarily have knowledge of what type <code>result</code> is.   It might be an <code>int</code>.  It might be a pointer.   It might be something else.    All of those support different operations.    You can also dramatically change the code by a minor change like</p>\n\n<pre><code>auto result = CallSomeFunction(a,y,z);\n</code></pre>\n\n<p>because, depending on what overloads exist for <code>CallSomeFunction()</code> the type of result might be completely different - and subsequent code may therefore behave completely differently than intended.    You might suddenly trigger error messages in later code(e.g. subsequently trying to dereference an <code>int</code>, trying to change something which is now <code>const</code>).  The more sinister change is where your change sails past the compiler, but subsequent code behaves in different and unknown - possibly buggy - ways.</p>\n\n<p>Not having explicit knowledge of the type of some variables therefore makes it harder to rigorously justify a claim that the code works as intended.  This means more effort to justify claims of \"fit for purpose\" in high-criticality (e.g. safety-critical or mission-critical) domains.</p>\n\n<p>The other, more common drawback, is the temptation for a programmer to use <code>auto</code> as a blunt instrument to force code to compile, rather than thinking about what the code is doing, and working to get it right.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34758249, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>This isn't a drawback of auto in a principled way exactly, but in practical terms it seems to be an issue for some. Basically, some people either: a) treat auto as a savior for types and shut their brain off when using it, or b) forget that auto always deduces to value types. This causes people to do things like this:</p>\n\n<pre><code>auto x = my_obj.method_that_returns_reference();\n</code></pre>\n\n<p>Oops, we just deep copied some object. It's often either a bug or a performance fail. Then, you can swing the other way too:</p>\n\n<pre><code>const auto&amp; stuff = *func_that_returns_unique_ptr();\n</code></pre>\n\n<p>Now you get a dangling reference. These problems aren't caused by auto at all, so I don't consider them legitimate arguments against it. But it does seem like auto makes these issue more common (from my personal experience), for the reasons I listed at the beginning. </p>\n\n<p>I think given time people will adjust, and understand the division of labor: auto deduces the underlying type, but you still want to think about reference-ness and const-ness. But it's taking a bit of time.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34758729, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>Other answers are mentioning drawbacks like \"you don't really know what the type of a variable is.\" I'd say that this is largely related to sloppy naming convention in code. If your interfaces are clearly-named, you shouldn't need to <em>care</em> what the exact type is. Sure, <code>auto result = callSomeFunction(a, b);</code> doesn't tell you much. But <code>auto valid = isValid(xmlFile, schema);</code> tells you enough to use <code>valid</code> without having to care what its exact type is. After all, with just <code>if (callSomeFunction(a, b))</code>, you wouldn't know the type either. The same with any other subexpression temporary objects. So I don't consider this a real drawback of <code>auto</code>.</p>\n\n<p><strong>I'd say its primary drawback is that sometimes, the exact return type is <em>not</em> what you want to work with.</strong> In effect, sometimes the actual return type differs from the \"logical\" return type as an implementation/optimisation detail. Expression templates are a prime example. Let's say we have this:</p>\n\n<pre><code>SomeType operator* (const Matrix &amp;lhs, const Vector &amp;rhs);\n</code></pre>\n\n<p>Logically, we would expect <code>SomeType</code> to be <code>Vector</code>, and we definitely want to treat it as such in our code. However, it is possible that for optimisation purposes, the algebra library we're using implements expression templates, and the actual return type is this:</p>\n\n<pre><code>MultExpression&lt;Matrix, Vector&gt; operator* (const Matrix &amp;lhs, const Vector &amp;rhs);\n</code></pre>\n\n<p>Now, the problem is that <code>MultExpression&lt;Matrix, Vector&gt;</code> will in all likelihood store a <code>const Matrix&amp;</code> and <code>const Vector&amp;</code> internally; it expects that it will convert to a <code>Vector</code> before the end of its full-expression. If we have this code, all is well:</p>\n\n<pre><code>extern Matrix a, b, c;\nextern Vector v;\n\nvoid compute()\n{\n  Vector res = a * (b * (c * v));\n  // do something with res\n}\n</code></pre>\n\n<p>However, if we had used <code>auto</code> here, we could get in trouble:</p>\n\n<pre><code>void compute()\n{\n  auto res = a * (b * (c * v));\n  // Oops! Now `res` is referring to temporaries (such as (c * v)) which no longer exist\n}\n</code></pre>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34762002, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>As I described in <a href=\"http://stackoverflow.com/a/34560436/1870760\">this answer</a> <code>auto</code> can sometimes result in funky situations you didn't intend.\nYou have to explictly say <code>auto&amp;</code> to have a reference type while doing just <code>auto</code> can create a pointer type. This can result in confusion by omitting the specifier all together, resulting in a copy of the reference instead of an actual reference.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34763042, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>What no one mentioned here so far, but for itself is worth an answer if you asked me.</p>\n\n<p>Since (even if everyone should be aware that <code>C != C++</code>) code written in C can easily be designed to provide a base for C++ code and therefore be designed without too much effort to be C++ compatible, this could be a requirement for design.</p>\n\n<p>I know about some rules where some well defined constructs from <code>C</code> are invalid for <code>C++</code> and vice versa. But this would simply result in broken executables and the known UB-clause applies which most times is noticed by strange loopings resulting in crashes or whatever (or even may stay undetected, but that doesn't matter here).</p>\n\n<p>But <code>auto</code> is the first time<sup>1</sup> this changes!</p>\n\n<p>Imagine you used <code>auto</code> as storage-class specifier before and transfer the code. It would not even necessarily (depending on the way it was used) \"break\"; it actually could silently change the behaviour of the program.</p>\n\n<p>That's something one should keep in mind.</p>\n\n<hr>\n\n<p><sup>1</sup><sub>At least the first time I'm aware of.</sub></p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34763081, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>One of the drawbacks is that sometimes you can't declare <code>const_iterator</code> with <code>auto</code>. You will get ordinary (non const) iterator in this example of code taken from <a href=\"http://stackoverflow.com/q/15233188/72178\">this question</a>:</p>\n\n<pre><code>map&lt;string,int&gt; usa;\n//...init usa\nauto city_it = usa.find(\"New York\");\n</code></pre>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34772170, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>One reason that I can think of is that you lose the opportunity to coerce the class that is returned. If your function or method returned a long 64 bit, and you only wanted a 32 unsigned int, then you lose the opportunity to control that.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34776063, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p><code>auto</code> does not have drawbacks <em>per se</em>, and I advocate to (hand-wavily) use it everywhere in new code. It allows your code to consistently type-check, and consistently avoid silent slicing. (If <code>B</code> derives from <code>A</code> and a function returning <code>A</code> suddenly returns <code>B</code>, then <code>auto</code> behaves as expected to store its return value)</p>\n\n<p>Although, pre-C++11 legacy code may rely on implicit conversions induced by the use of explicitly-typed variables. <strong>Changing an explicitly-typed variable to <code>auto</code> might change code behaviour</strong>, so you'd better be cautious.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34785236, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>Like <a href=\"http://www.randygaul.net/2015/12/06/i-hate-the-c-keyword-auto/\">this</a> developer, I hate <code>auto</code>. Or rather, I hate how people misuse <code>auto</code>.</p>\n\n<p>I'm of the (strong) opinion that <code>auto</code> is for helping you write generic code, <strong>not for reducing typing</strong>.<br>\nC++ is a language whose goal is to let you write robust code, <strong>not</strong> to minimize development time.<br>\nThis is fairly obvious from many features of C++, but unfortunately a few of the newer ones like <code>auto</code> that reduce typing mislead people into thinking they should start being lazy with typing.  </p>\n\n<p>In pre-<code>auto</code> days, people used <code>typedef</code>s, which was great because <code>typedef</code>  allowed the designer of the library to help you figure out what the return type should be, so that their library works as expected. When you use <code>auto</code>, you <em>take away that control</em> from the class's designer and instead ask the <em>compiler</em> to figure out what the type should be, which removes one of the most powerful C++ tools from the toolbox and risks <em>breaking</em> their code.</p>\n\n<p>Generally, if you use <code>auto</code>, it should be because <em>your code works for <strong>any</strong> reasonable type</em>, <strong><em>not</em></strong> because you're just too lazy to write down the type that it should work with. \nIf you use <code>auto</code> as a tool to help laziness, then what happens is that you eventually start introducing <strong>subtle bugs</strong> in your program, usually caused by implicit conversions that did not happen because you used <code>auto</code>.  </p>\n\n<p>Unfortunately, <em>these bugs are difficult to illustrate</em> in a short example here because their brevity makes them less convincing than the actual examples that come up in a user project -- however, they occur easily in template-heavy code that expect certain <strong>implicit conversions</strong> to take place.  </p>\n\n<p>If you want an example, there is one <a href=\"http://stackoverflow.com/a/28637941/541686\"><strong>here</strong></a>. A little note, though: before being tempted to jump and criticize the code: keep in mind that many well-known and mature libraries have been developed around such implicit conversions, and <em>they are there because they <strong>solve problems</strong> that can be difficult if not impossible</em> to solve otherwise. Try to figure out a <em>better solution</em> before criticizing them.</p>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34793117, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>Another irritating example:</p>\n\n<pre><code>for (auto i = 0; i &lt; s.size(); ++i)\n</code></pre>\n\n<p>generates a warning (<code>comparison between signed and unsigned integer expressions [-Wsign-compare]</code>), because <code>i</code> is a signed int. To avoid this you need to write e.g.</p>\n\n<pre><code>for (auto i = 0U; i &lt; s.size(); ++i)\n</code></pre>\n\n<p>or perhaps better:</p>\n\n<pre><code>for (auto i = 0ULL; i &lt; s.size(); ++i)\n</code></pre>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34888054, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}, {"body": "<p>I think <code>auto</code> is good when used in a localized context, where the reader easily &amp; obviously can deduct its type, or well documented with a comment of its type or a name that infer the actual type. Those who don't understand how it works might take it in the wrong ways, like using it instead of <code>template</code> or similar. Here are some good and bad use cases in my opinion.</p>\n\n<pre><code>void test (const int &amp; a)\n{\n    // b is not const\n    // b is not a reference\n\n    auto b = a;\n\n    // b type is decided by the compiler based on value of a\n    // a is int\n}\n</code></pre>\n\n<p><strong>Good Uses</strong></p>\n\n<p><em>Iterators</em></p>\n\n<pre><code>std::vector&lt;boost::tuple&lt;ClassWithLongName1,std::vector&lt;ClassWithLongName2&gt;,int&gt; v();\n\n..\n\nstd::vector&lt;boost::tuple&lt;ClassWithLongName1,std::vector&lt;ClassWithLongName2&gt;,int&gt;::iterator it = v.begin();\n\n// VS\n\nauto vi = v.begin();\n</code></pre>\n\n<p><em>Function Pointers</em></p>\n\n<pre><code>int test (ClassWithLongName1 a, ClassWithLongName2 b, int c)\n{\n    ..\n}\n\n..\n\nint (*fp)(ClassWithLongName1, ClassWithLongName2, int) = test;\n\n// VS\n\nauto *f = test;\n</code></pre>\n\n<p><strong>Bad Uses</strong></p>\n\n<p><em>Data Flow</em></p>\n\n<pre><code>auto input = \"\";\n\n..\n\nauto output = test(input);\n</code></pre>\n\n<p><em>Function Signature</em></p>\n\n<pre><code>auto test (auto a, auto b, auto c)\n{\n    ..\n}\n</code></pre>\n\n<p><em>Trivial Cases</em></p>\n\n<pre><code>for(auto i = 0; i &lt; 100; i++)\n{\n    ..\n}\n</code></pre>\n", "title": "Is there a downside to declaring variables with auto in C++?", "answer_id": 34897140, "question_id": 34758042, "tags": ["c++", "c++11", "type-inference", "auto"]}], "question_id": 34758042}, {"body": "<p>We have some build systems in production which no one cares about and these machines run ancient versions of GCC like GCC 3 or GCC 2.</p>\n\n<p>And I can't persuade the management to upgrade it to a more recent: they say, \"if ain't broke, don't fix it\". </p>\n\n<p>Since we maintain a very old code base (written in the 80s), this C89 code compiles just fine on these compilers. </p>\n\n<p>But I'm not sure it is good idea to use these old stuff.</p>\n\n<p>My question is:</p>\n\n<p>Can using an old C compiler compromise the security of the compiled program?  </p>\n\n<p>UPDATE:</p>\n\n<p>The same code is built by Visual Studio 2008 for Windows targets, and MSVC doesn't support C99 or C11 yet (I don't know if newer MSVC does), and I can build it on my Linux box using the latest GCC. So if we would just drop in a newer GCC it would probably build just as fine as before.</p>\n", "title": "Is using an outdated C compiler a security risk?", "tags": ["c", "security", "gcc"], "answer_count": 11, "link": "http://stackoverflow.com/questions/37479403/is-using-an-outdated-c-compiler-a-security-risk", "answers": [{"body": "<blockquote>\n  <p>Can using an old C compiler compromise the security of the compiled program? </p>\n</blockquote>\n\n<p>Of course it can, if the old compiler contains known bugs that you know would affect your program. </p>\n\n<p>The question is, does it? To know for sure, you would have to read the whole change log from your version to present date and check every single bug fixed over the years.</p>\n\n<p>If you find no evidence of compiler bugs that would affect your program, updating GCC just for the sake of it seems a bit paranoid. You would have to keep in mind that newer versions might contain new bugs, that are not yet discovered. Lots of changes were made recently with GCC 5 and C11 support.</p>\n\n<p>That being said, code written in the 80s is most likely already filled to the brim with security holes and reliance on poorly-defined behavior, no matter the compiler. We're talking of pre-standard C here.</p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37479659, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>There is a security risk where a malicious developer can sneak a back-door through a compiler bug. Depending on the quantity of known bugs in the compiler in use, the backdoor may look more or less inconspicuous (in any case, the point is that the code is correct, even if convoluted, at the source level. Source code reviews and tests using a non-buggy compiler will not find the backdoor, because the backdoor does not exist in these conditions). For extra deniability points, the malicious developer may also look for previously-unknown compiler bugs on their own. Again, the quality of the camouflage will depend on the choice of compiler bugs found.</p>\n\n<p>This attack is illustrated on the program sudo in <a href=\"https://www.alchemistowl.org/pocorgtfo/pocorgtfo08.pdf#page=7\" rel=\"nofollow\">this article</a>. bcrypt wrote a great follow-up for <a href=\"https://zyan.scripts.mit.edu/blog/backdooring-js/\" rel=\"nofollow\">Javascript minifiers</a>.</p>\n\n<p>Apart from this concern, the evolution of C compilers has been to exploit undefined behavior <a href=\"https://godbolt.org/g/wVEoTM\" rel=\"nofollow\">more</a> and <a href=\"https://lwn.net/Articles/342330/\" rel=\"nofollow\">more</a> and <a href=\"https://lwn.net/Articles/278137/\" rel=\"nofollow\">more</a> aggressively, so old C code that was written in good faith would actually be more secure compiled with a C compiler from the time, or compiled at -O0 (but some new program-breaking UB-exploiting optimizations <a href=\"http://blog.mycre.ws/articles/bind-and-gcc-49/\" rel=\"nofollow\">are introduced in new versions of compilers even at -O0</a>).</p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37479667, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p><em>If it aint broke, don't fix it</em></p>\n\n<p>Your boss sounds right in saying this, however, the more <strong>important</strong> factor, is safeguarding of inputs, outputs, buffer overflows. Lack of those is invariably the weakest link in the chain from that standpoint regardless of the compiler used.</p>\n\n<p>However, if the code base is ancient, and work was put in place to mitigate the weaknesses of the K&amp;R C used, such as lacking of type safety, insecure fgets, etc, weigh up the question \"<em>Would upgrading the compiler to more modern C99/C11 standards break everything?</em>\"</p>\n\n<p>Provided, that there's a clear path to migrate to the newer C standards, which could induce side effects, might be best to attempt a fork of the old codebase, assess it and put in extra type checks, sanity checks, and determine if upgrading to the newer compiler has any effect on input/output datasets. </p>\n\n<p>Then you can show it to your boss, \"<em>Here's the updated code base, refactored, more in line with industry accepted C99/C11 standards...</em>\".</p>\n\n<p>That's the gamble that would have to be weighed up on, <em>very carefully</em>, <em>resistence to change</em> might show there in that environment and may refuse to touch the newer stuff.</p>\n\n<p><strong>EDIT</strong></p>\n\n<p>Just sat back for a few minutes, realized this much, K&amp;R generated code could be running on a 16bit platform, chances are, upgrading to more modern compiler could actually break the code base, am thinking in terms of architecture, 32bit code would be generated, this could have funny side effects on the structures used for input/output datasets, that is another <strong>huge</strong> factor to weigh up carefully. </p>\n\n<p>Also, since OP has mentioned using Visual Studio 2008 to build the codebase, using gcc could induce bringing into the environment either MinGW or Cygwin, that could have an impact change on the environment, unless, the target is for Linux, then it would be worth a shot, may have to include additional switches to the compiler to minimize noise on old K&amp;R code base, the other important thing is to carry out a lot of testing to ensure no functionality is broken, may turn out to be a painful exercise. </p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37479799, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>Your compiled code contains bugs that could be exploited. The bugs come from three sources: Bugs in your source code, bugs in the compiler and libraries, and undefined behaviour in your source code that the compiler turns into a bug. (Undefined behaviour is a bug, but not a bug in the compiled code yet. As an example, i = i++; in C or C++ is a bug, but in your compiled code it may increase i by 1 and be Ok, or set i to some junk and be a bug). </p>\n\n<p>The rate of bugs in your compiled code is presumably low due to testing and to fixing bugs due to customer bug reports. So there may have been a large number of bugs initially, but that has gone down. </p>\n\n<p>If you upgrade to a newer compiler, you may lose bugs that were introduced by compiler bugs. But these bugs would all be bugs that to your knowledge nobody found and nobody exploited. But the new compiler may have bugs on its own, and importantly newer compilers have a stronger tendency to turn undefined behaviour into bugs in the compiled code. </p>\n\n<p>So you will have a whole lot of new bugs in your compiled code; all bugs that hackers could find and exploit. And unless you do a whole lot of testing, and leave your code with customers to find bugs for a long time, it will be less secure. </p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37480185, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>Well there is a higher probability that any bugs in the old compiler are well known and documented as opposed to using a new compiler so actions can be taken to avoid those bugs by coding around them. So in a way that is not enough as argument for upgrading. We have the same discussions where I work, we use GCC 4.6.1 on a code base for embedded software and there is a great reluctance (among management) to upgrade to the latest compiler because of fear for new, undocumented bugs. </p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37481035, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>Actually I would argue the opposite.</p>\n\n<p>There are a number of cases where behaviour is undefined by the C standard but where it is obvious what would happen with a \"dumb compiler\" on a given platform. Cases like allowing a signed integer to overflow or accessing the same memory though variables of two different types.</p>\n\n<p>Recent versions of gcc (and clang) have started treating these cases as optimisation opportunities not caring if they change how the binary behaves in the \"undefined behaviour\" condition.  This is very bad if your codebase was written by people who treated C like a \"portable assembler\". As time went on the optimisers have started looking at larger and larger chunks of code when doing these optimisations increasing the chance the binary will end up doing something other than \"what a binary built by a dumb compiler\" would do.</p>\n\n<p>There are compiler switches to restore \"traditional\" behaviour (-fwrapv and -fno-strict-aliasing for the two I mentioned above) , but first you have to know about them.</p>\n\n<p>While in principle a compiler bug could turn compliant code into a security hole I would consider the risk of this to be negligable in the grand scheme of things.</p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37484698, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>There are risks in both courses of action.</p>\n\n<hr>\n\n<p>Older compilers have the advantage of maturity, and whatever was broken in them has probably (but there's no guarantee) been worked around successfully.</p>\n\n<p>In this case, a new compiler is a potential source of new bugs.</p>\n\n<hr>\n\n<p>On the other hand, newer compilers come with <em>additional tooling</em>:</p>\n\n<ul>\n<li>GCC and Clang both now feature <em>sanitizers</em> which can instrument the runtime to detect undefined behaviors of various sorts (Chandler Carruth, of the Google Compiler team, claimed last year that he expects them to have reached full coverage)</li>\n<li>Clang, at least, features <em>hardening</em>, for example <a href=\"http://clang.llvm.org/docs/ControlFlowIntegrity.html\">Control Flow Integrity</a> is about detecting hi-jacks of control flow, there are also hardening implements to protect against stack smashing attacks (by separating the control-flow part of the stack from the data part); hardening features are generally low overhead (&lt; 1% CPU overhead)</li>\n<li>Clang/LLVM is also working on <a href=\"http://llvm.org/docs/LibFuzzer.html\">libFuzzer</a>, a tool to create instrumented fuzzing unit-tests that explore the input space of the function under test smartly (by tweaking the input to take not-as-yet explored execution paths)</li>\n</ul>\n\n<p>Instrumenting your binary with the sanitizers (Address Sanitizer, Memory Sanitizer or Undefined Behavior Sanitizer) and then fuzzing it (using <a href=\"https://en.wikipedia.org/wiki/American_fuzzy_lop_%28fuzzer%29\">American Fuzzy Lop</a> for example) has uncovered vulnerabilities in a number of high-profile softwares, see for example this <a href=\"https://lwn.net/Articles/657959/\">LWN.net article</a>.</p>\n\n<p>Those new tools, and all future tools, are inaccessible to you unless you upgrade your compiler.</p>\n\n<p>By staying on an underpowered compiler, you are putting your head in the sand and crossing fingers that no vulnerability is found. If your product is a high-value target, I urge you to reconsider.</p>\n\n<hr>\n\n<p><em>Note: even if you do NOT upgrade the production compiler, you might want to use a new compiler to check for vulnerability anyway; do be aware that since those are different compilers, the guarantees are lessened though.</em></p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37487394, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>Older compilers may not have protection against known hacking attacks. Stack smashing protection, for example, was not introduced <a href=\"https://en.wikipedia.org/wiki/Buffer_overflow_protection#GNU_Compiler_Collection_.28GCC.29\">until GCC 4.1</a>. So yeah, code compiled with older compilers may be vulnerable in ways that newer compilers protect against.</p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37497750, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>Another aspect to worry about is the <em>development of new code</em>.</p>\n\n<p>Older compilers may have different behavior for some language features than what is standardized and expected by the programmer. This mismatch can slow development and introduce subtle bugs that can be exploited.</p>\n\n<p>Older compilers offer fewer features (including language features!) and don't optimize as well. Programmers will hack their way around these deficiencies &mdash; e.g. by reimplementing missing features, or writing clever code that is obscure but runs faster &mdash; creating new opportunities for the creation of subtle bugs.</p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37501375, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<h2>Nope</h2>\n\n<p>The reason is simple, old compiler may have old bugs and exploits, but the new compiler will have new bugs and exploits. </p>\n\n<p>Your not \"fixing\" any bugs by upgrading to a new compiler. Your switching old bugs and exploits for new bugs and exploits. </p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37507367, "question_id": 37479403, "tags": ["c", "security", "gcc"]}, {"body": "<p>Your question falls into two parts:</p>\n\n<ul>\n<li>Explicit: \u201cIs there a greater risk in using the older compiler\u201d (more or less as in your title)</li>\n<li>Implicit: \u201cHow can I persuade management to upgrade\u201d</li>\n</ul>\n\n<p>Perhaps you can answer both by finding an exploitable flaw in your existing code base and showing that a newer compiler would have detected it. Of course your management may say \u201cyou found that with the old compiler\u201d, but you can point out that it cost considerable effort. Or you run it through the new compiler to find the vulnerability, then exploit it, if your are able/allowed to compile the code with the new compiler. You may want help from a friendly hacker, but that depends on trusting them and being able/allowed to show them the code (and use the new compiler).</p>\n\n<p>But if your system is not exposed to hackers, you should perhaps be more interested in whether a compiler upgrade would increase your effectiveness: MSVS 2013 Code Analysis quite often finds potential bugs much sooner than MSVS 2010, and it more or less supports C99/C11 \u2013 not sure if it does officially, but declarations can follow statements and you can declare variables in <code>for</code>-loops.</p>\n", "title": "Is using an outdated C compiler a security risk?", "answer_id": 37590533, "question_id": 37479403, "tags": ["c", "security", "gcc"]}], "question_id": 37479403}, {"body": "<p>I had an interview recently and one question asked was what is the use of <code>extern \"C\"</code> in C++ code. I replied that it is to use C functions in C++ code as C doesn't use name-mangling. I was asked why C doesn't use name-mangling and to be honest I couldn't answer.</p>\n\n<p>I understand that when the C++ compiler compiles functions, it gives a special name to the function mainly because we can have overloaded functions of the same name in C++ which must be resolved at compile time. In C, the name of the function will stay the same, or maybe with an _ before it.</p>\n\n<p>My query is: what's wrong with allowing the C++ compiler to mangle C functions also? I would have assumed that it doesn't matter what names the compiler gives to them. We call functions in the same way in C and C++. </p>\n", "title": "Why can&#39;t C functions be name-mangled?", "tags": ["c++", "c", "name-mangling", "extern-c"], "answer_count": 9, "link": "http://stackoverflow.com/questions/36621845/why-cant-c-functions-be-name-mangled", "answers": [{"body": "<p>MSVC in fact <em>does</em> mangle C names, although in a simple fashion. It sometimes appends <code>@4</code> or another small number. This relates to calling conventions and the need for stack cleanup. </p>\n\n<p>So the premise is just flawed.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36621938, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>It's not that they \"can't\", they <em>aren't</em>, in general.</p>\n\n<p>If you want to call a function in a C library called <code>foo(int x, const char *y)</code>, it's no good letting your C++ compiler mangle that into <code>foo_I_cCP()</code> (or whatever, just made up a mangling scheme on the spot here) just because it can.</p>\n\n<p>That name won't resolve, the function is in C and its name does not depend on its list of argument types. So the C++ compiler has to know this, and mark that function as being C to avoid doing the mangling.</p>\n\n<p>Remember that said C function might be in a library whose source code you don't have, all you have is the pre-compiled binary and the header. So your C++ compiler can't do \"it's own thing\", it can't change what's in the library after all.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36621944, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>C++ wants to be able to interop with C code that links against it, or that it links against.</p>\n\n<p>C expects non-name-mangled function names.</p>\n\n<p>If C++ mangled it, it would not find the exported non-mangled functions from C, or C would not find the functions C++ exported.  The C linker must get the name it itself expects, because it does not know it is coming from or going to C++.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36621978, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>C++ compilers use name mangling in order to allow for unique symbol names for overloaded functions whose signature would otherwise be the same. It basically encodes the types of arguments as well, which allows for polymorphism on a function-based level.</p>\n\n<p>C does not require this since it does not allow for the overloading of functions.</p>\n\n<p>Note that name mangling is one (but certainly not the only!) reason that one cannot rely on a 'C++ ABI'.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36621991, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<blockquote>\n  <p>what's wrong with allowing the C++ compiler to mangle C functions also?</p>\n</blockquote>\n\n<p>They wouldn't be C functions any more.</p>\n\n<p>A function is not just a signature and a definition; how a function works is largely determined by factors like the calling convention. The \"Application Binary Interface\" specified for use on your platform describes how systems talk to each other. The C++ ABI in use by your system specifies a name mangling scheme, so that programs on that system know how to invoke functions in libraries and so forth. <em>(Read the C++ Itanium ABI for a great example. You'll very quickly see why it's necessary.)</em></p>\n\n<p>The same applies for the C ABI on your system. Some C ABIs do actually have a name mangling scheme (e.g. Visual Studio), so this is less about \"turning off name mangling\" and more about switching from the C++ ABI to the C ABI, for certain functions. We mark C functions as being C functions, to which the C ABI (rather than the C++ ABI) is pertinent. The declaration must match the definition (be it in the same project or in some third-party library), otherwise the declaration is pointless. <strong>Without that, your system simply won't know how to locate/invoke those functions.</strong></p>\n\n<p>As for why platforms don't define C and C++ ABIs to be the same and get rid of this \"problem\", that's partially historical &mdash; the original C ABIs weren't sufficient for C++, which has namespaces, classes and operator overloading, all of which need to somehow be represented in a symbol's name in a computer-friendly manner &mdash; but one might also argue that making C programs now abide by the C++ is unfair on the C community, which would have to put up with a massively more complicated ABI just for the sake of some other people who want interoperability.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36621999, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>It was sort of answered above, but I'll try to put things into context.</p>\n\n<p>First, C came first. As such, what C does is, sort of, the \"default\". It does not mangle names because it just doesn't. A function name is a function name. A global is a global, and so on.</p>\n\n<p>Then C++ came along. C++ wanted to be able to use the same linker as C, and to be able to link with code written in C. But C++ could not leave the C \"mangling\" (or, lack there of) as is. Check out the following example:</p>\n\n<pre><code>int function(int a);\nint function();\n</code></pre>\n\n<p>In C++, these are distinct functions, with distinct bodies. If none of them are mangled, both will be called \"function\" (or \"_function\"), and the linker will complain about the redefinition of a symbol. C++ solution was to mangle the argument types into the function name. So, one is called <code>_function_int</code> and the other is called <code>_function_void</code> (not actual mangling scheme) and the collision is avoided.</p>\n\n<p>Now we're left with a problem. If <code>int function(int a)</code> was defined in a C module, and we're merely taking its header (i.e. declaration) in C++ code and using it, the compiler will generate an instruction to the linker to import <code>_function_int</code>. When the function was defined, in the C module, it was not called that. It was called <code>_function</code>. This will cause a linker error.</p>\n\n<p>To avoid that error, during the <em>declaration</em> of the function, we tell the compiler it is a function designed to be linked with, or compiled by, a C compiler:</p>\n\n<pre><code>extern \"C\" int function(int a);\n</code></pre>\n\n<p>The C++ compiler now knows to import <code>_function</code> rather than <code>_function_int</code>, and all is well.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36622579, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>It's very common to have programs which are partially written in C and partially written in some other language (often assembly language, but sometimes Pascal, FORTRAN, or something else).  It's also common to have programs contain different components written by different people who may not have the source code for everything.</p>\n\n<p>On most platforms, there is a specification--often called an ABI [Application Binary Interface] which describes what a compiler must do to produce a function with a particular name which accepts arguments of some particular types and returns a value of some particular type.  In some cases, an ABI may define more than one \"calling convention\"; compilers for such systems often provide a means of indicating which calling convention should be used for a particular function.  For example, on the Macintosh, most Toolbox routines use the Pascal calling convention, so the prototype for something like \"LineTo\" would be something like:</p>\n\n<pre><code>/* Note that there are no underscores before the \"pascal\" keyword because\n   the Toolbox was written in the early 1980s, before the Standard and its\n   underscore convention were published */\npascal void LineTo(short x, short y);\n</code></pre>\n\n<p>If all of the code in a project was compiled using the same compiler, it\nwouldn't matter what name the compiler exported for each function, but in\nmany situations it will be necessary for C code to call functions that were\ncompiled using other tools and cannot be recompiled with the present compiler\n[and may very well not even be in C].  Being able to define the linker name\nis thus critical to the use of such functions.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36626797, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>I'll add one other answer, to address some of the tangential discussions that took place.</p>\n\n<p>The C ABI (application binary interface) originally called for passing arguments on the stack in reverse order (i.e. - pushed from right to left), where the caller also frees the stack storage. Modern ABI actually uses registers for passing arguments, but many of the mangling considerations go back to that original stack argument passing.</p>\n\n<p>The original Pascal ABI, in contrast, pushed the arguments from left to right, and the callee had to pop the arguments. The original C ABI is superior to the original Pascal ABI in two important points. The argument push order means that the stack offset of the first argument is always known, allowing functions that have an unknown number of arguments, where the early arguments control how many other arguments there are (ala <code>printf</code>).</p>\n\n<p>The second way in which the C ABI is superior is the behavior in case the caller and callee do not agree on how many arguments there are. In the C case, so long as you don't actually access arguments past the last one, nothing bad happens. In Pascal, the wrong number of arguments is popped from the stack, and the entire stack is corrupted.</p>\n\n<p>The original Windows 3.1 ABI was based on Pascal. As such, it used the Pascal ABI (arguments in left to right order, callee pops). Since any mismatch in argument number might lead to stack corruption, a mangling scheme was formed. Each function name was mangled with a number indicating the size, in bytes, of its arguments. So, on 16 bit machine, the following function (C syntax):</p>\n\n<pre><code>int function(int a)\n</code></pre>\n\n<p>Was mangled to <code>function@2</code>, because <code>int</code> is two bytes wide. This was done so that if the declaration and definition mismatch, the linker will fail to find the function rather than corrupt the stack at run time. Conversely, if the program links, then you can be sure the correct number of bytes is popped from the stack at the end of the call.</p>\n\n<p>32 bit Windows and onward use the <a href=\"https://en.wikipedia.org/wiki/X86_calling_conventions#stdcall\"><code>stdcall</code></a> ABI instead. It is similar to the Pascal ABI, except push order is like in C, from right to left. Like the Pascal ABI, the name mangling mangles the arguments byte size into the function name to avoid stack corruption.</p>\n\n<p>Unlike claims made elsewhere here, the C ABI does not mangle the function names, even on Visual Studio. Conversely, mangling functions decorated with the <code>stdcall</code> ABI specification isn't unique to VS. GCC also supports this ABI, even when compiling for Linux. This is used extensively by <a href=\"https://www.winehq.org/\">Wine</a>, that uses it's own loader to allow run time linking of Linux compiled binaries to Windows compiled DLLs.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36646458, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}, {"body": "<p>Mangling the names of C functions and variables would allow their types to be checked at link time. Currently, all (?) C implementations allow you to define a variable in one file and call it as a function in another. Or you can declare a function with a wrong signature (e.g. <code>void fopen(double)</code> and then call it.</p>\n\n<p>I proposed <a href=\"http://www.dmst.aueb.gr/dds/pubs/jrnl/1991-SIGPLAN-CType/html/tsl.html\" rel=\"nofollow\">a scheme for the type-safe linkage of C variables and functions</a> through the use of mangling back in 1991. The scheme was never adopted, because, as other have noted here, this would destroy backward compatibility.</p>\n", "title": "Why can&#39;t C functions be name-mangled?", "answer_id": 36675912, "question_id": 36621845, "tags": ["c++", "c", "name-mangling", "extern-c"]}], "question_id": 36621845}, {"body": "<p>According to the docs, <a href=\"http://redux.js.org/docs/advanced/AsyncFlow.html\" rel=\"nofollow\">\"Without middleware, Redux store only supports synchronous data flow\"</a>. I don't understand why this is the case. Why can't the container component call the async API, and then <code>dispatch</code> the actions? </p>\n\n<p>For example, imagine a simple UI: a field and a button. When user pushes the button, the field gets populated with data from a remote server.</p>\n\n<p><a href=\"http://i.stack.imgur.com/GBI59.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/GBI59.png\" alt=\"A field and a button\"></a></p>\n\n<pre><code>import * as React from 'react';\nimport * as Redux from 'redux';\nimport { Provider, connect } from 'react-redux';\n\nconst ActionTypes = {\n    STARTED_UPDATING: 'STARTED_UPDATING',\n    UPDATED: 'UPDATED'\n};\n\nclass AsyncApi {\n    static getFieldValue() {\n        const promise = new Promise((resolve) =&gt; {\n            setTimeout(() =&gt; {\n                resolve(Math.floor(Math.random() * 100));\n            }, 1000);\n        });\n        return promise;\n    }\n}\n\nclass App extends React.Component {\n    render() {\n        return (\n            &lt;div&gt;\n                &lt;input value={this.props.field}/&gt;\n                &lt;button disabled={this.props.isWaiting} onClick={this.props.update}&gt;Fetch&lt;/button&gt;\n                {this.props.isWaiting &amp;&amp; &lt;div&gt;Waiting...&lt;/div&gt;}\n            &lt;/div&gt;\n        );\n    }\n}\nApp.propTypes = {\n    dispatch: React.PropTypes.func,\n    field: React.PropTypes.any,\n    isWaiting: React.PropTypes.bool\n};\n\nconst reducer = (state = { field: 'No data', isWaiting: false }, action) =&gt; {\n    switch (action.type) {\n        case ActionTypes.STARTED_UPDATING:\n            return { ...state, isWaiting: true };\n        case ActionTypes.UPDATED:\n            return { ...state, isWaiting: false, field: action.payload };\n        default:\n            return state;\n    }\n};\nconst store = Redux.createStore(reducer);\nconst ConnectedApp = connect(\n    (state) =&gt; {\n        return { ...state };\n    },\n    (dispatch) =&gt; {\n        return {\n            update: () =&gt; {\n                dispatch({\n                    type: ActionTypes.STARTED_UPDATING\n                });\n                AsyncApi.getFieldValue()\n                    .then(result =&gt; dispatch({\n                        type: ActionTypes.UPDATED,\n                        payload: result\n                    }));\n            }\n        };\n    })(App);\nexport default class extends React.Component {\n    render() {\n        return &lt;Provider store={store}&gt;&lt;ConnectedApp/&gt;&lt;/Provider&gt;;\n    }\n}\n</code></pre>\n\n<p>When the exported component is rendered, I can click the button and the input is updated correctly. </p>\n\n<p>Note the <code>update</code> function in the <code>connect</code> call. It dispatches an action that tells the App that it is updating, and then performs an async call. After the call finishes, the provided value is dispatched as a payload of another action.</p>\n\n<p>What is wrong with this approach? Why would I want to use Redux Thunk or Redux Promise, as the documentation suggests?</p>\n\n<p><strong>EDIT:</strong> I searched the Redux repo for clues, and found that Action Creators were required to be pure functions in the past. For example, <a href=\"https://github.com/rackt/redux/issues/533\" rel=\"nofollow\">here's a user trying to provide a better explanation for async data flow:</a></p>\n\n<blockquote>\n  <p>The action creator itself is still a pure function, but the thunk function it returns doesn't need to be, and it can do our async calls</p>\n</blockquote>\n\n<p><a href=\"https://github.com/rackt/redux/issues/1088\" rel=\"nofollow\">Action creators are no longer required to be pure.</a> So, thunk/promise middleware was definitely required in the past, but it seems that this is no longer the case?</p>\n", "title": "Why do we need middleware for async flow in Redux?", "tags": ["javascript", "asynchronous", "reactjs", "redux", "redux-thunk"], "answer_count": 4, "link": "http://stackoverflow.com/questions/34570758/why-do-we-need-middleware-for-async-flow-in-redux", "answers": [{"body": "<p><strong>The short answer</strong>: seems like a totally reasonable approach to the asynchrony problem to me. With a couple caveats.</p>\n\n<p>I had a very similar line of thought when working on a new project we just started at my job. I was a big fan of vanilla Redux's elegant system for updating the store and rerendering components in a way that stays out of the guts of a React component tree. It seemed weird to me to hook into that elegant <code>dispatch</code> mechanism to handle asynchrony.</p>\n\n<p>I ended up going with a really similar approach to what you have there in a library I factored out of our project, which we called <a href=\"https://github.com/artsy/react-redux-controller\">react-redux-controller</a>.</p>\n\n<p>I ended up not going with the exact approach you have above for a couple reasons:</p>\n\n<ol>\n<li>The way you have it written, those dispatching functions don't have access to the store. You can somewhat get around that by having your UI components pass in all of the info the dispatching function needs. But I'd argue that this couples those UI components to the dispatching logic unnecessarily. And more problematically, there's no obvious way for the dispatching function to access updated state in async continuations.</li>\n<li>The dispatching functions have access to <code>dispatch</code> itself via lexical scope. This limits the options for refactoring once that <code>connect</code> statement gets out of hand -- and it's looking pretty unwieldy with just that one <code>update</code> method. So you need some system for letting you compose those dispatcher functions if you break them up into separate modules.</li>\n</ol>\n\n<p>Take together, you have to rig up some system to allow <code>dispatch</code> and the store to be injected into your dispatching functions, along with the parameters of the event. I know of three reasonable approaches to this dependency injection:</p>\n\n<ul>\n<li><a href=\"https://github.com/gaearon/redux-thunk\">redux-thunk</a> does this in a functional way, by passing them into your thunks (making them not exactly thunks at all, by dome definitions). I haven't worked with the other <code>dispatch</code> middleware approaches, but I assume they're basically the same.</li>\n<li>react-redux-controller does this with a coroutine. As a bonus, it also gives you access to the \"selectors\", which are the functions you may have passed in as the first argument to <code>connect</code>, rather than having to work directly with the raw, normalized store.</li>\n<li>You could also do it the object-oriented way by injecting them into the <code>this</code> context, through a variety of possible mechanisms.</li>\n</ul>\n\n<p><strong>Update</strong></p>\n\n<p>It occurs to me that part of this conundrum is a limitation of <a href=\"https://github.com/rackt/react-redux\">react-redux</a>. The first argument to <a href=\"https://github.com/rackt/react-redux/blob/master/docs/api.md#connectmapstatetoprops-mapdispatchtoprops-mergeprops-options\"><code>connect</code></a> gets a state snapshot, but not dispatch. The second argument gets dispatch but not the state. Neither argument gets a thunk that closes over the current state, for being able to see updated state at the time of a continuation/callback.</p>\n", "title": "Why do we need middleware for async flow in Redux?", "answer_id": 34584313, "question_id": 34570758, "tags": ["javascript", "asynchronous", "reactjs", "redux", "redux-thunk"]}, {"body": "\n\n<p>To answer the question that is asked in the beginning:</p>\n\n<blockquote>\n  <p>Why can't the container component call the async API, and then dispatch the actions?</p>\n</blockquote>\n\n<p>Keep in mind that those docs are for Redux, not Redux plus React. Redux stores <em>hooked up to React components</em> can do exactly what you say, but a Plain Jane Redux store with no middleware doesn't accept arguments to <code>dispatch</code> except plain ol' objects.</p>\n\n<p>Without middleware you could of course still do</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>const store = createStore(reducer);\nMyAPI.doThing().then(resp =&gt; store.dispatch(...));\n</code></pre>\n\n<p>But it's a similar case where the asynchrony is wrapped <em>around</em> Redux rather than handled <em>by</em> Redux. So, middleware allows for asynchrony by modifying what can be passed directly to <code>dispatch</code>.</p>\n\n<hr>\n\n<p>That said, the spirit of your suggestion is, I think, valid. There are certainly other ways you could handle asynchrony in a Redux + React application.</p>\n\n<p>One benefit of using middleware is that you can continue to use action creators as normal without worrying about exactly how they're hooked up. For example, using <code>redux-thunk</code>, the code you wrote would look a lot like</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>function updateThing() {\n  return dispatch =&gt; {\n    dispatch({\n      type: ActionTypes.STARTED_UPDATING\n    });\n    AsyncApi.getFieldValue()\n      .then(result =&gt; dispatch({\n        type: ActionTypes.UPDATED,\n        payload: result\n      }));\n  }\n}\n\nconst ConnectedApp = connect(\n  (state) =&gt; { ...state },\n  { update: updateThing }\n)(App);\n</code></pre>\n\n<p>which doesn't look all that different from the original \u2014 it's just shuffled a bit \u2014 and <code>connect</code> doesn't know that <code>updateThing</code> is (or needs to be) asynchronous.</p>\n\n<p>If you also wanted to support <a href=\"https://github.com/acdlite/redux-promise\">promises</a>, <a href=\"https://github.com/acdlite/redux-rx\">observables</a>, <a href=\"https://github.com/yelouafi/redux-saga\">sagas</a>, or <a href=\"https://github.com/rackt/redux/blob/b35c1c95432e793df92609938ae79be96788e09e/examples/real-world/middleware/api.js#L76-L123\">crazy custom</a> and <a href=\"https://github.com/rackt/redux/blob/b35c1c95432e793df92609938ae79be96788e09e/examples/real-world/actions/index.js#L9-L17\">highly declarative</a> action creators, then Redux can do it just by changing what you pass to <code>dispatch</code> (aka, what you return from action creators). No mucking with the React components (or <code>connect</code> calls) necessary.</p>\n", "title": "Why do we need middleware for async flow in Redux?", "answer_id": 34585694, "question_id": 34570758, "tags": ["javascript", "asynchronous", "reactjs", "redux", "redux-thunk"]}, {"body": "<blockquote>\n  <p>What is wrong with this approach? Why would I want to use Redux Thunk or Redux Promise, as the documentation suggests?</p>\n</blockquote>\n\n<p>There is nothing wrong with this approach. It\u2019s just inconvenient in a large application because you\u2019ll have different components performing the same actions, you might want to debounce some actions, or keep some local state like auto-incrementing IDs close to action creators, etc. So it is just easier from the maintenance point of view to extract action creators into separate functions.</p>\n\n<p><strong>You can read <a href=\"http://stackoverflow.com/questions/35411423/how-to-dispatch-a-redux-action-with-a-timeout/35415559#35415559\">my answer to \u201cHow to dispatch a Redux action with a timeout\u201d</a> for a more detailed walkthrough.</strong></p>\n\n<p>Middleware like Redux Thunk or Redux Promise just gives you \u201csyntax sugar\u201d for dispatching thunks or promises, but you don\u2019t <em>have to</em> use it.</p>\n\n<p>So, without any middleware, your action creator might look like</p>\n\n<pre><code>// action creator\nfunction loadData(dispatch, userId) { // needs to dispatch, so it is first argument\n  return fetch(`http://data.com/${userId}`)\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'LOAD_DATA_SUCCESS', data }),\n      err =&gt; dispatch({ type: 'LOAD_DATA_FAILURE', err })\n    );\n}\n\n// component\ncomponentWillMount() {\n  loadData(this.props.dispatch, this.props.userId); // don't forget to pass dispatch\n}\n</code></pre>\n\n<p>But with Thunk Middleware you can write it like this:</p>\n\n<pre><code>// action creator\nfunction loadData(userId) {\n  return dispatch =&gt; fetch(`http://data.com/${userId}`) // Redux Thunk handles these\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'LOAD_DATA_SUCCESS', data }),\n      err =&gt; dispatch({ type: 'LOAD_DATA_FAILURE', err })\n    );\n}\n\n// component\ncomponentWillMount() {\n  this.props.dispatch(loadData(this.props.userId)); // dispatch like you usually do\n}\n</code></pre>\n\n<p>So there is no huge difference. One thing I like about the latter approach is that the component doesn\u2019t care that the action creator is async. It just calls <code>dispatch</code> normally, it can also use <code>mapDispatchToProps</code> to bind such action creator with a short syntax, etc. The components don\u2019t know how action creators are implemented, and you can switch between different async approaches (Redux Thunk, Redux Promise, Redux Saga) without changing the components. On the other hand, with the former, explicit approach, your components know <em>exactly</em> that a specific call is async, and needs <code>dispatch</code> to be passed by some convention (for example, as a sync parameter).</p>\n\n<p>Also think about how this code will change. Say we want to have a second data loading function, and to combine them in a single action creator.</p>\n\n<p>With the first approach we need to be mindful of what kind of action creator we are calling:</p>\n\n<pre><code>// action creators\nfunction loadSomeData(dispatch, userId) {\n  return fetch(`http://data.com/${userId}`)\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'LOAD_SOME_DATA_SUCCESS', data }),\n      err =&gt; dispatch({ type: 'LOAD_SOME_DATA_FAILURE', err })\n    );\n}\nfunction loadOtherData(dispatch, userId) {\n  return fetch(`http://data.com/${userId}`)\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'LOAD_OTHER_DATA_SUCCESS', data }),\n      err =&gt; dispatch({ type: 'LOAD_OTHER_DATA_FAILURE', err })\n    );\n}\nfunction loadAllData(dispatch, userId) {\n  return Promise.all(\n    loadSomeData(dispatch, userId), // pass dispatch first: it's async\n    loadOtherData(dispatch, userId) // pass dispatch first: it's async\n  );\n}\n\n\n// component\ncomponentWillMount() {\n  loadAllData(this.props.dispatch, this.props.userId); // pass dispatch first\n}\n</code></pre>\n\n<p>With Redux Thunk action creators can <code>dispatch</code> the result of other action creators and not even think whether those are synchronous or asynchronous:</p>\n\n<pre><code>// action creators\nfunction loadSomeData(userId) {\n  return dispatch =&gt; fetch(`http://data.com/${userId}`)\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'LOAD_SOME_DATA_SUCCESS', data }),\n      err =&gt; dispatch({ type: 'LOAD_SOME_DATA_FAILURE', err })\n    );\n}\nfunction loadOtherData(userId) {\n  return dispatch =&gt; fetch(`http://data.com/${userId}`)\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'LOAD_OTHER_DATA_SUCCESS', data }),\n      err =&gt; dispatch({ type: 'LOAD_OTHER_DATA_FAILURE', err })\n    );\n}\nfunction loadAllData(userId) {\n  return dispatch =&gt; Promise.all(\n    dispatch(loadSomeData(userId)), // just dispatch normally!\n    dispatch(loadOtherData(userId)) // just dispatch normally!\n  );\n}\n\n\n// component\ncomponentWillMount() {\n  this.props.dispatch(loadAllData(this.props.userId)); // just dispatch normally!\n}\n</code></pre>\n\n<p>With this approach, if you later want your action creators to look into current Redux state, you can just use the second <code>getState</code> argument passed to the thunks without modifying the calling code at all:</p>\n\n<pre><code>function loadSomeData(userId) {\n  // Thanks to Redux Thunk I can use getState() here without changing callers\n  return (dispatch, getState) =&gt; {\n    if (getState().data[userId].isLoaded) {\n      return Promise.resolve();\n    }\n\n    fetch(`http://data.com/${userId}`)\n      .then(res =&gt; res.json())\n      .then(\n        data =&gt; dispatch({ type: 'LOAD_SOME_DATA_SUCCESS', data }),\n        err =&gt; dispatch({ type: 'LOAD_SOME_DATA_FAILURE', err })\n      );\n  }\n}\n</code></pre>\n\n<p>If you need to change it to be synchronous, you can also do this without changing any calling code:</p>\n\n<pre><code>// I can change it to be a regular action creator without touching callers\nfunction loadSomeData(userId) {\n  return {\n    type: 'LOAD_SOME_DATA_SUCCESS',\n    data: localStorage.getItem('my-data')\n  }\n}\n</code></pre>\n\n<p>So the benefit of using middleware like Redux Thunk or Redux Promise is that components aren\u2019t aware of how action creators are implemented, and whether they care about Redux state, whether they are synchronous or asynchronous, and whether or not they call other action creators. The downside is a little bit of indirection, but we believe it\u2019s worth it in real applications.</p>\n\n<p>Finally, Redux Thunk and friends is just one possible approach to asynchronous requests in Redux apps. Another interesting approach is <a href=\"https://github.com/yelouafi/redux-saga\">Redux Saga</a> which lets you define long-running daemons (\u201csagas\u201d) that take actions as they come, and transform or perform requests before outputting actions. This moves the logic from action creators into sagas. You might want to check it out, and later pick what suits you the most.</p>\n\n<blockquote>\n  <p>I searched the Redux repo for clues, and found that Action Creators were required to be pure functions in the past. </p>\n</blockquote>\n\n<p>This is incorrect. The docs said this, but the docs were wrong.<br>\nAction creators were never required to be pure functions.<br>\nWe fixed the docs to reflect that.</p>\n", "title": "Why do we need middleware for async flow in Redux?", "answer_id": 34599594, "question_id": 34570758, "tags": ["javascript", "asynchronous", "reactjs", "redux", "redux-thunk"]}, {"body": "\n\n<h1>You don't.</h1>\n\n<h1>But... you should use redux-saga :)</h1>\n\n<p>Dan Abramov's answer is right about <code>redux-thunk</code> but I will talk a bit more about <a href=\"https://github.com/yelouafi/redux-saga\">redux-saga</a> that is quite similar but more powerful.</p>\n\n<h1>Imperative VS declarative</h1>\n\n<ul>\n<li><strong>DOM</strong>: jQuery is imperative / React is declarative</li>\n<li><strong>Monads</strong>: IO is imperative / Free is declarative</li>\n<li><strong>Redux effects</strong>: <code>redux-thunk</code> is imperative / <code>redux-saga</code> is declarative</li>\n</ul>\n\n<p>When you have a thunk in yours hands, like an IO monad or a promise, you can't easily know what it will do once you execute. The only way to test a thunk is to execute it, and mock the dispatcher (or the whole outside world if it interacts with more stuff...).</p>\n\n<p>If you are using mocks, then you are not doing functional programming. </p>\n\n<blockquote>\n  <p>Seen through the lens of side-effects, mocks are a flag that your code is impure, and in the functional programmer's eye, proof that something is wrong. Instead of downloading a library to help us check the iceberg is intact, we should be sailing around it.\n  A hardcore TDD/Java guy once asked me how you do mocking in Clojure. The answer is, we usually don't. We usually see it as a sign we need to refactor our code.</p>\n</blockquote>\n\n<p><a href=\"http://blog.jenkster.com/2015/12/which-programming-languages-are-functional.html\">Source</a></p>\n\n<p>The sagas (as they got implemented in <code>redux-saga</code>) are declarative and like the Free monad or React components, they are much easier to test without any mock.</p>\n\n<p>See also this <a href=\"http://degoes.net/articles/modern-fp\">article</a>:</p>\n\n<blockquote>\n  <p>in modern FP, we shouldn\u2019t write programs \u2014 we should write descriptions of programs, which we can then introspect, transform, and interpret at will.</p>\n</blockquote>\n\n<p>(Actually, Redux-saga is like a hybrid: the flow is imperative but the effects are declarative)</p>\n\n<h1>Confusion: actions/events/commands...</h1>\n\n<p>There is a lot of confusing in the frontend world on how some backend concepts like CQRS / EventSourcing and Flux / Redux may be related, mostly because in Flux we use the term \"action\" which can sometimes represent both imperative code (<code>LOAD_USER</code>) and events (<code>USER_LOADED</code>). I believe that like event-sourcing, you should only dispatch events.</p>\n\n<h1>Using sagas in practice</h1>\n\n<p>Imagine an app where there is somewhere a link to some user profile. The idiomatic way to handle this with both middlewares would be:</p>\n\n<h2><code>redux-thunk</code></h2>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;div onClick={e =&gt; dispatch(actions.loadUserProfile(123)}&gt;Robert&lt;/div&gt;\n\nfunction loadUserProfile(userId) {\n  return dispatch =&gt; fetch(`http://data.com/${userId}`)\n    .then(res =&gt; res.json())\n    .then(\n      data =&gt; dispatch({ type: 'USER_PROFILE_LOADED', data }),\n      err =&gt; dispatch({ type: 'USER_PROFILE_LOAD_FAILED', err })\n    );\n}\n</code></pre>\n\n<h2><code>redux-saga</code></h2>\n\n<pre class=\"lang-js prettyprint-override\"><code>&lt;div onClick={e =&gt; dispatch({ type: 'USER_NAME_CLICKED', payload: 123 })}&gt;Robert&lt;/div&gt;\n\n\nfunction* loadUserProfileOnNameClick() {\n  yield* takeLatest(\"USER_NAME_CLICKED\", fetchUser);\n}\n\nfunction* fetchUser(action) {\n  try {\n    const userProfile = yield fetch('http://data.com/${action.payload.userId }')\n    yield put({ type: 'USER_PROFILE_LOADED', userProfile })\n  } \n  catch(err) {\n    yield put({ type: 'USER_PROFILE_LOAD_FAILED', err })\n  }\n}\n</code></pre>\n\n<p>This saga translates to:</p>\n\n<blockquote>\n  <p>every time a username gets clicked, fetch the user profile and then dispatch an event with the loaded profile. </p>\n</blockquote>\n\n<p>As you can see, there are some advantages of <code>redux-saga</code>.</p>\n\n<p>The usage of <code>takeLatest</code> permits to express that you are only interested to get the data of the last username clicked (handle concurrency problems in case the user click very fast on a lot of usernames). This kind of stuff is hard with thunks. You could have used <code>takeEvery</code> if you don't want this behavior.</p>\n\n<p>You keep action creators pure. Note it's still useful to keep actionCreators (in sagas <code>put</code> and components <code>dispatch</code>), as it might help you to add action validation (assertions/flow/typescript) in the future.</p>\n\n<p>Your code becomes much more testable as the effects are declarative</p>\n\n<p>You don't need anymore to trigger rpc-like calls like <code>actions.loadUser()</code>. Your UI just needs to dispatch what HAS HAPPENED. We only fire <strong>events</strong> (always in the past tense!) and not actions anymore. This means that you can create decoupled <a href=\"https://github.com/erikras/ducks-modular-redux\">\"ducks\"</a> or <a href=\"http://martinfowler.com/bliki/BoundedContext.html\">Bounded Contexts</a> and that the saga can act as the coupling point between these modular components.</p>\n\n<p>This means that your views are more easy to manage because they don't need anymore to contain that translation layer between what has happened and what should happen as an effect </p>\n\n<p>For example imagine an infinite scroll view. <code>CONTAINER_SCROLLED</code> can lead to <code>NEXT_PAGE_LOADED</code>, but is it really the responsibility of the scrollable container to decide whether or not we should load another page? Then he has to be aware of more complicated stuff like whether or not the last page was loaded successfully or if there is already a page that tries to load, or if there is no more items left to load? I don't think so: for maximum reusability the scrollable container should just describe that it has been scrolled. The loading of a page is a \"business effect\" of that scroll</p>\n\n<p>Some might argue that generators can inherently hide state outside of redux store with local variables, but if you start to orchestrate complex things inside thunks by starting timers etc you would have the same problem anyway. And there's a <code>select</code> effect that now permits to get some state from your Redux store.</p>\n\n<p>Sagas can be time-traveled and also enables complex flow logging and dev-tools that are currently being worked on. Here is some simple async flow logging that is already implemented:</p>\n\n<p><a href=\"http://i.stack.imgur.com/F9Jsh.png\"><img src=\"http://i.stack.imgur.com/F9Jsh.png\" alt=\"saga flow logging\"></a></p>\n\n<h1>Decoupling</h1>\n\n<p>Sagas are not only replacing redux thunks. They come from backend / distributed systems / event-sourcing.</p>\n\n<p>It is a very common misconception that sagas are just here to replace your redux thunks with better testability. Actually this is just an implementation detail of redux-saga. Using declarative effects is better than thunks for testability, but the saga pattern can be implemented on top of imperative or declarative code.</p>\n\n<p>In the first place, the saga  is a piece of software that permits to coordinate long running transactions (eventual consistency), and transactions across different bounded contexts (domain driven design jargon).</p>\n\n<p>To simplify this for frontend world, imagine there is widget1 and widget2. When some button on widget1 is clicked, then it should have an effect on widget2. Instead of coupling the 2 widgets together (ie widget1 dispatch an action that targets widget2), widget1 only dispatch that its button was clicked. Then the saga listen for this button click and then update widget2 by dispaching a new event that widget2 is aware of.</p>\n\n<p>This adds a level of indirection that is unnecessary for simple apps, but make it more easy to scale complex applications. You can now publish widget1 and widget2 to different npm repositories so that they never have to know about each others, without having them to share a global registry of actions. The 2 widgets are now bounded contexts that can live separately. They do not need each others to be consistent and can be reused in other apps as well. The saga is the coupling point between the two widgets that coordinate them in a meaningful way for your business.</p>\n\n<p>Some nice articles on how to structure your Redux app, on which you can use Redux-saga for decoupling reasons:</p>\n\n<ul>\n<li><a href=\"http://jaysoo.ca/2016/02/28/organizing-redux-application/\">http://jaysoo.ca/2016/02/28/organizing-redux-application/</a></li>\n<li><a href=\"http://marmelab.com/blog/2015/12/17/react-directory-structure.html\">http://marmelab.com/blog/2015/12/17/react-directory-structure.html</a></li>\n<li><a href=\"https://github.com/slorber/scalable-frontend-with-elm-or-redux\">https://github.com/slorber/scalable-frontend-with-elm-or-redux</a></li>\n</ul>\n\n<h1>A concrete usecase: notification system</h1>\n\n<p>I want my components to be able to trigger the display of in-app notifications. But I don't want my components to be highly coupled to the notification system that has its own business rules (max 3 notifications displayed at the same time,  notification queueing, 4 seconds display-time etc...).</p>\n\n<p>I don't want my JSX components to decide when a notification will show/hide. I just give it the ability to request a notification, and leave the complex rules inside the saga. This kind of stuff is quite hard to implement with thunks or promises.</p>\n\n<p><a href=\"http://i.stack.imgur.com/XQ8lx.png\"><img src=\"http://i.stack.imgur.com/XQ8lx.png\" alt=\"notifications\"></a></p>\n\n<p>I've described <a href=\"http://stackoverflow.com/a/38574266/82609\">here</a> how this can be done with saga</p>\n\n<h1>Why is it called a Saga?</h1>\n\n<p>The term saga comes from the backend world. I initially introduced Yassine (the author of Redux-saga) to that term in a <a href=\"https://github.com/paldepind/functional-frontend-architecture/issues/20#issuecomment-158694062\">long discussion</a>.</p>\n\n<p>Initially, that term was introduced with a <a href=\"http://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf\">paper</a>, the saga pattern was supposed to be used to handle eventual consistency in distributed transactions, but its usage has been extended to a broader definition by backend developers so that it now also covers the \"process manager\" pattern (somehow the original saga pattern is a specialized form of process manager).</p>\n\n<p>Today, the term \"saga\" is confusing as it can describe 2 different things. As it is used in redux-saga, it does not describe a way to handle distributed transactions but rather a way to coordinate actions in your app. <code>redux-saga</code> could also have been called <code>redux-process-manager</code>. </p>\n\n<p>See also:</p>\n\n<ul>\n<li><a href=\"http://kellabyte.com/2012/05/30/clarifying-the-saga-pattern/\">Kella Byte: Claryfing the Saga pattern</a></li>\n<li><a href=\"https://msdn.microsoft.com/en-us/library/jj591569.aspx\">Microsoft CQRS Journey: A Saga on Sagas</a></li>\n<li><a href=\"https://medium.com/@yelouafi/agree-for-the-saga-confusion-3fb2fc375322#.dxe1prl4y\">Medium response of Yassine</a></li>\n</ul>\n\n<h1>Alternatives</h1>\n\n<p>If you don't like the idea of using generators but you are interested by the saga pattern and its decoupling properties, you can also achieve the same with <a href=\"https://github.com/salsita/redux-saga-rxjs\">redux-saga-rxjs</a></p>\n\n<pre class=\"lang-js prettyprint-override\"><code>const loadUserProfileOnNameClick = iterable =&gt; iterable\n   // Everytime a username gets clicked\n  .filter(({ action, state }) =&gt; action.type === 'USER_NAME_CLICKED')\n   // Fetch the user, but only load data for the last clicked username\n  .flatmapLatest(({ action, state }) =&gt; fetchUser(action.payload.userId));\n\n\nfunction fetchUser(userId) {\n  return Observable.fromPromise(fetch('http://data.com/${userId }'))\n    .map(userProfile =&gt; \n        ({ type: 'USER_PROFILE_LOADED', userProfile })\n     )\n    .catch(error =&gt; \n        Observable.of({ type: 'USER_PROFILE_LOAD_FAILED', error })\n    );\n}\n</code></pre>\n\n<p>Note also <a href=\"https://github.com/redux-observable/redux-observable\">redux-observable</a> is using a very similar pattern, but decided to not use the term \"saga\" in favor of \"epic\"</p>\n\n<h1>Some redux-saga useful resources</h1>\n\n<ul>\n<li><a href=\"http://stackoverflow.com/questions/34930735/pros-cons-of-using-redux-saga-with-es6-generators-vs-redux-thunk-with-es7-async/34933395\">Redux-saga vs Redux-thunk with async/await</a></li>\n<li><a href=\"http://jaysoo.ca/2016/01/03/managing-processes-in-redux-using-sagas\">Managing processes in Redux Saga</a></li>\n<li><a href=\"http://riadbenguella.com/from-actions-creators-to-sagas-redux-upgraded/\">From actionsCreators to Sagas</a></li>\n<li><a href=\"http://codepen.io/CrocoDillon/pen/pgaRwb?editors=0010\">Snake game implemented with Redux-saga</a></li>\n</ul>\n", "title": "Why do we need middleware for async flow in Redux?", "answer_id": 34623840, "question_id": 34570758, "tags": ["javascript", "asynchronous", "reactjs", "redux", "redux-thunk"]}], "question_id": 34570758}, {"body": "<p>I expected <code>array.array</code> to be faster than lists, as arrays seem to be unboxed.</p>\n\n<p>However, I get the following result:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>In [1]: import array\n\nIn [2]: L = list(range(100000000))\n\nIn [3]: A = array.array('l', range(100000000))\n\nIn [4]: %timeit sum(L)\n1 loop, best of 3: 667 ms per loop\n\nIn [5]: %timeit sum(A)\n1 loop, best of 3: 1.41 s per loop\n\nIn [6]: %timeit sum(L)\n1 loop, best of 3: 627 ms per loop\n\nIn [7]: %timeit sum(A)\n1 loop, best of 3: 1.39 s per loop\n</code></pre>\n\n<p>What could be the cause of such a difference?</p>\n", "title": "Why are Python&#39;s arrays slow?", "tags": ["python", "arrays", "performance", "boxing", "python-internals"], "answer_count": 2, "link": "http://stackoverflow.com/questions/36778568/why-are-pythons-arrays-slow", "answers": [{"body": "<p>The <em>storage</em> is \"unboxed\", but every time you access an element Python has to \"box\" it (embed it in a regular Python object) in order to do anything with it.  For example, your <code>sum(A)</code> iterates over the array, and boxes each integer, one at a time, in a regular Python <code>int</code> object.  That costs time.  In your <code>sum(L)</code>, all the boxing was done at the time the list was created.</p>\n\n<p>So, in the end, an array is generally slower, but requires substantially less memory.</p>\n\n<hr>\n\n<p>Here's the relevant code from a recent version of Python 3, but the same basic ideas apply to all CPython implementations since Python was first released.</p>\n\n<p>Here's the code to access a list item:</p>\n\n<pre><code>PyObject *\nPyList_GetItem(PyObject *op, Py_ssize_t i)\n{\n    /* error checking omitted */\n    return ((PyListObject *)op) -&gt; ob_item[i];\n}\n</code></pre>\n\n<p>There's very little to it:  <code>somelist[i]</code> just returns the <code>i</code>'th object in the list (and all Python objects in CPython are pointers to a struct whose initial segment conforms to the layout of a <code>struct PyObject</code>).</p>\n\n<p>And here's the <code>__getitem__</code> implementation for an <code>array</code> with type code <code>l</code>:</p>\n\n<pre><code>static PyObject *\nl_getitem(arrayobject *ap, Py_ssize_t i)\n{\n    return PyLong_FromLong(((long *)ap-&gt;ob_item)[i]);\n}\n</code></pre>\n\n<p>The raw memory is treated as a vector of platform-native <code>C</code> <code>long</code> integers; the <code>i</code>'th <code>C long</code> is read up; and then <code>PyLong_FromLong()</code> is called to wrap (\"box\") the native <code>C long</code> in a Python <code>long</code> object (which, in Python 3, which eliminates Python 2's distinction between <code>int</code> and <code>long</code>, is actually shown as type <code>int</code>).</p>\n\n<p>This boxing has to allocate new memory for a Python <code>int</code> object, and spray the native <code>C long</code>'s bits into it.  In the context of the original example, this object's lifetime is very brief (just long enough for <code>sum()</code> to add the contents into a running total), and then more time is required to deallocate the new <code>int</code> object.</p>\n\n<p>This is where the speed difference comes from, always has come from, and always will come from in the CPython implementation.</p>\n", "title": "Why are Python&#39;s arrays slow?", "answer_id": 36778655, "question_id": 36778568, "tags": ["python", "arrays", "performance", "boxing", "python-internals"]}, {"body": "<p>To add to Tim Peters' excellent answer, arrays implement the <a href=\"https://docs.python.org/3/c-api/buffer.html\">buffer protocol</a>, while lists do not.  This means that, <em>if you are writing a C extension</em> (or the moral equivalent, such as writing a <a href=\"http://cython.org\">Cython</a> module), then you can access and work with the elements of an array much faster than anything Python can do.  This will give you considerable speed improvements, possibly well over an order of magnitude.  However, it has a number of downsides:</p>\n\n<ol>\n<li>You are now in the business of writing C instead of Python.  Cython is one way to ameliorate this, but it does not eliminate many fundamental differences between the languages; you need to be familiar with C semantics and understand what it is doing.</li>\n<li>PyPy's C API works <a href=\"http://morepypy.blogspot.ie/2010/04/using-cpython-extension-modules-with.html\">to some extent</a>, but isn't very fast.  If you are targeting PyPy, you should probably just write simple code with regular lists, and then let the JITter optimize it for you.</li>\n<li>C extensions are harder to distribute than pure Python code because they need to be compiled.  Compilation tends to be architecture and operating-system dependent, so you will need to ensure you are compiling for your target platform.</li>\n</ol>\n\n<p>Going straight to C extensions may be using a sledgehammer to swat a fly, depending on your use case.  You should first investigate <a href=\"http://docs.scipy.org/doc/numpy/about.html\">NumPy</a> and see if it is powerful enough to do whatever math you're trying to do.  It will also be much faster than native Python, if used correctly.</p>\n", "title": "Why are Python&#39;s arrays slow?", "answer_id": 36781207, "question_id": 36778568, "tags": ["python", "arrays", "performance", "boxing", "python-internals"]}], "question_id": 36778568}, {"body": "<p><a href=\"https://gcc.gnu.org/gcc-6/changes.html\">GCC 6 has a new optimizer feature</a>: It assumes that <code>this</code> is always not null and optimizes based on that.</p>\n\n<blockquote>\n  <p>Value range propagation now assumes that the this pointer of C++ member functions is non-null. This eliminates common null pointer checks <strong>but also breaks some non-conforming code-bases (such as Qt-5, Chromium, KDevelop)</strong>. As a temporary work-around -fno-delete-null-pointer-checks can be used. Wrong code can be identified by using -fsanitize=undefined.</p>\n</blockquote>\n\n<p>The change document clearly calls this out as dangerous because it breaks a surprising amount of frequently used code.</p>\n\n<p><strong>Why would this new assumption break practical C++ code?</strong> Are there particular patterns where careless or uninformed programmers rely on this particular undefined behavior? I cannot imagine anyone writing <code>if (this == NULL)</code> because that is so unnatural.</p>\n", "title": "Why does the enhanced GCC 6 optimizer break practical C++ code?", "tags": ["c++", "gcc", "compiler-optimization", "undefined-behavior"], "answer_count": 5, "link": "http://stackoverflow.com/questions/36893251/why-does-the-enhanced-gcc-6-optimizer-break-practical-c-code", "answers": [{"body": "<p>It does so because the \"practical\" code was broken and involved undefined behavior to begin with. There's no reason to use a null <code>this</code>, other than as a micro-optimization, usually a very premature one.</p>\n\n<p>It's a dangerous practice, since <a href=\"http://www.viva64.com/en/b/0226/\">adjustment of pointers due to class hierarchy traversal</a> can turn a null <code>this</code> into a non-null one. So, at the very least, the class whose methods are supposed to work with a null <code>this</code> must be a final class with no base class: it can't derive from anything, and it can't be derived from. We're quickly departing from practical to <a href=\"http://programmers.stackexchange.com/a/73733/55606\">ugly-hack-land</a>.</p>\n\n<p>In practical terms, the code doesn't have to be ugly:</p>\n\n<pre><code>struct Node\n{\n  Node* left;\n  Node* right;\n  void process();\n  void traverse_in_order() {\n    traverse_in_order_impl(this);\n  }\nprivate:\n  static void traverse_in_order_impl(Node * n)\n    if (!n) return;\n    traverse_in_order_impl(n-&gt;left);\n    n-&gt;process();\n    traverse_in_order_impl(n-&gt;right);\n  }\n};\n</code></pre>\n\n<blockquote>\n  <p>If you had an empty tree (eg. root is nullptr), this solution is still relying on undefined behavior by calling traverse_in_order with a nullptr.</p>\n</blockquote>\n\n<p>If the tree is empty, a.k.a. a null <code>Node* root</code>, you aren't supposed to be calling any non-static methods on it. Period. It's perfectly fine to have C-like tree code that takes an instance pointer by an explicit parameter.</p>\n\n<p>The argument here seems to boil down to somehow needing to write non-static methods on objects that could be called from a null instance pointer. There's no such need. The C-with-objects way of writing such code is still way nicer in the C++ world, because it can be type safe at the very least. Basically, the null <code>this</code> is such a micro-optimization, with such narrow field of use, that disallowing it is IMHO perfectly fine. No public API should depend on a null <code>this</code>.</p>\n", "title": "Why does the enhanced GCC 6 optimizer break practical C++ code?", "answer_id": 36893587, "question_id": 36893251, "tags": ["c++", "gcc", "compiler-optimization", "undefined-behavior"]}, {"body": "<p>I guess the question that needs to be answered why well-intentioned people would write the checks in the first place.</p>\n\n<p>The most common case is probably if you have a class that is part of a naturally occurring recursive call.</p>\n\n<p>If you had:</p>\n\n<pre><code>struct Node\n{\n  Node* left;\n  Node* right;\n};\n</code></pre>\n\n<p>in C, you might write:</p>\n\n<pre><code>void traverse_in_order(Node* n) {\n    if(!n) return;\n    traverse_in_order(n-&gt;left);\n    process(n);\n    traverse_in_order(n-&gt;right);\n}\n</code></pre>\n\n<p>In C++, it's nice to make this a member function:</p>\n\n<pre><code>void Node::traverse_in_order() {\n    // &lt;--- What check should be put here?\n    left-&gt;traverse_in_order();\n    process();\n    right-&gt;traverse_in_order();\n}\n</code></pre>\n\n<p>In the early days of C++ (prior to standardization), it was emphasized that that member functions were syntactic sugar for a function where the this parameter is implicit. Code was written in C++, converted to equivalent C and compiled. There were even explicit examples that comparing this to null was meaningful and the original Cfront compiler took advantage of this too. So coming from a C background, the obvious choice for the check is:</p>\n\n<pre><code>if(this == nullptr) return;      \n</code></pre>\n\n<p>Note: Bjarne Stroustrup even mentions that the rules for <code>this</code> have changed over the years <a href=\"http://www.i-programmer.info/programming/cc/9212-finding-bugs-in-the-first-c-compiler-what-does-bjarne-think.html\" rel=\"nofollow\">here</a></p>\n\n<p>And this worked on many compilers for many years. When standardization happened, this changed. And more recently, compilers started taking advantage of calling a member function where <code>this</code> being <code>nullptr</code> is undefined behavior, which means that this condition is always <code>false</code>, and the compiler is free to omit it.</p>\n\n<p>That means that to do any traversal of this tree, you need to either:</p>\n\n<ul>\n<li><p>Do all of the checks before calling traverse_in_order</p>\n\n<pre><code>void Node::traverse_in_order() {\n    if(left) left-&gt;traverse_in_order();\n    process();\n    if(right) right-&gt;traverse_in_order();\n}\n</code></pre>\n\n<p>This means also checking at EVERY call site if you could have a null root.</p></li>\n<li><p>Don't use a member function</p>\n\n<p>This means that you're writing the old C style code (perhaps as a static method), and calling it with the object explicitly as a parameter. eg. you're back to writing <code>Node::traverse_in_order(node);</code> rather than <code>node-&gt;traverse_in_order();</code> at the call site.</p></li>\n<li><p>I believe the easiest/neatest way to fix this particular example in a way that is standards compliant is to actually use a sentinel node rather than a nullptr.</p>\n\n<pre><code>// static class, or global variable\nNode sentinel;\n\nvoid Node::traverse_in_order() {\n    if(this == &amp;sentinel) return;\n    ...\n}\n</code></pre></li>\n</ul>\n\n<p>Neither of the first two options seem that appealing, and while code could get away with it, they wrote bad code with <code>this == nullptr</code> instead of using a proper fix.</p>\n\n<p>I'm guessing that's how some of these code bases evolved to have <code>this == nullptr</code> checks in them.</p>\n\n<p>edit: Added a proper solution, since people seem to want to provide one for this response that was just written as an example as to why people might have written nullptr checks.</p>\n\n<p>Edit May 9: Added link to Bjarne Stroustroup's comments on the change in <code>this</code></p>\n", "title": "Why does the enhanced GCC 6 optimizer break practical C++ code?", "answer_id": 36893681, "question_id": 36893251, "tags": ["c++", "gcc", "compiler-optimization", "undefined-behavior"]}, {"body": "<blockquote>\n  <p>The change document clearly calls this out as dangerous because it breaks a surprising amount of frequently used code.</p>\n</blockquote>\n\n<p>The document doesn't call it dangerous. Nor does it claim that it breaks <em>a surprising amount of code</em>. It simply points out a few popular code bases which are known rely on this undefined behaviour and will break due to the change unless the workaround option is used.</p>\n\n<blockquote>\n  <p>Why would this new assumption break practical C++ code?</p>\n</blockquote>\n\n<p>If <em>practical</em> c++ code relies on undefined behaviour, then changes to that undefined behaviour can break it. This is why UB is to be avoided, even when a program relying on it appears to work as intended.</p>\n\n<blockquote>\n  <p>Are there particular patterns where careless or uninformed programmers rely on this particular undefined behavior?</p>\n</blockquote>\n\n<p>I don't know if it's wide spread <strong>anti</strong>-pattern, but an uninformed programmer might think that they can fix their program from crashing by doing:</p>\n\n<pre><code>if (this)\n    member_variable = 42;\n</code></pre>\n\n<p>When the actual bug is dereferencing a null pointer somewhere else.</p>\n\n<p>I'm sure that if programmer is uninformed enough, they will be able to come up with more advanced (anti)-patterns that rely on this UB.</p>\n\n<blockquote>\n  <p>I cannot imagine anyone writing <code>if (this == NULL)</code> because that is so unnatural.</p>\n</blockquote>\n\n<p>I can.</p>\n", "title": "Why does the enhanced GCC 6 optimizer break practical C++ code?", "answer_id": 36893891, "question_id": 36893251, "tags": ["c++", "gcc", "compiler-optimization", "undefined-behavior"]}, {"body": "<p>The C++ standard is broken in important ways. Unfortunately, rather than protect the users from these problems, the GCC developers have chosen to use undefined behaviour as an excuse to implement marginal optimisations, even when it has been clearly explained to them how harmful it is.</p>\n\n<p>Here a much cleverer person than I explains in great detail. (He's talking about C but the situation is the same there).</p>\n\n<ul>\n<li><a href=\"https://groups.google.com/forum/m/#!msg/boring-crypto/48qa1kWignU/o8GGp2K1DAAJ\" rel=\"nofollow\">https://groups.google.com/forum/m/#!msg/boring-crypto/48qa1kWignU/o8GGp2K1DAAJ</a></li>\n</ul>\n\n<h3>Why is it harmful?</h3>\n\n<p><strong>Simply recompiling previously working, secure code with a newer version of the compiler can introduce security vulnerabilities</strong>. While the new behaviour can be disabled with a flag, existing makefiles do not have that flag set, obviously. And since no warning is produced, it is not obvious to the developer that the previously reasonable behaviour has changed.</p>\n\n<p>In this example, the developer has included a check for integer overflow, using <code>assert</code>, which will terminate the program if an invalid length is supplied. The GCC team removed the check on the basis that integer overflow is undefined, therefore the check can be removed. This resulted in real in-the-wild instances of this codebase being re-made vulnerable after the issue had been fixed.</p>\n\n<ul>\n<li><a href=\"https://gcc.gnu.org/bugzilla/show_bug.cgi?id=30475\" rel=\"nofollow\">https://gcc.gnu.org/bugzilla/show_bug.cgi?id=30475</a></li>\n</ul>\n\n<p>Read the whole thing. It's enough to make you weep.</p>\n\n<h3>OK, but what about this one?</h3>\n\n<p>Way back when, there was a fairly common idiom which went something like this:</p>\n\n<pre><code> OPAQUEHANDLE ObjectType::GetHandle(){\n    if(this==NULL)return DEFAULTHANDLE;\n    return mHandle;\n\n }\n\n void DoThing(ObjectType* pObj){\n     osfunction(pObj-&gt;GetHandle(), \"BLAH\");\n }\n</code></pre>\n\n<p>So the idiom is: If <code>pObj</code> is not null, you use the handle it contains, otherwise you use a default handle. This is encapsulated in the <code>GetHandle</code> function.</p>\n\n<p>The trick is that calling a non-virtual function doesn't actually make any use of the <code>this</code> pointer, so there is no access violation. </p>\n\n<h2>I still don't get it</h2>\n\n<p>A lot of code exists which is written like that. If someone simply recompiles it, without changing a line, every call to <code>DoThing(NULL)</code> is a crashing bug - if you are lucky. </p>\n\n<p><strong>If you are not  lucky, calls to crashing bugs become remote execution vulnerabilities.</strong></p>\n\n<p>This can occur even automatically. You've got an automated build system, right? Upgrading it to the latest compiler is harmless, right? But now it's not - not if your compiler is GCC.</p>\n\n<h2>OK so tell them!</h2>\n\n<p>They've been told. They are doing this in the full knowledge of the consequences.</p>\n\n<h2>but... why?</h2>\n\n<p>Who can say? Perhaps:</p>\n\n<ul>\n<li>They value the ideal purity of the C++ language over actual code</li>\n<li>They believe people should be punished for not following the standard</li>\n<li>They have no understanding of the reality of the world</li>\n<li>They are ... introducing bugs on purpose. Perhaps for a foreign government. Where do you live? All governments are foreign to most of the world, and most are hostile to some of the world.</li>\n</ul>\n\n<p>Or perhaps something else. Who can say?</p>\n", "title": "Why does the enhanced GCC 6 optimizer break practical C++ code?", "answer_id": 36894819, "question_id": 36893251, "tags": ["c++", "gcc", "compiler-optimization", "undefined-behavior"]}, {"body": "<p>Some of the \"practical\" (funny way to spell \"buggy\") code that was broken looked like this:</p>\n\n<pre><code>void foo(X* p) {\n  p-&gt;bar()-&gt;baz();\n}\n</code></pre>\n\n<p>and it forgot to account for the fact that <code>p-&gt;bar()</code> sometimes returns a null pointer, which means that dereferencing it to call <code>baz()</code> is undefined.</p>\n\n<p>Not all the code that was broken contained explicit <code>if (this == nullptr)</code> or <code>if (!p) return;</code> checks. Some cases were simply functions that didn't access any member variables, and so <em>appeared</em> to work OK. For example:</p>\n\n<pre><code>struct DummyImpl {\n  bool valid() const { return false; }\n  int m_data;\n};\nstruct RealImpl {\n  bool valid() const { return m_valid; }\n  bool m_valid;\n  int m_data;\n};\n\ntemplate&lt;typename T&gt;\nvoid do_something_else(T* p) {\n  if (p) {\n    use(p-&gt;m_data);\n  }\n}\n\ntemplate&lt;typename T&gt;\nvoid func(T* p) {\n  if (p-&gt;valid())\n    do_something(p);\n  else \n    do_something_else(p);\n}\n</code></pre>\n\n<p>In this code when you call <code>func&lt;DummyImpl*&gt;(DummyImpl*)</code> with a null pointer there is a \"conceptual\" dereference of the pointer to call <code>p-&gt;DummyImpl::valid()</code>, but in fact that member function just returns <code>false</code> without accessing <code>*this</code>. That <code>return false</code> can be inlined and so in practice the pointer doesn't need to be accessed at all. So with some compilers it appears to work OK: there's no segfault for dereferencing null, <code>p-&gt;valid()</code> is false, so the code calls <code>do_something_else(p)</code>, which checks for null pointers, and so does nothing. No crash or unexpected behaviour is observed.</p>\n\n<p>With GCC 6 you still get the call to <code>p-&gt;valid()</code>, but the compiler now infers from that expression that <code>p</code> must be non-null (otherwise <code>p-&gt;valid()</code> would be undefined behaviour) and makes a note of that information. That inferred information is used by the optimizer so that if the call to <code>do_something_else(p)</code> gets inlined, the <code>if (p)</code> check is now considered redundant, because the compiler remembers that it is not null, and so inlines the code to:</p>\n\n<pre><code>template&lt;typename T&gt;\nvoid func(T* p) {\n  if (p-&gt;valid())\n    do_something(p);\n  else {\n    // inlined body of do_something_else(p) with value propagation\n    // optimization performed to remove null check.\n    use(p-&gt;m_data);\n  }\n}\n</code></pre>\n\n<p>This now really does dereference a null pointer, and so code that previously appeared to work stops working.</p>\n\n<p>In this example the bug is in <code>func</code>, which should have checked for null first (or the callers should never have called it with null):</p>\n\n<pre><code>template&lt;typename T&gt;\nvoid func(T* p) {\n  if (p &amp;&amp; p-&gt;valid())\n    do_something(p);\n  else \n    do_something_else(p);\n}\n</code></pre>\n\n<p>An important point to remember is that most optimizations like this are not a case of the compiler saying \"ah, the programmer tested this pointer against null, I will remove it just to be annoying\". What happens is that various  run-of-the-mill optimizations like inlining and value range propagation combine to make those checks redundant, because they come after an earlier check, or a dereference. If the compiler knows that a pointer is non-null at point A in a function, and the pointer isn't changed before a later point B in the same function, then it knows it is also non-null at B. When inlining happens points A and B might actually be pieces of code that were originally in separate functions, but are now combined into one piece of code, and the compiler is able to apply its knowledge that the pointer is non-null in more places. This is a basic, but very important optimization, and if compilers didn't do that everyday code would be considerably slower and people would complain about unnecessary branches to re-test the same conditions repeatedly.</p>\n", "title": "Why does the enhanced GCC 6 optimizer break practical C++ code?", "answer_id": 36916651, "question_id": 36893251, "tags": ["c++", "gcc", "compiler-optimization", "undefined-behavior"]}], "question_id": 36893251}, {"body": "<p>My podfile was working but after updating to cocoapods version 1.0.0.beta.1</p>\n\n<p>pod install displays following error</p>\n\n<pre><code>MacBook-Pro:iOS-TuneIn home$ pod install\nFully deintegrating due to major version update\nDeleted 1 'Copy Pods Resources' build phases.\nDeleted 1 'Check Pods Manifest.lock' build phases.\nDeleted 1 'Embed Pods Frameworks' build phases.\n- libPods.a\n- Pods.debug.xcconfig\n- Pods.release.xcconfig\nDeleted 1 'Copy Pods Resources' build phases.\nDeleted 1 'Check Pods Manifest.lock' build phases.\n- libPods.a\nDeleted 1 'Copy Pods Resources' build phases.\nDeleted 1 'Check Pods Manifest.lock' build phases.\n- libPods.a\nDeleted 1 'Copy Pods Resources' build phases.\nDeleted 1 'Check Pods Manifest.lock' build phases.\n- libPods.a\nDeleted 1 'Copy Pods Resources' build phases.\nDeleted 1 'Check Pods Manifest.lock' build phases.\n- libPods.a\n- libPods.a\nDeleted 1 empty `Pods` groups from project.\nRemoving `Pods` directory.\n\nProject has been deintegrated. No traces of CocoaPods left in project.\nNote: The workspace referencing the Pods project still remains.\nUpdating local specs repositories\nAnalyzing dependencies\n[!] The dependency `AFNetworking (= 2.6.3)` is not used in any concrete target.\nThe dependency `MBProgressHUD (~&gt; 0.9.1)` is not used in any concrete target.\nThe dependency `PDKeychainBindingsController (~&gt; 0.0.1)` is not used in any concrete target.\nThe dependency `FMDB/SQLCipher` is not used in any concrete target.\nThe dependency `ZXingObjC (~&gt; 3.1.0)` is not used in any concrete target.\nThe dependency `SDWebImage (~&gt; 3.7.2)` is not used in any concrete target.\nThe dependency `SignalR-ObjC (~&gt; 2.0.0.beta3)` is not used in any concrete target.\nThe dependency `CJPAdController (from `https://github.com/nabeelarif100/CJPAdController.git`)` is not used in any concrete target.\nThe dependency `ECSlidingViewController (~&gt; 2.0.3)` is not used in any concrete target.\nThe dependency `VGParallaxHeader` is not used in any concrete target.\nThe dependency `EMString` is not used in any concrete target.\nThe dependency `Google/SignIn` is not used in any concrete target.\nThe dependency `VIPhotoView (~&gt; 0.1)` is not used in any concrete target.\nThe dependency `EncryptedCoreData (from `https://github.com/project-imas/encrypted-core-data.git`)` is not used in any concrete target.\nMacBook-Pro:iOS-TuneIn home$ \n</code></pre>\n\n<p>Podfile:</p>\n\n<pre><code>source 'https://github.com/CocoaPods/Specs.git'\nplatform :ios, '7.0'\npod 'AFNetworking', '2.6.3'\npod 'MBProgressHUD', '~&gt; 0.9.1'\npod 'PDKeychainBindingsController', '~&gt; 0.0.1'\npod 'FMDB/SQLCipher'\npod 'ZXingObjC', '~&gt; 3.1.0'\npod 'SDWebImage', '~&gt;3.7.2'\npod 'SignalR-ObjC','~&gt;2.0.0.beta3'\npod 'CJPAdController', :git =&gt; 'https://github.com/nabeelarif100/CJPAdController.git'\npod 'ECSlidingViewController', '~&gt; 2.0.3'\npod 'VGParallaxHeader'\npod 'EMString'\npod 'Google/SignIn'\npod 'VIPhotoView', '~&gt; 0.1'\npod 'EncryptedCoreData', :git =&gt; 'https://github.com/project-imas/encrypted-core-data.git'\n</code></pre>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "tags": ["ios", "cocoapods"], "answer_count": 10, "link": "http://stackoverflow.com/questions/34556991/pod-install-displaying-error-in-cocoapods-version-1-0-0-beta-1", "answers": [{"body": "<p>I got the same issue today. For mitigation, I unintall cocoapod, then install again version 0.39.</p>\n\n<p>here is the link how to uninstall: <a href=\"http://superuser.com/questions/686317/how-to-fully-uninstall-the-cocoapods-from-the-mac-machine\">http://superuser.com/questions/686317/how-to-fully-uninstall-the-cocoapods-from-the-mac-machine</a></p>\n\n<p>This answer does not fix the root cause, but can get you unblocked. I don't have enough reputation to leave comments, so I put an answer here to unblock you.</p>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 34557731, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>You have to specify a target for each pod.</p>\n\n<p>e.g. if before you had your Podfile written like this:</p>\n\n<pre><code>pod 'Alamofire', '~&gt; 3.1.4'\npod 'SwiftyJSON', '~&gt; 2.3.2'\n</code></pre>\n\n<p>just change it to</p>\n\n<pre><code>target \"TargetName\" do\n    pod 'Alamofire', '~&gt; 3.1.4'\n    pod 'SwiftyJSON', '~&gt; 2.3.2'\nend\n</code></pre>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 34558516, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>My podfile was formatted correctly, so the answer did not work for me.  What I <strong>had</strong> to do was all of the following:\nFirst, </p>\n\n<ol>\n<li>gem uninstall cocoapods</li>\n<li>rvm get stable --auto-dotfiles </li>\n<li>rvm use ruby-2.1.2 </li>\n<li>rvm osx-ssl-certs update all </li>\n<li>rvm rubygems latest </li>\n<li>sudo gem sources -r <a href=\"https://rubygems.org/\" rel=\"nofollow\">https://rubygems.org/</a> </li>\n<li>sudo gem sources -a <a href=\"http://rubygems.org/\" rel=\"nofollow\">http://rubygems.org/</a> </li>\n<li>gem install cocoapods -v 1.0.0.beta.1 --pre -V</li>\n</ol>\n\n<p>I had SSL errors, timeout errors, and path errors.  This fixed all of these.  I am adding this answer in hopes that it will help someone - most people with this issue will NOT need to go through all of these steps, and should not do so if it is not neccesary.  Keep in mind, that this is changing the d/l link to not use https, so be sure to change it back once you have resolved this issue.  <a href=\"https://stackoverflow.com/questions/19612185/unable-to-install-cocoapods-gem-from-rubygems-org-bad-response-backend-read-e\">This</a>, <a href=\"https://stackoverflow.com/questions/19150017/ssl-error-when-installing-rubygems-unable-to-pull-data-from-https-rubygems-o?rq=1\">this</a>, and <a href=\"https://stackoverflow.com/questions/14355165/xcode-installing-cocoapods-no-response\">this</a> Stack Overflow question helped me finally resolve these issues.  </p>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 34732465, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>From the CocoaPods website:</p>\n\n<blockquote>\n  <p>CocoaPods provides a <code>pod init</code> command to create a Podfile with smart defaults. You should use it.</p>\n</blockquote>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 35760117, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>After the new changes to cocoapods, You have to add the following lines to your podfile.</p>\n\n<pre><code>target \"YOUR_PROJECT_NAME\" do\n\n     pod \"YOUR_POD\"\n\nend\n</code></pre>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 37045040, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>I have the same problem, and even I changed to </p>\n\n<p>target \"TargetName\" do\n    pod 'Alamofire', '~> 3.1.4'\n    pod 'SwiftyJSON', '~> 2.3.2'\nend</p>\n\n<p>It seems has some cache problem, it always read old version of PodFile, even I remove PodFile, the same error show up. It's weird.</p>\n\n<p>However, when I open a new terminal , running pod install, it works.</p>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 37245804, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>you must add <code>target 'your target' do</code> and <code>end</code> around you pod like below.</p>\n\n<pre><code>target 'your target' do\npod 'AFNetworking', '2.6.3'\npod 'MBProgressHUD', '~&gt; 0.9.1'\npod 'PDKeychainBindingsController', '~&gt; 0.0.1'\nend\n</code></pre>\n\n<p>plus: You may be need remove the pods dir, Podfile.lock and xcworkspace file, run the <code>pod install</code> again.</p>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 37500300, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>1) Add and Open <strong><em>Podfile</em></strong> in Xcode instead of TextEdit or any other editor. (<em>Syntax highlighting while viewing a pod file will simplify the process of finding syntax errors</em>)</p>\n\n<p>2) Add project dependancies as follows in your <strong><em>Podfile</em></strong></p>\n\n<pre><code>def pods\n  pod 'AFNetworking', '~&gt; 2.6'\n  pod 'ORStackView', '~&gt; 3.0'\n  pod 'SwiftyJSON', '~&gt; 2.3'\nend\n</code></pre>\n\n<p>3) Add above define <strong><em>pods</em></strong> in <strong><em>project target</em></strong> as follows</p>\n\n<pre><code>target 'App_Target_Name' do\n  pods\nend\n</code></pre>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 38411575, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>for New version of cocoapods i.i 1.0.1</p>\n\n<p>pod 'SlideMenuControllerSwift'\npod 'SDWebImage'\npod 'SearchTextField'</p>\n\n<p>I was getting error: </p>\n\n<p>The dependency <code>SlideMenuControllerSwift</code> is not used in any concrete target.\nThe dependency <code>SDWebImage</code> is not used in any concrete target.\nThe dependency <code>SearchTextField</code> is not used in any concrete target.</p>\n\n<p>than i changed it to </p>\n\n<p>target \"YOUR_PROJECT_NAME\" do</p>\n\n<pre><code> pod \"YOUR_POD\"\n</code></pre>\n\n<p>end</p>\n\n<p>than it worked</p>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 39101172, "question_id": 34556991, "tags": ["ios", "cocoapods"]}, {"body": "<p>I was this operation in the podfile\uff1a</p>\n\n<p>source '<a href=\"https://github.com/CocoaPods/Specs.git\" rel=\"nofollow\">https://github.com/CocoaPods/Specs.git</a>'</p>\n\n<p>platform :ios, '8.0'</p>\n\n<p>target \"targetprojectname\" do</p>\n\n<p>pod \"AFNetworking\"</p>\n\n<p>end</p>\n", "title": "Pod install displaying error in cocoapods version 1.0.0.beta.1", "answer_id": 39218302, "question_id": 34556991, "tags": ["ios", "cocoapods"]}], "question_id": 34556991}, {"body": "<p>I just started to study C, and when doing one example about passing pointer to pointer as a function's parameter, I found a problem. </p>\n\n<p>This is my sample code : </p>\n\n<pre><code>#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n\nint* allocateIntArray(int* ptr, int size){\n    if (ptr != NULL){\n        for (int i = 0; i &lt; size; i++){\n            ptr[i] = i;\n        }\n    }\n    return ptr;\n}\n\nvoid increasePointer(int** ptr){\n    if (ptr != NULL){\n        *ptr += 1; /* &lt;----------------------------- This is line 16 */\n    }\n}\n\nint main()\n{\n    int* p1 = (int*)malloc(sizeof(int)* 10);\n    allocateIntArray(p1, 10);\n\n    for (int i = 0; i &lt; 10; i++){\n        printf(\"%d\\n\", p1[i]);\n    }\n\n    increasePointer(&amp;p1);\n    printf(\"%d\\n\", *p1);\n    p1--;\n    free(p1);\n    fgets(string, sizeof(string), stdin);\n    return 0;\n}\n</code></pre>\n\n<p>The problem occurs in line 16, when I modify <code>*ptr+=1</code> to <code>*ptr++</code>. The expected result should be the whole array and number 1 but when I use <code>*ptr++</code> the result is 0. </p>\n\n<p>Is there any diffirence between <code>+=1</code> and <code>++</code>? I thought that both of them are the same.</p>\n", "title": "Difference between *ptr += 1 and *ptr++ in C", "tags": ["c", "pointers", "post-increment"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35306391/difference-between-ptr-1-and-ptr-in-c", "answers": [{"body": "<p>The difference is due to operator precedence.</p>\n\n<p>The post-increment operator <code>++</code> has higher precedence than the dereference operator <code>*</code>. So <code>*ptr++</code> is equivalent to <code>*(ptr++)</code>. In other words, the post increment modifies the pointer, not what it points to.</p>\n\n<p>The assignment operator <code>+=</code> has lower precedence than the dereference operator <code>*</code>, so <code>*ptr+=1</code> is equivalent to <code>(*ptr)+=1</code>. In other words, the assignment operator modifies the value that the pointer points to, and does not change the pointer itself. </p>\n", "title": "Difference between *ptr += 1 and *ptr++ in C", "answer_id": 35306447, "question_id": 35306391, "tags": ["c", "pointers", "post-increment"]}, {"body": "<p>The order of precedence for the 3 operators involved in your question is the following :</p>\n\n<p>post-increment <code>++</code> > dereference <code>*</code> > assignment <code>+=</code></p>\n\n<p>You can check this <a href=\"http://en.cppreference.com/w/c/language/operator_precedence\">page</a> for further details on the subject.</p>\n\n<blockquote>\n  <p>When parsing an expression, an operator which is listed on some row will be bound tighter (as if by parentheses) to its arguments than any operator that is listed on a row further below it. For example, the expression <code>*p++</code> is parsed as <code>*(p++)</code>, and not as <code>(*p)++</code>.</p>\n</blockquote>\n\n<p>Long story short, in order to express this assignment <code>*ptr+=1</code> using the post-increment operator you need to add parentheses to the dereference operator to give that operation precedence over <code>++</code> as in this <strong><code>(*ptr)++</code></strong></p>\n", "title": "Difference between *ptr += 1 and *ptr++ in C", "answer_id": 35336032, "question_id": 35306391, "tags": ["c", "pointers", "post-increment"]}, {"body": "<p>Let's apply parentheses to show the <a href=\"http://en.cppreference.com/w/c/language/operator_precedence\">order of operations</a></p>\n\n<pre><code>a + b / c\na + (b/c)\n</code></pre>\n\n<p>Let's do it again with</p>\n\n<pre><code>*ptr   += 1\n(*ptr) += 1\n</code></pre>\n\n<p>And again with</p>\n\n<pre><code>*ptr++\n*(ptr++)\n</code></pre>\n\n<ul>\n<li>In <code>*ptr += 1</code>, we increment the value of the variable our pointer <em>points</em> to.</li>\n<li>In <code>*ptr++</code>, we increment the pointer <em>after</em> our entire statement (line of code) is done, and return a reference to the variable our pointer <em>points</em> to.</li>\n</ul>\n\n<p>The latter allows you to do things like:</p>\n\n<pre><code>for(int i = 0; i &lt; length; i++)\n{\n    // Copy value from *src and store it in *dest\n    *dest++ = *src++;\n\n    // Keep in mind that the above is equivalent to\n    *(dest++) = *(src++);\n}\n</code></pre>\n\n<p>This is a common method used to copy a <code>src</code> array into another <code>dest</code> array.</p>\n", "title": "Difference between *ptr += 1 and *ptr++ in C", "answer_id": 35346040, "question_id": 35306391, "tags": ["c", "pointers", "post-increment"]}, {"body": "<p>Very good question.</p>\n\n<p>In K&amp;R \"C programming language\" \"5.1 Pointers and Addresses\", we can get an answer for this.</p>\n\n<p>\"The unary operators * and &amp; bind more tightly than arithmetic operators\"</p>\n\n<pre><code>*ptr += 1      //Increment what ptr points to.\n</code></pre>\n\n<p>\"Unary operators like * and ++ associate <strong>right to left</strong>.\"</p>\n\n<pre><code>*ptr++        //Increment prt instead of what ptr point to.\n</code></pre>\n\n<p>//It works like *(ptr++).</p>\n\n<p>The correct way is:</p>\n\n<pre><code>(*ptr)++      //This will work.\n</code></pre>\n", "title": "Difference between *ptr += 1 and *ptr++ in C", "answer_id": 35787541, "question_id": 35306391, "tags": ["c", "pointers", "post-increment"]}, {"body": "<p>*ptr += 1 : Increment data that ptr points to.\n*ptr++ : Increment pointer that is point to next memory location instead of the data that pointer points to.</p>\n", "title": "Difference between *ptr += 1 and *ptr++ in C", "answer_id": 35857516, "question_id": 35306391, "tags": ["c", "pointers", "post-increment"]}], "question_id": 35306391}, {"body": "<p>I was looking at the source of <a href=\"https://github.com/grantjenks/sorted_containers/blob/master/sortedcontainers/sortedlist.py\">sorted_containers</a> and was surprised to see <a href=\"https://github.com/grantjenks/sorted_containers/blob/master/sortedcontainers/sortedlist.py#L72\">this line</a>:</p>\n\n<pre><code>self._load, self._twice, self._half = load, load * 2, load &gt;&gt; 1\n</code></pre>\n\n<p>Here <code>load</code> is an integer. Why use bit shift in one place, and multiplication in another? It seems reasonable that bit shifting may be faster than integral division by 2, but why not replace the multiplication by a shift as well? I benchmarked the the following cases:</p>\n\n<ol>\n<li>(times, divide)</li>\n<li>(shift, shift)</li>\n<li>(times, shift)</li>\n<li>(shift, divide)</li>\n</ol>\n\n<p>and found that #3 is consistently faster than other alternatives:</p>\n\n<pre><code># self._load, self._twice, self._half = load, load * 2, load &gt;&gt; 1\n\nimport random\nimport timeit\nimport pandas as pd\n\nx = random.randint(10 ** 3, 10 ** 6)\n\ndef test_naive():\n    a, b, c = x, 2 * x, x // 2\n\n\ndef test_shift():\n    a, b, c = x, x &lt;&lt; 1, x &gt;&gt; 1\n\n\ndef test_mixed():\n    a, b, c = x, x * 2, x &gt;&gt; 1\n\n\ndef test_mixed_swaped():\n    a, b, c = x, x &lt;&lt; 1, x // 2\n\n\ndef observe(k):\n    print(k)\n    return {\n        'naive': timeit.timeit(test_naive),\n        'shift': timeit.timeit(test_shift),\n        'mixed': timeit.timeit(test_mixed),\n        'mixed_swapped': timeit.timeit(test_mixed_swaped),\n    }\n\n\ndef get_observations():\n    return pd.DataFrame([observe(k) for k in range(100)])\n</code></pre>\n\n<p><a href=\"http://i.stack.imgur.com/mrSuI.png\"><img src=\"http://i.stack.imgur.com/mrSuI.png\" alt=\"enter image description here\"></a>\n<a href=\"http://i.stack.imgur.com/Yo4wL.png\"><img src=\"http://i.stack.imgur.com/Yo4wL.png\" alt=\"enter image description here\"></a></p>\n\n<p>The question:</p>\n\n<p>Is my test valid? If so, why is (multiply, shift) faster than (shift, shift)?</p>\n\n<p>I run Python 3.5 on Ubuntu 14.04.</p>\n\n<p><strong>Edit</strong></p>\n\n<p>Above is the original statement of the question. Dan Getz provides an excellent explanation in his answer.</p>\n\n<p>For the sake of completeness, here are sample illustrations for larger <code>x</code> when multiplication optimizations do not apply.</p>\n\n<p><a href=\"http://i.stack.imgur.com/2btSH.png\"><img src=\"http://i.stack.imgur.com/2btSH.png\" alt=\"enter image description here\"></a>\n<a href=\"http://i.stack.imgur.com/ItTVG.png\"><img src=\"http://i.stack.imgur.com/ItTVG.png\" alt=\"enter image description here\"></a></p>\n", "title": "Times two faster than bit shift?", "tags": ["python", "performance", "python-3.x", "bit-shift"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37053379/times-two-faster-than-bit-shift", "answers": [{"body": "\n\n<p>This seems to be because multiplication of small numbers is optimized in CPython 3.5, in a way that left shifts by small numbers are not. Positive left shifts always create a larger integer object to store the result, as part of the calculation, while for multiplications of the sort you used in your test, a special optimization avoids this and creates an integer object of the correct size. This can be seen in <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c\">the source code of Python's integer implementation</a>.</p>\n\n<p>Because integers in Python are arbitrary-precision, they are stored as arrays of integer \"digits\", with a limit on the number of bits per integer digit. So in the general case, operations involving integers are not single operations, but instead need to handle the case of multiple \"digits\". In <em>pyport.h</em>, this bit limit <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Include/pyport.h#l134\">is defined as</a> 30 bits on 64-bit platform, or 15 bits otherwise. (I'll just call this 30 from here on to keep the explanation simple. But note that if you were using Python compiled for 32-bit, your benchmark's result would depend on if <code>x</code> were less than 32,768 or not.)</p>\n\n<p>When an operation's inputs and outputs stay within this 30-bit limit, the operation can be handled in an optimized way instead of the general way. The beginning of the <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l3401\">integer multiplication implementation</a> is as follows:</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>static PyObject *\nlong_mul(PyLongObject *a, PyLongObject *b)\n{\n    PyLongObject *z;\n\n    CHECK_BINOP(a, b);\n\n    /* fast path for single-digit multiplication */\n    if (Py_ABS(Py_SIZE(a)) &lt;= 1 &amp;&amp; Py_ABS(Py_SIZE(b)) &lt;= 1) {\n        stwodigits v = (stwodigits)(MEDIUM_VALUE(a)) * MEDIUM_VALUE(b);\n#ifdef HAVE_LONG_LONG\n        return PyLong_FromLongLong((PY_LONG_LONG)v);\n#else\n        /* if we don't have long long then we're almost certainly\n           using 15-bit digits, so v will fit in a long.  In the\n           unlikely event that we're using 30-bit digits on a platform\n           without long long, a large v will just cause us to fall\n           through to the general multiplication code below. */\n        if (v &gt;= LONG_MIN &amp;&amp; v &lt;= LONG_MAX)\n            return PyLong_FromLong((long)v);\n#endif\n    }\n</code></pre>\n\n<p>So when multiplying two integers where each fits in a 30-bit digit, this is done as a direct multiplication by the CPython interpreter, instead of working with the integers as arrays. (<a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l19\"><code>MEDIUM_VALUE()</code></a> called on a positive integer object simply gets its first 30-bit digit.) If the result fits in a single 30-bit digit, <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l1048\"><code>PyLong_FromLongLong()</code></a> will notice this in a relatively small number of operations, and create a single-digit integer object to store it.</p>\n\n<p>In contrast, left shifts are not optimized this way, and every left shift deals with the integer being shifted as an array. In particular, if you look at the source code for <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l4101\"><code>long_lshift()</code></a>, in the case of a small but positive left shift, a 2-digit integer object is always created, if only to have its length truncated to 1 later: <em>(my comments in <code>/*** ***/</code>)</em></p>\n\n<pre class=\"lang-c prettyprint-override\"><code>static PyObject *\nlong_lshift(PyObject *v, PyObject *w)\n{\n    /*** ... ***/\n\n    wordshift = shiftby / PyLong_SHIFT;   /*** zero for small w ***/\n    remshift  = shiftby - wordshift * PyLong_SHIFT;   /*** w for small w ***/\n\n    oldsize = Py_ABS(Py_SIZE(a));   /*** 1 for small v &gt; 0 ***/\n    newsize = oldsize + wordshift;\n    if (remshift)\n        ++newsize;   /*** here newsize becomes at least 2 for w &gt; 0, v &gt; 0 ***/\n    z = _PyLong_New(newsize);\n\n    /*** ... ***/\n}\n</code></pre>\n\n<hr>\n\n<h1>Integer division</h1>\n\n<p>You didn't ask about the worse performance of integer floor division compared to right shifts, because that fit your (and my) expectations. But dividing a small positive number by another small positive number is not as optimized as small multiplications, either. Every <code>//</code> computes both the quotient <em>and</em> the remainder using the function <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l2376\"><code>long_divrem()</code></a>. This remainder is computed for a small divisor with <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l1537\">a multiplication</a>, and <a href=\"https://hg.python.org/cpython/file/580ddeccd689/Objects/longobject.c#l2406\">is stored in a newly-allocated integer object</a>, which in this situation is immediately discarded.</p>\n", "title": "Times two faster than bit shift?", "answer_id": 37054723, "question_id": 37053379, "tags": ["python", "performance", "python-3.x", "bit-shift"]}], "question_id": 37053379}, {"body": "<p>I've been curious. What are the differences between:</p>\n\n<ol>\n<li><p><code>SELECT * FROM `tablename`</code></p></li>\n<li><p><code>SELECT * FROM `tablename` WHERE 1</code></p></li>\n<li><p><code>SELECT * FROM `tablename` WHERE 1=1</code></p></li>\n</ol>\n", "title": "SELECT * FROM tablename WHERE 1", "tags": ["sql", "select-query"], "answer_count": 10, "link": "http://stackoverflow.com/questions/38813819/select-from-tablename-where-1", "answers": [{"body": "<p>2 and 3 are the same in MySQL, functionally 1 is also the same. </p>\n\n<p><code>where 1</code> is not standard so, as others have pointed out, will not work in other dialects.</p>\n\n<p>People add <code>where 1</code> or <code>where 1 = 1</code> so <code>where</code> conditions can be easily added or removed to/from a query by adding in/commenting out some \"<code>and</code> ...\" components.</p>\n\n<p>i.e.</p>\n\n<pre><code>SELECT * FROM `tablename` WHERE 1=1\n--AND Column1 = 'Value1'\nAND Column2 = 'Value2'\n</code></pre>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38813846, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>If you are asking about the differences in performances and results, there isn't any , 2 and 3 are the same <code>WHERE TRUE</code> , and they will result the same as the first one.</p>\n\n<pre><code>1 - SELECT * FROM table_name\n</code></pre>\n\n<p>Results in all the data from <code>table_name</code> (no filter)</p>\n\n<pre><code>2 - SELECT * FROM table_name WHERE 1\n</code></pre>\n\n<p>1 will be evaluated as <code>TRUE</code> , therefore - no filter - every record will be returned .</p>\n\n<pre><code>3 - SELECT * FROM table_name where 1=1\n</code></pre>\n\n<p>Same as the last one, 1=1 is a <code>TRUE</code> expression , therefore - no filter - every record will be selected.</p>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38813851, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>In 1, MySQL does not need to evaluate any WHERE conditions.</p>\n\n<p>In 2 and 3, the where condition is static and not based on the rows' values. It will be evaluated with boolean logic and always be true.</p>\n\n<p>Functionally, there is no difference. You should choose 1 for code clarity.</p>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38813874, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>As you know, all three produce the same results.  (In a boolean context, MySQL treats the integer \"1\" as true -- in fact, any number that is not \"0\" is treated as true).</p>\n\n<p>The MySQL optimizer is explicitly <a href=\"http://dev.mysql.com/doc/refman/5.7/en/where-optimizations.html\">documented</a> to remove constant conditions in the <code>WHERE</code> clause:</p>\n\n<blockquote>\n  <ul>\n  <li><p>Constant condition removal  . . .:</p>\n  \n  <p>(B>=5 AND B=5) OR (B=6 AND 5=5) OR (B=7 AND 5=6)\n  -> B=5 OR B=6</p></li>\n  </ul>\n</blockquote>\n\n<p>Hence, all three will be compiled into exactly the same code.</p>\n\n<p>They are all functionally equivalent and should have the same performance characteristics.</p>\n\n<p>That said, the first and third are standard SQL.  The second will cause some sort of boolean expression error in many databases.  So, I would advise you to avoid that (I'm not sure whether it works or not in MySQL's strict SQL mode).</p>\n\n<p>Often the third is used when constructing dynamic <code>WHERE</code> clauses.  It makes it easy to add additional conditions as <code>AND &lt;condition&gt;</code> without worrying about lingering <code>AND</code>s.</p>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38813887, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>all are the same but 2 and 3 are used to easly handle AND/OR conditions </p>\n\n<p>like</p>\n\n<pre><code>SELECT * FROM `tablename` WHERE 1=1 AND (columnname1 = 'Value' OR columnname2 = 'Value')\n</code></pre>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38813889, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>All are the same but 2 and 3 are used to create Dynamic queries for AND/OR conditions</p>\n\n<pre><code>sqlquery =\"  SELECT * FROM `tablename` where 1 =1 \"\n</code></pre>\n\n<p>we use  2 and 3 format to make dynamic query so we already know \"where\" keyword  is added and we keep adding  more filters .\nLike </p>\n\n<pre><code>sqlquery  = sqlquery + \"and columna =a\"\n\"AND columna =a \" then\n</code></pre>\n\n<p>after few lines if we have new filters we add  \"AND coulmnb =b \" and so on </p>\n\n<p>You don't have to check the sql query for where keyword as its placed in first or initial query </p>\n\n<pre><code>SELECT * FROM `tablename` WHERE 1=1 AND (columnname1 = 'Value' OR columnname2 = 'Value')\n</code></pre>\n\n<p>Otherwise  we can write <code>sqlquery = \"SELECT * FROM tablename\"</code></p>\n\n<p>then </p>\n\n<p>if there is no 'where' clause in <code>sqlquery</code>  then </p>\n\n<pre><code>sqlquery  = sqlquery + \"where columna =a\"\n</code></pre>\n\n<p>else</p>\n\n<pre><code>sqlquery  = sqlquery + \"and columna =a\"\n</code></pre>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38835035, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>They all output the same answer. However the way 2 and 3 are written is mostly is in order to have control of the \"Where\" statement so it would make it easier to add it or remove it later. </p>\n\n<p>I think that the first and third way are the proper way of writing it. If you need a where statement you do like in number 3 otherwise number 1 would be good enough. </p>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38838211, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>In MS SQL 1 and 3 are same , however, option 2 will not work , option 2 is an invalid statement as in MS SQL, WHERE is used to compare some values. For Example: </p>\n\n<ol>\n<li>Select * from 'myTable where ID = 3 (valid)</li>\n<li>Select * from 'myTable where 1 = 1 is same as Select * from 'myTable where 2= 2 is same as Select * from 'myTable where 3= 3 you get the idea (valid) is same as Select * From 'myTable'</li>\n</ol>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38885630, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p>1 - SELECT * FROM table_name\nit will give you all the records of the table with running any where statement.</p>\n\n<p>2 - SELECT * FROM table_name WHERE 1\nthis where condition is always true, its mostly used by hacker to get into any system. If you heard about sql injections than 2 &amp; 3 are scenarios which are forced to build by hacker to get all the records of table.</p>\n\n<p>3 - SELECT * FROM table_name where 1=1\nThis will give you all the records of the table but it will compare the where statement and then move forward, it's basically added to add or removed more statements after that.</p>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 38977934, "question_id": 38813819, "tags": ["sql", "select-query"]}, {"body": "<p><strong>Result</strong> - Gives all the records in the table specified instead of tablename for all three queries</p>\n\n<p><code>SELECT * FROM tablename WHERE 1</code> - Check this <a href=\"http://stackoverflow.com/a/3720750/6426617\">answer</a> </p>\n\n<p><code>SELECT * FROM tablename WHERE 1=1</code> - Check this <a href=\"http://stackoverflow.com/a/242831/6426617\">answer</a> </p>\n\n<p>For more Info about WHERE clause optimizations check these : <a href=\"http://dev.mysql.com/doc/refman/5.7/en/where-optimizations.html\" rel=\"nofollow\">MYSQL</a>, <a href=\"https://www.sqlite.org/optoverview.html\" rel=\"nofollow\">SQLite</a>, <a href=\"http://oracle.readthedocs.io/en/latest/sql/indexes/where-clause.html\" rel=\"nofollow\">SQL</a></p>\n", "title": "SELECT * FROM tablename WHERE 1", "answer_id": 39131093, "question_id": 38813819, "tags": ["sql", "select-query"]}], "question_id": 38813819}, {"body": "<p>I have recently updated my Android Studio to 2.1.1 and I noticed my layout previews weren't working anymore, so when I checked it showed a message saying:</p>\n\n<blockquote>\n  <p>Rendering Problems</p>\n  \n  <p>Exception raised during rendering: com/android/util/PropertiesMap (Details)</p>\n</blockquote>\n\n<p>When I clicked on details a message window popped up dislaying this error:</p>\n\n<pre><code>java.lang.NoClassDefFoundError: com/android/util/PropertiesMap\nat com.android.layoutlib.bridge.android.BridgeContext.createStyleBasedTypedArray(BridgeContext.java:940)\nat com.android.layoutlib.bridge.android.BridgeContext.obtainStyledAttributes(BridgeContext.java:638)\nat android.content.res.Resources_Theme_Delegate.obtainStyledAttributes(Resources_Theme_Delegate.java:71)\nat android.content.res.Resources$Theme.obtainStyledAttributes(Resources.java:1436)\nat android.widget.TextView.&lt;init&gt;(TextView.java:761)\nat android.widget.TextView.&lt;init&gt;(TextView.java:704)\nat android.widget.TextView.&lt;init&gt;(TextView.java:700)\nat com.android.layoutlib.bridge.MockView.&lt;init&gt;(MockView.java:50)\nat com.android.layoutlib.bridge.MockView.&lt;init&gt;(MockView.java:45)\nat com.android.layoutlib.bridge.MockView.&lt;init&gt;(MockView.java:41)\nat android.view.BridgeInflater.createViewFromTag(BridgeInflater.java:163)\nat android.view.LayoutInflater.createViewFromTag(LayoutInflater.java:727)\nat android.view.LayoutInflater.rInflate_Original(LayoutInflater.java:858)\nat android.view.LayoutInflater_Delegate.rInflate(LayoutInflater_Delegate.java:70)\nat android.view.LayoutInflater.rInflate(LayoutInflater.java:834)\nat android.view.LayoutInflater.inflate(LayoutInflater.java:492)\nat com.android.layoutlib.bridge.bars.CustomBar.&lt;init&gt;(CustomBar.java:95)\nat com.android.layoutlib.bridge.bars.StatusBar.&lt;init&gt;(StatusBar.java:67)\nat com.android.layoutlib.bridge.impl.Layout.createStatusBar(Layout.java:222)\nat com.android.layoutlib.bridge.impl.Layout.&lt;init&gt;(Layout.java:144)\nat com.android.layoutlib.bridge.impl.RenderSessionImpl.inflate(RenderSessionImpl.java:297)\nat com.android.layoutlib.bridge.Bridge.createSession(Bridge.java:429)\nat com.android.ide.common.rendering.LayoutLibrary.createSession(LayoutLibrary.java:350)\nat com.android.tools.idea.rendering.RenderTask$2.compute(RenderTask.java:520)\nat com.android.tools.idea.rendering.RenderTask$2.compute(RenderTask.java:508)\nat com.intellij.openapi.application.impl.ApplicationImpl.runReadAction(ApplicationImpl.java:967)\nat com.android.tools.idea.rendering.RenderTask.createRenderSession(RenderTask.java:508)\nat com.android.tools.idea.rendering.RenderTask.access$600(RenderTask.java:75)\nat com.android.tools.idea.rendering.RenderTask$3.call(RenderTask.java:620)\nat com.android.tools.idea.rendering.RenderTask$3.call(RenderTask.java:617)\nat com.android.tools.idea.rendering.RenderService.runRenderAction(RenderService.java:371)\nat com.android.tools.idea.rendering.RenderTask.render(RenderTask.java:617)\nat com.android.tools.idea.rendering.RenderTask.render(RenderTask.java:639)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager.doRender(AndroidLayoutPreviewToolWindowManager.java:654)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager.access$1700(AndroidLayoutPreviewToolWindowManager.java:82)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager$7$1.run(AndroidLayoutPreviewToolWindowManager.java:596)\nat com.intellij.openapi.progress.impl.CoreProgressManager$2.run(CoreProgressManager.java:142)\nat com.intellij.openapi.progress.impl.CoreProgressManager.registerIndicatorAndRun(CoreProgressManager.java:446)\nat com.intellij.openapi.progress.impl.CoreProgressManager.executeProcessUnderProgress(CoreProgressManager.java:392)\nat com.intellij.openapi.progress.impl.ProgressManagerImpl.executeProcessUnderProgress(ProgressManagerImpl.java:54)\nat com.intellij.openapi.progress.impl.CoreProgressManager.runProcess(CoreProgressManager.java:127)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager$7.run(AndroidLayoutPreviewToolWindowManager.java:591)\nat com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:337)\nat com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:327)\nat com.intellij.util.ui.update.MergingUpdateQueue$3.run(MergingUpdateQueue.java:271)\nat com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:286)\nat com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:244)\nat com.intellij.util.ui.update.MergingUpdateQueue.run(MergingUpdateQueue.java:234)\nat com.intellij.util.concurrency.QueueProcessor.runSafely(QueueProcessor.java:238)\nat com.intellij.util.Alarm$Request$1.run(Alarm.java:352)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\n</code></pre>\n\n<p>Does anyone know how to solve this?</p>\n", "title": "Rendering Problems Exception raised during rendering: com/android/util/PropertiesMap", "tags": ["android", "android-studio-2.1"], "answer_count": 1, "link": "http://stackoverflow.com/questions/37318561/rendering-problems-exception-raised-during-rendering-com-android-util-propertie", "answers": [{"body": "<p>I had the same problem,and you can change your version of android to render the layouts in the preview window .</p>\n\n<p>See <a href=\"http://stackoverflow.com/questions/35914565/android-studio-rendering-problems-exception-raised-during-rendering-com-androi\">this</a> post:</p>\n\n<blockquote>\n  <p><a href=\"http://i.stack.imgur.com/aD9m2.png\"><img src=\"http://i.stack.imgur.com/aD9m2.png\" alt=\"enter image description here\"></a></p>\n  \n  <p>change the android version used to render layouts from Android Studio\n  back to \"API 23: Android 6.0</p>\n</blockquote>\n", "title": "Rendering Problems Exception raised during rendering: com/android/util/PropertiesMap", "answer_id": 37319338, "question_id": 37318561, "tags": ["android", "android-studio-2.1"]}], "question_id": 37318561}, {"body": "<p>For some reason I was sneaking into the .NET Framework source for the class <a href=\"http://referencesource.microsoft.com/#mscorlib/system/double.cs,159\" rel=\"nofollow\"><code>Double</code></a> and found out that the declaration of <code>==</code> is:</p>\n\n<pre><code>public static bool operator ==(Double left, Double right) {\n    return left == right;\n}\n</code></pre>\n\n<p>The same logic applies for <strong>every</strong> operator.</p>\n\n<hr>\n\n<ul>\n<li>What's the point of such a definition? </li>\n<li>How does it work?</li>\n<li>Why doesn't it create an infinite recursion?</li>\n</ul>\n", "title": "Definition of &quot;==&quot; operator for Double", "tags": ["c#", ".net", "language-lawyer"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35133777/definition-of-operator-for-double", "answers": [{"body": "<p>I took a look at the <a href=\"http://en.wikipedia.org/wiki/Common_Intermediate_Language\" rel=\"nofollow\">CIL</a> with JustDecompile. The inner <code>==</code> gets translated to the CIL <a href=\"https://msdn.microsoft.com/en-us/library/system.reflection.emit.opcodes.ceq.aspx\" rel=\"nofollow\">ceq</a> op code. In other words, it's primitive CLR equality.</p>\n\n<p>I was curious to see if the C# compiler would reference <code>ceq</code> or the <code>==</code> operator when comparing two double values. In the trivial example I came up with (below), it used <code>ceq</code>.</p>\n\n<p>This program:</p>\n\n<pre><code>void Main()\n{\n    double x = 1;\n    double y = 2;\n\n    if (x == y)\n        Console.WriteLine(\"Something bad happened!\");\n    else\n        Console.WriteLine(\"All is right with the world\");\n}\n</code></pre>\n\n<p>generates the following CIL (note the statement with label <code>IL_0017</code>):</p>\n\n<pre><code>IL_0000:  nop\nIL_0001:  ldc.r8      00 00 00 00 00 00 F0 3F\nIL_000A:  stloc.0     // x\nIL_000B:  ldc.r8      00 00 00 00 00 00 00 40\nIL_0014:  stloc.1     // y\nIL_0015:  ldloc.0     // x\nIL_0016:  ldloc.1     // y\nIL_0017:  ceq\nIL_0019:  stloc.2\nIL_001A:  ldloc.2\nIL_001B:  brfalse.s   IL_002A\nIL_001D:  ldstr       \"Something bad happened!\"\nIL_0022:  call        System.Console.WriteLine\nIL_0027:  nop\nIL_0028:  br.s        IL_0035\nIL_002A:  ldstr       \"All is right with the world\"\nIL_002F:  call        System.Console.WriteLine\nIL_0034:  nop\nIL_0035:  ret\n</code></pre>\n", "title": "Definition of &quot;==&quot; operator for Double", "answer_id": 35134123, "question_id": 35133777, "tags": ["c#", ".net", "language-lawyer"]}, {"body": "<p>In reality, the compiler will turn the <code>==</code> operator into a <code>ceq</code> IL code, and the operator you mention will not be called.</p>\n\n<p>The reason for the operator in the source code is likely so it can be called from languages other than C# that do not translate it into a <code>CEQ</code> call directly (or through reflection).  The code <em>within</em> the operator <em>will</em> be compiled to a <code>CEQ</code>, so there is no infinite recursion.</p>\n\n<p>In fact, if you call the operator via reflection, you can see that the operator is called (rather than a <code>CEQ</code> instruction), and obviously is not infinitely recursive (since the program terminates as expected):</p>\n\n<pre><code>double d1 = 1.1;\ndouble d2 = 2.2;\n\nMethodInfo mi = typeof(Double).GetMethod(\"op_Equality\", BindingFlags.Static | BindingFlags.Public );\n\nbool b = (bool)(mi.Invoke(null, new object[] {d1,d2}));\n</code></pre>\n\n<p>Resulting IL (compiled by LinqPad 4):</p>\n\n<pre><code>IL_0000:  nop         \nIL_0001:  ldc.r8      9A 99 99 99 99 99 F1 3F \nIL_000A:  stloc.0     // d1\nIL_000B:  ldc.r8      9A 99 99 99 99 99 01 40 \nIL_0014:  stloc.1     // d2\nIL_0015:  ldtoken     System.Double\nIL_001A:  call        System.Type.GetTypeFromHandle\nIL_001F:  ldstr       \"op_Equality\"\nIL_0024:  ldc.i4.s    18 \nIL_0026:  call        System.Type.GetMethod\nIL_002B:  stloc.2     // mi\nIL_002C:  ldloc.2     // mi\nIL_002D:  ldnull      \nIL_002E:  ldc.i4.2    \nIL_002F:  newarr      System.Object\nIL_0034:  stloc.s     04 // CS$0$0000\nIL_0036:  ldloc.s     04 // CS$0$0000\nIL_0038:  ldc.i4.0    \nIL_0039:  ldloc.0     // d1\nIL_003A:  box         System.Double\nIL_003F:  stelem.ref  \nIL_0040:  ldloc.s     04 // CS$0$0000\nIL_0042:  ldc.i4.1    \nIL_0043:  ldloc.1     // d2\nIL_0044:  box         System.Double\nIL_0049:  stelem.ref  \nIL_004A:  ldloc.s     04 // CS$0$0000\nIL_004C:  callvirt    System.Reflection.MethodBase.Invoke\nIL_0051:  unbox.any   System.Boolean\nIL_0056:  stloc.3     // b\nIL_0057:  ret \n</code></pre>\n\n<p>Interestingly - the same operators do NOT exist (either in the reference source or via reflection) for integral types, only <code>Single</code>, <code>Double</code>, <code>Decimal</code>, <code>String</code>, and <code>DateTime</code>, which disproves my theory that they exist to be called from other languages.  Obviously you can equate two integers in other languages without these operators, so we're back to the question \"why do they exist for <code>double</code>\"?</p>\n", "title": "Definition of &quot;==&quot; operator for Double", "answer_id": 35134124, "question_id": 35133777, "tags": ["c#", ".net", "language-lawyer"]}, {"body": "<p>The source of the primitive types can be confusing. Have you seen the very first line of the <code>Double</code> struct?</p>\n\n<p>Normally you cannot define a recursive struct like this:</p>\n\n<pre><code>public struct Double : IComparable, IFormattable, IConvertible\n        , IComparable&lt;Double&gt;, IEquatable&lt;Double&gt;\n{\n    internal double m_value; // Self-recursion with endless loop?\n    // ...\n}\n</code></pre>\n\n<p>Primitive types have their native support in CIL as well. Normally they are not treated like object-oriented types. A double is just a 64-bit value if it is used as <code>float64</code> in CIL. However, if it is handled as a usual .NET type, it contains an actual value and it contains methods like any other types.</p>\n\n<p>So what you see here is the same situation for operators. Normally if you use the double type type directly, it will never be called. BTW, its source looks like this in CIL:</p>\n\n<pre><code>.method public hidebysig specialname static bool op_Equality(float64 left, float64 right) cil managed\n{\n    .custom instance void System.Runtime.Versioning.NonVersionableAttribute::.ctor()\n    .custom instance void __DynamicallyInvokableAttribute::.ctor()\n    .maxstack 8\n    L_0000: ldarg.0\n    L_0001: ldarg.1\n    L_0002: ceq\n    L_0004: ret\n}\n</code></pre>\n\n<p>As you can see, there is no endless loop (the <code>ceq</code> instrument is used instead of calling the <code>System.Double::op_Equality</code>). So when a double is treated like an object, the operator method will be called, which will eventually handle it as the <code>float64</code> primitive type on the CIL level.</p>\n", "title": "Definition of &quot;==&quot; operator for Double", "answer_id": 35134278, "question_id": 35133777, "tags": ["c#", ".net", "language-lawyer"]}, {"body": "<p>The main confusion here is that you're assuming that all .NET libraries (in this case, the Extended Numerics Library, which is <em>not</em> a part of the BCL) are written in standard C#. This isn't always the case, and different languages have different rules.</p>\n\n<p>In standard C#, the piece of code you're seeing would result in a stack overflow, due to the way operator overload resolution works. However, the code isn't actually in standard C# - it basically uses undocumented features of the C# compiler. Instead of calling the operator, it emits this code:</p>\n\n<pre><code>ldarg.0\nldarg.1\nceq\nret\n</code></pre>\n\n<p>That's it :) There is no 100% equivalent C# code - this simply isn't possible in C# with <em>your own</em> type.</p>\n\n<p>Even then, the actual operator isn't used when compiling C# code - the compiler does a bunch of optimizations, like in this case, where it replaces the <code>op_Equality</code> call with just the simple <code>ceq</code>. Again, you can't replicate this in your own <code>DoubleEx</code> struct - it's compiler magic.</p>\n\n<p>This certainly isn't a unique situation in .NET - there's plenty of code that isn't valid, standard C#. The reasons are usually (a) compiler hacks and (b) a different language, with the odd (c) runtime hacks (I'm looking at you, <code>Nullable</code>!).</p>\n\n<p>Since the Roslyn C# compiler is oepn source, I can actually point you at the place where overload resolution is decided: </p>\n\n<p><a href=\"https://github.com/dotnet/roslyn/blob/a4e375b95953e471660e9686a46893c97db70b0e/src/Compilers/CSharp/Portable/Binder/Semantics/Operators/BinaryOperatorOverloadResolution.cs\">The place where all binary operators are resolved</a></p>\n\n<p><a href=\"https://github.com/dotnet/roslyn/blob/a4e375b95953e471660e9686a46893c97db70b0e/src/Compilers/CSharp/Portable/Binder/Semantics/Operators/BinaryOperatorEasyOut.cs\">The \"shortcuts\" for intrinsic operators</a></p>\n\n<p>When you look at the shortcuts, you'll see that equality between double and double results in the intrinsic double operator, <em>never</em> in the  actual <code>==</code> operator defined on the type. The .NET type system has to pretend that <code>Double</code> is a type like any other, but C# doesn't - <code>double</code> is a primitive in C#.</p>\n", "title": "Definition of &quot;==&quot; operator for Double", "answer_id": 35135611, "question_id": 35133777, "tags": ["c#", ".net", "language-lawyer"]}, {"body": "<p>As indicated in the Microsoft documentation for the System.Runtime.Versioning Namespace:The types found in this namespace are intended for use within the .NET Framework and not for user applications.The System.Runtime.Versioning namespace contains advanced types that support versioning in side by side implementations of the .NET Framework.</p>\n", "title": "Definition of &quot;==&quot; operator for Double", "answer_id": 35172829, "question_id": 35133777, "tags": ["c#", ".net", "language-lawyer"]}], "question_id": 35133777}, {"body": "<p><strong>Overview:</strong></p>\n\n<p>My company has developed a rate-limited API. Our goal is twofold:</p>\n\n<ul>\n<li>A: Create a strong developer ecosystem around our product. </li>\n<li>B: Demonstrate the power of our API by using it to drive our own application.</li>\n</ul>\n\n<p><em>Clarification: Why rate-limit at all?</em></p>\n\n<p><em>We rate limit our API, because we sell it as an addition to our product. Anonymous access to our API has a very low threshold for API calls per hour, whereas our paid customers are permitted upwards of 1000 calls per hour or more.</em></p>\n\n<p><strong>The Problem:</strong></p>\n\n<p>Our rate-limited API is great for the developer eco-system, but in order for us to dogfood it we can't allow it to be restricted to the same rate-limiting. The front end of our API is all JavaScript, making direct Ajax calls to the API. </p>\n\n<p>So the question is:</p>\n\n<blockquote>\n  <p><em>How do you secure an api so that rate-limiting can be removed where in the process in removing such rate-limiting can't be easily spoofed?</em></p>\n</blockquote>\n\n<p><strong>Explored Solutions (and why they didn't work)</strong></p>\n\n<ol>\n<li><p>Verify the referrer against the host header. -- <em>Flawed because the <strong>referrer</strong> is easily faked.</em></p></li>\n<li><p>Use an <a href=\"https://en.wikipedia.org/wiki/Hash-based_message_authentication_code\" rel=\"nofollow\">HMAC</a> to create a signature based off the request and a shared secret, then verify the request on the server. -- <em>Flawed because the secret and algorithm would be easily determined by looking into the front end JavaScript.</em></p></li>\n<li><p>Proxy the request and sign the request in the proxy -- <em>Still flawed, as the proxy itself exposes the API.</em></p></li>\n</ol>\n\n<p><strong>The Question:</strong></p>\n\n<p>I am looking to the brilliant minds on Stack&nbsp;Overflow to present alternate solutions. How would you solve this problem?</p>\n", "title": "Dogfooding our own rate-limited API", "tags": ["javascript", "api", "rest"], "answer_count": 9, "link": "http://stackoverflow.com/questions/34939229/dogfooding-our-own-rate-limited-api", "answers": [{"body": "<p>Unfortunately, there is no perfect solution to this. </p>\n\n<p>The general approach is typically to provide a <em>spoofable</em> way for clients to identify themselves (e.g. an identifier, version, and API key -- for example), for clients to register information about themselves that can be used to limit access (e.g. the client is a server in a given IP address range, so only allow callers in that range; e.g. the client is JavaScript, but delivered only to a specific category of browser, so only allow access to HTTP requests that specify certain user agent strings; etc.), and then to use machine learning/pattern recognition to detect anomalous usage that is likely a spoofed client and then to reject traffic from these spoofed clients (or confirm with clients that these usages are indeed not coming from the legitimate client, replace their spoofable credentials, and then disallow further traffic using the older spoofed credentials). </p>\n\n<p>You can make it slightly more difficult to spoof by using multiple layers of key. For example, you give out a longer-lived credential that lives on a server (and that can only be used in a limited set of IP address ranges) to make an API call that records information about the client (e.g. the user agent) and returns a shorter-lived client-side key that is syndicated in JavaScript for use on the client for client-side API requests. This, too, is imperfect (a spoofer could issue the same server call to get the credential), but it will be more difficult if the returned API key is included in obfuscated (and frequently changing) JavaScript or HTML (which would make it difficult to reliably extract from the response). That also provides a way to more easily detect spoofing; the client-side key is now tied to a particular client (e.g. specific user agent, perhaps even a specific cookie jar) that makes reuse in another client easy to detect and the expiration also limits the duration in which the spoofed key may be reused.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34939478, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>Since your own JavaScript client is accessing the API directly, anyone's going to be able to look at what it's doing and mimic it, including use the same API key. You can try to make it more difficult, like by obfuscating your code or putting various hurdles in the way, but you and the person you're trying to restrain have fundamentally the same access. Instead of trying to create a difference in privileges, you'll need to construct a system where it's totally OK that the unofficial client uses all the access in its scope, but the system is arranged in such a way that official use across all clients is greater.</p>\n\n<p>This is often done with per-user access tokens, as opposed to one token for the entire application. Each token's limit should be plenty for typical use of your API, but restrictive for someone trying to abuse it. For example, 100 calls per minute might be more than enough to support typical browsing, but if I want to scrape you, I can't do it effectively on that budget.</p>\n\n<p>There will always be an arms race - I can get around the limit by creating lots of bot user accounts. That, though, is a pretty solved problem if you just add a captcha to your signup flow, at a tiny bit of expense to the real human. When you get into these scenarios, everything's just a tradeoff between convenience and restriction. You'll never find something totally bulletproof, so focus on making it good enough and wait until someone exploits you to learn where the holes were.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34939633, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>Can you stand up a separate instance of the UI and throttle-free API, and then restrict access to IP addresses coming from your organisation?</p>\n\n<p>E.g., deploy the whole thing behind your corporate firewall, and attach the application to the same database as the public-facing instance if you need to share data between instances.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34939771, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<ul>\n<li>Whitelist source IP addresses</li>\n<li>Use a <a href=\"http://en.wikipedia.org/wiki/Virtual_private_network\" rel=\"nofollow\">VPN</a>, whitelist VPN members</li>\n<li>Proxy solution or browser addon that adds HTTP headers should be fine if you can secure the proxy and aren't concerned about <a href=\"https://en.wikipedia.org/wiki/Man-in-the-middle_attack\" rel=\"nofollow\">MITM</a> attacks sniffing the traffic</li>\n<li>Any solution involving secrets can mitigate the impact of leaks by rotating secrets on a daily basis</li>\n</ul>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34939875, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>Assuming the app in question must be publicly open, you don\u2019t have much choice:</p>\n\n<p><strong>Pick another way to demonstrate the power of your API.</strong> For example, write such an app and share its source, but don\u2019t actually run that code. Make sure it\u2019s well-documented though, so that anyone can deploy it and see it working (subject to throttling).</p>\n\n<p>The app you run would need to be refactored to avoid client-side API requests and be more server-rendered. You can still dogfood your API, but not in an obvious way\u2014make secure requests to throttle-free API from the server side.</p>\n\n<p><strong>Adjust rate limitation</strong> to allow for your app to work and invest into performance optimization to handle the load.</p>\n\n<p>And yeah, have the core API throttle-free in the first place, and keep it inside a private network. Throttle in a separate publicly accessible layer.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34940147, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>If this is causing you a problem, it will cause your putative ecosystem of developers a problem (e.g. when they try to develop an alternative UI). If you are really eating your own dog food, make the API (and the rate limiting) work for your application. Here are some suggestions:</p>\n\n<ul>\n<li><p>Do not rate limit by IP address. Rather, rate limit by something associated with the user, e.g. their user ID. Apply the rate limit at the authentication stage.</p></li>\n<li><p>Design your API so that users do not need to call it continuously (e.g. give a list call that returns many results, rather than a repeated call that returns one item each time)</p></li>\n<li><p>Design your web app with the same constraints you expect your developer ecosystem to have, i.e. ensure you can design it within reasonable throttling rates.</p></li>\n<li><p>Ensure your back end is scalable (horizontally preferably) so you don't need to impose throttling at levels so low it actually causes a problem to a UI.</p></li>\n<li><p>Ensure your throttling has the ability to cope with bursts, as well as limiting longer term abuse.</p></li>\n<li><p>Ensure your throttling performs sensible actions tailored to the abuse you are seeking to remove. For instance, consider queuing or delaying mild abusers rather than refusing the connection. Most web front ends will only open four simultaneous connections at once. If you delay an attempt to open a fifth you'll only hit the case where they are using a CLI at the same time as the web client (ot two web clients). If you delay the n-th API call without a gap rather than failing it, the end user will see things slow down rather than break. If you combine this with only queuing N API calls at once, you will only hit people who are parallelising large numbers of API calls, which is probably not the behaviour you want - e.g. 100 simultaneous API calls then a gap for an hour is normally far worse than 100 sequential API calls over an hour.</p></li>\n</ul>\n\n<p>Did this not answer your question? Well, if you really <em>need</em> to do what you are asking, rate-limit at the authentication stage and apply a different rate limit based on the group your user fits into. If you are using one set of credentials (used by your devs and QA team), you get a higher rate limit. But you can immediately see why this will inevitably lead you to your ecosystem seeing issues that your dev and QA team do not see.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34945053, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>You could try to generate a unique session ID, bound to a certain IP address/user and limited time to live. When a user downloads your application frontend JavaScript code inject the generated session ID into the JavaScript source code. The session ID will be attached to every request to your API and the rate-limit is lifted.</p>\n\n<p>The ID cannot be simply copied for spoofing, because it is only valid for a single IP address, user and limited amount of time. So an adversary would have to call your page and filter out the key from your JavaScript source or from intercepting the Ajax request every time a new user wants to use it.</p>\n\n<p><strong>Another Option:</strong></p>\n\n<p>Set up a proxy for your own application and use obfuscation. The Ajax requests to the proxy use different names from the real API-calls and the proxy translates them back. So your application would not call <code>getDocument</code> on your real API, but it will call <code>getFELSUFDSKJE</code> on your proxy. The proxy will translate this call back to getDocument and forward it to actual rate-limited API.</p>\n\n<p>Your actual API will not rate-limit requests by the proxy.</p>\n\n<p>And so that other people don't use your proxy for their own application you change the obfuscation scheme daily. The obfuscated call-names can be generated automatically in you JavaScript source code and configured in the proxy.</p>\n\n<p>A client wishing to use this, would also need to keep up with your changing obfuscation to use your proxy. And you can still use referrer-headers and similar for logging, so you can find people using your proxy. Or catch them when changing the obfuscation scheme.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34948888, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>Setup multiple accounts, and pick one of them at random at every request, or change which one you use every hour or so. This way you can distribute the load over <code>n</code> accounts, giving you up to <code>n</code> times higher limits. </p>\n\n<p>Be careful about accidentally shutting yourself down if you are trying to find other users doing this, if it's not allowed for customers. </p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34955506, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}, {"body": "<p>Buy your product. Become a paid customer of yourself.</p>\n\n<p>\"Anonymous access to our API has a very low threshold for API calls per hour, whereas our paid customers are permitted upwards of 1000 calls per hour or more.\"</p>\n\n<p>This also helps test the system from a customer's perspective.</p>\n", "title": "Dogfooding our own rate-limited API", "answer_id": 34972076, "question_id": 34939229, "tags": ["javascript", "api", "rest"]}], "question_id": 34939229}, {"body": "<p>After I uploaded my build to iTunes Connect by using Application Loader, I got an email about an issue \"Unexpected Machine Code\" see below.</p>\n\n<blockquote>\n  <p>Unexpected Machine Code - Your upload contains both bitcode and native\n  machine code. When you provide bitcode, it's not necessary to include\n  machine code as well. To reduce the size of your upload, use Xcode 7.3\n  or later, or any other toolchain that removes machine code.</p>\n</blockquote>\n\n<p>I am so confused about this warning. Is this a new thing in iTunes Connect? Why it doesn't give me the warning when I upload QA build but Release build?</p>\n\n<p>Update: I upload QA build last week, it wasn't any warning. And today (June 27th 2016) I upload Release build, it send me the warning.</p>\n", "title": "&quot;Unexpected Machine Code&quot; warning from iTunes Connect", "tags": ["ios", "itunesconnect"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38061045/unexpected-machine-code-warning-from-itunes-connect", "answers": [{"body": "<p>Just got this as well, it's a bug. I uploaded the near exact same binary as I did a couple of days ago which went through fine (only change was to the version and build number). Developer support confirmed this.</p>\n\n<p>If you do get the e-mail, don't worry about it. Your binary will still process and you'll still be able to submit it to the app store.</p>\n\n<p>Update: both of my binaries which received this message passed review and have been released to the app store.</p>\n", "title": "&quot;Unexpected Machine Code&quot; warning from iTunes Connect", "answer_id": 38061592, "question_id": 38061045, "tags": ["ios", "itunesconnect"]}], "question_id": 38061045}, {"body": "<p>Following the question <a href=\"http://stackoverflow.com/questions/38407760/extending-string-prototype-performance-shows-that-function-calls-are-10x-faster\">Extending String.prototype performance</a> I am really intrigued, because just adding <code>\"use strict\"</code> to a <code>String.prototype</code> method improved performance 10 times. The <a href=\"http://stackoverflow.com/a/38410199/1968972\">explanation</a> by <a href=\"http://stackoverflow.com/users/1048572/bergi\">bergi</a> is short and does not explain it to me. Why there is such a dramatic difference between two almost identical methods, that only differ in <code>\"use strict\"</code> at the top? Can you explain in more detail and with the theory behind this?</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>String.prototype.count = function(char) {\r\n  var n = 0;\r\n  for (var i = 0; i &lt; this.length; i++)\r\n    if (this[i] == char) n++;\r\n  return n;\r\n};\r\n\r\nString.prototype.count_strict = function(char) {\r\n  \"use strict\";\r\n  var n = 0;\r\n  for (var i = 0; i &lt; this.length; i++)\r\n    if (this[i] == char) n++;\r\n  return n;\r\n};\r\n// Here is how I measued speed, using Node.js 6.1.0\r\n\r\nvar STR = '0110101110010110100111010011101010101111110001010110010101011101101010101010111111000';\r\nvar REP = 1e4;\r\n\r\nconsole.time('proto');\r\nfor (var i = 0; i &lt; REP; i++) STR.count('1');\r\nconsole.timeEnd('proto');\r\n\r\nconsole.time('proto-strict');\r\nfor (var i = 0; i &lt; REP; i++) STR.count_strict('1');\r\nconsole.timeEnd('proto-strict');</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>Result:</p>\n\n<pre><code>proto: 101 ms\nproto-strict: 7.5 ms\n</code></pre>\n", "title": "Why &quot;use strict&quot; improves performance 10x in this example?", "tags": ["javascript", "performance"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38411552/why-use-strict-improves-performance-10x-in-this-example", "answers": [{"body": "<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Strict_mode#Securing_JavaScript\">In strict mode, the <code>this</code> context is not forced to be an object.</a> If you call a function on a non-object, <code>this</code> will just be that non-object.</p>\n\n<p>In contrast, in non-strict mode, the <code>this</code> context is always first wrapped in an object if it's not already an object. For example, <code>(42).toString()</code> first wraps <code>42</code> in a <code>Number</code> object and then calls <code>Number.prototype.toString</code> with the <code>Number</code> object as <code>this</code> context. In strict mode, the <code>this</code> context is left untouched and just calls <code>Number.prototype.toString</code> with <code>42</code> as <code>this</code> context.</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>(function() {\r\n  console.log(typeof this);\r\n}).call(42); // 'object'\r\n\r\n(function() {\r\n  'use strict';\r\n  console.log(typeof this);\r\n}).call(42); // 'number'</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>In your case, the non-strict mode version spends a lot of time wrapping and unwrapping primitive <code>string</code>s into <code>String</code> object wrappers and back. The strict mode version on the other hand directly works on the primitive <code>string</code>, which improves performance.</p>\n", "title": "Why &quot;use strict&quot; improves performance 10x in this example?", "answer_id": 38411722, "question_id": 38411552, "tags": ["javascript", "performance"]}], "question_id": 38411552}, {"body": "<p>\nOur organization has a <strong>required</strong> coding rule (without any explanation) that: </p>\n\n<blockquote>\n  <p>if \u2026 else if constructs should be terminated with an else clause</p>\n</blockquote>\n\n<p><strong>Example 1:</strong></p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>if ( x &lt; 0 )\n{\n   x = 0;\n} /* else not needed */\n</code></pre>\n\n<p><strong>Example 2:</strong></p>\n\n<pre class=\"lang-cpp prettyprint-override\"><code>if ( x &lt; 0 )\n{\n    x = 0;\n}\nelse if ( y &lt; 0 )\n{\n    x = 3;\n}\nelse    /* this else clause is required, even if the */\n{       /* programmer expects this will never be reached */\n        /* no change in value of x */\n}\n</code></pre>\n\n<p>What edge case is this designed to handle? </p>\n\n<p>What also concerns me about the reason is that <strong>Example 1</strong> does not need an <code>else</code> but <strong>Example 2</strong> does. If the reason is re-usability and extensibility, I think <code>else</code> should be used in both cases.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "tags": ["c", "misra"], "answer_count": 12, "link": "http://stackoverflow.com/questions/35053371/what-is-the-benefit-of-terminating-if-else-if-constructs-with-an-else-clause", "answers": [{"body": "<p>Your company followed MISRA coding guidance. There are a few versions of these guidelines that contain this rule, but from <a href=\"http://www.misra.org.uk/Publications/tabid/57/Default.aspx#label-c2\">MISRA-C:2004</a><sup>\u2020</sup>:</p>\n\n<blockquote>\n  <p>Rule 14.10 (required): All if \u2026 else if constructs shall be terminated\n  with an else clause.</p>\n  \n  <p>This rule applies whenever an if statement is followed by one or more\n  else if statements; the final else <code>if</code> shall be followed by an <code>else</code>\n  statement. In the case of a simple <code>if</code> statement then the <code>else</code>\n  statement need not be included. The requirement for a final <code>else</code>\n  statement is defensive programming. The <code>else</code> statement shall either\n  take appropriate action or contain a suitable comment as to why no\n  action is taken.  This is consistent with the requirement to have a\n  final <code>default</code> clause in a <code>switch</code> statement. For example this code\n  is a simple if statement:</p>\n\n<pre><code>if ( x &lt; 0 )\n{\n log_error(3);\n x = 0;\n} /* else not needed */\n</code></pre>\n  \n  <p>whereas the following code demonstrates an <code>if</code>, <code>else if</code> construct</p>\n\n<pre><code>if ( x &lt; 0 )\n{\n log_error(3);\n x = 0;\n}\nelse if ( y &lt; 0 )\n{\n x = 3;\n}\nelse /* this else clause is required, even if the */\n{ /* programmer expects this will never be reached */\n /* no change in value of x */\n}\n</code></pre>\n</blockquote>\n\n<p>In <a href=\"http://www.misra.org.uk/Publications/tabid/57/Default.aspx#label-c3\">MISRA-C:2012</a>, which supersedes the 2004 version and is the current recommendation for new projects, <a href=\"http://stackoverflow.com/questions/35053371/why-should-if-else-if-constructs-be-terminated-with-an-else-clause/35053448?noredirect=1#comment57852463_35053448\">the same rule exists but is numbered 15.7</a>.</p>\n\n<p><strong>Example 1:</strong>\nin a single if statement programmer may need to check n number of conditions and performs single operation. </p>\n\n<pre><code>if(condition_1 || condition_2 || ... condition_n)\n{\n   //operation_1\n}\n</code></pre>\n\n<p>In a regular usage performing a operation is not needed all the time when <code>if</code> is used. </p>\n\n<p><strong>Example 2:</strong>\nHere programmer checks n number of conditions and performing multiple operations. In regular usage <code>if..else if</code> is like <code>switch</code> you may need to perform a operation like default. So usage <code>else</code> is needed as per misra standard</p>\n\n<pre><code>if(condition_1 || condition_2 || ... condition_n)\n{\n   //operation_1\n}\nelse if(condition_1 || condition_2 || ... condition_n)\n{\n  //operation_2\n}\n....\nelse\n{\n   //default cause\n}\n</code></pre>\n\n<hr>\n\n<p><sup>\u2020</sup> Current and past versions of these publications are available for purchase via the <a href=\"http://www.misra.org.uk/shop/\">MISRA webstore</a> (<a href=\"http://www.misra.org.uk/Buyonline/tabid/58/Default.aspx\">via</a>).</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35053448, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>This, is done to make the code more readable, for later references and to make it clear, to a later reviewer, that the remaining cases handled by the last <code>else</code>, are <strong>do nothing</strong> cases, so that they are not overlooked somehow at first sight.</p>\n\n<p>This is a good programming practice, which makes code <strong>reusable</strong> and <strong>extend-able</strong>.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35053472, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<blockquote>\n  <p>This is the equivalent of requiring a default case in every switch.</p>\n</blockquote>\n\n<p><strong>This extra else will Decrease <a href=\"https://en.wikipedia.org/wiki/Code_coverage\">code coverage</a> of your program.</strong></p>\n\n<hr>\n\n<p>In my experience with porting linux kernel , or android code to different platform many time we do something wrong and in logcat we see some error like </p>\n\n<pre><code>if ( x &lt; 0 )\n{\n    x = 0;\n}\nelse if ( y &lt; 0 )\n{\n    x = 3;\n}\nelse    /* this else clause is required, even if the */\n{       /* programmer expects this will never be reached */\n        /* no change in value of x */\n        printk(\" \\n [function or module name]: this should never happen \\n\");\n\n        /* It is always good to mention function/module name with the \n           logs. If you end up with \"this should never happen\" message\n           and the same message is used in many places in the software\n           it will be hard to track/debug.\n        */\n}\n</code></pre>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35053497, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>As mentioned in another answer, this is from the MISRA-C coding guidelines. The purpose is defensive programming, a concept which is often used in mission-critical programming.</p>\n\n<p>That is, every <code>if - else if</code> must end with an <code>else</code>, and every <code>switch</code> must end with a <code>default</code>.</p>\n\n<p>There are two reasons for this:</p>\n\n<ul>\n<li><p>Self-documenting code. If you write an <code>else</code> but leave it empty it means: \"I have definitely considered the scenario when neither <code>if</code> nor <code>else if</code> are true\". </p>\n\n<p>Not writing an <code>else</code> there means: \"either I considered the scenario where neither <code>if</code> nor <code>else if</code> are true, or I completely forgot to consider it and there's potentially a fat bug right here in my code\".</p></li>\n<li><p>Stop runaway code. In mission-critical software, you need to write robust programs that account even for the highly unlikely. So you could see code like </p>\n\n<pre><code>if (mybool == TRUE) \n{\n} \nelse if (mybool == FALSE) \n{\n}\nelse\n{\n  // handle error\n}\n</code></pre>\n\n<p>This code will be completely alien to PC programmers and computer scientists, but it makes perfect sense in mission-critical software, because it catches the case where the \"mybool\" has gone corrupt, for whatever reason. </p>\n\n<p>Historically, you would fear corruption of the RAM memory because of EMI/noise. This is not much of an issue today. Far more likely, memory corruption occurs because of bugs elsewhere in the code: pointers to wrong locations, array-out-of-bounds bugs, stack overflow, runaway code etc.</p>\n\n<p>So most of the time, code like this comes back to slap yourself in the face when you have written bugs during the implementation stage. Meaning it could also be used as a debug technique: the program you are writing tells you when you have written bugs. </p></li>\n</ul>\n\n<hr>\n\n<p><strong>EDIT</strong></p>\n\n<p>Regarding why <code>else</code> is not needed after every single <code>if</code>: </p>\n\n<p>An <code>if-else</code> or <code>if-else if-else</code> completely covers all possible values that a variable can have. But a plain <code>if</code> statement is not necessarily there to cover all possible values, it has a much broader usage. Most often you just wish to check a certain condition and if it is not met, then do nothing. Then it is simply not meaningful to write defensive programming to cover the <code>else</code> case. </p>\n\n<p>Plus it would clutter up the code completely if you wrote an empty <code>else</code> after each and every <code>if</code>.</p>\n\n<p>MISRA-C:2012 15.7 gives no rationale why <code>else</code> is not needed, it just states: </p>\n\n<blockquote>\n  <p>Note: a final <code>else</code> statement is not required for a simple <code>if</code>\n  statement.</p>\n</blockquote>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35055736, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>Logically any test implies two branches.  What do you do if it is true, and what do you do if it is false. </p>\n\n<p>For those cases where either branch has no functionality, it is reasonable to add a comment about why it doesn't need to have functionality.  </p>\n\n<p>This may be of benefit for the next maintenance programmer to come along.  They should not have to search too far to decide if the code is correct.   You can kind of <a href=\"http://linuxmafia.com/humour/hunting-an-elephant.html\" rel=\"nofollow\">Prehunt the Elephant</a>.</p>\n\n<p>Personally, it helps me as it forces me to look at the else case, and evaluate it.  It may be an impossible condition, in which case i may throw an exception as the contract is violated.   It may be benign, in which case a comment may be enough.</p>\n\n<p>Your mileage may vary.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35067515, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>The basic reason is probably code coverage and the implicit else: how will the code behave if the condition is not true? For genuine testing, you need some way to see that you have tested with the condition false. If every test case you have goes through the if clause, your code could have problems in the real world because of a condition that you did not test.</p>\n\n<p>However, some conditions may properly be like Example 1, like on a tax return: \"If the result is less than 0, enter 0.\" You still need to have a test where the condition is false.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35068156, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>Only a brief explanation, since I did this all about 5 years ago.</p>\n\n<p>There is (with most languages) no syntactic requirement to include \"null\" <code>else</code> statement (and unnecessary <code>{..}</code>), and in \"simple little programs\" there is no need.  But real programmers don't write \"simple little programs\", and, just as importantly, they don't write programs that will be used once and then discarded.</p>\n\n<p>When one write an if/else:</p>\n\n<pre><code>if(something)\n  doSomething;\nelse\n  doSomethingElse;\n</code></pre>\n\n<p>it all seems simple and one hardly sees even the point of adding <code>{..}</code>.</p>\n\n<p>But some day, a few months from now, some other programmer (you would never make such a mistake!) will need to \"enhance\" the program and will add a statement.</p>\n\n<pre><code>if(something)\n  doSomething;\nelse\n  doSomethingIForgot;\n  doSomethingElse;\n</code></pre>\n\n<p>Suddenly <code>doSomethingElse</code> kinda forgets that it's supposed to be in the <code>else</code> leg.</p>\n\n<p>So you're a good little programmer and you always use <code>{..}</code>.  But you write:</p>\n\n<pre><code>if(something) {\n  if(anotherThing) {\n    doSomething;\n  }\n}\n</code></pre>\n\n<p>All's well and good until that new kid makes a midnight modification:</p>\n\n<pre><code>if(something) {\n  if(!notMyThing) {\n  if(anotherThing) {\n    doSomething;\n  }\n  else {\n    dontDoAnything;  // Because it's not my thing.\n  }}\n}\n</code></pre>\n\n<p>Yes, it's improperly formatted, but so is half the code in the project, and the \"auto formatter\" gets bollixed up by all the <code>#ifdef</code> statements.  And, of course, the real code is far more complicated than this toy example.</p>\n\n<p>Unfortunately (or not), I've been out of this sort of thing for a few years now, so I don't have a fresh \"real\" example in mind -- the above is (obviously) contrived and a bit hokey.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35093134, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>I'm currently working with PHP. Creating a registration form and a login form. I am just purely using if and else. No else if or anything that is unnecessary. </p>\n\n<p>If user clicks submits button -> it goes to the next if statement... if username is less than than 'X' amount of characters then alert. If successful then check password length and so on.</p>\n\n<p>No need for extra code such as an else if that could dismiss reliability for server load time to check all the extra code.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35164837, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>I would like to add to &ndash; and partly contradict &ndash; the previous answers. While it is certainly common to use if-else if in a switch-like manner that should cover the full range of thinkable values for an expression, it is by no means guaranteed that any range of possible conditions is fully covered. The same can be said about the switch construct itself, hence the requirement to use a default clause, which catches all remaining values and can, if not otherwise required anyway, be used as an assertion safeguard. </p>\n\n<p>The question itself features a good counter-example: The second condition does not relate to x at all (which is the reason why I often prefer the more flexible if-based variant over the switch-based variant). From the example it is obvious that if condition A is met, x should be set to a certain value. Should A not be met, then condition B is tested. If it is met, then x should receive another value. If neither A nor B are met, then x should remain unchanged.</p>\n\n<p>Here we can see that an empty else branch should be used to comment on the programmer's intention for the reader.</p>\n\n<p>On the other hand, I cannot see why there must be an else clause especially for the latest and innermost if statement. <strong>In C, there is no such thing as an 'else if'. There is only if and else.</strong> Instead, according to MISRA, the construct should formally be indented this way (and I should have put the opening curly braces on their own lines, but I don't like that):</p>\n\n<pre><code>if (A) {\n    // do something\n}\nelse {\n    if (B) {\n        // do something else (no pun intended)\n    }\n    else {\n        // don't do anything here\n    }\n}\n</code></pre>\n\n<p>When MISRA asks to put curly braces around every branch, then it contradicts itself by mentioning \"if ... else if constructs\".</p>\n\n<p>Anyone can imagine the ugliness of deeply nested if else trees, <a href=\"http://stackoverflow.com/questions/268132/invert-if-statement-to-reduce-nesting/6342924#6342924\">see here on a side note</a>. Now imagine that this construct can be arbitrarily extended anywhere. Then asking for an else clause in the end, but not anywhere else, becomes absurd.</p>\n\n<pre><code>if (A) {\n    if (B) {\n        // do something\n    }\n    // you could to something here\n}\nelse {\n    // or here\n    if (B) { // or C?\n        // do something else (no pun intended)\n    }\n    else {\n        // don't do anything here, if you don't want to\n    }\n    // what if I wanted to do something here? I need brackets for that.\n}\n</code></pre>\n\n<p>So I am sure that the people who developed the MISRA guidelines had the switch-like if-else if intention in mind.</p>\n\n<p>In the end, it comes down for them to defining precisely what is meant with an \"if ... else if construct\"</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35166295, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>Our software was not mission critical, yet we also decided to use this rule because of defensive programming.\nWe added a throw exception to the theoretically unreachable code (switch + if-else). And it saved us many times as the software failed fast e.g. when a new type has been added and we forgot to change one-or-two if-else or switch. As a bonus it made super easy to find the issue.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35168336, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>Well, my example involves undefined behavior, but sometimes some people try to be fancy and fails hard, take a look:</p>\n\n<pre><code>int a = 0;\nbool b = true;\nuint8_t* bPtr = (uint8_t*)&amp;b;\n*bPtr = 0xCC;\nif(b == true)\n{\n    a += 3;\n}\nelse if(b == false)\n{\n    a += 5;\n}\nelse\n{\n    exit(3);\n}\n</code></pre>\n\n<p>You probably would never expect to have <code>bool</code> which is not <code>true</code> nor <code>false</code>, however it may happen. Personally I believe this is problem caused by person who decides to do something fancy, but additional <code>else</code> statement can prevent any further issues.</p>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35173717, "question_id": 35053371, "tags": ["c", "misra"]}, {"body": "<p>Most the time when you just have a single <code>if</code> statement, it's probably one of reasons such as:</p>\n\n<ul>\n<li>Function guard checks</li>\n<li>Initialization option</li>\n<li>Optional processing branch</li>\n</ul>\n\n<p><em>Example</em></p>\n\n<pre><code>void print (char * text)\n{\n    if (text == null) return; // guard check\n\n    printf(text);\n}\n</code></pre>\n\n<p>But when you do <code>if .. else if</code>, it's probably one of reasons such as:</p>\n\n<ul>\n<li>Dynamic switch-case</li>\n<li>Processing fork</li>\n<li>Handling a processing parameter</li>\n</ul>\n\n<p>And in case your <code>if .. else if</code> covers all possibilities, in that case your last <code>if (...)</code> is not needed, you can just remove it, because at that point the only possible values are the ones covered by that condition.</p>\n\n<p><em>Example</em></p>\n\n<pre><code>int absolute_value (int n)\n{\n    if (n == 0)\n    {\n        return 0;\n    }\n    else if (n &gt; 0)\n    {\n        return n;\n    }\n    else /* if (n &lt; 0) */ // redundant check\n    {\n        return (n * (-1));\n    }\n}\n</code></pre>\n\n<p>And in most of these reasons, it's possible something doesn't fit into any of the categories in your <code>if .. else if</code>, thus the need to handle them in a final <code>else</code> clause, handling can be done through business-level procedure, user notification, internal error mechanism, ..etc.</p>\n\n<p><em>Example</em></p>\n\n<pre><code>#DEFINE SQRT_TWO   1.41421356237309504880\n#DEFINE SQRT_THREE 1.73205080756887729352\n#DEFINE SQRT_FIVE  2.23606797749978969641\n\ndouble square_root (int n)\n{\n         if (n  &gt; 5)   return sqrt((double)n);\n    else if (n == 5)   return SQRT_FIVE;\n    else if (n == 4)   return 2.0;\n    else if (n == 3)   return SQRT_THREE;\n    else if (n == 2)   return SQRT_TWO;\n    else if (n == 1)   return 1.0;\n    else if (n == 0)   return 0.0;\n    else               return sqrt(-1); // error handling\n}\n</code></pre>\n\n<p>This final <code>else</code> clause is quite similar to few other things in languages such as <code>Java</code> and <code>C++</code>, such as:</p>\n\n<ul>\n<li><code>default</code> case in a switch statement</li>\n<li><code>catch(...)</code> that comes after all specific <code>catch</code> blocks</li>\n<li><code>finally</code> in a try-catch clause</li>\n</ul>\n", "title": "What is the benefit of terminating if \u2026 else if constructs with an else clause?", "answer_id": 35251278, "question_id": 35053371, "tags": ["c", "misra"]}], "question_id": 35053371}, {"body": "<p>In a project, somebody pushed this line:</p>\n\n<pre><code>double (*e)[n+1] = malloc((n+1) * sizeof(*e));\n</code></pre>\n\n<p>Which supposedly creates a two-dimensional array of (n+1)*(n+1) doubles.</p>\n\n<p><em>Supposedly</em>, I say, because so far, nobody I asked could tell me what this does, exactly, nor where it originated from or why it should work (which allegedly, it does, but I'm not yet buying it).</p>\n\n<p>Perhaps I'm missing something obvious, but I'd appreciate it if somebody could explain above line to me. Because personally, I'd feel much better if we'd use something we actually understand.</p>\n", "title": "Freaky way of allocating two-dimensional array?", "tags": ["c", "arrays", "multidimensional-array", "malloc", "allocation"], "answer_count": 3, "link": "http://stackoverflow.com/questions/36794202/freaky-way-of-allocating-two-dimensional-array", "answers": [{"body": "<p>This is the typical way you should allocate 2D arrays dynamically.</p>\n\n<ul>\n<li><code>e</code> is an array pointer to an array of type <code>double [n+1]</code>.</li>\n<li><code>sizeof(*e)</code> therefore gives the type of the pointed-at type, which is the size of one <code>double [n+1]</code> array.</li>\n<li>You allocate room for <code>n+1</code> such arrays.</li>\n<li>You set the array pointer <code>e</code> to point at the first array in this array of arrays.</li>\n<li>This allows you to use <code>e</code> as <code>e[i][j]</code> to access individual items in the 2D array.</li>\n</ul>\n\n<p>Personally I think this style is much easier to read:</p>\n\n<pre><code>double (*e)[n+1] = malloc( sizeof(double[n+1][n+1]) );\n</code></pre>\n", "title": "Freaky way of allocating two-dimensional array?", "answer_id": 36794360, "question_id": 36794202, "tags": ["c", "arrays", "multidimensional-array", "malloc", "allocation"]}, {"body": "<p>The variable <code>e</code> is a pointer to an array of <code>n + 1</code> elements of type <code>double</code>.</p>\n\n<p>Using the dereference operator on <code>e</code> gives you the base-type of <code>e</code> which is \" array of <code>n + 1</code> elements of type <code>double</code>\".</p>\n\n<p>The <code>malloc</code> call simply takes the base-type of <code>e</code> (explained above) and gets its size, multiplies it by <code>n + 1</code>, and passing that size to the <code>malloc</code> function. Essentially allocating an array of <code>n + 1</code> arrays of <code>n + 1</code> elements of <code>double</code>.</p>\n", "title": "Freaky way of allocating two-dimensional array?", "answer_id": 36794369, "question_id": 36794202, "tags": ["c", "arrays", "multidimensional-array", "malloc", "allocation"]}, {"body": "<p>This idiom falls naturally out of 1D array allocation.  Let's start with allocating a 1D array of some arbitrary type <code>T</code>:</p>\n\n<pre><code>T *p = malloc( sizeof *p * N );\n</code></pre>\n\n<p>Simple, right?  The <em>expression</em> <code>*p</code> has type <code>T</code>, so <code>sizeof *p</code> gives the same result as <code>sizeof (T)</code>, so we're allocating enough space for an <code>N</code>-element array of <code>T</code>.  This is true for <em>any type <code>T</code></em>.  </p>\n\n<p>Now, let's substitute <code>T</code> with an array type like <code>R [10]</code>.  Then our allocation becomes</p>\n\n<pre><code>R (*p)[10] = malloc( sizeof *p * N);\n</code></pre>\n\n<p>The semantics here are <em>exactly the same</em> as the 1D allocation method; all that's changed is the type of <code>p</code>.  Instead of <code>T *</code>, it's now <code>R (*)[10]</code>.  The expression <code>*p</code> has type <code>T</code> which is type <code>R [10]</code>, so <code>sizeof *p</code> is equivalent to <code>sizeof (T)</code> which is equivalent to <code>sizeof (R [10])</code>.  So we're allocating enough space for an <code>N</code> by <code>10</code> element array of <code>R</code>.  </p>\n\n<p>We can take this even further if we want; suppose <code>R</code> is itself an array type <code>int [5]</code>.  Substitute that for <code>R</code> and we get</p>\n\n<pre><code>int (*p)[10][5] = malloc( sizeof *p * N);\n</code></pre>\n\n<p>Same deal - <code>sizeof *p</code> is the same as <code>sizeof (int [10][5])</code>, and we wind up allocating a contiguous chunk of memory large enough to hold a <code>N</code> by <code>10</code> by <code>5</code> array of <code>int</code>.  </p>\n\n<p>So that's the allocation side; what about the access side?</p>\n\n<p>Remember that the <code>[]</code> subscript operation is <em>defined</em> in terms of pointer arithmetic: <code>a[i]</code> is defined as <code>*(a + i)</code><sup>1</sup>.  Thus, the subscript operator <code>[]</code> <em>implicitly</em> dereferences a pointer.  If <code>p</code> is a pointer to <code>T</code>, you can access the pointed-to value either by explicitly dereferencing with the unary <code>*</code> operator:</p>\n\n<pre><code>T x = *p;\n</code></pre>\n\n<p><em>or</em> by using the <code>[]</code> subscript operator:</p>\n\n<pre><code>T x = p[0]; // identical to *p\n</code></pre>\n\n<p>Thus, if <code>p</code> points to the first element of an <em>array</em>, you can access any element of that array by using a subscript on the pointer <code>p</code>:</p>\n\n<pre><code>T arr[N];\nT *p = arr; // expression arr \"decays\" from type T [N] to T *\n...\nT x = p[i]; // access the i'th element of arr through pointer p\n</code></pre>\n\n<p>Now, let's do our substitution operation again and replace <code>T</code> with the array type <code>R [10]</code>:</p>\n\n<pre><code>R arr[N][10];\nR (*p)[10] = arr; // expression arr \"decays\" from type R [N][10] to R (*)[10]\n...\nR x = (*p)[i];\n</code></pre>\n\n<p>One immediately apparent difference; we're explicitly dereferencing <code>p</code> before applying the subscript operator.  We don't want to subscript into <code>p</code>, we want to subscript into what <code>p</code> <em>points to</em> (in this case, the <em>array</em> <code>arr[0]</code>).  Since unary <code>*</code> has lower precedence than the subscript <code>[]</code> operator, we have to use parentheses to explicitly group <code>p</code> with <code>*</code>.  But remember from above that <code>*p</code> is the same as <code>p[0]</code>, so we can substitute that with</p>\n\n<pre><code>R x = (p[0])[i];\n</code></pre>\n\n<p>or just</p>\n\n<pre><code>R x = p[0][i];\n</code></pre>\n\n<p>Thus, if <code>p</code> points to a 2D array, we can index into that array through <code>p</code> like so:</p>\n\n<pre><code>R x = p[i][j]; // access the i'th element of arr through pointer p;\n               // each arr[i] is a 10-element array of R\n</code></pre>\n\n<p>Taking this to the same conclusion as above and substituting <code>R</code> with <code>int [5]</code>:</p>\n\n<pre><code>int arr[N][10][5];\nint (*p)[10][5]; // expression arr \"decays\" from type int [N][5][10] to int (*)[10][5]\n...\nint x = p[i][j][k];\n</code></pre>\n\n<p>This works <em>just the same</em> if <code>p</code> points to a regular array, or if it points to memory allocated through <code>malloc</code>.  </p>\n\n<p>This idiom has the following benefits:</p>\n\n<ol>\n<li>It's simple - just one line of code, as opposed to the piecemeal allocation method<pre><code>T **arr = malloc( sizeof *arr * N );\nif ( arr )\n{\n  for ( size_t i = 0; i &lt; N; i++ )\n  {\n    arr[i] = malloc( sizeof *arr[i] * M );\n  }\n}\n</code></pre>\n<li>All the rows of the allocated array are *contiguous*, which is not the case with the piecemeal allocation method above;\n<li>Deallocating the array is just as easy with a single call to <code>free</code>.  Again, not true with the piecemeal allocation method, where you have to deallocate each <code>arr[i]</code> before you can deallocate <code>arr</code>.  \n</ol>\n\n<p>Sometimes the piecemeal allocation method is preferable, such as when your heap is badly fragmented and you can't allocate your memory as a contiguous chunk, or you want to allocate a \"jagged\" array where each row can have a different length.  But in general, this is the better way to go.  </p>\n\n<p><hr>\n<sup>1.  Remember that arrays <em>are not</em> pointers - instead, array <em>expressions</em> are converted to pointer expressions as necessary.\n</sup>  </p>\n", "title": "Freaky way of allocating two-dimensional array?", "answer_id": 36799070, "question_id": 36794202, "tags": ["c", "arrays", "multidimensional-array", "malloc", "allocation"]}], "question_id": 36794202}, {"body": "<p>Running <code>brew update</code> I get the following:</p>\n\n<pre><code>/usr/local/Library/brew.sh: line 32: /usr/local/Library/ENV/scm/git: No such file or directory\n/usr/local/Library/brew.sh: line 32: /usr/local/Library/ENV/scm/git: No such file or directory\n[etc..]\nError: update-report should not be called directly!\n</code></pre>\n\n<p>Calling <code>brew config</code> reports the following:</p>\n\n<pre><code>HOMEBREW_VERSION: 0.9.9\nORIGIN: https://github.com/Homebrew/brew.git\nHEAD: 90e84453f9adda65de6b9274987d06e46caa5d37\nLast commit: 4 hours ago\nCore tap ORIGIN: https://github.com/Homebrew/homebrew-core\nCore tap HEAD: 05ce2548bad01807c6be2aece21ab70d221755e6\nCore tap last commit: 8 weeks ago\nHOMEBREW_PREFIX: /usr/local\nHOMEBREW_REPOSITORY: /usr/local\nHOMEBREW_CELLAR: /usr/local/Cellar\nHOMEBREW_BOTTLE_DOMAIN: https://homebrew.bintray.com\nCPU: quad-core 64-bit ivybridge\nHomebrew Ruby: 2.0.0-p648\nClang: 7.3 build 703\nGit: 2.7.0 =&gt; /usr/local/bin/git\nPerl: /usr/bin/perl\nPython: /Users/username/anaconda/bin/python =&gt; /Users/username/anaconda/bin/python2.7\nRuby: /usr/bin/ruby =&gt; /System/Library/Frameworks/Ruby.framework/Versions/2.0/usr/bin/ruby\nJava: 1.7.0_79, 1.6.0_65-b14-468\nOS X: 10.11.4-x86_64\nXcode: 7.3\nCLT: 7.3.0.0.1.1457485338\nX11: 2.7.8 =&gt; /opt/X11\n</code></pre>\n\n<p>Any idea how to fix this?  I wonder if Sophos Antivirus might be causing problems again..</p>\n", "title": "Homebrew: Error: update-report should not be called directly", "tags": ["homebrew"], "answer_count": 3, "link": "http://stackoverflow.com/questions/38410020/homebrew-error-update-report-should-not-be-called-directly", "answers": [{"body": "<p>I just got this error. I ran <code>brew upgrade</code> and then <code>brew update</code> again and the problem seemed to resolve itself.</p>\n\n<p><code>brew upgrade</code> will upgrade <em>all</em> installed packages. If you need to maintain a specific version of any binary, consider using @thanh-h\u1ea3i's answer: <a href=\"http://stackoverflow.com/a/38464247/868724\">http://stackoverflow.com/a/38464247/868724</a></p>\n\n<p>Alternatively, try just running <code>brew update</code> again as suggested by @ashley-willis.</p>\n", "title": "Homebrew: Error: update-report should not be called directly", "answer_id": 38412195, "question_id": 38410020, "tags": ["homebrew"]}, {"body": "<p>Edit : run <code>cd \"$(brew --repository)\" &amp;&amp; git fetch &amp;&amp; git reset --hard origin/master</code> will solve!</p>\n\n<p>Refer to <a href=\"https://github.com/Homebrew/brew/issues/557\">https://github.com/Homebrew/brew/issues/557</a></p>\n", "title": "Homebrew: Error: update-report should not be called directly", "answer_id": 38464247, "question_id": 38410020, "tags": ["homebrew"]}, {"body": "<p>Running <code>brew update</code> again after getting this error works, as I saw suggested on <a href=\"https://discuss.circleci.com/t/brew-update-command-fails/5211\">https://discuss.circleci.com/t/brew-update-command-fails/5211</a> and worked for me as well, without doing any upgrades nor git-fu</p>\n", "title": "Homebrew: Error: update-report should not be called directly", "answer_id": 38489296, "question_id": 38410020, "tags": ["homebrew"]}], "question_id": 38410020}, {"body": "<p>I was wondering about the difference between <code>\\</code> and <code>/</code> in file paths. I have noticed that sometimes a path contains <code>/</code>and sometimes it is with <code>\\</code>.</p>\n\n<p>It would be great if anyone can explain when to use <code>\\</code> and <code>/</code>.</p>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "tags": ["c#", "path", "filepath", "backslash", "slash"], "answer_count": 7, "link": "http://stackoverflow.com/questions/38428561/difference-between-forward-slash-and-backslash-in-file-path", "answers": [{"body": "<p><code>\\</code> is used for Windows local file paths and network paths as in </p>\n\n<p><code>C:\\Windows\\Temp\\</code> or <code>\\\\NetworkSharedDisk\\Documents\\Archive\\</code></p>\n\n<p><code>/</code> is what is required by standard URIs as in :</p>\n\n<p><a href=\"http://www.stackoverflow.com/\">http://www.stackoverflow.com/</a></p>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38428607, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}, {"body": "<p>On Unix-based systems <code>\\</code> is an escape character, that is, <code>\\</code> tells the parser that this is a space and not the end of the statement. On Unix systems <code>/</code> is the directory separator.</p>\n\n<p>On Windows <code>\\</code> is the directory separator, but the <code>/</code> cannot be used in file or directory names.</p>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38428656, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}, {"body": "<ul>\n<li>A URL, standardized in RFC 1738, always uses forward slashes,\nregardless of platform.</li>\n<li>A file path and a URI are different. <code>\\</code> is correct in a Windows file\npath and <code>/</code> is correct in a URI.</li>\n<li>Several browsers (namely, Firefox &amp; Opera) fail catastrophically when\nencountering URIs with backslashes.</li>\n<li>System.IO.Path.DirectorySeparatorChar to get current path separator</li>\n</ul>\n\n<p><a href=\"http://stackoverflow.com/questions/1589930/so-what-is-the-right-direction-of-the-paths-slash-or-under-windows\">This</a> can be relevant resource. </p>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38428670, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}, {"body": "<p><code>/</code> is the path separator on Unix and Unix-like systems. Modern Windows can generally use both <code>\\</code> and <code>/</code> interchangeably for filepaths, but Microsoft has advocated for the use of <code>\\</code> as the path separator for decades.</p>\n\n<p>This is done for historical reasons that date as far back as the 1970s, predating Windows by over a decade. In the beginning, MS-DOS (the foundation to early Windows) didn't support directories. Unix had directory support using the <code>/</code> character since the beginning. However, when directories were added in MS-DOS 2.0, Microsoft and IBM were already using the <code>/</code> character for <a href=\"https://en.wikipedia.org/wiki/Command-line_interface#Command-line_option\">command switches</a>, and because of DOS's lightweight parser (descended from <a href=\"https://en.wikipedia.org/wiki/86-DOS\">QDOS</a>, designed to run on lower end hardware), they couldn't find a feasible way to use the <code>/</code> character without breaking compatibility with their existing applications.</p>\n\n<p>So, to avoid errors about \"missing a switch\" or \"invalid switch\" when passing filepaths as arguments to commands such as these:</p>\n\n<pre><code>cd/                        &lt;---- no switch specified\ndir folder1/folder2        &lt;---- /folder2 is not a switch for dir\n</code></pre>\n\n<p>it was decided that the <code>\\</code> character would be used instead, so you could write those commands like this</p>\n\n<pre><code>cd\\\ndir folder1\\folder2\n</code></pre>\n\n<p>without error.</p>\n\n<p>Later, Microsoft and IBM collaborated on an operating system unrelated to DOS called <a href=\"https://en.wikipedia.org/wiki/OS/2\">OS/2</a>. OS/2 had the ability to use both separators, probably to attract more Unix developers. When <a href=\"https://en.wikipedia.org/wiki/OS/2#1990:_Breakup\">Microsoft and IBM parted ways in 1990</a>, Microsoft took what code they had and created <a href=\"https://en.wikipedia.org/wiki/Windows_NT\">Windows NT</a>, on which all modern versions of Windows are based, carrying this separator agnosticism with it.</p>\n\n<hr>\n\n<p>As backward compatibility has been the name of the game for Microsoft from all of the major OS transitions that they've undertaken (DOS to Win16/DOS, to Win16/Win32, to Win32/WinNT), this peculiarity stuck, and it will probably exist for a while yet.</p>\n\n<p>It's for this reason that this discrepancy exists. It should really have no effect on what you're doing because, like I said, the WinAPI can generally use them interchangeably. However, 3rd party applications will probably break if you pass a <code>/</code> when they expect a <code>\\</code> between directory names. If you're using Windows, stick with <code>\\</code>. If you're using Unix or <a href=\"https://en.wikipedia.org/wiki/Uniform_Resource_Identifier\">URI</a>s (which have their foundation in Unix paths, but that's another story entirely), then use <code>/</code>.</p>\n\n<hr>\n\n<p><strong>In the context of C#:</strong> It should be noted, since this <em>is</em> technically a C# question, that if you want to write more \"portable\" C# code that works on both Unix and Windows (even if C# is predominantly a Windows language), you might want to use the <code>Path.DirectorySeparatorChar</code> field so your code uses the preferred separator on that system, and use <code>Path.Combine()</code> to append paths properly.</p>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38428899, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}, {"body": "<p><a href=\"http://www.computerhistory.org/atchm/microsoft-ms-dos-early-source-code/\" rel=\"nofollow\">MS-DOS 1.0</a> retained the command line option (or switch) character convention of '/' from CP/M. At that time there was no directory structure in the file system and no conflict.</p>\n\n<p>When Microsoft developed the more Unix like environment with MS-DOS (and PC-DOS) 2.0, they needed to represent the path separator using something that did not conflict with existing command line options. Internally, the system works equally well with either '/' or '\\'. The command processor (and many applications) continued to use the '/' as a switch character.</p>\n\n<p>A <a href=\"https://en.wikipedia.org/wiki/CONFIG.SYS#CONFIG.SYS_directives\" rel=\"nofollow\"><code>CONFIG.SYS</code></a> entry <code>SWITCHAR=-</code> could be used to override the <code>/</code> default to improve Unix compatibility. This makes built in commands and standard utilities use the alternate character. The Unix path separator could then be unambiguously used for file and directory names. This entry was removed later versions, but a DOS call was documented to set the value after booting.</p>\n\n<p>This was little used and most third-party tools continued unchanged. The confusion persists. Many ports of Unix tools retain the '-' switch character and some support both conventions.</p>\n\n<p>The follow-on PowerShell command processor implements rigorous escaping and switch parameters and largely avoids the confusion except where legacy tools are used.</p>\n\n<blockquote>\n  <p>Neither the question nor the answer relate to C#.</p>\n</blockquote>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38435152, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}, {"body": "<p>Apart from the answers given, it is worth mentioning that <code>\\</code> is widely used for special characters (such as <code>\\n</code> <code>\\t</code>) in programming languages, text editors and general systems that apply lexical analysis.</p>\n\n<p>If you are programming for instance, it is inconvenient at times to need to even need to escape backslash with another one (<code>\\\\</code>) in order to use it properly - or need to use escaping strings, such as C# <code>@\"\\test\"</code>.</p>\n\n<p>Of course, as mentioned before, web URIs use forward slash by standard, <del>but both slashes work in the latest and most common command line tools.</del></p>\n\n<p>UPDATE: After searching a little bit, it seems out the whole story between <code>/</code> and <code>\\</code> goes back in \"computer history\", in the ages of DOS and the Unix-based systems at that time. <em>HowToGeek</em> has an interesting <a href=\"http://www.howtogeek.com/181774/why-windows-uses-backslashes-and-everything-else-uses-forward-slashes/\" rel=\"nofollow\">article</a> about this story.</p>\n\n<p>In short terms, DOS 1.0 was initially released by IBM with no directory support, and <code>/</code> was used for another (\"switching\") command functionality. When directories were introduced in 2.0 version, <code>/</code> was already in use, so IBM chose the visually closest symbol, which was <code>\\</code>. On the other hand, Unix standardly used <code>/</code> for directories.</p>\n\n<p>When users started to use many different systems, they started becoming confused, making the OS developers to attempt making the systems work in both cases - this even applies in the part of URLs, as some browsers support the <em>http:\\www.test.com\\go</em> format. This had drawbacks though in general, but the whole thing stands today still for backward compartibility causes, with an attempt for support of both slashes on Windows, even though they are not based on DOS anymore.</p>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38440215, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}, {"body": "<p>You shouldn't be using either in C#. You should always use the <a href=\"https://msdn.microsoft.com/en-us/library/system.io.path(v=vs.110).aspx\" rel=\"nofollow\"><code>Path</code> class</a>. This contains a method called <code>Path.Combine</code> that can be used to create paths without specifying the separator yourself.</p>\n\n<p>Example usage:</p>\n\n<pre><code>string fullPath = System.IO.Path.Combine(\"C:\", \"Folder1\", \"Folder2\", \"file.txt\");\n</code></pre>\n", "title": "Difference between forward slash (/) and backslash (\\) in file path", "answer_id": 38454955, "question_id": 38428561, "tags": ["c#", "path", "filepath", "backslash", "slash"]}], "question_id": 38428561}, {"body": "<p>When I read the source code from <code>java.io.BufferedInputStream.getInIfOpen()</code>, I am confused about why it wrote code like this:</p>\n\n<pre><code>/**\n * Check to make sure that underlying input stream has not been\n * nulled out due to close; if not return it;\n */\nprivate InputStream getInIfOpen() throws IOException {\n    InputStream input = in;\n    if (input == null)\n        throw new IOException(\"Stream closed\");\n    return input;\n}\n</code></pre>\n\n<p>Why is it using the alias instead of using the field variable <code>in</code> directly like below:</p>\n\n<pre><code>/**\n * Check to make sure that underlying input stream has not been\n * nulled out due to close; if not return it;\n */\nprivate InputStream getInIfOpen() throws IOException {\n    if (in == null)\n        throw new IOException(\"Stream closed\");\n    return in;\n}\n</code></pre>\n\n<p>Can someone give a reasonable explanation?</p>\n", "title": "Why does BufferedInputStream copy a field to a local variable rather than use the field directly", "tags": ["java", "bufferedinputstream"], "answer_count": 4, "link": "http://stackoverflow.com/questions/36231047/why-does-bufferedinputstream-copy-a-field-to-a-local-variable-rather-than-use-th", "answers": [{"body": "<p>If you look at this code out of context there is no good explanation for that \"alias\". It is simply redundant code or poor code style.</p>\n\n<p>But the context is that <code>BufferedInputStream</code> is a class that can be subclassed, and that it needs to work in a multi-threaded context.</p>\n\n<p>The clue is that <code>in</code> is declared in <code>FilterInputStream</code> is <code>protected volatile</code>. That means that there is a chance that a subclass could reach in and assign <code>null</code> to <code>in</code>. Given that possibility, the \"alias\" is actually there to prevent a race condition.</p>\n\n<p>Consider the code without the \"alias\"</p>\n\n<pre><code>private InputStream getInIfOpen() throws IOException {\n    if (in == null)\n        throw new IOException(\"Stream closed\");\n    return in;\n}\n</code></pre>\n\n<ol>\n<li>Thread A calls <code>getInIfOpen()</code></li>\n<li>Thread A evaluates <code>in == null</code> and sees that <code>in</code> is not <code>null</code>.</li>\n<li>Thread B assigns <code>null</code> to <code>in</code>.</li>\n<li>Thread A executes <code>return in</code>. Which returns <code>null</code> because <code>a</code> is a <code>volatile</code>.</li>\n</ol>\n\n<p>The \"alias\" prevents this. Now <code>in</code> is read just once by thread A. If thread B assigns <code>null</code> after thread A has <code>in</code> it doesn't matter. Thread A will either throw an exception or return a (guaranteed) non-null value.</p>\n", "title": "Why does BufferedInputStream copy a field to a local variable rather than use the field directly", "answer_id": 36231109, "question_id": 36231047, "tags": ["java", "bufferedinputstream"]}, {"body": "<p>This is because the class <code>BufferedInputStream</code> is designed for multi-threaded usage.</p>\n\n<p>Here, you see the declaration of <code>in</code>, which is placed in the parent class <code>FilterInputStream</code>:</p>\n\n<pre><code>protected volatile InputStream in;\n</code></pre>\n\n<p>Since it is <code>protected</code>, its value can be changed by any subclass of <code>FilterInputStream</code>, including <code>BufferedInputStream</code> and its subclasses. Also, it is declared <code>volatile</code>, which means that if any thread changes the value of the variable, this change will immediately be reflected in all other threads. This combination is bad, since it means the class <code>BufferedInputStream</code> has no way to control or know when <code>in</code> is changed. Thus, the value can even be changed between the check for null and the return statement in <code>BufferedInputStream::getInIfOpen</code>, which effectively makes the check for null useless. By reading the value of <code>in</code> only once to cache it in the local variable <code>input</code>, the method <code>BufferedInputStream::getInIfOpen</code> is safe against changes from other threads, since local variables are always owned by a single thread.</p>\n\n<p>There is an example in <code>BufferedInputStream::close</code>, which sets <code>in</code> to null:</p>\n\n<pre><code>public void close() throws IOException {\n    byte[] buffer;\n    while ( (buffer = buf) != null) {\n        if (bufUpdater.compareAndSet(this, buffer, null)) {\n            InputStream input = in;\n            in = null;\n            if (input != null)\n                input.close();\n            return;\n        }\n        // Else retry in case a new buf was CASed in fill()\n    }\n}\n</code></pre>\n\n<p>If <code>BufferedInputStream::close</code> is called by another thread while <code>BufferedInputStream::getInIfOpen</code> is executed, this would result in the race condition described above.</p>\n", "title": "Why does BufferedInputStream copy a field to a local variable rather than use the field directly", "answer_id": 36231156, "question_id": 36231047, "tags": ["java", "bufferedinputstream"]}, {"body": "<p>This is such a short code, but, theoretically, in a multi-threaded environment, <code>in</code> may change right after the comparison, so the method could return something it didn't check (it could return <code>null</code>, thus doing the exact thing it was meant to prevent).</p>\n", "title": "Why does BufferedInputStream copy a field to a local variable rather than use the field directly", "answer_id": 36231158, "question_id": 36231047, "tags": ["java", "bufferedinputstream"]}, {"body": "<p>I believe capturing the class variable <code>in</code> to the local variable <code>input</code> is to prevent inconsistent behavior if <code>in</code> is change by another thread while <code>getInIfOpen()</code> is running.</p>\n\n<p>Notice that the owner of <code>in</code> is the parent class and does not mark it as <code>final</code>.</p>\n\n<p>This pattern is replicated in other parts of the class and seems to be reasonable defensive coding.</p>\n", "title": "Why does BufferedInputStream copy a field to a local variable rather than use the field directly", "answer_id": 36231170, "question_id": 36231047, "tags": ["java", "bufferedinputstream"]}], "question_id": 36231047}, {"body": "<p>Adding two 32-bit integers can result an integer overflow:</p>\n\n<pre><code>uint64_t u64_z = u32_x + u32_y;\n</code></pre>\n\n<p>This overflow can be avoided if one of the 32-bit integers is first casted or added to a 64-bit integer.</p>\n\n<pre><code>uint64_t u64_z = u32_x + u64_a + u32_y;\n</code></pre>\n\n<p>However, if the compiler decides to reorder the addition:</p>\n\n<pre><code>uint64_t u64_z = u32_x + u32_y + u64_a;\n</code></pre>\n\n<p>the integer overflow might still happen.</p>\n\n<p>Are compilers allowed to do such a reordering or can we trust them to notice the result inconsistency and keep the expression order as is?</p>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "tags": ["c++", "c", "compiler-construction", "integer-overflow"], "answer_count": 6, "link": "http://stackoverflow.com/questions/38563707/a-b-c-%e2%89%a0-a-c-b-and-compiler-reordering", "answers": [{"body": "<p>If the optimiser does such a reordering it is still bound to the C specification, so such a reordering would become:</p>\n\n<pre><code>uint64_t u64_z = (uint64_t)u32_x + (uint64_t)u32_y + u64_a;\n</code></pre>\n\n<p>Rationale: </p>\n\n<p>We start with</p>\n\n<pre><code>uint64_t u64_z = u32_x + u64_a + u32_y;\n</code></pre>\n\n<p>Addition is performed left-to-right.</p>\n\n<p>The integer promotion rules state that in the first addition in the original expression, <code>u32_x</code> be promoted to <code>uint64_t</code>. In the second addition, <code>u32_y</code> will also be promoted to <code>uint64_t</code>.</p>\n\n<p>So, in order to be compliant with the C specification, any optimiser must promote <code>u32_x</code> and <code>u32_y</code> to 64 bit unsigned values. This is equivalent to adding a cast. (The actual optimising is not done at the C level, but I use C notation because that is a notation that we understand.)</p>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "answer_id": 38563853, "question_id": 38563707, "tags": ["c++", "c", "compiler-construction", "integer-overflow"]}, {"body": "<p>There is the \"as if\" rule in C, C++, and Objective-C: The compiler may do whatever it likes as long as no conforming program can tell the difference. </p>\n\n<p>In these languages, a + b + c is defined to be the same as (a + b) + c. If you can tell the difference between this and for example a + (b + c) then the compiler cannot change the order. If you can't tell the difference, then the compiler is free to change the order, but that's fine, because you can't tell the difference. </p>\n\n<p>In your example, with b = 64 bit, a and c 32 bit, the compiler would be allowed to evaluate (b + a) + c or even (b + c) + a, because you couldn't tell the difference, but not (a + c) + b because you can tell the difference. </p>\n\n<p>In other words, the compiler isn't allowed to do anything that makes your code behave different from what it should. It is not required to produce the code that you think it would produce, or that you think it should produce, but the code <em>will</em> give you exactly the results it should. </p>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "answer_id": 38563861, "question_id": 38563707, "tags": ["c++", "c", "compiler-construction", "integer-overflow"]}, {"body": "<p>A compiler is only allowed to re-order under the <em>as if</em> rule.  That is, if the reordering will always give the same result as the specified ordering, then it is allowed.  Otherwise (as in your example), not.</p>\n\n<p>For example, given the following expression</p>\n\n<pre><code>i32big1 - i32big2 + i32small\n</code></pre>\n\n<p>which has been carefully constructed to subtract the two values which are known to be large but similar, and only <em>then</em> add the other small value (thus avoiding any overflow), the compiler may choose to reorder into:</p>\n\n<pre><code>(i32small - i32big2) + i32big1\n</code></pre>\n\n<p>and rely on the fact that the target platform is using two-complement arithmetic with wrap-round to prevent problems.  (Such a reordering might be sensible if the compiler is pressed for registers, and happens to have <code>i32small</code> in a register already).</p>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "answer_id": 38563866, "question_id": 38563707, "tags": ["c++", "c", "compiler-construction", "integer-overflow"]}, {"body": "<blockquote>\n  <p>Are compilers allowed to do such a reordering or can we trust them to notice the result inconsistency and keep the expression order as is?</p>\n</blockquote>\n\n<p>The compiler can reorder only if it gives the same result - here, as you observed, it doesn't.</p>\n\n<hr>\n\n<p>It's possible to write a function template, if you want one, which promotes all arguments to <code>std::common_type</code> before adding - this would be safe, and not rely on either argument order or manual casting, but it's pretty clunky.</p>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "answer_id": 38563956, "question_id": 38563707, "tags": ["c++", "c", "compiler-construction", "integer-overflow"]}, {"body": "<p>It depends on bit width of <code>unsigned/int</code>.</p>\n\n<p>The below 2 are not the same (when <code>unsigned &lt;= 32</code> bits).  <code>u32_x + u32_y</code> becomes 0.</p>\n\n<pre><code>u64_a = 0; u32_x = 1; u32_y = 0xFFFFFFFF;\nuint64_t u64_z = u32_x + u64_a + u32_y;\nuint64_t u64_z = u32_x + u32_y + u64_a;  // u32_x + u32_y carry does not add to sum.\n</code></pre>\n\n<p>They are the same (when <code>unsigned &gt;= 34</code> bits).  Integer promotions caused  <code>u32_x + u32_y</code> addition to occur at 64-bit math.  Order is irrelevant.</p>\n\n<p>It is UB (when <code>unsigned == 33</code> bits).  Integer promotions caused addition to occur at signed 33-bit math and signed overflow is UB.</p>\n\n<blockquote>\n  <p>Are compilers allowed to do such a reordering ...?</p>\n</blockquote>\n\n<p>(32 bit math): Re-order yes, but same results must occur, so not <em>that</em> re-ordering OP proposes.  Below are the same  </p>\n\n<pre><code>// Same\nu32_x + u64_a + u32_y;\nu64_a + u32_x + u32_y;\nu32_x + (uint64_t) u32_y + u64_a;\n...\n\n// Same as each other below, but not the same as the 3 above.\nuint64_t u64_z = u32_x + u32_y + u64_a;\nuint64_t u64_z = u64_a + (u32_x + u32_y);\n</code></pre>\n\n<blockquote>\n  <p>...  can we trust them to notice the result inconsistency and keep the expression order as is?</p>\n</blockquote>\n\n<p>Trust yes, but OP's coding goal is not crystal clear.  Should <code>u32_x + u32_y</code> carry contribute?  If OP wants that contribution, code should be</p>\n\n<pre><code>uint64_t u64_z = u64_a + u32_x + u32_y;\nuint64_t u64_z = u32_x + u64_a + u32_y;\nuint64_t u64_z = u32_x + (u32_y + u64_a);\n</code></pre>\n\n<p>But not </p>\n\n<pre><code>uint64_t u64_z = u32_x + u32_y + u64_a;\n</code></pre>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "answer_id": 38569184, "question_id": 38563707, "tags": ["c++", "c", "compiler-construction", "integer-overflow"]}, {"body": "<p>Quoting from the <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf\">standards</a>:</p>\n\n<blockquote>\n  <p>[ Note: Operators can be regrouped according to the usual\n  mathematical rules only where the operators really are associative or\n  commutative.7 For example, in the following fragment int a, b;</p>\n\n<pre><code>/\u2217 ... \u2217/\na = a + 32760 + b + 5;\n</code></pre>\n  \n  <p>the expression statement behaves exactly the same as</p>\n\n<pre><code>a = (((a + 32760) + b) + 5);\n</code></pre>\n  \n  <p>due to the associativity and precedence of these operators. Thus, the\n  result of the sum (a + 32760) is next added to b, and that result is\n  then added to 5 which results in the value assigned to a. On a machine\n  in which overflows produce an exception and in which the range of\n  values representable by an int is [-32768,+32767], the implementation\n  cannot rewrite this expression as</p>\n\n<pre><code>a = ((a + b) + 32765);\n</code></pre>\n  \n  <p>since if the values for a and b were, respectively, -32754 and -15,\n  the sum a + b would produce an exception while the original expression\n  would not; nor can the expression be rewritten either as</p>\n\n<pre><code>a = ((a + 32765) + b);\n</code></pre>\n  \n  <p>or</p>\n\n<pre><code>a = (a + (b + 32765));\n</code></pre>\n  \n  <p>since the values for a and b might have been, respectively, 4 and -8\n  or -17 and 12. However on a machine in which overflows do not produce\n  an exception and in which the results of overflows are reversible, the\n  above expression statement can be rewritten by the implementation in\n  any of the above ways because the same result will occur. \u2014 end note ]</p>\n</blockquote>\n", "title": "(A + B + C) \u2260 (A + C + B\u200b) and compiler reordering", "answer_id": 38581553, "question_id": 38563707, "tags": ["c++", "c", "compiler-construction", "integer-overflow"]}], "question_id": 38563707}, {"body": "<p>I'm testing the tuple structure, and I found it's strange when I use the <code>==</code> operator like:</p>\n\n<pre><code>&gt;&gt;&gt;  (1,) == 1,\nOut: (False,)\n</code></pre>\n\n<p>When I assign these two expressions to a variable, the result is true:</p>\n\n<pre><code>&gt;&gt;&gt; a = (1,)\n&gt;&gt;&gt; b = 1,\n&gt;&gt;&gt; a==b\nOut: True\n</code></pre>\n\n<p>This questions is different from <a href=\"http://stackoverflow.com/questions/7992559/python-tuple-trailing-comma-syntax-rule\">Python tuple trailing comma syntax rule</a> in my view. I ask the group of expressions between <code>==</code> operator. </p>\n", "title": "What&#39;s the meaning of &quot;(1,) == 1,&quot; in Python?", "tags": ["python", "tuples", "equals-operator"], "answer_count": 3, "link": "http://stackoverflow.com/questions/37313471/whats-the-meaning-of-1-1-in-python", "answers": [{"body": "<p>This is just operator precedence.  Your first</p>\n\n<pre><code>(1,) == 1,\n</code></pre>\n\n<p>groups like so:</p>\n\n<pre><code>((1,) == 1),\n</code></pre>\n\n<p>so builds a tuple with a single element from the result of comparing the one-element tuple <code>1,</code> to the integer <code>1</code> for equality  They're not equal, so you get the 1-tuple <code>False,</code> for a result.</p>\n", "title": "What&#39;s the meaning of &quot;(1,) == 1,&quot; in Python?", "answer_id": 37313506, "question_id": 37313471, "tags": ["python", "tuples", "equals-operator"]}, {"body": "<p>When you do </p>\n\n<pre><code>&gt;&gt;&gt; (1,) == 1,\n</code></pre>\n\n<p>it builds a tuple with the result from comparing the <em>tuple</em> <code>(1,)</code> with an <em>integer</em> and thus returning <code>False</code>.</p>\n\n<p>Instead when you assign to variables, the two <em>equal tuples</em> are compared with each other.</p>\n\n<p>You can try:</p>\n\n<pre><code>&gt;&gt;&gt; x = 1,\n&gt;&gt;&gt; x\n(1,)\n</code></pre>\n", "title": "What&#39;s the meaning of &quot;(1,) == 1,&quot; in Python?", "answer_id": 37313562, "question_id": 37313471, "tags": ["python", "tuples", "equals-operator"]}, {"body": "<p>Other answers have already shown you that the behaviour is due to operator precedence, as documented <a href=\"https://docs.python.org/3/reference/expressions.html#operator-precedence\">here</a>.  </p>\n\n<p>I'm going to show you how to find the answer yourself next time you have a question similar to this.  You can deconstruct how the expression parses using the <a href=\"https://docs.python.org/3/library/ast.html\"><code>ast</code></a> module:</p>\n\n<pre><code>&gt;&gt;&gt; import ast\n&gt;&gt;&gt; source_code = '(1,) == 1,'\n&gt;&gt;&gt; print(ast.dump(ast.parse(source_code), annotate_fields=False))\nModule([Expr(Tuple([Compare(Tuple([Num(1)], Load()), [Eq()], [Num(1)])], Load()))])\n</code></pre>\n\n<p>From this we can see that the code gets parsed <a href=\"http://stackoverflow.com/a/37313506/674039\">as Tim Peters explained</a>: </p>\n\n<pre><code>Module([Expr(\n    Tuple([\n        Compare(\n            Tuple([Num(1)], Load()), \n            [Eq()], \n            [Num(1)]\n        )\n    ], Load())\n)])\n</code></pre>\n", "title": "What&#39;s the meaning of &quot;(1,) == 1,&quot; in Python?", "answer_id": 37313614, "question_id": 37313471, "tags": ["python", "tuples", "equals-operator"]}], "question_id": 37313471}, {"body": "<p>In Google Chrome's developer tools, when I select an element, I see     <code>==$0</code> next to the selected element. What does that mean?</p>\n\n<p><a href=\"http://i.stack.imgur.com/C2eGI.jpg\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/C2eGI.jpg\" alt=\"Screenshot\"></a></p>\n", "title": "What does ==$0 (double equals dollar zero) mean in Chrome Developer Tools?", "tags": ["google-chrome", "google-chrome-devtools", "code-inspection"], "answer_count": 3, "link": "http://stackoverflow.com/questions/36999739/what-does-0-double-equals-dollar-zero-mean-in-chrome-developer-tools", "answers": [{"body": "<p>It's the last selected DOM node index. Chrome assigns an index to each DOM node you select. So <code>$0</code> will always point to the last node you selected, while <code>$1</code> will point to the node you selected before that. Think of it like a stack of most recently selected nodes.</p>\n\n<p>As an example, consider the following</p>\n\n<pre><code>&lt;div id=\"sunday\"&gt;&lt;/div&gt;\n&lt;div id=\"monday\"&gt;&lt;/div&gt;\n&lt;div id=\"tuesday\"&gt;&lt;/div&gt;\n</code></pre>\n\n<p>Now you opened the devtools console and selected <code>#sunday</code>, <code>#monday</code> and <code>#tuesday</code> in the mentioned order, you will get ids like:</p>\n\n<pre><code>$0 -&gt; &lt;div id=\"tuesday\"&gt;&lt;/div&gt; \n$1 -&gt; &lt;div id=\"monday\"&gt;&lt;/div&gt;\n$2 -&gt; &lt;div id=\"sunday\"&gt;&lt;/div&gt;\n</code></pre>\n\n<p><strong>Note:</strong>\nIt Might be useful to know that the node is selectable in your scripts (or console), for example one popular use for this is angular element selector, so you can simply pick your node, and run this:</p>\n\n<pre><code>angular.element($0).scope()\n</code></pre>\n\n<p>Voila you got access to node scope via console.</p>\n", "title": "What does ==$0 (double equals dollar zero) mean in Chrome Developer Tools?", "answer_id": 36999842, "question_id": 36999739, "tags": ["google-chrome", "google-chrome-devtools", "code-inspection"]}, {"body": "<blockquote>\n  <p>$0 returns the most recently selected element or JavaScript object,\n  $1 returns the second most recently selected one, and so on.</p>\n</blockquote>\n\n<p>Refer : <a href=\"https://developer.chrome.com/devtools/docs/commandline-api\">Command Line API Reference</a></p>\n", "title": "What does ==$0 (double equals dollar zero) mean in Chrome Developer Tools?", "answer_id": 36999871, "question_id": 36999739, "tags": ["google-chrome", "google-chrome-devtools", "code-inspection"]}, {"body": "<p><strong>$0 - $4</strong>\nThe <strong>$0, $1, $2, $3 and $4</strong> commands work as a historical reference to the last five DOM elements inspected within the Elements panel or the last five JavaScript heap objects selected in the Profiles panel. $0 returns the most recently selected element or JavaScript object, $1 returns the second most recently selected one, and so on.</p>\n\n<p>User console: give input $0 and check output.\nRefer\n<a href=\"https://developers.google.com/web/tools/chrome-devtools/debug/command-line/command-line-reference?hl=en#section-1\">LINK</a></p>\n", "title": "What does ==$0 (double equals dollar zero) mean in Chrome Developer Tools?", "answer_id": 37180120, "question_id": 36999739, "tags": ["google-chrome", "google-chrome-devtools", "code-inspection"]}], "question_id": 36999739}, {"body": "<p>My <code>XML</code> layout is not rendering with this error message. I am already using Java 8.</p>\n\n<p><a href=\"http://i.stack.imgur.com/qNGLL.png\"><img src=\"http://i.stack.imgur.com/qNGLL.png\" alt=\"enter image description here\"></a></p>\n\n<p>Also using latest build tools in Gradle.</p>\n\n<pre><code>android {\n    compileSdkVersion 'android-N'\n    buildToolsVersion 24.0.0 rc1\n    ...\n}\n</code></pre>\n\n<p>XML Error - </p>\n\n<p><a href=\"http://i.stack.imgur.com/bj7SN.png\"><img src=\"http://i.stack.imgur.com/bj7SN.png\" alt=\"enter image description here\"></a></p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "tags": ["android", "android-n"], "answer_count": 13, "link": "http://stackoverflow.com/questions/35928580/android-n-requires-the-ide-to-be-running-with-java-1-8-or-later", "answers": [{"body": "<p>I got the same error you describe but on OS X. I know this may not solve your problem (because I don't know the equivalent for Windows), but since this is not OS specific I am leaving the solution for any OS X fellow out there.\nEdit the file <code>/Applications/Android\\ Studio.app/Contents/Info.plist</code>, and remove the 1.6 version (in my case), you may place 1.6+, 1.7+ or 1.8, whichever you prefer. Mine ended up like this</p>\n\n<pre><code>...\n&lt;key&gt;JVMVersion&lt;/key&gt;\n&lt;string&gt;1.7+&lt;/string&gt;\n...\n</code></pre>\n\n<p>Be aware there's an incompatibility with drag and drop with java versions 1.8_60 to 1.8_75 (<a href=\"https://youtrack.jetbrains.com/issue/IDEA-146691\">IDEA-146691</a>) which the IDE will report once you successfully change the version.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 35935433, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>Click on the letter N, next to the android icon in the Preview window and select API level 23 or lower. That should solve it.</p>\n\n<p><a href=\"http://i.stack.imgur.com/C7QZj.png\"><img src=\"http://i.stack.imgur.com/C7QZj.png\" alt=\"First row of icons in the rpeview window\"></a></p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 35952099, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>Set <code>JDK_HOME</code> and <code>JAVA_HOME</code> environment variables, in my case <code>C:\\Program Files\\Java\\jdk1.8.0_60</code>.</p>\n\n<p><a href=\"http://i.stack.imgur.com/FtEV8.png\"><img src=\"http://i.stack.imgur.com/FtEV8.png\" alt=\"enter image description here\"></a></p>\n\n<p>Check Java version in <code>Help &gt; About</code>. It should show the JRE version you just set.</p>\n\n<p><a href=\"http://i.stack.imgur.com/EOWtQ.png\"><img src=\"http://i.stack.imgur.com/EOWtQ.png\" alt=\"enter image description here\"></a></p>\n\n<p>That is it, now you can preview your layout in <code>N</code>.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 35974928, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>From the docs:\n<a href=\"http://tools.android.com/tech-docs/configuration\" rel=\"nofollow\">http://tools.android.com/tech-docs/configuration</a></p>\n\n<blockquote>\n  <p>You can also place use environment variables to point to specific\n  override files elsewhere: STUDIO_JDK, which JDK to run studio with</p>\n</blockquote>\n\n<p>I prefer using the <strong>STUDIO_JDK</strong> variable and leave other default java home variables alone.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 36884694, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>I'm using Ubuntu 14.04 and I was facing a similar error, with the difference I don't have NDK. My solution was, after setting up Java 8 in project definition, reboot Android Studio, no more.</p>\n\n<p>Check system Java version, in console \"java -version\", output should say something like this</p>\n\n<pre><code>java version \"1.8.0_91\"\n</code></pre>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 37850174, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>I met the same issue.</p>\n\n<p>I installed jdk1.6,jdk1.7,jdk1.8, and deployed Studio as what you did.</p>\n\n<p>My solution is uninstall jdk1.6 and jdk1.7, only leave jdk1.8</p>\n\n<pre><code>sudo rm -rf ${path to jdk1.6}\nsudo rm -rf ${path to jdk1.7}\n</code></pre>\n\n<p>That's all.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 37871750, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>My issue was that the Android N was \"partially installed\".</p>\n\n<p>My solution was to go to -\nPreferences/Appearances and Behavior/System Settings/Android SDK and check the Status. If it's \"partially installed\" then you have to uninstall then reinstall. (Uncheck the checkbox and hit apply; it will uninstall; then you can check the box and install it again) Worked for me. If you go to lower version(like 23) then you have to change the buildToolsVersion variable to \"23.0.0\" in the build.gradle.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 37935109, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>I was facing the same problem recently. Try changing the API preview level to 23 or lower. That worked for me.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 38041513, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>I faced the problem \"Android N requires the IDE to be running with Java 1.8 or later.\" The error kept showing up in spite of setting <code>C:\\Program Files\\Java\\jdk1.8.0_92</code> as the project's JDK in <code>File &gt; Project Structure</code>. </p>\n\n<p>I solved it by updating JAVA_HOME in my environment variables from <code>C:\\Program Files\\Java\\jdk1.7.0_67</code> to <code>C:\\Program Files\\Java\\jdk1.8.0_92</code>. When I did that and restarted Android Studio, the design mode became available.</p>\n\n<p>OS: Windows 10</p>\n\n<p>Android Studio: 2.1.2</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 38367730, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>Had the same problem with Ubunutu. The problem was not running the IDE with Java8.</p>\n\n<p>I fixed it with updating the default java program of the OS, using:</p>\n\n<pre><code>sudo update-alternatives --config java\n</code></pre>\n\n<p>And choosing the relevant Java8 path.</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 38393230, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>For <strong>Mac users</strong> with this issue (I'm running OS X 10.11.5 and Android Studio 2.1.2), here's how you can solve it:</p>\n\n<p>Currently, the Mac version of Android Studio is run with Java 6 (not necessarily the same as the JDK version for the app), because Java 6 has better font rendering than Java 7 &amp; 8, and that's apparently more important than rendering your layout.</p>\n\n<p>First of all, make sure you have <a href=\"http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\">JDK 8 installed</a>.</p>\n\n<p>Then run the following commands:</p>\n\n<pre><code>$ export STUDIO_JDK=/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk\n$ open /Applications/Android\\ Studio.app/\n</code></pre>\n\n<p>You may need to change the <code>91</code> in <code>jdk1.8.0_91.jdk</code> to a different version. You can see which JDK versions you have with:</p>\n\n<pre><code>$ ls /Library/Java/JavaVirtualMachines/\n</code></pre>\n\n<p>Unfortunately, this method requires opening the app from the terminal, but you could always create a script for it.</p>\n\n<p><a href=\"http://tools.android.com/tech-docs/configuration/osx-jdk\">More info</a></p>\n\n<p><strong>Edit:</strong> And if you really don't want to open it from the terminal:</p>\n\n<ol>\n<li>Create an AppleScript with contents:\n<code>do shell script \"export STUDIO_JDK=/Library/Java/JavaVirtualMachines/jdk1.8.0_91.jdk; open /Applications/Android\\\\ Studio.app/\"</code></li>\n<li><strong>File > Save</strong> Choose file format <em>Application</em>. Save it somewhere.</li>\n<li>If you want to change the icon, <a href=\"https://discussions.apple.com/thread/827809?start=0&amp;tstart=0\">see here.</a></li>\n<li>Drag the app to your launchpad if you like.</li>\n</ol>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 38493792, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>machine: windows8 64bit\nin my case the jdk1.7 I had installed was 32bit hence I was using studio.exe (32bit) so even when I downloaded the new jdk1.8 (64bit) the studio instance I was using (32bit) couldn't detect it, but when I uninstalled the older jdk and tried to rerun studio.exe I was told to install a 32bit jdk. \nso I ran studio64.exe and it ran smoothly.\nensure you have the correct path in your environment variables. cheers android N</p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 39004660, "question_id": 35928580, "tags": ["android", "android-n"]}, {"body": "<p>Make sure you have installed jdk for x64 and not x84 when starting Android Studio (x64)</p>\n\n<p><a href=\"http://i.stack.imgur.com/sWTU1.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/sWTU1.png\" alt=\"enter image description here\"></a></p>\n", "title": "Android N requires the IDE to be running with Java 1.8 or later?", "answer_id": 39279615, "question_id": 35928580, "tags": ["android", "android-n"]}], "question_id": 35928580}, {"body": "<p>The following code snippet executes two threads, one is a simple timer logging every second, the second is an infinite loop that executes a remainder operation:</p>\n\n<pre><code>public class TestBlockingThread {\n    private static final Logger LOGGER = LoggerFactory.getLogger(TestBlockingThread.class);\n\n    public static final void main(String[] args) throws InterruptedException {\n        Runnable task = () -&gt; {\n            int i = 0;\n            while (true) {\n                i++;\n                if (i != 0) {\n                    boolean b = 1 % i == 0;\n                }\n            }\n        };\n\n        new Thread(new LogTimer()).start();\n        Thread.sleep(2000);\n        new Thread(task).start();\n    }\n\n    public static class LogTimer implements Runnable {\n        @Override\n        public void run() {\n            while (true) {\n                long start = System.currentTimeMillis();\n                try {\n                    Thread.sleep(1000);\n                } catch (InterruptedException e) {\n                    // do nothing\n                }\n                LOGGER.info(\"timeElapsed={}\", System.currentTimeMillis() - start);\n            }\n        }\n    }\n}\n</code></pre>\n\n<p>This gives the following result:</p>\n\n<pre><code>[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=1004\n[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=1003\n[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=13331\n[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=1006\n[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=1003\n[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=1004\n[Thread-0] INFO  c.m.c.concurrent.TestBlockingThread - timeElapsed=1004\n</code></pre>\n\n<p>I don't understand why the infinite task blocks all other threads for 13.3 seconds. I tried to change thread priorities and other settings, nothing worked.</p>\n\n<p>If you have any suggestions to fix this (including tweaking OS context switching settings) please let me know. </p>\n", "title": "Java thread executing remainder operation in a loop blocks all other threads", "tags": ["java", "multithreading"], "answer_count": 4, "link": "http://stackoverflow.com/questions/39298474/java-thread-executing-remainder-operation-in-a-loop-blocks-all-other-threads", "answers": [{"body": "<p>After&nbsp;all the explanations here (thanks to <a href=\"http://stackoverflow.com/users/57695/peter-lawrey\">Peter Lawrey</a>) we found that the main source of this pause is that safepoint inside the loop is reached rather rarely so it takes a long time to stop all threads for JIT-compiled code replacement. </p>\n\n<p>But I decided to go deeper and find <strong><em>why</em></strong> safepoint is reached rarely. I found it a bit confusing why the back jump of <code>while</code> loop is not \"safe\" in this case.</p>\n\n<p>So I summon <code>-XX:+PrintAssembly</code> in all its glory to help</p>\n\n<pre><code>-XX:+UnlockDiagnosticVMOptions \\\n-XX:+TraceClassLoading \\\n-XX:+DebugNonSafepoints \\\n-XX:+PrintCompilation \\\n-XX:+PrintGCDetails \\\n-XX:+PrintStubCode \\\n-XX:+PrintAssembly \\\n-XX:PrintAssemblyOptions=-Mintel\n</code></pre>\n\n<p>After some investigation I found that after third recompilation of lambda <code>C2</code> compiler threw away safepoint polls inside loop completely.</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>During the profiling stage variable <code>i</code> was never seen equal to 0. That's why <code>C2</code> speculatively optimized this branch away, so that the loop was transformed to something like</p>\n\n<pre><code>for (int i = OSR_value; i != 0; i++) {\n    if (1 % i == 0) {\n        uncommon_trap();\n    }\n}\nuncommon_trap();\n</code></pre>\n\n<p>Note that originally infinite loop was reshaped to a regular finite loop with a counter! Due to JIT optimization to eliminate safepoint polls in finite counted loops, there was no safepoint poll in this loop either.</p>\n\n<p>After some time, <code>i</code> wrapped back to <code>0</code>, and the uncommon trap was taken. The method was deoptimized and continued execution in the interpreter. During recompilation with a new knowledge <code>C2</code> recognized the infinite loop and gave up compilation. The rest of the method proceeded in the interpreter with proper safepoints.</p>\n\n<p>There is a great must-read blog post <a href=\"http://psy-lob-saw.blogspot.ru/2015/12/safepoints.html\">\"Safepoints: Meaning, Side Effects and Overheads\"</a> by <a href=\"http://stackoverflow.com/users/1047667/nitsan-wakart\">Nitsan Wakart</a> covering safepoints and this particular issue.</p>\n\n<p>Safepoint elimination in very long counted loops is known to be a problem. The bug <a href=\"https://bugs.openjdk.java.net/browse/JDK-5014723\"><code>JDK-5014723</code></a> (thanks to <a href=\"https://twitter.com/iwan0www\">Vladimir Ivanov</a>) addresses this problem.</p>\n\n<p>The workaround is available until the bug is finally fixed.</p>\n\n<ol>\n<li>You can try using <a href=\"https://bugs.openjdk.java.net/browse/JDK-6869327\"><code>-XX:+UseCountedLoopSafepoints</code></a> (it <em>will</em> cause overall performance penalty and <strong>may lead to JVM crash</strong> <a href=\"https://bugs.openjdk.java.net/browse/JDK-8161147\"><code>JDK-8161147</code></a>). After using it <code>C2</code> compiler continue keeping safepoints at the back jumps and original pause disappears completely.</li>\n<li><p>You can explicitly disable compilation of problematic method by using<br/>\n<code>-XX:CompileCommand='exclude,binary/class/Name,methodName'</code></p></li>\n<li><p>Or you can rewrite your code by adding safepoint manually. For example <code>Thread.yield()</code> call at the end of cycle or even changing <code>int i</code> to <code>long i</code> (thanks, <a href=\"http://stackoverflow.com/users/1047667/nitsan-wakart\">Nitsan Wakart</a>) will also fix pause.</p></li>\n</ol>\n", "title": "Java thread executing remainder operation in a loop blocks all other threads", "answer_id": 39298768, "question_id": 39298474, "tags": ["java", "multithreading"]}, {"body": "<p>In short, the loop you have has no safe point inside it except when <code>i == 0</code> is reached. When this method is compiled and triggers the code to be replaced it needs to bring all the threads to a safe point, but this takes a very long time, locking up not just the thread running the code but all threads in the JVM.</p>\n\n<p>I added the following command line options.</p>\n\n<pre><code>-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintCompilation\n</code></pre>\n\n<p>I also modified the code to use floating point which appears to take longer.</p>\n\n<pre><code>boolean b = 1.0 / i == 0;\n</code></pre>\n\n<p>And what I see in the output is</p>\n\n<pre><code>timeElapsed=100\nApplication time: 0.9560686 seconds\n  41423  280 %     4       TestBlockingThread::lambda$main$0 @ -2 (27 bytes)   made not entrant\nTotal time for which application threads were stopped: 40.3971116 seconds, Stopping threads took: 40.3967755 seconds\nApplication time: 0.0000219 seconds\nTotal time for which application threads were stopped: 0.0005840 seconds, Stopping threads took: 0.0000383 seconds\n  41424  281 %     3       TestBlockingThread::lambda$main$0 @ 2 (27 bytes)\ntimeElapsed=40473\n  41425  282 %     4       TestBlockingThread::lambda$main$0 @ 2 (27 bytes)\n  41426  281 %     3       TestBlockingThread::lambda$main$0 @ -2 (27 bytes)   made not entrant\ntimeElapsed=100\n</code></pre>\n\n<p>Note: for code to be replaced, threads have to be stopped at a safe point.  However it appears here that such a safe point is reached very rarely (possibly only when <code>i == 0</code> Changing the task to</p>\n\n<pre><code>Runnable task = () -&gt; {\n    for (int i = 1; i != 0 ; i++) {\n        boolean b = 1.0 / i == 0;\n    }\n};\n</code></pre>\n\n<p>I see a similar delay.</p>\n\n<pre><code>timeElapsed=100\nApplication time: 0.9587419 seconds\n  39044  280 %     4       TestBlockingThread::lambda$main$0 @ -2 (28 bytes)   made not entrant\nTotal time for which application threads were stopped: 38.0227039 seconds, Stopping threads took: 38.0225761 seconds\nApplication time: 0.0000087 seconds\nTotal time for which application threads were stopped: 0.0003102 seconds, Stopping threads took: 0.0000105 seconds\ntimeElapsed=38100\ntimeElapsed=100\n</code></pre>\n\n<hr>\n\n<p>Adding code to the loop carefully you get a longer delay.</p>\n\n<pre><code>for (int i = 1; i != 0 ; i++) {\n    boolean b = 1.0 / i / i == 0;\n}\n</code></pre>\n\n<p>gets</p>\n\n<pre><code> Total time for which application threads were stopped: 59.6034546 seconds, Stopping threads took: 59.6030773 seconds\n</code></pre>\n\n<p>However, change the code to use a native method which always has a safe point (if it is not an intrinsic)</p>\n\n<pre><code>for (int i = 1; i != 0 ; i++) {\n    boolean b = Math.cos(1.0 / i) == 0;\n}\n</code></pre>\n\n<p>prints</p>\n\n<pre><code>Total time for which application threads were stopped: 0.0001444 seconds, Stopping threads took: 0.0000615 seconds\n</code></pre>\n\n<p>Note: adding <code>if (Thread.currentThread().isInterrupted()) { ... }</code> to a loop adds a safe point.</p>\n\n<p>Note: This happened on a 16 core machine so there is no lack of CPU resources.</p>\n", "title": "Java thread executing remainder operation in a loop blocks all other threads", "answer_id": 39299027, "question_id": 39298474, "tags": ["java", "multithreading"]}, {"body": "<p>After following the comment threads and some testing on my own, I believe that the pause is caused by the JIT compiler. Why the JIT compiler is taking such a long time is beyond my ability to debug.</p>\n\n<p>However, since you only asked for how to prevent this, I have a solution:</p>\n\n<p>Pull your infinite loop into a method where it can be excluded from the JIT compiler</p>\n\n<pre><code>public class TestBlockingThread {\n    private static final Logger LOGGER = Logger.getLogger(TestBlockingThread.class.getName());\n\n    public static final void main(String[] args) throws InterruptedException     {\n        Runnable task = () -&gt; {\n            infLoop();\n        };\n        new Thread(new LogTimer()).start();\n        Thread.sleep(2000);\n        new Thread(task).start();\n    }\n\n    private static void infLoop()\n    {\n        int i = 0;\n        while (true) {\n            i++;\n            if (i != 0) {\n                boolean b = 1 % i == 0;\n            }\n        }\n    }\n</code></pre>\n\n<p>Run your program with this VM argument: </p>\n\n<p>-XX:CompileCommand=exclude,PACKAGE.TestBlockingThread::infLoop (replace PACKAGE with your package information)</p>\n\n<p>You should get a message like this to indicate when the method would have been JIT-compiled: <br/>### Excluding compile: static blocking.TestBlockingThread::infLoop\n<br/>you may notice that I put the class into a package called blocking</p>\n", "title": "Java thread executing remainder operation in a loop blocks all other threads", "answer_id": 39299135, "question_id": 39298474, "tags": ["java", "multithreading"]}, {"body": "<p>Found the answer of <em>why</em>. They are called safepoints, and are best known as the Stop-The-World that happens because of GC.</p>\n\n<p>See this articles: <a href=\"https://plumbr.eu/blog/performance-blog/logging-stop-the-world-pauses-in-jvm\">Logging stop-the-world pauses in JVM</a></p>\n\n<blockquote>\n  <p>Different events can cause the JVM to pause all the application threads. Such pauses are called Stop-The-World (STW) pauses. The most common cause for an STW pause to be triggered is garbage collection (example in github) , but different <strong>JIT actions</strong> (example), biased lock revocation (example), certain JVMTI operations , and many more also require the application to be stopped.</p>\n  \n  <p>The points at which the application threads may be safely stopped are called, surprise, <strong>safepoints</strong>. This term is also often used to refer to all the STW pauses.</p>\n  \n  <p>It is more or less common that GC logs are enabled. However, this does not capture information on all the safepoints. To get it all, use these JVM options:</p>\n\n<pre><code>-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime\n</code></pre>\n  \n  <p>If you are wondering about the naming explicitly referring to GC, don\u2019t be alarmed \u2013 turning on these options logs all of the safepoints, not just garbage collection pauses. If you run a following example (source in github) with the flags specified above.</p>\n</blockquote>\n\n<p>Reading the <a href=\"http://openjdk.java.net/groups/hotspot/docs/HotSpotGlossary.html\">HotSpot Glossary of Terms</a>, it defines this:</p>\n\n<blockquote>\n  <p><strong>safepoint</strong></p>\n  \n  <p>A point during program execution at which all GC roots are known and all heap object contents are consistent. From a global point of view, all threads must block at a safepoint before the GC can run. (As a special case, threads running JNI code can continue to run, because they use only handles. During a safepoint they must block instead of loading the contents of the handle.) From a local point of view, a safepoint is a distinguished point in a block of code where the executing thread may block for the GC. <strong>Most call sites qualify as safepoints.</strong> There are strong invariants which hold true at every safepoint, which may be disregarded at non-safepoints. Both compiled Java code and C/C++ code be optimized between safepoints, but less so across safepoints. The JIT compiler emits a GC map at each safepoint. C/C++ code in the VM uses stylized macro-based conventions (e.g., TRAPS) to mark potential safepoints.</p>\n</blockquote>\n\n<p>Running with the above mentioned flags, I get this output:</p>\n\n<pre class=\"lang-none prettyprint-override\"><code>Application time: 0.9668750 seconds\nTotal time for which application threads were stopped: 0.0000747 seconds, Stopping threads took: 0.0000291 seconds\ntimeElapsed=1015\nApplication time: 1.0148568 seconds\nTotal time for which application threads were stopped: 0.0000556 seconds, Stopping threads took: 0.0000168 seconds\ntimeElapsed=1015\ntimeElapsed=1014\nApplication time: 2.0453971 seconds\nTotal time for which application threads were stopped: 10.7951187 seconds, Stopping threads took: 10.7950774 seconds\ntimeElapsed=11732\nApplication time: 1.0149263 seconds\nTotal time for which application threads were stopped: 0.0000644 seconds, Stopping threads took: 0.0000368 seconds\ntimeElapsed=1015\n</code></pre>\n\n<p>Notice the third STW event:<br>\n<em>Total time stopped:</em> 10.7951187 seconds<br>\n<em>Stopping threads took:</em> <strong>10.7950774 seconds</strong></p>\n\n<p>JIT itself took virtually no time, but once the JVM had decided to perform a JIT compilation, it entered STW mode, however since the code to be compiled (the infinite loop) doesn't have a <em>call site</em>, no safepoint was ever reached.</p>\n\n<p>The STW ends when JIT eventually gives up waiting and concludes the code is in an infinite loop.</p>\n", "title": "Java thread executing remainder operation in a loop blocks all other threads", "answer_id": 39299153, "question_id": 39298474, "tags": ["java", "multithreading"]}], "question_id": 39298474}, {"body": "<p>There is a lot of talk about the latest kid in redux town right now, <a href=\"https://github.com/yelouafi/redux-saga\">yelouafi/redux-saga</a>. It uses generator functions for listening to/dispatching actions.</p>\n\n<p>Before I wrap my head around it, I would like to know the pros/cons of using <code>redux-saga</code> instead of the approach below where I'm using <code>redux-thunk</code> with async/await.</p>\n\n<p>A component might look like this, dispatch actions like usual.</p>\n\n<pre><code>import { login } from 'redux/auth';\n\nclass LoginForm extends Component {\n\n  onClick(e) {\n    e.preventDefault();\n    const { user, pass } = this.refs;\n    this.props.dispatch(login(user.value, pass.value));\n  }\n\n  render() {\n    return (&lt;div&gt;\n        &lt;input type=\"text\" ref=\"user\" /&gt;\n        &lt;input type=\"password\" ref=\"pass\" /&gt;\n        &lt;button onClick={::this.onClick}&gt;Sign In&lt;/button&gt;\n    &lt;/div&gt;);\n  } \n}\n\nexport default connect((state) =&gt; ({}))(LoginForm);\n</code></pre>\n\n<p>Then my actions look something like this:</p>\n\n<pre><code>// auth.js\n\nimport request from 'axios';\nimport { loadUserData } from './user';\n\n// define constants\n// define initial state\n// export default reducer\n\nexport const login = (user, pass) =&gt; async (dispatch) =&gt; {\n    try {\n        dispatch({ type: LOGIN_REQUEST });\n        let { data } = await request.post('/login', { user, pass });\n        await dispatch(loadUserData(data.uid));\n        dispatch({ type: LOGIN_SUCCESS, data });\n    } catch(error) {\n        dispatch({ type: LOGIN_ERROR, error });\n    }\n}\n\n// more actions...\n</code></pre>\n\n<hr>\n\n<pre><code>// user.js\n\nimport request from 'axios';\n\n// define constants\n// define initial state\n// export default reducer\n\nexport const loadUserData = (uid) =&gt; async (dispatch) =&gt; {\n    try {\n        dispatch({ type: USERDATA_REQUEST });\n        let { data } = await request.get(`/users/${uid}`);\n        dispatch({ type: USERDATA_SUCCESS, data });\n    } catch(error) {\n        dispatch({ type: USERDATA_ERROR, error });\n    }\n}\n\n// more actions...\n</code></pre>\n", "title": "Pros/cons of using redux-saga with ES6 generators vs redux-thunk with ES7 async/await", "tags": ["javascript", "reactjs", "redux", "redux-thunk", "redux-saga"], "answer_count": 3, "link": "http://stackoverflow.com/questions/34930735/pros-cons-of-using-redux-saga-with-es6-generators-vs-redux-thunk-with-es7-async", "answers": [{"body": "<p>In redux-saga, the equivalent of the above example would be</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>export function* loginSaga() {\n  while(true) {\n    const { user, pass } = yield take(LOGIN_REQUEST)\n    try {\n      let { data } = yield call(request.post, '/login', { user, pass });\n      yield fork(loadUserData, data.uid);\n      yield put({ type: LOGIN_SUCCESS, data });\n    } catch(error) {\n      yield put({ type: LOGIN_ERROR, error });\n    }  \n  }\n}\n\nexport function* loadUserData(uid) {\n  try {\n    yield put({ type: USERDATA_REQUEST });\n    let { data } = yield call(request.get, `/users/${uid}`);\n    yield put({ type: USERDATA_SUCCESS, data });\n  } catch(error) {\n    yield put({ type: USERDATA_ERROR, error });\n  }\n}\n</code></pre>\n\n<p>The first thing to notice is that we're calling the api functions using the form <code>yield call(func, ...args)</code>. <code>call</code> doesn't execute the effect, it just creates a plain object like <code>{type: 'CALL', func, args}</code>. The execution is delegated to the redux-saga middleware which takes care of executing the function and resuming the generator with its result.</p>\n\n<p>The main advantage is that you can test the generator outside of Redux using simple equality checks</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>const iterator = loginSaga()\n\nassert.deepEqual(iterator.next().value, take(LOGIN_REQUEST))\n\n// resume the generator with some dummy action\nconst mockAction = {user: '...', pass: '...'}\nassert.deepEqual(\n  iterator.next(mockAction).value, \n  call(request.post, '/login', mockAction)\n)\n\n// simulate an error result\nconst mockError = 'invalid user/password'\nassert.deepEqual(\n  iterator.throw(mockError).value, \n  put({ type: LOGIN_ERROR, error: mockError })\n)\n</code></pre>\n\n<p>Note we're mocking the api call result by simply injecting the mocked data into the <code>next</code> method of the iterator. Mocking data is way simpler than mocking functions.</p>\n\n<p>The second thing to notice is the call to <code>yield take(ACTION)</code>. Thunks are called by the action creator on each new action (e.g. <code>LOGIN_REQUEST</code>). i.e. actions are continually <em>pushed</em> to thunks, and thunks have no control on when to stop handling those actions. </p>\n\n<p>In redux-saga, generators <em>pull</em> the next action. i.e. they have control when to listen for some action, and when to not. In the above example the flow instructions are placed inside a <code>while(true)</code> loop, so it'll listen for each incoming action, which somewhat mimics the thunk pushing behavior.</p>\n\n<p>The pull approach allows implementing complex control flows. Suppose for example we want to add the following requirements </p>\n\n<ul>\n<li><p>Handle LOGOUT user action</p></li>\n<li><p>upon the first successful login, the server returns a token which expires in some delay stored in a <code>expires_in</code> field. We'll have to refresh the authorization in the background on each <code>expires_in</code> milliseconds</p></li>\n<li><p>Take into account that when waiting for the result of api calls (either initial login or refresh) the user may logout in-between.</p></li>\n</ul>\n\n<p>How would you implement that with thunks; while also providing full test coverage for the entire flow? Here is how it may look with Sagas:</p>\n\n<pre class=\"lang-js prettyprint-override\"><code>function* authorize(credentials) {\n  const token = yield call(api.authorize, credentials)\n  yield put( login.success(token) )\n  return token\n}\n\nfunction* authAndRefreshTokenOnExpiry(name, password) {\n  let token = yield call(authorize, {name, password})\n  while(true) {\n    yield call(delay, token.expires_in)\n    token = yield call(authorize, {token})\n  }\n}\n\nfunction* watchAuth() {\n  while(true) {\n    try {\n      const {name, password} = yield take(LOGIN_REQUEST)\n\n      yield race([\n        take(LOGOUT),\n        call(authAndRefreshTokenOnExpiry, name, password)\n      ])\n\n      // user logged out, next while iteration will wait for the\n      // next LOGIN_REQUEST action\n\n    } catch(error) {\n      yield put( login.error(error) )\n    }\n  }\n}\n</code></pre>\n\n<p>In the above example, we're expressing our concurrency requirement using <code>race</code>. If <code>take(LOGOUT)</code> wins the race (i.e. user clicked on a Logout Button). The race will automatically cancel the <code>authAndRefreshTokenOnExpiry</code> background task. And if the <code>authAndRefreshTokenOnExpiry</code> was blocked in middle of a <code>call(authorize, {token})</code> call it'll also be cancelled. Cancellation propagates downward automatically.</p>\n\n<p>You can find a <a href=\"http://jsbin.com/sodowa/9/edit?js,console\">runnable demo of the above flow</a></p>\n", "title": "Pros/cons of using redux-saga with ES6 generators vs redux-thunk with ES7 async/await", "answer_id": 34933395, "question_id": 34930735, "tags": ["javascript", "reactjs", "redux", "redux-thunk", "redux-saga"]}, {"body": "<p>I will add my experience using saga in production system in addition to the library author's rather thorough answer.</p>\n\n<p>Pro (using saga):</p>\n\n<ul>\n<li><p>Testability. It's very easy to test sagas as call() returns a pure object. Testing thunks normally requires you to include a mockStore inside your test.</p></li>\n<li><p>redux-saga comes with lots of useful helper functions about tasks. It seems to me that the concept of saga is to create some kind of background worker/thread for your app, which act as a missing piece in react redux architecture(actionCreators and reducers must be pure functions.) Which leads to next point.</p></li>\n<li><p>Sagas offer independent place to handle all side effects. It is usually easier to modify and manage than thunk actions in my experience.</p></li>\n</ul>\n\n<p>Con:</p>\n\n<ul>\n<li><p>Generator syntax.</p></li>\n<li><p>Lots of concepts to learn.</p></li>\n<li><p>API stability. It seems redux-saga is still adding features (eg Channels?) and the community is not as big. There is a concern if the library makes a non backward compatible update some day.</p></li>\n</ul>\n", "title": "Pros/cons of using redux-saga with ES6 generators vs redux-thunk with ES7 async/await", "answer_id": 37742622, "question_id": 34930735, "tags": ["javascript", "reactjs", "redux", "redux-thunk", "redux-saga"]}, {"body": "<p>Pros:</p>\n\n<ul>\n<li>Professional asynchronous workflow controlling.</li>\n</ul>\n\n<p>Cons:</p>\n\n<ul>\n<li><p>Does not support Internet Explorer 8.</p>\n\n<p>See: <a href=\"https://github.com/yelouafi/redux-saga/issues/313\" rel=\"nofollow\">https://github.com/yelouafi/redux-saga/issues/313</a></p>\n\n<blockquote>\n  <p><strong><em>yelouafi</strong> commented on 9 May</em></p>\n  \n  <p>didn't have in mind to support IE8. This would lead to unnecessary complications in the code</p>\n</blockquote></li>\n</ul>\n", "title": "Pros/cons of using redux-saga with ES6 generators vs redux-thunk with ES7 async/await", "answer_id": 37913529, "question_id": 34930735, "tags": ["javascript", "reactjs", "redux", "redux-thunk", "redux-saga"]}], "question_id": 34930735}, {"body": "<p>Executing this snippet in the Chrome console:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>function foo() {\r\n    return typeof null === 'undefined';\r\n}\r\nfor(var i = 0; i &lt; 1000; i++) console.log(foo());</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>should print 1000 times <code>false</code>, but on some machines will print <code>false</code> for a number of iterations, then <code>true</code> for the rest. </p>\n\n<p><a href=\"http://i.stack.imgur.com/FuFKT.jpg\"><img src=\"http://i.stack.imgur.com/FuFKT.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>Why is this happening? Is it just a bug? </p>\n", "title": "Why does the value of typeof null change inside a loop?", "tags": ["javascript", "v8"], "answer_count": 4, "link": "http://stackoverflow.com/questions/37939455/why-does-the-value-of-typeof-null-change-inside-a-loop", "answers": [{"body": "<p>There is a chromium bug open for this:</p>\n\n<p><a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=604033\">Issue 604033 - JIT compiler not preserving method behavior</a></p>\n\n<p>So yes It's just a bug!</p>\n", "title": "Why does the value of typeof null change inside a loop?", "answer_id": 37939890, "question_id": 37939455, "tags": ["javascript", "v8"]}, {"body": "<p>It's actually a <a href=\"https://developers.google.com/v8/\">V8</a> JavaScript engine (<a href=\"https://en.wikipedia.org/wiki/V8_(JavaScript_engine)\">Wiki</a>) bug.</p>\n\n<p>This engine is used in Chromium, Maxthron, Android OS, Node.js etc.</p>\n\n<p>Relatively simple <a href=\"https://www.reddit.com/r/javascript/comments/4oxtgk/javascript_developers_be_warned_about_this_crazy/d4grsr0\">bug description</a> you can find in this <a href=\"https://www.reddit.com/r/javascript/comments/4oxtgk/javascript_developers_be_warned_about_this_crazy/\">Reddit topic</a>:</p>\n\n<blockquote>\n  <p>Modern JavaScript engines compile JS code into optimized machine code\n  when it is executed (Just In Time compilation) to make it run faster.\n  However, the optimization step has some initial performance cost in\n  exchange for a long term speedup, so the engine dynamically decides\n  whether a method is worth it depending on how commonly it is used.</p>\n  \n  <p>In this case there appears to be a bug only in the optimized path,\n  while the unoptimized path works fine. So at first the method works as\n  intended, but if it's called in a loop often enough at some point the\n  engine will decide to optimize it and replaces it with the buggy\n  version.</p>\n</blockquote>\n\n<p>This bug seems to have been fixed in V8 itself (<a href=\"https://github.com/v8/v8/commit/7dfb5beeec8821521beeb2b8eac36707a663064c\">commit</a>), aswell as in Chromium (<a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=604033\">bug report</a>) and NodeJS (<a href=\"https://github.com/nodejs/node/pull/7348\">commit</a>).</p>\n", "title": "Why does the value of typeof null change inside a loop?", "answer_id": 37944460, "question_id": 37939455, "tags": ["javascript", "v8"]}, {"body": "<p>To answer the direct question of why it changes, the bug is in the \"JIT\" optimisation routine of the V8 JS engine used by Chrome. At first, the code is run exactly as written, but the more you run it, the more potential there is for the benefits of optimisation to outweigh the costs of analysis.</p>\n\n<p>In this case, after repeated execution in the loop, the JIT compiler analyses the function, and replaces it with an optimised version. Unfortunately, the analysis makes an incorrect assumption, and the optimised version doesn't actually produce the correct result.</p>\n\n<p>Specifically, <a href=\"https://www.reddit.com/r/javascript/comments/4oxtgk/javascript_developers_be_warned_about_this_crazy/d4grqo1\">Reddit user RainHappens suggests</a> that it is an error in <em>type propagation</em>:</p>\n\n<blockquote>\n  <p>It also does some type propagation (as in what types a variable etc can be). There's a special \"undetectable\" type for when a variable is undefined or null. In this case the optimizer goes \"null is undetectable, so it can be replaced with the \"undefined\" string for the comparison.</p>\n</blockquote>\n\n<p>This is one of the hard problems with optimising code: how to guarantee that code which has been rearranged for performance will still have the same effect as the original.</p>\n", "title": "Why does the value of typeof null change inside a loop?", "answer_id": 37948799, "question_id": 37939455, "tags": ["javascript", "v8"]}, {"body": "<p>This was fixed two month ago and will land in Chrome soon (already in Canary). </p>\n\n<p><a href=\"https://codereview.chromium.org/1912553002\" rel=\"nofollow\">V8 Issue 1912553002 - Fix 'typeof null' canonicalization in crankshaft</a></p>\n\n<p><a href=\"https://bugs.chromium.org/p/chromium/issues/detail?id=604033\" rel=\"nofollow\">Chromium Issue 604033 - JIT compiler not preserving method behavior</a></p>\n", "title": "Why does the value of typeof null change inside a loop?", "answer_id": 38089992, "question_id": 37939455, "tags": ["javascript", "v8"]}], "question_id": 37939455}, {"body": "<p>I have been looking at the <a href=\"http://en.wikipedia.org/wiki/Boost_%28C%2B%2B_libraries%29\">Boost</a> libraries source code, and I have noticed that often there are single pound signs without any preprocessor directives attached to them. I read through the GCC preprocessor manual and specification guide and can't find anything about it.</p>\n\n<pre><code>(1) #ifndef BOOST_CONFIG_HPP\n(2) #  include &lt;boost/config.hpp&gt;\n(3) #endif\n(4) #\n(5) #if defined(BOOST_HAS_PRAGMA_ONCE)\n(6) #  pragma once\n(7) #endif\n</code></pre>\n\n<p>On line 4, there is nothing after the pound sign. What effect does this have? Is it defined in the C preprocessor (CPP) specification?</p>\n\n<p>As Boost is a cross-platform library, I would assume that any CPP should parse it correctly. What would the effect/side-effects be of having random pound/hash signs throughout the code?</p>\n", "title": "What is the purpose of a single pound/hash sign (#) on its own line in the C/C++ preprocessor?", "tags": ["c++", "c", "boost", "c-preprocessor"], "answer_count": 3, "link": "http://stackoverflow.com/questions/35207515/what-is-the-purpose-of-a-single-pound-hash-sign-on-its-own-line-in-the-c-c", "answers": [{"body": "<p>It makes the source code look pretty, that's all.</p>\n\n<p>Highlights the fact that the whole block is a preprocessor section.</p>\n\n<p>And indeed, both the C and C++ preprocessors <em>must</em> ignore <code>#</code> on a line.</p>\n", "title": "What is the purpose of a single pound/hash sign (#) on its own line in the C/C++ preprocessor?", "answer_id": 35207547, "question_id": 35207515, "tags": ["c++", "c", "boost", "c-preprocessor"]}, {"body": "<p>Always check an authoritative source instead of relying on other resources. C is standardised as ISO 9899::2011, C++ also has an ISO standard. Both are well accepted and the final drafts available by a short search. The C standard states in <a href=\"http://port70.net/~nsz/c/c11/n1570.html#6.10.7\" rel=\"nofollow\">6.10.7</a> (C++ has much the same text):</p>\n\n<blockquote>\n  <p>A preprocessing directive of the form</p>\n\n<pre><code># new-line\n</code></pre>\n  \n  <p>has no effect.</p>\n</blockquote>\n\n<p>This is a <em>null directive</em>, as much as an <code>;</code> without a preceeding expression in the core-language is a <em>null statement</em> .</p>\n\n<p>For the preprocessor it is just for formatting/readability to highlight that the lines belong semantically together. (the semicolon OTOH is semantically relevant).</p>\n", "title": "What is the purpose of a single pound/hash sign (#) on its own line in the C/C++ preprocessor?", "answer_id": 35207649, "question_id": 35207515, "tags": ["c++", "c", "boost", "c-preprocessor"]}, {"body": "<p>A <code>#</code> on its own on a line has no effect at all. I assume it's being used for aesthetic value.</p>\n\n<p>The C standard says:</p>\n\n<blockquote>\n  <p><strong>6.10.7 Null directive</strong></p>\n  \n  <p><strong>Semantics</strong></p>\n  \n  <p>A preprocessing directive of the form</p>\n  \n  <p><code># new-line</code></p>\n  \n  <p>has no effect.</p>\n</blockquote>\n\n<p>The C++ standard says the same thing:</p>\n\n<blockquote>\n  <p><strong>16.7  Null directive  [cpp.null]</strong></p>\n  \n  <p>A preprocessing directive of the form</p>\n  \n  <p><code># new-line</code></p>\n  \n  <p>has no effect.</p>\n</blockquote>\n", "title": "What is the purpose of a single pound/hash sign (#) on its own line in the C/C++ preprocessor?", "answer_id": 35207652, "question_id": 35207515, "tags": ["c++", "c", "boost", "c-preprocessor"]}], "question_id": 35207515}, {"body": "<p>I get the following  error while upgrading a firebase project from old domain to new google firebase domain.</p>\n\n<blockquote>\n  <p>Failed to resolve: com.google.firebase:firebase-core:9.0.0</p>\n</blockquote>\n\n<p>I followed the steps mentioned on the <a href=\"http://firebase.google.com\">Firebase documentation</a>, in the section <a href=\"https://firebase.google.com/docs/android/setup#available_libraries\">Add Firebase to your Android Project, topic Available libraries</a>.</p>\n\n<p>What are my options to resolve this error?</p>\n", "title": "Failed to resolve: com.google.firebase:firebase-core:9.0.0", "tags": ["android", "firebase"], "answer_count": 4, "link": "http://stackoverflow.com/questions/37310188/failed-to-resolve-com-google-firebasefirebase-core9-0-0", "answers": [{"body": "<p>The <code>9.0.0</code> version of Firebase was built using Google Play services 9.0 and is now available under the new packaging <code>com.google.firebase:*</code></p>\n\n<p>See Release Notes for Google Play services 9.0\n<a href=\"https://developers.google.com/android/guides/releases#may_2016_-_v90\">https://developers.google.com/android/guides/releases#may_2016_-_v90</a></p>\n\n<p>New versions of packages <strong>Google Play Services</strong> (rev 30) and <strong>Google Repository</strong> (rev 26) were just released in the SDK manager so it's likely you just need to update.</p>\n\n<p>In Android Studio:</p>\n\n<ul>\n<li>Click <strong>Tools</strong> > <strong>Android</strong> > <strong>SDK Manager</strong></li>\n<li>Click into the <strong>SDK Tools</strong> tab</li>\n<li>Select and install <strong>Google Play Services</strong> (rev 30) and <strong>Google Repository</strong> (rev 26).  See the image below.</li>\n<li><strong>Sync</strong> and <strong>Build</strong> your project.</li>\n</ul>\n\n<p><a href=\"http://i.stack.imgur.com/qc3AG.png\"><img src=\"http://i.stack.imgur.com/qc3AG.png\" alt=\"enter image description here\"></a></p>\n", "title": "Failed to resolve: com.google.firebase:firebase-core:9.0.0", "answer_id": 37310513, "question_id": 37310188, "tags": ["android", "firebase"]}, {"body": "<p>Just add <code>compile 'com.google.android.gms:play-services:9.0.0'</code>\ninto your build.gradle.</p>\n\n<p>Sync and Build your project.</p>\n\n<p>See <a href=\"https://firebase.google.com/docs/android/setup#add_the_sdk\" rel=\"nofollow\">Add Firebase to your Android Project</a></p>\n", "title": "Failed to resolve: com.google.firebase:firebase-core:9.0.0", "answer_id": 37404172, "question_id": 37310188, "tags": ["android", "firebase"]}, {"body": "<p>Following are the prerequisites if you want to add firebase to your project. </p>\n\n<ol>\n<li>For working with Firebase you should install Android Studio 1.5 or higher. </li>\n<li>Download the latest Google Play services SDK from through Android SDK Manager.</li>\n<li>The device should be running Android 2.3 (Gingerbread) or newer, and Google Play services 9.2.0 or newer.</li>\n</ol>\n\n<p>I could only find out all this after hours of struggle.</p>\n\n<p>Source: <a href=\"http://www.androidtutorialpoint.com/material-design/firebase-android-tutorial-getting-started/\" rel=\"nofollow\">Adding Firebase to your Android App</a></p>\n", "title": "Failed to resolve: com.google.firebase:firebase-core:9.0.0", "answer_id": 38359210, "question_id": 37310188, "tags": ["android", "firebase"]}, {"body": "<p>Go to Android SDK Manager and install the latest version of below two libraries</p>\n\n<ol>\n<li>Google Play Services</li>\n<li>Google Repository</li>\n</ol>\n", "title": "Failed to resolve: com.google.firebase:firebase-core:9.0.0", "answer_id": 39069909, "question_id": 37310188, "tags": ["android", "firebase"]}], "question_id": 37310188}, {"body": "<p>So I was trying to write the <em>n</em>th number in the Fibonacci sequence in as compact a function as possible:</p>\n\n<pre><code>public uint fibn ( uint N ) \n{\n   return (N == 0 || N == 1) ? 1 : fibn(N-1) + fibn(N-2);\n}\n</code></pre>\n\n<p>But I'm wondering if I can make this even more compact and efficient by changing </p>\n\n<pre><code>(N == 0 || N == 1)\n</code></pre>\n\n<p>into a single comparison. Is there some fancy bit shift operation that can do this?</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"], "answer_count": 14, "link": "http://stackoverflow.com/questions/36359610/is-it-possible-to-simplify-x-0-x-1-into-a-single-operation", "answers": [{"body": "<p>Since argument is <code>uint</code> (<em>unsigned</em>) you can put</p>\n\n<pre><code>  return (N &lt;= 1) ? 1 : N * fibn(N-1);\n</code></pre>\n\n<p>Less readable (IMHO) but if you count each character (<em>Code Golf</em> or alike)</p>\n\n<pre><code>  return N &lt; 2 ? 1 : N * fibn(N-1);\n</code></pre>\n\n<p><strong>Edit</strong>: for your <em>edited question</em>:</p>\n\n<pre><code>  return (N &lt;= 1) ? 1 : fibn(N-1) + fibn(N-2);\n</code></pre>\n\n<p>Or </p>\n\n<pre><code>  return N &lt; 2 ? 1 : fibn(N-1) + fibn(N-2);\n</code></pre>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36359655, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>You could also check that all other bits are 0 like this:</p>\n\n<pre><code>return (N &amp; ~1) == 0 ? 1 : N * fibn(N-1);\n</code></pre>\n\n<p>For completeness thanks to <a href=\"http://stackoverflow.com/users/2700898/matt\">Matt</a> the even better solution:</p>\n\n<pre><code>return (N | 1) == 1 ? 1 : N * fibn(N-1);\n</code></pre>\n\n<p>In both cases you need to take care of the parenthesis because bitwise operators have lower priority than <code>==</code>.</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36359698, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>As you use an uint, which can't get negative, you could check if <code>n &lt; 2</code></p>\n\n<p>EDIT</p>\n\n<p>Or for that special function case you could write it as follows:</p>\n\n<pre><code>public uint fibn(uint N)\n    return (N == 0) ? 1 : N * fibn(N-1);\n}\n</code></pre>\n\n<p>which will lead to the same result, of course at the cost of an additional recursion step.</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36359699, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>Dmitry's answer is best but if it was an Int32 return type and you had a larger set of integers to choose from you could do this. </p>\n\n<pre><code>return new List&lt;int&gt;() { -1, 0, 1, 2 }.Contains(N) ? 1 : N * fibn(N-1);\n</code></pre>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36359876, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>Simply check to see if <code>N</code> is &lt;= 1 since you know N is unsigned there can only be 2 conditions that <code>N &lt;= 1</code> that results in <code>TRUE</code>: 0 and 1</p>\n\n<pre><code>public uint fibn ( uint N ) \n{\n   return (N &lt;= 1) ? 1 : fibn(N-1) + finb(N-2);\n}\n</code></pre>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36364064, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<h3>How to do it with bitshift</h3>\n\n<p>If you want to use bitshift and make the code somewhat obscure (but short) you could do:</p>\n\n<pre><code>public uint fibn ( uint N ) {\n   return N &gt;&gt; 1 != 0? fibn(N-1) + finb(N-2): 1;\n}\n</code></pre>\n\n<p>For an unsigned integer <code>N</code> in the language c, <code>N&gt;&gt;1</code> tosses off the low order bit. If that result is non-zero, it implies N is greater than 1.</p>\n\n<p>Note: this algorithm is horribly inefficient as it needlessly recalculates values in the sequence that have already been calculated.</p>\n\n<h3>Something WAY WAY faster</h3>\n\n<p>Calculate it one pass rather than implicitly building a fibonaci(N) sized tree:</p>\n\n<pre><code>uint faster_fibn(uint N) { //requires N &gt; 1 to work\n  uint a = 1, b = 1, c = 1;\n  while(--N != 0) {\n    c = b + a;\n    a = b;\n    b = c;\n  }\n  return c;\n}\n</code></pre>\n\n<p>As some people have mentioned, it doesn't take long to overflow even a 64 bit unsigned integer. Depending on how large you're trying to go, you'll need to use arbitrary precision integers.</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36365103, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>There are a number of ways to implement your arithmetic test using bitwise arithmetic. Your expression:</p>\n\n<ul>\n<li><code>x == 0 || x == 1</code></li>\n</ul>\n\n<p>is logically equivalent to each one of these:</p>\n\n<ul>\n<li><code>(x &amp; 1) == x</code></li>\n<li><code>(x &amp; ~1) == 0</code></li>\n<li><code>(x | 1) == 1</code></li>\n<li><code>(~x | 1) == (uint)-1</code></li>\n<li><code>x &gt;&gt; 1 == 0</code></li>\n</ul>\n\n<p>Bonus:</p>\n\n<ul>\n<li><code>x * x == x</code> (the proof takes a bit of effort)</li>\n</ul>\n\n<p>But practically speaking, these forms are the most readable, and the tiny difference in performance isn't really worth using bitwise arithmetic:</p>\n\n<ul>\n<li><code>x == 0 || x == 1</code></li>\n<li><code>x &lt;= 1</code> (because <code>x</code> is an unsigned integer)</li>\n<li><code>x &lt; 2</code> (because <code>x</code> is an unsigned integer)</li>\n</ul>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36367623, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p><strong>Disclaimer: I don't know C#, and didn't test this code:</strong></p>\n\n<blockquote>\n  <p>But I'm wondering if I can make this even more compact and efficient by changing [...] into a single comparison...</p>\n</blockquote>\n\n<p>No need for bitshifting or such, this uses just one comparison, and it should be a lot more <em>efficient</em> ( O(n) vs O(2^n) I think? ). The body of the function is <em>more compact</em>, though it ends being a bit longer with the declaration.</p>\n\n<p>(To remove overhead from recursion, there's the iterative version, as in <a href=\"https://stackoverflow.com/a/36365103/4309858\">Mathew Gunn's answer</a>)</p>\n\n<pre><code>public uint fibn ( uint N, uint B=1, uint A=0 ) \n{\n    return N == 0 ? A : fibn( N--, A+B, B );\n}\n\n                     fibn( 5 ) =\n                     fibn( 5,   1,   0 ) =\nreturn 5  == 0 ? 0 : fibn( 5--, 0+1, 1 ) =\n                     fibn( 4,   1,   1 ) =\nreturn 4  == 0 ? 1 : fibn( 4--, 1+1, 1 ) =\n                     fibn( 3,   2,   1 ) =\nreturn 3  == 0 ? 1 : fibn( 3--, 1+2, 2 ) =\n                     fibn( 2,   3,   2 ) =\nreturn 2  == 0 ? 2 : fibn( 2--, 2+3, 3 ) =\n                     fibn( 1,   5,   3 ) =\nreturn 1  == 0 ? 3 : fibn( 1--, 3+5, 5 ) =\n                     fibn( 0,   8,   5 ) =\nreturn 0  == 0 ? 5 : fibn( 0--, 5+8, 8 ) =\n                 5\nfibn(5)=5\n</code></pre>\n\n<p>PS: This is a common functional pattern for iteration with accumulators. If you replace <code>N--</code> with <code>N-1</code> you're effectively using no mutation, which makes it usable in a pure functional approach. </p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36378528, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>If what you want to do is to make the function more efficient, then use a lookup table. The lookup table is surprisingly small at only 47 entries - the next entry would overflow a 32-bit unsigned integer. It also of course makes the function trivial to write.</p>\n\n<pre><code>class Sequences\n{\n    // Store the complete list of values that will fit in a 32-bit unsigned integer without overflow.\n    private static readonly uint[] FibonacciSequence = { 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,\n        233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418,\n        317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169,\n        63245986, 102334155, 165580141, 267914296, 433494437, 701408733, 1134903170, 1836311903, 2971215073\n    };\n\n    public uint fibn(uint N)\n    {\n        return FibonacciSequence[N];\n    }\n}\n</code></pre>\n\n<p>You can obviously do the same thing for factorials.</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36385413, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>Bit late to the party, but you could also do <code>(x==!!x)</code> </p>\n\n<p><code>!!x</code> converts the a value to <code>1</code> if it's not <code>0</code>, and leaves it at <code>0</code> if it is.<br>\nI use this kinda thing in C obfuscation a lot.  </p>\n\n<p>Note: This is C, not sure if it works in C#</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36391195, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>So I created a <code>List</code> of these special integers and checked if <code>N</code> pertains to it.</p>\n\n<pre><code>static List&lt;uint&gt; ints = new List&lt;uint&gt; { 0, 1 };\n\npublic uint fibn(uint N) \n{\n   return ints.Contains(N) ? 1 : fibn(N-1) + fibn(N-2);\n}\n</code></pre>\n\n<p>You could also use an extension method for different purposes where <code>Contains</code> is called only once (e. g. when your application is starting and loading data). This provides a clearer style and clarifies the primary relation to your value (<code>N</code>):</p>\n\n<pre><code>static class ObjectHelper\n{\n    public static bool PertainsTo&lt;T&gt;(this T obj, IEnumerable&lt;T&gt; enumerable)\n    {\n        return (enumerable is List&lt;T&gt; ? (List&lt;T&gt;) enumerable : enumerable.ToList()).Contains(obj);\n    }\n}\n</code></pre>\n\n<p>Apply it:</p>\n\n<pre><code>N.PertainsTo(ints)\n</code></pre>\n\n<p>This might be not the fastest way to do it, but to me, it appears to be a better style.</p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36434584, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>Here's my solution, there's not much in optimizing this simple function, on the other hand what I'm offering here is readability as a mathematical definition of the recursive function.</p>\n\n<pre><code>public uint fibn(uint N) \n{\n    switch(N)\n    {\n        case  0: return 1;\n\n        case  1: return 1;\n\n        default: return fibn(N-1) + fibn(N-2);\n    }\n}\n</code></pre>\n\n<p>The mathematical definition of Fibonacci number in a similar fashion..</p>\n\n<p><a href=\"http://i.stack.imgur.com/R8q2g.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/R8q2g.png\" alt=\"enter image description here\"></a></p>\n\n<p>Taking it further to force the switch case to build a lookup table.</p>\n\n<pre><code>public uint fibn(uint N) \n{\n    switch(N)\n    {\n        case  0: return 1;\n        case  1: return 1;\n        case  2: return 2;\n        case  3: return 3;\n        case  4: return 5;\n        case  5: return 8;\n        case  6: return 13;\n        case  7: return 21;\n        case  8: return 34;\n        case  9: return 55;\n        case 10: return 89;\n        case 11: return 144;\n        case 12: return 233;\n        case 13: return 377;\n        case 14: return 610;\n        case 15: return 987;\n        case 16: return 1597;\n        case 17: return 2584;\n        case 18: return 4181;\n        case 19: return 6765;\n        case 20: return 10946;\n        case 21: return 17711;\n        case 22: return 28657;\n        case 23: return 46368;\n        case 24: return 75025;\n        case 25: return 121393;\n        case 26: return 196418;\n        case 27: return 317811;\n        case 28: return 514229;\n        case 29: return 832040;\n        case 30: return 1346269;\n        case 31: return 2178309;\n        case 32: return 3524578;\n        case 33: return 5702887;\n        case 34: return 9227465;\n        case 35: return 14930352;\n        case 36: return 24157817;\n        case 37: return 39088169;\n        case 38: return 63245986;\n        case 39: return 102334155;\n        case 40: return 165580141;\n        case 41: return 267914296;\n        case 42: return 433494437;\n        case 43: return 701408733;\n        case 44: return 1134903170;\n        case 45: return 1836311903;\n        case 46: return 2971215073;\n\n        default: return fibn(N-1) + fibn(N-2);\n    }\n}\n</code></pre>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36444222, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>This one also work</p>\n\n<pre><code>Math.Sqrt(N) == N \n</code></pre>\n\n<p>square root of 0 and 1 will return 0 and 1 respectively . </p>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 36906544, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}, {"body": "<p>for N is uint, just use</p>\n\n<pre><code>N &lt;= 1\n</code></pre>\n", "title": "Is it possible to simplify (x == 0 || x == 1) into a single operation?", "answer_id": 37362503, "question_id": 36359610, "tags": ["c#", "algorithm", "optimization", "arithmetic-expressions"]}], "question_id": 36359610}, {"body": "<p>I just had a rather unpleasant experience in our production environment, causing <code>OutOfMemoryErrors: heapspace..</code></p>\n\n<p>I traced the issue to my use of <code>ArrayList::new</code> in a function.</p>\n\n<p>To verify that this is actually performing worse than normal creation via a declared constructor (<code>t -&gt; new ArrayList&lt;&gt;()</code>), I wrote the following small method:</p>\n\n<pre><code>public class TestMain {\n  public static void main(String[] args) {\n    boolean newMethod = false;\n    Map&lt;Integer,List&lt;Integer&gt;&gt; map = new HashMap&lt;&gt;();\n    int index = 0;\n\n    while(true){\n      if (newMethod) {\n        map.computeIfAbsent(index, ArrayList::new).add(index);\n     } else {\n        map.computeIfAbsent(index, i-&gt;new ArrayList&lt;&gt;()).add(index);\n      }\n      if (index++ % 100 == 0) {\n        System.out.println(\"Reached index \"+index);\n      }\n    }\n  }\n}\n</code></pre>\n\n<p>Running the method with <code>newMethod=true;</code> will cause the method to fail with <code>OutOfMemoryError</code> just after index hits 30k. With <code>newMethod=false;</code> the program does not fail, but keeps pounding away until killed (index easily reaches 1.5 milion).</p>\n\n<p>Why does <code>ArrayList::new</code> create so many <code>Object[]</code> elements on the heap that it causes <code>OutOfMemoryError</code> so fast?</p>\n\n<p>(By the way - it also happens when the collection type is <code>HashSet</code>.)</p>\n", "title": "Horrendous performance &amp; large heap footprint of Java 8 constructor reference?", "tags": ["java", "constructor", "java-8", "out-of-memory", "method-reference"], "answer_count": 2, "link": "http://stackoverflow.com/questions/35296734/horrendous-performance-large-heap-footprint-of-java-8-constructor-reference", "answers": [{"body": "<p>In the first case (<code>ArrayList::new</code>) you are using the <a href=\"https://docs.oracle.com/javase/8/docs/api/java/util/ArrayList.html#ArrayList-int-\">constructor</a> which takes an initial capacity argument, in the second case you are not. A large initial capacity (<code>index</code> in your code) causes a large <code>Object[]</code> to be allocated, resulting in your <code>OutOfMemoryError</code>s.</p>\n\n<p>Here are the two constructors' current implementations:</p>\n\n<pre><code>public ArrayList(int initialCapacity) {\n    if (initialCapacity &gt; 0) {\n        this.elementData = new Object[initialCapacity];\n    } else if (initialCapacity == 0) {\n        this.elementData = EMPTY_ELEMENTDATA;\n    } else {\n        throw new IllegalArgumentException(\"Illegal Capacity: \"+\n                                           initialCapacity);\n    }\n}\npublic ArrayList() {\n    this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;\n}\n</code></pre>\n\n<p>Something similar happens in <code>HashSet</code>, except the array is not allocated until <code>add</code> is called.</p>\n", "title": "Horrendous performance &amp; large heap footprint of Java 8 constructor reference?", "answer_id": 35296974, "question_id": 35296734, "tags": ["java", "constructor", "java-8", "out-of-memory", "method-reference"]}, {"body": "<p>The <code>computeIfAbsent</code> signature is the following:</p>\n\n<pre><code>V computeIfAbsent(K key, Function&lt;? super K, ? extends V&gt; mappingFunction)\n</code></pre>\n\n<p>So the <code>mappingFunction</code> is the function which <em>receives</em> one argument. In your case <code>K = Integer</code> and <code>V = List&lt;Integer&gt;</code>, so the signature becomes (omitting PECS):</p>\n\n<pre><code>Function&lt;Integer, List&lt;Integer&gt;&gt; mappingFunction\n</code></pre>\n\n<p>When you write <code>ArrayList::new</code> in the place where <code>Function&lt;Integer, List&lt;Integer&gt;&gt;</code> is necessary, compiler looks for the suitable constructor which is:</p>\n\n<pre><code>public ArrayList(int initialCapacity)\n</code></pre>\n\n<p>So essentially your code is equivalent to</p>\n\n<pre><code>map.computeIfAbsent(index, i-&gt;new ArrayList&lt;&gt;(i)).add(index);\n</code></pre>\n\n<p>And your keys are treated as <code>initialCapacity</code> values which leads to pre-allocation of arrays of ever increasing size, which, of course, quite fast leads to <code>OutOfMemoryError</code>.</p>\n\n<p>In this particular case constructor references are not suitable. Use lambdas instead. Were the <code>Supplier&lt;? extends V&gt;</code> used in <code>computeIfAbsent</code>, then <code>ArrayList::new</code> would be appropriate.</p>\n", "title": "Horrendous performance &amp; large heap footprint of Java 8 constructor reference?", "answer_id": 35297235, "question_id": 35296734, "tags": ["java", "constructor", "java-8", "out-of-memory", "method-reference"]}], "question_id": 35296734}, {"body": "<p>Consider this code:</p>\n\n<pre><code>struct A\n{\n    void foo() const\n    {\n        std::cout &lt;&lt; \"const\" &lt;&lt; std::endl;\n    }\n\n    private:\n\n        void foo()\n        {\n            std::cout &lt;&lt; \"non - const\" &lt;&lt; std::endl;\n        }\n};\n\nint main()\n{\n    A a;\n    a.foo();\n}\n</code></pre>\n\n<p>The compiler error is:</p>\n\n<blockquote>\n  <p>error: 'void A::foo()' is private`.</p>\n</blockquote>\n\n<p>But when I delete the private one it just works. Why is the public const method not called when the non-const one is private?</p>\n\n<p>In other words, why does overload resolution come before access control? This is strange. Do you think it is consistent? My code works and then I add a method, and my working code does not compile at all.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "tags": ["c++", "function-overloading", "overload-resolution"], "answer_count": 11, "link": "http://stackoverflow.com/questions/39042240/why-is-a-public-const-method-not-called-when-the-non-const-one-is-private", "answers": [{"body": "<p>Because the variable <code>a</code> in the <code>main</code> function is not declared as <code>const</code>.</p>\n\n<p>Constant member functions are called on constant objects.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042299, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>Since the implicit <code>this</code> pointer is non-<code>const</code>, the compiler will first check for the presence of a non-<code>const</code> version of the function before a <code>const</code> version.</p>\n\n<p>If you explicitly mark the non-<code>const</code> one <code>private</code> then the resolution will fail, and the compiler will not continue searching.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042314, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>In this call:</p>\n\n<pre><code>a.foo();\n</code></pre>\n\n<p>There is always an implicit <code>this</code> pointer available in every member function. And the <code>const</code> qualification of <code>this</code> is taken from the calling reference/object. The above call <em>is treated</em> by the compiler as:</p>\n\n<pre><code>A::foo(a);\n</code></pre>\n\n<p>But you have two declarations of <code>A::foo</code> which is <em>treated like</em>:</p>\n\n<pre><code>A::foo(A* );\nA::foo(A const* );\n</code></pre>\n\n<p>By overload resolution, the first will be selected for non-const <code>this</code>, the second will be selected for a <code>const this</code>. If you remove the first, the second will bind to both <code>const</code> and <code>non-const</code> <code>this</code>.</p>\n\n<p>After overload resolution to select the best viable function, comes access control. Since you specified access to the chosen overload as <code>private</code>, the compiler will then complain.</p>\n\n<p>The standard says so:</p>\n\n<blockquote>\n  <p><strong><a href=\"http://eel.is/c++draft/class.access#4\">[class.access/4]</a>:</strong> ...In the case of overloaded function names, access control is applied to\n  the function selected by overload resolution....</p>\n</blockquote>\n\n<p>But if you do this:</p>\n\n<pre><code>A a;\nconst A&amp; ac = a;\nac.foo();\n</code></pre>\n\n<p>Then, only the <code>const</code> overload will be fit.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042473, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>When you call <code>a.foo();</code>, the compiler goes through overload resolution to find the best function to use. When it builds the overload set it finds</p>\n\n<pre><code>void foo() const\n</code></pre>\n\n<p>and</p>\n\n<pre><code>void foo()\n</code></pre>\n\n<p>Now, since <code>a</code> is not <code>const</code>, the non-const version is the best match, so the compiler picks <code>void foo()</code>. Then the access restrictions are put in place and you get a compiler error, since <code>void foo()</code> is private.</p>\n\n<p>Remember, in overload resolution it is not 'find the best usable function'. It is 'find the best function and try to use it'. If it can't because of access restrictions or being deleted, then you get a compiler error.</p>\n\n<blockquote>\n  <p>In other words why does overload resolution comes before access control?</p>\n</blockquote>\n\n<p>Well, let's look at:</p>\n\n<pre><code>struct Base\n{\n    void foo() { std::cout &lt;&lt; \"Base\\n\"; }\n};\n\nstruct Derived : Base\n{\n    void foo() { std::cout &lt;&lt; \"Derived\\n\"; }\n};\n\nstruct Foo\n{\n    void foo(Base * b) { b-&gt;foo(); }\nprivate:\n    void foo(Derived * d) { d-&gt;foo(); }\n};\n\nint main()\n{\n    Derived d;\n    Foo f;\n    f.foo(&amp;d);\n}\n</code></pre>\n\n<p>Now let's say that I did not actually mean to make <code>void foo(Derived * d)</code> private. If access control came first then this program would compile and run and <code>Base</code> would be printed. This could be very hard to track down in a large code base. Since access control comes after overload resolution I get a nice compiler error telling me the function I want it to call cannot be called, and I can find the bug a lot easier.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042574, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>Access controls (<code>public</code>, <code>protected</code>, <code>private</code>) do not affect overload resolution. The compiler chooses <code>void foo()</code> because it's the best match. The fact that it's not accessible doesn't change that. Removing it leaves only <code>void foo() const</code>, which is then the best (i.e., only) match.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042619, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>This comes down to a fairly basic design decision in C++.</p>\n\n<p>When looking up the function to satisfy a call, the compiler carries out a search like this:</p>\n\n<ol>\n<li><p>It searches to find the first<sup>1</sup> scope at which there's <em>something</em> with that name.</p></li>\n<li><p>The compiler finds <em>all</em> the functions (or functors, etc.) with that name in that scope.</p></li>\n<li><p>Then the compiler does overload resolution to find the best candidate among those it found (whether they're accessible or not).</p></li>\n<li><p>Finally, the compiler checks whether that chosen function is accessible.</p></li>\n</ol>\n\n<p>Because of that ordering, yes, it's possible that the compiler will choose an overload that's not accessible, even though there's another overload that's accessible (but not chosen during overload resolution).</p>\n\n<p>As to whether it would be <em>possible</em> to do things differently: yes, it's undoubtedly possible. It would definitely lead to quite a different language than C++ though. It turns out that a lot of seemingly rather minor decisions can have ramifications that affect a lot more than might be initially obvious.</p>\n\n<hr>\n\n<ol>\n<li>\"First\" can be a little complex in itself, especially when/if templates get involved, since they can lead to two-phase lookup, meaning there are two entirely separate \"roots\" to start from when doing the search. The <em>basic</em> idea is pretty simple though: start from the smallest enclosing scope, and work your way outward to larger and larger enclosing scopes.</li>\n</ol>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042622, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>It's important to keep in mind the order of things that happen, which is:</p>\n\n<ol>\n<li>Find all the viable functions.</li>\n<li>Pick the best viable function.</li>\n<li>If there isn't exactly one best viable, or if you can't actually call the best viable function (due to access violations or the function being <code>delete</code>d), fail. </li>\n</ol>\n\n<p>(3) happens after (2). Which is really important, because otherwise making functions <code>delete</code>d or <code>private</code> would become sort of meaningless and much harder to reason about. </p>\n\n<p>In this case:</p>\n\n<ol>\n<li>The viable functions are <code>A::foo()</code> and <code>A::foo() const</code>.</li>\n<li>The best viable function is <code>A::foo()</code> because the latter involves a qualification conversion on the implicit <code>this</code> argument.</li>\n<li>But <code>A::foo()</code> is <code>private</code> and you don't have access to it, hence the code is ill-formed. </li>\n</ol>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042625, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>Access specifiers do not affect name-lookup and function-call resolution, ever. The function is selected before the compiler checks whether the call should trigger an access violation. </p>\n\n<p>This way, if you change an access specifier, you'll be alerted at compile-time if there is a violation in existing code; if privacy were taken into account for function call resolution, your program's behavior could silently change. </p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042694, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p><em>The technical reason has been answered by other answers. I'll only focus on this question:</em></p>\n\n<blockquote>\n  <p>In other words why overload resolution comes before access control? This is strange. Do you think it is consistent? My code works and then I add a method and my working code does not compile at all.</p>\n</blockquote>\n\n<p>That's how the language was designed. The intent is trying to call the best viable overload, as far as possible. If it fails, an error will be triggered to remind you to consider the design again.</p>\n\n<p>On the other hand, suppose your code compiled and worked well with the <code>const</code> member function being invoked. Someday, someone (maybe yourself) then decides to change the accessibility of the non-<code>const</code> member function from <code>private</code> to <code>public</code>. Then, the behavior would change without any compile errors! This would be a <em>surprise</em>.</p>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042913, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>Ultimately this comes down to the assertion in the standard that <strong>accessibility should not be taken into consideration when performing overload resolution</strong>. This assertion may be found in <a href=\"http://eel.is/c++draft/over.match#3\">[over.match]</a> clause 3:</p>\n\n<blockquote>\n  <p>... When overload resolution succeeds, and the best viable function is not accessible (Clause [class.access]) in the context in which it is used, the program is ill-formed.</p>\n</blockquote>\n\n<p>and also the <em>Note</em> in clause 1 of the same section:</p>\n\n<blockquote>\n  <p>[ Note: The function selected by overload resolution is not guaranteed to be appropriate for the context. Other restrictions, such as the accessibility of the function, can make its use in the calling context ill-formed.  \u2014 end note ]</p>\n</blockquote>\n\n<p>As for why, I can think of a couple of possible motivations:</p>\n\n<ol>\n<li>It prevents unexpected changes of behaviour as a result of changing the accessibility of an overload candidate (instead, a compile error will occur).</li>\n<li>It removes context-dependence from the overload resolution process (i.e. overload resolution would have the same result whether inside or outside the class).</li>\n</ol>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39042976, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}, {"body": "<p>Suppose access control came before overload resolution. Effectively, this would mean that <code>public/protected/private</code> controlled visibility rather than accessibility. </p>\n\n<p>Section 2.10 of <a href=\"http://rads.stackoverflow.com/amzn/click/0201543303\"><strong>Design and Evolution of C++ by Stroustrup</strong></a> has a passage on this where he discusses the following example</p>\n\n<pre><code>int a; // global a\n\nclass X {\nprivate:\n    int a; // member X::a\n};\n\nclass XX : public X {\n    void f() { a = 1; } // which a?\n};\n</code></pre>\n\n<p>Stroustrup mentions that a benefit of the current rules (visibility before accessibility) is that (temporarily) chaning the <code>private</code> inside <code>class X</code> into <code>public</code> (e.g. for the purposes of debugging) is that there is no quiet change in the meaning of the above program (i.e. <code>X::a</code> is attempted to be accessed in both cases, which gives an access error in the above example). If <code>public/protected/private</code> would control visibility, the meaning of the program would change (global <code>a</code> would be called with <code>private</code>, otherwise <code>X::a</code>).</p>\n\n<p>He then states that he does not recall whether it was by explicit design or a side effect of the preprocessor technology used to implement the C with Classess predecessor to Standard C++.</p>\n\n<p>How is this related to your example? Basically because the Standard made overload resolution conform to the general rule that name lookup comes before access control.</p>\n\n<blockquote>\n  <p><strong>10.2 Member name lookup [class.member.lookup]</strong></p>\n  \n  <p>1 Member name lookup determines the meaning of a name (id-expression)\n  in a class scope (3.3.7). Name lookup can result in an ambiguity, in\n  which case the program is ill-formed. For an id-expression, name\n  lookup begins in the class scope of this; for a qualified-id, name\n  lookup begins in the scope of the nestedname- specifier. <strong>Name lookup\n  takes place before access control</strong> (3.4, Clause 11).</p>\n  \n  <p>8 If the name of an overloaded function is unambiguously found,\n  <strong>overloading resolution (13.3) also takes place before access control</strong>.\n  Ambiguities can often be resolved by qualifying a name with its class\n  name.</p>\n</blockquote>\n", "title": "Why is a public const method not called when the non-const one is private?", "answer_id": 39046515, "question_id": 39042240, "tags": ["c++", "function-overloading", "overload-resolution"]}], "question_id": 39042240}, {"body": "<p>While reading <a href=\"http://thbecker.net/articles/rvalue_references/section_01.html\">this explanation</a> on lvalues and rvalues, these lines of code stuck out to me:</p>\n\n<pre><code>int&amp; foo();\nfoo() = 42; // OK, foo() is an lvalue\n</code></pre>\n\n<p>I tried it in g++, but the compiler says \"undefined reference to foo()\". If I add</p>\n\n<pre><code>int foo()\n{\n  return 2;\n}\n\nint main()\n{\n  int&amp; foo();\n  foo() = 42;\n}\n</code></pre>\n\n<p>It compiles fine, but running it gives a <a href=\"http://en.wikipedia.org/wiki/Segmentation_fault\">segmentation fault</a>. Just the line</p>\n\n<pre><code>int&amp; foo();\n</code></pre>\n\n<p>by itself both compiles and runs without any problems.</p>\n\n<p>What does this code mean? How can you assign a value to a function call, and why is it not an rvalue?</p>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "tags": ["c++"], "answer_count": 9, "link": "http://stackoverflow.com/questions/36477542/what-does-int-foo-mean-in-c", "answers": [{"body": "<p><code>int&amp; foo();</code> is a function returning a reference to <code>int</code>. Your provided function returns <code>int</code> without reference.</p>\n\n<p>You may do</p>\n\n<pre><code>int&amp; foo()\n{\n    static int i = 42;\n    return i;\n}\n\nint main()\n{\n    int&amp; foo();\n    foo() = 42;\n}\n</code></pre>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477619, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p>The explanation is assuming that there is some reasonable implementation for <code>foo</code> which returns an lvalue reference to a valid <code>int</code>.</p>\n\n<p>Such an implementation might be:</p>\n\n<pre><code>int a = 2; //global variable, lives until program termination\n\nint&amp; foo() {\n    return a;\n} \n</code></pre>\n\n<p>Now, since <code>foo</code> returns an lvalue reference, we can assign something to the return value, like so:</p>\n\n<pre><code>foo() = 42;\n</code></pre>\n\n<p>This will update the global <code>a</code> with the value <code>42</code>, which we can check by accessing the variable directly or calling <code>foo</code> again:</p>\n\n<pre><code>int main() {\n    foo() = 42;\n    std::cout &lt;&lt; a;     //prints 42\n    std::cout &lt;&lt; foo(); //also prints 42\n}\n</code></pre>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477626, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p>The example code at the linked page is just a dummy function declaration. It does not compile, but if you had some function defined, it would work generally. The example meant \"If you had a function with this signature, you could use it like that\".</p>\n\n<p>In your example, <code>foo</code> is clearly returning an lvalue based on the signature, but you return an rvalue that is converted to an lvalue. This clearly is determined to fail. You could do:</p>\n\n<pre><code>int&amp; foo()\n{\n    static int x;\n    return x;\n}\n</code></pre>\n\n<p>and would succeed by changing the value of x, when saying:</p>\n\n<pre><code>foo() = 10;\n</code></pre>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477658, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p><code>int &amp; foo();</code> means that <code>foo()</code> returns a reference to a variable. </p>\n\n<p>Consider this code:</p>\n\n<pre><code>#include &lt;iostream&gt;\nint k = 0;\n\nint &amp;foo()\n{\n    return k;\n}\n\nint main(int argc,char **argv)\n{\n    k = 4;\n    foo() = 5;\n    std::cout &lt;&lt; \"k=\" &lt;&lt; k &lt;&lt; \"\\n\";\n    return 0;\n}\n</code></pre>\n\n<p>This code prints:</p>\n\n<p>$ ./a.out\n   k=5</p>\n\n<p>Because <code>foo()</code> returns a reference to the global variable <code>k</code>.</p>\n\n<p>In your revised code, you are casting the returned value to a reference, which is then invalid. </p>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477674, "question_id": 36477542, "tags": ["c++"]}, {"body": "<pre><code>int&amp; foo();\n</code></pre>\n\n<p>Declares a function named foo that returns a reference to an <code>int</code>.  What that examples fails to do is give you a definition of that function that you could compile.  If we use</p>\n\n<pre><code>int &amp; foo()\n{\n    static int bar = 0;\n    return bar;\n}\n</code></pre>\n\n<p>Now we have a function that returns a reference to <code>bar</code>.  since bar is <code>static</code> it will live on after the call to the function so returning a reference to it is safe.  Now if we do</p>\n\n<pre><code>foo() = 42;\n</code></pre>\n\n<p>What happens is we assign 42 to <code>bar</code> since we assign to the reference and the reference is just an alias for <code>bar</code>.  If we call the function again like</p>\n\n<pre><code>std::cout &lt;&lt; foo();\n</code></pre>\n\n<p>It would print 42 since we set <code>bar</code> to that above.</p>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477686, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p><code>int &amp;foo();</code> declares a function called <code>foo()</code> with return type <code>int&amp;</code>.  If you call this function without providing a body then you are likely to get an undefined reference error.</p>\n\n<p>In your second attempt you provided a function <code>int foo()</code>.  This has a different return type to the function declared by <code>int&amp; foo();</code>.  So you have two declarations of the same <code>foo</code> that don't match, which violates the One Definition Rule causing undefined behaviour (no diagnostic required).</p>\n\n<p>For something that works, take out the local function declaration. They can lead to silent undefined behaviour as you have seen. Instead, only use function declarations outside of any function. Your program could look like:</p>\n\n<pre><code>int &amp;foo()\n{\n    static int i = 2;\n    return i;\n}  \n\nint main()\n{\n    ++foo();  \n    std::cout &lt;&lt; foo() &lt;&lt; '\\n';\n}\n</code></pre>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477701, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p>In that context the &amp; means a reference - so foo returns a reference to an int, rather than an int.</p>\n\n<p>I'm not sure if you'd have worked with pointers yet, but it's a similar idea, you're not actually returning the value out of the function - instead you're passing the information needed to find the location in memory where that int is. </p>\n\n<p>So to summarize you're not assigning a value to a function call - you're using a function to get a reference, and then assigning the value being referenced to a new value. It's easy to think everything happens at once, but in reality the computer does everything in a precise order.</p>\n\n<p>If you're wondering - the reason you're getting a segfault is because you're returning a numeric literal '2' - so it's the exact error you'd get if you were to define a const int and then try to modify its value.</p>\n\n<p>If you haven't learned about pointers and dynamic memory yet then I'd recommend that first as there's a few concepts that I think are hard to understand unless you're learning them all at once.</p>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477852, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p>The function you have, foo(), is a function that returns a reference to an integer.</p>\n\n<p>So let's say originally foo returned 5, and later on, in your main function, you say <code>foo() = 10;</code>, then prints out foo, it will print 10 instead of 5.</p>\n\n<p>I hope that makes sense :)</p>\n\n<p>I'm new to programming as well. It's interesting to see questions like this that makes you think! :)</p>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36477854, "question_id": 36477542, "tags": ["c++"]}, {"body": "<p>All the other answers declare a static inside the function. I think that might confuse you, so take a look at this:</p>\n\n<pre><code>int&amp; highest(int  &amp; i, int  &amp; j)\n{\n    if (i &gt; j)\n    {\n        return i;\n    }\n    return j;\n}\n\nint main()\n{\n    int a{ 3};\n    int b{ 4 };\n    highest(a, b) = 11;\n    return 0;\n}\n</code></pre>\n\n<p>Because <code>highest()</code> returns a reference, you can assign a value to it. When this runs, <code>b</code> will be changed to 11. If you changed the initialization so that <code>a</code> was, say, 8, then <code>a</code> would be changed to 11. This is some code that might actually serve a purpose, unlike the other examples.</p>\n", "title": "What does &quot;int&amp; foo()&quot; mean in C++?", "answer_id": 36488088, "question_id": 36477542, "tags": ["c++"]}], "question_id": 36477542}, {"body": "<p>Does this <code>for</code> loop ever stop?</p>\n\n<pre><code>for (var i=0; 1/i &gt; 0; i++) {\n}\n</code></pre>\n\n<p>If so, when and why? I was told that it stops, but I was given no reason for that.</p>\n", "title": "Does this &#39;for&#39; loop stop, and why/why not? for (var i=0; 1/i &gt; 0; i++) { }", "tags": ["javascript"], "answer_count": 4, "link": "http://stackoverflow.com/questions/37827073/does-this-for-loop-stop-and-why-why-not-for-var-i-0-1-i-0-i", "answers": [{"body": "<p>The <code>Number.MAX_SAFE_INTEGER</code> constant represents the maximum safe integer in JavaScript. The <code>MAX_SAFE_INTEGER</code> constant has a value of <code>9007199254740991</code>. The reasoning behind that number is that JavaScript uses <a href=\"http://en.wikipedia.org/wiki/Double_precision_floating-point_format\">double-precision floating-point format numbers</a> as specified in <a href=\"http://en.wikipedia.org/wiki/IEEE_floating_point\">IEEE 754</a> and can only safely represent numbers between -(2<sup>53</sup> - 1) and 2<sup>53</sup> - 1.</p>\n\n<p>Safe in this context refers to the ability to represent integers exactly and to correctly compare them. For example, <code>Number.MAX_SAFE_INTEGER + 1 === Number.MAX_SAFE_INTEGER + 2</code> will evaluate to <code>true</code>, which is mathematically incorrect. See <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Number/isSafeInteger\"><code>Number.isSafeInteger()</code></a> for more information.</p>\n\n<p>Because <code>MAX_SAFE_INTEGER</code> is a static property of <code>Number</code>, you always use it as <code>Number.MAX_SAFE_INTEGER</code>, rather than as a property of a <code>Number</code> object you created.</p>\n\n<p><strong>UPDATE:</strong></p>\n\n<p>Someone in an answer that was deleted mentioned: <code>i</code> will never reach infinity. Once it reaches <code>Number.MAX_SAFE_INTEGER</code>, <code>i++</code> doesn't increment the variable anymore. This is in fact <em>not</em> correct.</p>\n\n<p>@T.J. Crowder comments that <code>i = Number.MAX_SAFE_INTEGER; i++; i == Number.MAX_SAFE_INTEGER;</code> is <code>false</code>. But the next iteration reaches an unchanging state, so the answer in main is correct.</p>\n\n<p><code>i</code> in the example never reaches <code>Infinity</code>.</p>\n", "title": "Does this &#39;for&#39; loop stop, and why/why not? for (var i=0; 1/i &gt; 0; i++) { }", "answer_id": 37827271, "question_id": 37827073, "tags": ["javascript"]}, {"body": "<h3>Answer:</h3>\n\n<p>The condition <code>1/i &gt; 0</code> will always evaluate to true:</p>\n\n<ul>\n<li><p>Initially it's true because <code>1/0</code> evaluates to <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Infinity\"><code>Infinity</code></a> and <code>Infinity &gt; 0</code> is true</p></li>\n<li><p>It stays true since <code>1/i &gt; 0</code> is true for all <code>i &lt; Infinity</code> and <code>i++</code> never reaches <code>Infinity</code>.</p></li>\n</ul>\n\n<p>Why does <code>i++</code> never reach <code>Infinity</code>? Due to the limited precision of the <code>Number</code> datatype, there is a value for which <code>i + 1 == i</code>:</p>\n\n<pre><code>9007199254740992 + 1 == 9007199254740992 // true\n</code></pre>\n\n<p>Once <code>i</code> reaches that value (which corresponds to <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Number/MAX_SAFE_INTEGER\"><code>Number.MAX_SAFE_INTEGER</code></a><code>+ 1</code>), it will stay the same even after <code>i++</code>.</p>\n\n<p>We therefore have an infinite loop.</p>\n\n<hr>\n\n<h3>Appendix:</h3>\n\n<p>Why is <code>9007199254740992 + 1 == 9007199254740992</code>?</p>\n\n<p>JavaScript's <code>Number</code> datatype is actually an 64-bit <a href=\"https://en.wikipedia.org/wiki/Double-precision_floating-point_format\">IEEE 754 double precision float</a>. Each <code>Number</code> is disassembled and stored as three parts: 1-bit sign, 11-bit exponent, and 52-bit mantissa. Its value is -1<sup> sign</sup> \u00d7 mantissa \u00d7 2<sup> exponent</sup>.</p>\n\n<p>How is <strong>9007199254740992</strong> represented? As <strong>1.0 \u00d7 2<sup> 53</sup></strong>, or in binary:</p>\n\n<p><img src=\"http://i.stack.imgur.com/YLmDe.png\" alt=\"enter image description here\"></p>\n\n<p>Incrementing the mantissa's least significant bit, we get the next higher number:</p>\n\n<p><img src=\"http://i.stack.imgur.com/Fs8yo.png\" alt=\"enter image description here\"></p>\n\n<p>The value of that number is 1.00000000000000022\u2026 \u00d7 2<sup> 53</sup> = <strong>9007199254740994</strong></p>\n\n<p>What does that mean? <code>Number</code> can either be 900719925474099<strong>2</strong> or 900719925474099<strong>4</strong>, but nothing in between.</p>\n\n<p>Now, which one shall we chose to represent 900719925474099<strong>2 + 1</strong>? The <a href=\"https://en.wikipedia.org/wiki/IEEE_floating_point#Roundings_to_nearest\">IEEE 754 rounding rules</a> give the answer: 900719925474099<strong>2</strong>.</p>\n", "title": "Does this &#39;for&#39; loop stop, and why/why not? for (var i=0; 1/i &gt; 0; i++) { }", "answer_id": 37827313, "question_id": 37827073, "tags": ["javascript"]}, {"body": "<p><em>(I'm not a fan of meta-content, but: <a href=\"http://stackoverflow.com/a/37827271/157247\">gotnull's</a> and <a href=\"http://stackoverflow.com/a/37827313/157247\">le_m's</a> answers are both correct and useful. They were originally, and are <strong>even more so</strong> with the edits made after this Community Wiki was posted. The original motivation for this CW is largely gone as a result of those edits, but it remains useful, so... Also: While there are only a couple of authors listed, many other community members have helped greatly with comments which have been folded in and cleaned up. This isn't just a CW in name.)</em></p>\n\n<hr>\n\n<p>The loop won't stop in a correctly-implemented JavaScript engine. <em>(The engine's host environment might eventually terminate it because it's endless, but that's another thing.)</em></p>\n\n<p>Here's why:</p>\n\n<ol>\n<li><p>Initially, when <code>i</code> is <code>0</code>, the condition <code>1/i &gt; 0</code> is true because in JavaScript, <code>1/0</code> is <code>Infinity</code>, and <code>Infinity &gt; 0</code> is true.</p></li>\n<li><p>After that, <code>i</code> will be incremented and continue to grow as a positive integer value for a long time (a further 9,007,199,254,740,991 iterations). In all of those cases, <code>1/i</code> will remain <code>&gt; 0</code> (although the values for <code>1/i</code> get <strong>really</strong> small toward the end!) and so the loop continues up to and including the loop where <code>i</code> reaches the value <code>Number.MAX_SAFE_INTEGER</code>.</p></li>\n<li><p>Numbers in JavaScript are IEEE-754 double-precision binary floating point, a fairly compact format (64 bits) which provides for fast calculations and a vast range. It does this by storing the number as a sign bit, an 11-bit exponent, and a 52-bit significand (although through cleverness it actually gets 53 bits of precision). It's <em>binary</em> (base 2) floating point: The significand (plus some cleverness) gives us the value, and the exponent gives us the magnitude of the number.</p>\n\n<p>Naturally, with just so many significant bits, not every number can be stored. Here is the number 1, and the next highest number after 1 that the format can store, 1 + 2<sup>-52</sup> \u2248 1.00000000000000022, and the next highest after that 1 + 2 \u00d7 2<sup>-52</sup> \u2248 1.00000000000000044:</p>\n\n<pre>\n&nbsp;&nbsp;&nbsp;+--------------------------------------------------------------- sign bit\n&nbsp;&nbsp;/ +-------+------------------------------------------------------ exponent\n&nbsp;/ /        |  +-------------------------------------------------+- significand\n/ /         | /                                                  |\n0 01111111111 0000000000000000000000000000000000000000000000000000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= 1\n0 01111111111 0000000000000000000000000000000000000000000000000001\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u2248 1.00000000000000022\n0 01111111111 0000000000000000000000000000000000000000000000000010\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\u2248 1.00000000000000044\n</pre>\n\n<p>Note the jump from 1.00000000000000022 to 1.00000000000000044; there's no way to store 1.0000000000000003. That can happen with integers, too: <code>Number.MAX_SAFE_INTEGER</code> (9,007,199,254,740,991) is the highest positive integer value that the format can hold where <code>i</code> and <code>i + 1</code> are both exactly representable (<a href=\"http://www.ecma-international.org/ecma-262/6.0/index.html#sec-number.max_safe_integer\">spec</a>). Both 9,007,199,254,740,991 and 9,007,199,254,740,992 can be represented, but the <em>next</em> integer, 9,007,199,254,740,993, cannot; the next integer we can represent after 9,007,199,254,740,992 is 9,007,199,254,740,994. Here are the bit patterns, note the rightmost (least significant) bit:</p>\n\n<pre>\n&nbsp;&nbsp;&nbsp;+--------------------------------------------------------------- sign bit\n&nbsp;&nbsp;/ +-------+------------------------------------------------------ exponent\n&nbsp;/ /        |  +-------------------------------------------------+- significand\n/ /         | /                                                  |\n0 10000110011 1111111111111111111111111111111111111111111111111111\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= 9007199254740991 (Number.MAX_SAFE_INTEGER)\n0 10000110100 0000000000000000000000000000000000000000000000000000\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= 9007199254740992 (Number.MAX_SAFE_INTEGER + 1)\nx xxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;9007199254740993 (Number.MAX_SAFE_INTEGER + 2) can't be stored\n0 10000110100 0000000000000000000000000000000000000000000000000001\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= 9007199254740994 (Number.MAX_SAFE_INTEGER + 3)\n</pre>\n\n<p>Remember, the format is base 2, and with that exponent the least significant bit is no longer fractional; it has a value of 2. It can be off (9,007,199,254,740,992) or on (9,007,199,254,740,994); so at this point, we've started to lose precision even at the whole number (integer) scale. Which has implications for our loop!</p></li>\n<li><p>After completing the <code>i = 9,007,199,254,740,992</code> loop, <code>i++</code> gives us ... <code>i = 9,007,199,254,740,992</code> again; there's no change in <code>i</code>, because the next integer can't be stored and the calculation ends up rounding down. <code>i</code> would change if we did <code>i += 2</code>, but <code>i++</code> can't change it. So we've reached steady-state: <code>i</code> never changes, and the loop never terminates.</p></li>\n</ol>\n\n<p>Here are the various relevant calculations:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>if (!Number.MAX_SAFE_INTEGER) {\r\n  // Browser doesn't have the Number.MAX_SAFE_INTEGER\r\n  // property; shim it. Should use Object.defineProperty\r\n  // but hey, maybe it's so old it doesn't have that either\r\n  Number.MAX_SAFE_INTEGER = 9007199254740991;\r\n}\r\nvar i = 0;\r\nconsole.log(i, 1/i, 1/i &gt; 0); // 0, Infinity, true\r\ni++;\r\nconsole.log(i, 1/i, 1/i &gt; 0); // 1, 1, true\r\n// ...eventually i is incremented all the way to Number.MAX_SAFE_INTEGER\r\ni = Number.MAX_SAFE_INTEGER;\r\nconsole.log(i, 1/i, 1/i &gt; 0); // 9007199254740991 1.1102230246251568e-16, true\r\ni++;\r\nconsole.log(i, 1/i, 1/i &gt; 0); // 9007199254740992 1.1102230246251565e-16, true\r\ni++;\r\nconsole.log(i, 1/i, 1/i &gt; 0); // 9007199254740992 1.1102230246251565e-16, true (no change)\r\nconsole.log(i == i + 1);      // true</code></pre>\r\n</div>\r\n</div>\r\n</p>\n", "title": "Does this &#39;for&#39; loop stop, and why/why not? for (var i=0; 1/i &gt; 0; i++) { }", "answer_id": 37830614, "question_id": 37827073, "tags": ["javascript"]}, {"body": "<p>Who told you that it will stops, ask him the condition where it will stop.\nSince condition in for loop is always true for any value of i, it will never stops..</p>\n", "title": "Does this &#39;for&#39; loop stop, and why/why not? for (var i=0; 1/i &gt; 0; i++) { }", "answer_id": 37959278, "question_id": 37827073, "tags": ["javascript"]}], "question_id": 37827073}, {"body": "<p>With almost all code I write, I am often dealing with set reduction problems on collections that ultimately end up with naive \"if\" conditions inside of them. Here's a simple example:</p>\n\n<pre><code>for(int i=0; i&lt;myCollection.size(); i++)\n{\n     if (myCollection[i] == SOMETHING)\n     {\n           DoStuff();\n     }\n}\n</code></pre>\n\n<p>With functional languages, I can solve the problem by reducing the collection to another collection (easily) and then perform all operations on my reduced set. In pseudocode:</p>\n\n<pre><code>newCollection &lt;- myCollection where &lt;x=true\nmap DoStuff newCollection\n</code></pre>\n\n<p>And in other C variants, like C#, I could reduce with a where clause like</p>\n\n<pre><code>foreach (var x in myCollection.Where(c=&gt; c == SOMETHING)) \n{\n   DoStuff();\n}\n</code></pre>\n\n<p>Or better (at least to my eyes)</p>\n\n<pre><code>myCollection.Where(c=&gt;c == Something).ToList().ForEach(d=&gt; DoStuff(d));\n</code></pre>\n\n<p>Admittedly, I am doing a lot of paradigm mixing and subjective/opinion based style, but I can't help but feel that I am missing something really fundamental that could allow me to use this preferred technique with C++. Could someone enlighten me?</p>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "tags": ["c++", "c++11", "c++14"], "answer_count": 11, "link": "http://stackoverflow.com/questions/38399334/how-can-i-avoid-for-loops-with-an-if-condition-inside-them-with-c", "answers": [{"body": "<p>Boost provides ranges that can be used w/ range-based for. Ranges have the advantage that they don't copy the underlying data structure, they merely provide a 'view' (that is, <code>begin()</code>, <code>end()</code> for the range and <code>operator++()</code>, <code>operator==()</code> for the iterator). This might be of your interest: <a href=\"http://www.boost.org/libs/range/doc/html/range/reference/adaptors/reference/filtered.html\">http://www.boost.org/libs/range/doc/html/range/reference/adaptors/reference/filtered.html</a></p>\n\n<pre><code>#include &lt;boost/range/adaptor/filtered.hpp&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nstruct is_even\n{\n    bool operator()( int x ) const { return x % 2 == 0; }\n};\n\nint main(int argc, const char* argv[])\n{\n    using namespace boost::adaptors;\n\n    std::vector&lt;int&gt; myCollection{1,2,3,4,5,6,7,8,9};\n\n    for( int i: myCollection | filtered( is_even() ) )\n    {\n        std::cout &lt;&lt; i;\n    }\n}\n</code></pre>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38399472, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>IMHO it's more straight forward and more readable to use a for loop with an if inside it. However, if this is annoying for you, you could use a <code>for_each_if</code> like the one below:</p>\n\n<pre><code>template&lt;typename Iter, typename Pred, typename Op&gt; \nvoid for_each_if(Iter first, Iter last, Pred p, Op op) {\n  while(first != last) {\n    if (p(*first)) op(*first);\n    ++first;\n  }\n}\n</code></pre>\n\n<p><em>Usecase:</em></p>\n\n<pre><code>std::vector&lt;int&gt; v {10, 2, 10, 3};\nfor_each_if(v.begin(), v.end(), [](int i){ return i &gt; 5; }, [](int &amp;i){ ++i; });\n</code></pre>\n\n<p><a href=\"http://coliru.stacked-crooked.com/a/9f418d1c5e0d82f0\"><strong>Live Demo</strong></a></p>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38399676, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>Instead of creating a new algorithm, as the accepted answer does, you can use an existing one with a function that applies the condition:</p>\n\n<pre><code>std::for_each(first, last, [](auto&amp;&amp; x){ if (cond(x)) { ... } });\n</code></pre>\n\n<p>Or if you really want a new algorithm, at least reuse <code>for_each</code> there instead of duplicating the iteration logic:</p>\n\n<pre><code>template&lt;typename Iter, typename Pred, typename Op&gt; \n  void\n  for_each_if(Iter first, Iter last, Pred p, Op op) {\n    std::for_each(first, last, [&amp;](auto&amp; x) { if (p(x)) op(x); });\n  }\n</code></pre>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38400434, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>Here is a quick relatively minimal <code>filter</code> function.</p>\n\n<p>It takes a predicate.  It returns a function object that takes an iterable.</p>\n\n<p>It returns an iterable that can be used in a <code>for(:)</code> loop.</p>\n\n<pre><code>template&lt;class It&gt;\nstruct range_t {\n  It b, e;\n  It begin() const { return b; }\n  It end() const { return e; }\n  bool empty() const { return begin()==end(); }\n};\ntemplate&lt;class It&gt;\nrange_t&lt;It&gt; range( It b, It e ) { return {std::move(b), std::move(e)}; }\n\ntemplate&lt;class It, class F&gt;\nstruct filter_helper:range_t&lt;It&gt; {\n  F f;\n  void advance() {\n    while(true) {\n      (range_t&lt;It&gt;&amp;)*this = range( std::next(this-&gt;begin()), this-&gt;end() );\n      if (this-&gt;empty())\n        return;\n      if (f(*this-&gt;begin()))\n        return;\n    }\n  }\n  filter_helper(range_t&lt;It&gt; r, F fin):\n    range_t&lt;It&gt;(r), f(std::move(fin))\n  {\n      while(true)\n      {\n          if (this-&gt;empty()) return;\n          if (f(*this-&gt;begin())) return;\n          (range_t&lt;It&gt;&amp;)*this = range( std::next(this-&gt;begin()), this-&gt;end() );\n      }\n  }\n};\n\ntemplate&lt;class It, class F&gt;\nstruct filter_psuedo_iterator {\n  using iterator_category=std::input_iterator_tag;\n  filter_helper&lt;It, F&gt;* helper = nullptr;\n  bool m_is_end = true;\n  bool is_end() const {\n    return m_is_end || !helper || helper-&gt;empty();\n  }\n\n  void operator++() {\n    helper-&gt;advance();\n  }\n  typename std::iterator_traits&lt;It&gt;::reference\n  operator*() const {\n    return *(helper-&gt;begin());\n  }\n  It base() const {\n      if (!helper) return {};\n      if (is_end()) return helper-&gt;end();\n      return helper-&gt;begin();\n  }\n  friend bool operator==(filter_psuedo_iterator const&amp; lhs, filter_psuedo_iterator const&amp; rhs) {\n    if (lhs.is_end() &amp;&amp; rhs.is_end()) return true;\n    if (lhs.is_end() || rhs.is_end()) return false;\n    return lhs.helper-&gt;begin() == rhs.helper-&gt;begin();\n  }\n  friend bool operator!=(filter_psuedo_iterator const&amp; lhs, filter_psuedo_iterator const&amp; rhs) {\n    return !(lhs==rhs);\n  }\n};\ntemplate&lt;class It, class F&gt;\nstruct filter_range:\n  private filter_helper&lt;It, F&gt;,\n  range_t&lt;filter_psuedo_iterator&lt;It, F&gt;&gt;\n{\n  using helper=filter_helper&lt;It, F&gt;;\n  using range=range_t&lt;filter_psuedo_iterator&lt;It, F&gt;&gt;;\n\n  using range::begin; using range::end; using range::empty;\n\n  filter_range( range_t&lt;It&gt; r, F f ):\n    helper{{r}, std::forward&lt;F&gt;(f)},\n    range{ {this, false}, {this, true} }\n  {}\n};\n\ntemplate&lt;class F&gt;\nauto filter( F&amp;&amp; f ) {\n    return [f=std::forward&lt;F&gt;(f)](auto&amp;&amp; r)\n    {\n        using std::begin; using std::end;\n        using iterator = decltype(begin(r));\n        return filter_range&lt;iterator, std::decay_t&lt;decltype(f)&gt;&gt;{\n            range(begin(r), end(r)), f\n        };\n    };\n};\n</code></pre>\n\n<p>I took short cuts.  A real library should make real iterators, not the <code>for(:)</code>-qualifying pseudo-fascades I did.</p>\n\n<p>At point of use, it looks like this:</p>\n\n<pre><code>int main()\n{\n  std::vector&lt;int&gt; test = {1,2,3,4,5};\n  for( auto i: filter([](auto x){return x%2;})( test ) )\n    std::cout &lt;&lt; i &lt;&lt; '\\n';\n}\n</code></pre>\n\n<p>which is pretty nice, and prints</p>\n\n<pre><code>1\n3\n5\n</code></pre>\n\n<p><a href=\"http://coliru.stacked-crooked.com/a/31d7911a3cb214ed\">Live example</a>.</p>\n\n<p>There is a proposed addition to C++ called Rangesv3 which does this kind of thing and more.  <code>boost</code> also has filter ranges/iterators available.  boost also has helpers that make writing the above much shorter.</p>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38402087, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>One style that gets used enough to mention, but hasn't been mentioned yet, is:</p>\n\n<pre><code>for(int i=0; i&lt;myCollection.size(); i++) {\n  if (myCollection[i] != SOMETHING)\n    continue;\n\n  DoStuff();\n}\n</code></pre>\n\n<p>Advantages:</p>\n\n<ul>\n<li>Doesn't change the indentation level of <code>DoStuff();</code> when condition complexity increases. Logically, <code>DoStuff();</code> should be at the top-level of the <code>for</code> loop, and it is.</li>\n<li>Immediately makes it clear that the loop iterates over the <code>SOMETHING</code>s of the collection, without requiring the reader to verify that there is nothing after the closing <code>}</code> of the <code>if</code> block.</li>\n<li>Doesn't require any libraries or helper macros or functions.</li>\n</ul>\n\n<p>Disadvantages:</p>\n\n<ul>\n<li><code>continue</code>, like other flow control statements, gets misused in ways that lead to hard-to-follow code so much that some people are opposed to <em>any</em> use of them: there is a valid style of coding that some follow that avoids <code>continue</code>, that avoids <code>break</code> other than in a <code>switch</code>, that avoids <code>return</code> other than at the end of a function.</li>\n</ul>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38409085, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>If DoStuff() would be dependent on i somehow in the future then I'd propose this guaranteed branch-free bit-masking variant.</p>\n\n<pre><code>unsigned int times = 0;\nconst int kSize = sizeof(unsigned int)*8;\nfor(int i = 0; i &lt; myCollection.size()/kSize; i++){\n  unsigned int mask = 0;\n  for (int j = 0; j&lt;kSize; j++){\n    mask |= (myCollection[i*kSize+j]==SOMETHING) &lt;&lt; j;\n  }\n  times+=popcount(mask);\n}\n\nfor(int i=0;i&lt;times;i++)\n   DoStuff();\n</code></pre>\n\n<p>Where popcount is any function doing a population count ( count number of bits = 1 ). There will be some freedom to put more advanced constraints with i and their neighbors. If that is not needed we can strip the inner loop and remake the outer loop</p>\n\n<pre><code>for(int i = 0; i &lt; myCollection.size(); i++)\n  times += (myCollection[i]==SOMETHING);\n</code></pre>\n\n<p>followed by a</p>\n\n<pre><code>for(int i=0;i&lt;times;i++)\n   DoStuff();\n</code></pre>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38409791, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<pre><code>for(auto const &amp;x: myCollection) if(x == something) doStuff();\n</code></pre>\n\n<p>Looks pretty much like a C++-specific <code>for</code> comprehension to me. To you?</p>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38409896, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>Also, if you don't care reordering the collection, std::partition is cheap.</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n#include &lt;algorithm&gt;\n#include &lt;functional&gt;\n\nvoid DoStuff(int i)\n{\n    std::cout &lt;&lt; i &lt;&lt; '\\n';\n}\n\nint main()\n{\n    using namespace std::placeholders;\n\n    std::vector&lt;int&gt; v {1, 2, 5, 0, 9, 5, 5};\n    const int SOMETHING = 5;\n\n    std::for_each(v.begin(),\n                  std::partition(v.begin(), v.end(),\n                                 std::bind(std::equal_to&lt;int&gt; {}, _1, SOMETHING)), // some condition\n                  DoStuff); // action\n}\n</code></pre>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38410741, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>I am in awe of the complexity of the above solutions. I was going to suggest a simple <code>#define foreach(a,b,c,d) for(a; b; c)if(d)</code> but it has a few obvious deficits, for example, you have to remember to use commas instead of semicolons in your loop, and you can't use the comma operator in <code>a</code> or <code>c</code>.</p>\n\n<pre><code>#include &lt;list&gt;\n#include &lt;iostream&gt;\n\nusing namespace std; \n\n#define foreach(a,b,c,d) for(a; b; c)if(d)\n\nint main(){\n  list&lt;int&gt; a;\n\n  for(int i=0; i&lt;10; i++)\n    a.push_back(i);\n\n  for(auto i=a.begin(); i!=a.end(); i++)\n    if((*i)&amp;1)\n      cout &lt;&lt; *i &lt;&lt; ' ';\n  cout &lt;&lt; endl;\n\n  foreach(auto i=a.begin(), i!=a.end(), i++, (*i)&amp;1)\n    cout &lt;&lt; *i &lt;&lt; ' ';\n  cout &lt;&lt; endl;\n\n  return 0;\n}\n</code></pre>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38413839, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>The idea of avoiding</p>\n\n<pre><code>for(...)\n    if(...)\n</code></pre>\n\n<p>constructs as an antipattern is too broad.</p>\n\n<p>It is completely fine to process multiple items that match a certain expression from inside a loop, and the code cannot get much clearer than that. If the processing grows too large to fit on screen, that is a good reason to use a subroutine, but still the conditional is best placed inside the loop, i.e.</p>\n\n<pre><code>for(...)\n    if(...)\n        do_process(...);\n</code></pre>\n\n<p>is vastly preferable to</p>\n\n<pre><code>for(...)\n    maybe_process(...);\n</code></pre>\n\n<p>It becomes an antipattern when only one element will match, because then it would be clearer to first search for the element, and perform the processing outside of the loop.</p>\n\n<pre><code>for(int i = 0; i &lt; size; ++i)\n    if(i == 5)\n</code></pre>\n\n<p>is an extreme and obvious example of this. More subtle, and thus more common, is a factory pattern like</p>\n\n<pre><code>for(creator &amp;c : creators)\n    if(c.name == requested_name)\n    {\n        unique_ptr&lt;object&gt; obj = c.create_object();\n        obj.owner = this;\n        return std::move(obj);\n    }\n</code></pre>\n\n<p>This is hard to read, because it isn't obvious that the body code will be executed once only. In this case, it would be better to separate the lookup:</p>\n\n<pre><code>creator &amp;lookup(string const &amp;requested_name)\n{\n    for(creator &amp;c : creators)\n        if(c.name == requested_name)\n            return c;\n}\n\ncreator &amp;c = lookup(requested_name);\nunique_ptr obj = c.create_object();\n</code></pre>\n\n<p>There is still an <code>if</code> within a <code>for</code>, but from the context it becomes clear what it does, there is no need to change this code unless the lookup changes (e.g. to a <code>map</code>), and it is immediately clear that <code>create_object()</code> is called only once, because it is not inside a loop.</p>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38424427, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}, {"body": "<p>Another solution in case the i:s are important. This one builds a list that fills in the indexes of which to call doStuff() for. Once again the main point is to avoid the branching and trade it for pipelineable arithmetic costs.</p>\n\n<pre><code>int buffer[someSafeSize];\nint cnt = 0; // counter to keep track where we are in list.\nfor( int i = 0; i &lt; container.size(); i++ ){\n   int lDecision = (container[i] == SOMETHING);\n   buffer[cnt] = lDecision*i + (1-lDecision)*buffer[cnt];\n   cnt += lDecision;\n}\n\nfor( int i=0; i&lt;cnt; i++ )\n   doStuff(buffer[i]); // now we could pass the index or a pointer as an argument.\n</code></pre>\n\n<p>The \"magical\" line is the buffer loading line that arithmetically calculates wether to keep the value and stay in position or to count up position and add value. So we trade away a potential branch for some logics and arithmetics and maybe some cache hits. A typical scenario when this would be useful is if doStuff() does a small amount of pipelineable calculations and any branch in between calls could interrupt those pipelines.</p>\n\n<p>Then just loop over the buffer and run doStuff() until we reach cnt. This time we will have the current i stored in the buffer so we can use it in the call to doStuff() if we would need to.</p>\n", "title": "How can I avoid &quot;for&quot; loops with an &quot;if&quot; condition inside them with C++?", "answer_id": 38570434, "question_id": 38399334, "tags": ["c++", "c++11", "c++14"]}], "question_id": 38399334}, {"body": "<p>If you want to use <a href=\"https://en.wikipedia.org/wiki/Qt_%28software%29\">Qt</a>, you have to embrace <code>quint8</code>, <code>quint16</code> and so forth.</p>\n\n<p>If you want to use <strong><a href=\"https://en.wikipedia.org/wiki/GLib\">GLib</a></strong>, you have to welcome <code>guint8</code>, <code>guint16</code> and so forth.</p>\n\n<p>On <a href=\"http://en.wikipedia.org/wiki/Linux\">Linux</a> there are <code>u32</code>, <code>s16</code> and so forth.</p>\n\n<p><strong>uC/OS</strong> defines <code>SINT32</code>, <code>UINT16</code> and so forth.</p>\n\n<p>And if you have to use some combination of those things, you better be prepared for trouble. Because on your machine <code>u32</code> will be <code>typedef</code>d over <code>long</code> and <code>quint32</code> will be <code>typedef</code>d over <code>int</code> and the compiler <em>will complain</em>.</p>\n\n<p>Why does everybody do this, if there is <code>&lt;stdint.h&gt;</code>? Is this some kind of tradition for libraries?</p>\n", "title": "Why does everybody typedef over standard C types?", "tags": ["c++", "c", "stdint"], "answer_count": 4, "link": "http://stackoverflow.com/questions/38552314/why-does-everybody-typedef-over-standard-c-types", "answers": [{"body": "<p><code>stdint.h</code> didn't exist back when these libraries were being developed. So each library made its own <code>typedef</code>s.</p>\n", "title": "Why does everybody typedef over standard C types?", "answer_id": 38552322, "question_id": 38552314, "tags": ["c++", "c", "stdint"]}, {"body": "<p>For the older libraries, this is needed because the header in question (<code>stdint.h</code>) didn't exist.</p>\n\n<p>There's still, however, a problem around: those types (<code>uint64_t</code> and others) are <em>an optional feature</em> in the standard. So a complying implementation might not ship with them -- and thus force libraries to still include them nowadays.</p>\n", "title": "Why does everybody typedef over standard C types?", "answer_id": 38552329, "question_id": 38552314, "tags": ["c++", "c", "stdint"]}, {"body": "<p><code>stdint.h</code> has been standardised since 1999. It is more likely that many applications define (effectively alias) types to maintain partial independence from the underlying machine architecture.</p>\n\n<p>They provide developers confidence that types used in their application matches their project specific assumptions on behavior that may not match either the language standard or compiler implementation.</p>\n\n<p>The practice is mirrored in the object oriented <a href=\"http://c2.com/cgi/wiki?FacadePattern\">Fa\u00e7ade</a> design pattern and is much abused by developers invariably writing wrapper classes for all imported libraries.</p>\n\n<p>When compliers were much less standard and machine architectures could vary from 16-bit, <a href=\"http://www.linfo.org/pdp-7.html\">18-bit</a> through <a href=\"https://en.wikipedia.org/wiki/PDP-10\">36-bit</a> word length mainframes this was much more of a consideration. The practice is much less relevant now in a world converging on 32-bit ARM embedded systems. It remains a concern for low-end microcontrollers with <a href=\"https://en.wikipedia.org/wiki/Harvard_architecture\">odd</a> memory maps.</p>\n", "title": "Why does everybody typedef over standard C types?", "answer_id": 38554684, "question_id": 38552314, "tags": ["c++", "c", "stdint"]}, {"body": "<p><em>So you have the power to typedef char to int.</em></p>\n\n<p>One \"coding horror\" mentioned that one companies header had a point where a programmer wanted a boolean value, and a char was the logical native type for the job, and so wrote <code>typedef bool char</code>. Then later on someone found an integer to be the most logical choice, and wrote <code>typedef bool int</code>. The result, ages before Unicode, was virtually <code>typedef char int</code>.</p>\n\n<p>Quite a lot of forward-thinking, forward compatibility, I think.</p>\n", "title": "Why does everybody typedef over standard C types?", "answer_id": 38565498, "question_id": 38552314, "tags": ["c++", "c", "stdint"]}], "question_id": 38552314}, {"body": "<p>What's the most pythonic way to mesh two strings together?</p>\n\n<p>For example:</p>\n\n<p>Input:</p>\n\n<pre><code>u = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nl = 'abcdefghijklmnopqrstuvwxyz'\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "tags": ["python", "string", "python-2.7", "python-3.x"], "answer_count": 14, "link": "http://stackoverflow.com/questions/34756145/most-pythonic-way-to-interleave-two-strings", "answers": [{"body": "<p>With <code>join()</code> and <code>zip()</code>.</p>\n\n<pre><code>&gt;&gt;&gt; ''.join(''.join(item) for item in zip(u,l))\n'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34756161, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>For me, the most pythonic* way is the following which <em>pretty much does the same thing</em> but uses the <code>+</code> operator for concatenating the individual characters in each string:</p>\n\n<pre><code>res = \"\".join(i + j for i, j in zip(u, l))\nprint(res)\n# 'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'\n</code></pre>\n\n<p>It is also faster than using two <code>join()</code> calls:</p>\n\n<pre><code>In [5]: l1 = 'A' * 1000000; l2 = 'a' * 1000000\n\nIn [6]: %timeit \"\".join(\"\".join(item) for item in zip(l1, l2))\n1 loops, best of 3: 442 ms per loop\n\nIn [7]: %timeit \"\".join(i + j for i, j in zip(l1, l2))\n1 loops, best of 3: 360 ms per loop\n</code></pre>\n\n<p>Faster approaches exist, but they often obfuscate the code.</p>\n\n<p><strong>Note:</strong> If the two input strings are <em>not</em> the same length then the longer one will be truncated as <strong><a href=\"https://docs.python.org/3/library/functions.html#zip\"><code>zip</code></a></strong> stops iterating at the end of the shorter string. In this case instead of <code>zip</code> one should use <strong><a href=\"https://docs.python.org/3/library/itertools.html#itertools.zip_longest\"><code>zip_longest</code></a></strong> (<strong><a href=\"https://docs.python.org/2.7/library/itertools.html#itertools.izip_longest\"><code>izip_longest</code></a></strong> in Python 2) from the <a href=\"https://docs.python.org/3/library/itertools.html\"><code>itertools</code></a> module to ensure that both strings are fully exhausted.</p>\n\n<hr>\n\n<p><sub>*To take a quote from <strong><em><a href=\"https://www.python.org/dev/peps/pep-0020/\">the Zen of Python</a></em></strong>: <strong>Readability counts</strong>. <br>\nPythonic = <strong>readability</strong> for me; <code>i + j</code> is just visually parsed more easily, at least for my eyes.</sub></p>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34756226, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>If you want the fastest way, you can combine <a href=\"https://docs.python.org/2/library/itertools.html\">itertools</a> with <code>operator.add</code>:</p>\n\n<pre><code>In [36]: from operator import add\n\nIn [37]: from itertools import  starmap, izip\n\nIn [38]: timeit \"\".join([i + j for i, j in uzip(l1, l2)])\n1 loops, best of 3: 142 ms per loop\n\nIn [39]: timeit \"\".join(starmap(add, izip(l1,l2)))\n1 loops, best of 3: 117 ms per loop\n\nIn [40]: timeit \"\".join([\"\".join(item) for item in zip(l1, l2)])\n1 loops, best of 3: 196 ms per loop\n\nIn [41]:  \"\".join(starmap(add, izip(l1,l2))) ==  \"\".join([i + j   for i, j in izip(l1, l2)]) ==  \"\".join([\"\".join(item) for item in izip(l1, l2)])\nOut[42]: True\n</code></pre>\n\n<p>But combining <code>izip</code> and <code>chain.from_iterable</code> is faster again</p>\n\n<pre><code>In [2]: from itertools import  chain, izip\n\nIn [3]: timeit \"\".join(chain.from_iterable(izip(l1, l2)))\n10 loops, best of 3: 98.7 ms per loop\n</code></pre>\n\n<p>There is also a substantial difference between \n<code>chain(*</code> and <code>chain.from_iterable(...</code>.</p>\n\n<pre><code>In [5]: timeit \"\".join(chain(*izip(l1, l2)))\n1 loops, best of 3: 212 ms per loop\n</code></pre>\n\n<p>There is no such thing as a generator with join, passing one is always going to be slower as python  will first build a list using the content because it does two passes over the data, one to figure out the size needed and one to actually do the join which would not be possible using a generator:</p>\n\n<p><a href=\"https://github.com/python/cpython/blob/master/Objects/stringlib/join.h#L54\">join.h</a>:</p>\n\n<pre><code> /* Here is the general case.  Do a pre-pass to figure out the total\n  * amount of space we'll need (sz), and see whether all arguments are\n  * bytes-like.\n   */\n</code></pre>\n\n<p>Also if you  have different length strings and you don't want to lose data you can use <a href=\"https://docs.python.org/2/library/itertools.html#itertools.izip_longest\">izip_longest</a> :</p>\n\n<pre><code>In [22]: from itertools import izip_longest    \nIn [23]: a,b = \"hlo\",\"elworld\"\n\nIn [24]:  \"\".join(chain.from_iterable(izip_longest(a, b,fillvalue=\"\")))\nOut[24]: 'helloworld'\n</code></pre>\n\n<p>For python 3 it is called <code>zip_longest</code></p>\n\n<p>But  for python2, veedrac's suggestion is  by far the fastest:</p>\n\n<pre><code>In [18]: %%timeit\nres = bytearray(len(u) * 2)\nres[::2] = u\nres[1::2] = l\nstr(res)\n   ....: \n100 loops, best of 3: 2.68 ms per loop\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34756604, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<h2>Faster Alternative</h2>\n\n<p>Another way:</p>\n\n<pre><code>res = [''] * len(u) * 2\nres[::2] = u\nres[1::2] = l\nprint(''.join(res))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'\n</code></pre>\n\n<h2>Speed</h2>\n\n<p>Looks like it is faster:</p>\n\n<pre><code>%%timeit\nres = [''] * len(u) * 2\nres[::2] = u\nres[1::2] = l\n''.join(res)\n\n100000 loops, best of 3: 4.75 \u00b5s per loop\n</code></pre>\n\n<p>than the fastest solution so far:</p>\n\n<pre><code>%timeit \"\".join(list(chain.from_iterable(zip(u, l))))\n\n100000 loops, best of 3: 6.52 \u00b5s per loop\n</code></pre>\n\n<p>Also for the larger strings:</p>\n\n<pre><code>l1 = 'A' * 1000000; l2 = 'a' * 1000000\n\n%timeit \"\".join(list(chain.from_iterable(zip(l1, l2))))\n1 loops, best of 3: 151 ms per loop\n\n\n%%timeit\nres = [''] * len(l1) * 2\nres[::2] = l1\nres[1::2] = l2\n''.join(res)\n\n10 loops, best of 3: 92 ms per loop\n</code></pre>\n\n<p>Python 3.5.1.</p>\n\n<h2>Variation for strings with different lengths</h2>\n\n<pre><code>u = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nl = 'abcdefghijkl'\n</code></pre>\n\n<h3>Shorter one determines length (<code>zip()</code> equivalent)</h3>\n\n<pre><code>min_len = min(len(u), len(l))\nres = [''] * min_len * 2 \nres[::2] = u[:min_len]\nres[1::2] = l[:min_len]\nprint(''.join(res))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>AaBbCcDdEeFfGgHhIiJjKkLl\n</code></pre>\n\n<h3>Longer one determines length (<code>itertools.zip_longest(fillvalue='')</code> equivalent)</h3>\n\n<pre><code>min_len = min(len(u), len(l))\nres = [''] * min_len * 2 \nres[::2] = u[:min_len]\nres[1::2] = l[:min_len]\nres += u[min_len:] + l[min_len:]\nprint(''.join(res))\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>AaBbCcDdEeFfGgHhIiJjKkLlMNOPQRSTUVWXYZ\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34756930, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>I like using two <code>for</code>s, the variable names can give a hint/reminder to what is going on:</p>\n\n<pre><code>\"\".join(char for pair in zip(u,l) for char in pair)\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34756956, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>Just to add another, more basic approach:</p>\n\n<pre><code>st = \"\"\nfor char in u:\n    st = \"{0}{1}{2}\".format( st, char, l[ u.index( char ) ] )\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34759182, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>Since readability and simplicity are at the core of doing things the \"pythonic\" way; I would suggest that string concatenation be done with the <code>+</code> operator.</p>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34760748, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>A lot of these suggestions assume the strings are of equal length. Maybe that covers all reasonable use cases, but at least to me it seems that you might want to accomodate strings of differing lengths too. Or am I the only one thinking the mesh should work a bit like this:</p>\n\n<pre><code>u = \"foobar\"\nl = \"baz\"\nmesh(u,l) = \"fboaozbar\"\n</code></pre>\n\n<p>One way to do this would be the following:</p>\n\n<pre><code>def mesh(a,b):\n    minlen = min(len(a),len(b))\n    return \"\".join([\"\".join(x+y for x,y in zip(a,b)),a[minlen:],b[minlen:]])\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34761090, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>On Python 2, by <em>far</em> the faster way to do things, at ~3x the speed of list slicing for small strings and ~30x for long ones, is</p>\n\n<pre><code>res = bytearray(len(u) * 2)\nres[::2] = u\nres[1::2] = l\nstr(res)\n</code></pre>\n\n<p>This wouldn't work on Python 3, though. You could implement something like</p>\n\n<pre><code>res = bytearray(len(u) * 2)\nres[::2] = u.encode(\"ascii\")\nres[1::2] = l.encode(\"ascii\")\nres.decode(\"ascii\")\n</code></pre>\n\n<p>but by then you've already lost the gains over list slicing for small strings (it's still 20x the speed for long strings) and this doesn't even work for non-ASCII characters yet.</p>\n\n<p>FWIW, if you <em>are</em> doing this on massive strings and need every cycle, <em>and</em> for some reason have to use Python strings... here's how to do it:</p>\n\n<pre><code>res = bytearray(len(u) * 4 * 2)\n\nu_utf32 = u.encode(\"utf_32_be\")\nres[0::8] = u_utf32[0::4]\nres[1::8] = u_utf32[1::4]\nres[2::8] = u_utf32[2::4]\nres[3::8] = u_utf32[3::4]\n\nl_utf32 = l.encode(\"utf_32_be\")\nres[4::8] = l_utf32[0::4]\nres[5::8] = l_utf32[1::4]\nres[6::8] = l_utf32[2::4]\nres[7::8] = l_utf32[3::4]\n\nres.decode(\"utf_32_be\")\n</code></pre>\n\n<p>Special-casing the common case of smaller types will help too. FWIW, this is only 3x the speed of list slicing for long strings and a factor of 4 to 5 <em>slower</em> for small strings.</p>\n\n<p>Either way I prefer the <code>join</code> solutions, but since timings were mentioned elsewhere I thought I might as well join in.</p>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34761165, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>You could also do this using <a href=\"https://docs.python.org/3/library/functions.html#map\" rel=\"nofollow\"><code>map</code></a> and <a href=\"https://docs.python.org/3/library/operator.html#operator.add\" rel=\"nofollow\"><code>operator.add</code></a>:</p>\n\n<pre><code>from operator import add\n\nu = 'AAAAA'\nl = 'aaaaa'\n\ns = \"\".join(map(add, u, l))\n</code></pre>\n\n<p><strong>Output</strong>:</p>\n\n<pre><code>'AaAaAaAaAa'\n</code></pre>\n\n<p>What map does is it takes every element from the first iterable <code>u</code> and the first elements from the second iterable <code>l</code> and applies the function supplied as the first argument <code>add</code>. Then join just joins them.</p>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 34784700, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>Potentially faster and shorter than the current leading solution:</p>\n\n<pre><code>from itertools import chain\n\nu = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nl = 'abcdefghijklmnopqrstuvwxyz'\n\nres = \"\".join(chain(*zip(u, l)))\n</code></pre>\n\n<p>Strategy speed-wise is to do as much at the C-level as possible.  Same zip_longest() fix for uneven strings and it would be coming out of the same module as chain() so can't ding me too many points there!</p>\n\n<p>Other solutions I came up with along the way:</p>\n\n<pre><code>res = \"\".join(u[x] + l[x] for x in range(len(u)))\n\nres = \"\".join(k + l[i] for i, k in enumerate(u))\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 35146714, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>Jim's answer is great, but here's my favorite option, if you don't mind a couple of imports:</p>\n\n<pre><code>from functools import reduce\nfrom operator import add\n\nreduce(add, map(add, u, l))\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 35147811, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>I would use zip() to get a readable and easy way:</p>\n\n<pre><code>result = ''\nfor cha, chb in zip(u, l):\n    result += '%s%s' % (cha, chb)\n\nprint result\n# 'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 35180531, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}, {"body": "<p>Feels a bit un-pythonic not to consider the double-list-comprehension answer here, to handle n string with O(1) effort:</p>\n\n<pre><code>\"\".join(c for cs in itertools.zip_longest(*all_strings) for c in cs)\n</code></pre>\n\n<p>where <code>all_strings</code> is a list of the strings you want to interleave. In your case, <code>all_strings = [u, l]</code>. A full use example would look like this:</p>\n\n<pre><code>import itertools\na = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nb = 'abcdefghijklmnopqrstuvwxyz'\nall_strings = [a,b]\ninterleaved = \"\".join(c for cs in itertools.zip_longest(*all_strings) for c in cs)\nprint(interleaved)\n# 'AaBbCcDdEeFfGgHhIiJjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'\n</code></pre>\n\n<p>Like many answers, fastest? Probably not, but simple and flexible. Also, without too much added complexity, this is slightly faster than the accepted answer (in general, string addition is a bit slow in python):</p>\n\n<pre><code>In [7]: l1 = 'A' * 1000000; l2 = 'a' * 1000000;\n\nIn [8]: %timeit \"\".join(a + b for i, j in zip(l1, l2))\n1 loops, best of 3: 227 ms per loop\n\nIn [9]: %timeit \"\".join(c for cs in zip(*(l1, l2)) for c in cs)\n1 loops, best of 3: 198 ms per loop\n</code></pre>\n", "title": "Most pythonic way to interleave two strings", "answer_id": 35245385, "question_id": 34756145, "tags": ["python", "string", "python-2.7", "python-3.x"]}], "question_id": 34756145}, {"body": "<p>We use <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Statements/return\">return</a> statements optionally in JavaScript functions. It's a keyword. But what is the actual type of <code>return</code> itself. Actually I got confused, seeing the example:</p>\n\n<pre><code>function add(a, b) {\n  return (\n    console.log(a + b),\n    console.log(arguments)\n  );\n}\n\nadd(2, 2);\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>4\n[2, 2]\n</code></pre>\n\n<p>So, we can pass comma separated expressions into the <code>return</code> statement. Is this a function?</p>\n\n<p>And starting with this, can we take a wild guess that every keyword in JavaScript are ultimately a function?</p>\n\n<p>I've written a small blog as a gist of this discussion. You may want to check it <a href=\"http://coderuse.com/2016/04/JavaScript-What-is-the-type-of-return/\">here</a>.</p>\n", "title": "What type is the &#39;return&#39; keyword?", "tags": ["javascript", "return", "keyword"], "answer_count": 6, "link": "http://stackoverflow.com/questions/36690704/what-type-is-the-return-keyword", "answers": [{"body": "<blockquote>\n  <p>But what is the actual type of 'return' itself.</p>\n</blockquote>\n\n<p>It doesn't have a type, it isn't a value.</p>\n\n<p>Attempting <code>typeof return;</code> will give you <code>Unexpected token return</code>.</p>\n\n<blockquote>\n  <p>So, we can pass comma separated expressions into the return statement. Is this a function?</p>\n</blockquote>\n\n<p>No, while parenthesis <em>can</em> be used to call a function, here they are a <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Grouping\">grouping operator</a> containing a couple of expressions seperated by a <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Operators/Comma_Operator\">comma operator</a>.</p>\n\n<p>A more useful demonstration would be:</p>\n\n<pre><code>function add(a, b) {\n  return (\n    (a + b),\n    (a - b)\n  );\n}\n\nconsole.log(add(2, 2));\n</code></pre>\n\n<p>Which outputs <code>0</code> because the result of <code>a + b</code> is ignored (it is on the LHS of the comma operator) and <code>a - b</code> is returned.</p>\n", "title": "What type is the &#39;return&#39; keyword?", "answer_id": 36690764, "question_id": 36690704, "tags": ["javascript", "return", "keyword"]}, {"body": "<p>Testing what happens when you return parenthesiesed values:</p>\n\n<pre><code>function foo() {\n    return (1, 2);\n}\n\nconsole.log(foo());\n</code></pre>\n\n<p>Gives the answer <code>2</code>, so it appears that a comma separated list of values evaluates to the last element in the list.</p>\n\n<p>Really, the parenthesis are irrelevant here, they're grouping operations instead of signifying a function call. What's possibly surprising, though, is that the comma is legal here. I found an interesting blog post on how the comma is deal with here:</p>\n\n<p><a href=\"https://javascriptweblog.wordpress.com/2011/04/04/the-javascript-comma-operator/\">https://javascriptweblog.wordpress.com/2011/04/04/the-javascript-comma-operator/</a></p>\n", "title": "What type is the &#39;return&#39; keyword?", "answer_id": 36690896, "question_id": 36690704, "tags": ["javascript", "return", "keyword"]}, {"body": "<p><code>return</code> is not a function. It's the <em>continuation</em> of the function in which it occurs. </p>\n\n<p>Think about the statement <code>alert (2 * foo(bar));</code> where <code>foo</code> is the name of a function. When you're evaluating it, you see that you have to set aside the rest of the statement for a moment to concentrate on evaluating <code>foo(bar)</code>. You could visualize the part you set aside as something like <code>alert (2 * _)</code>, with a blank to fill in. When you know what the value of <code>foo(bar)</code> is, you pick it up again. </p>\n\n<p>The thing you set aside was the <em>continuation</em> of the call <code>foo(bar)</code>. </p>\n\n<p>Calling <code>return</code> feeds a value to that continuation. </p>\n\n<p>When you evaluate a <em>function</em> inside <code>foo</code>, the rest of <code>foo</code> waits for that function to reduce to a value, and then <code>foo</code> picks up again. You still have a goal to evaluate <code>foo(bar)</code>, it's just paused. </p>\n\n<p>When you evaluate <code>return</code> inside <code>foo</code>, no part of <code>foo</code> waits around for a value. <code>return</code> doesn't reduce to a value at the place inside <code>foo</code> where you used it. Instead, it causes the entire <em>call</em> <code>foo(bar)</code> to reduce to a value, and the goal \"evaluate <code>foo(bar)</code>\" is deemed complete and blown away.</p>\n\n<p>People don't usually tell you about continuations when you're new to programming. They think of it as an advanced topic, just because there <em>are</em> some very advanced things that people eventually <em>do</em> with continuations. But the truth is, you're using them all along, every time you call a function. </p>\n", "title": "What type is the &#39;return&#39; keyword?", "answer_id": 36698950, "question_id": 36690704, "tags": ["javascript", "return", "keyword"]}, {"body": "<p>the <code>return</code> here is a red herring. Perhaps interesting ist the following variation:</p>\n\n<pre><code>function add(a, b) {\n  return (\n    console.log(a + b),\n    console.log(arguments)\n  );\n}\n\nconsole.log(add(2, 2));\n</code></pre>\n\n<p>which outputs as the last line</p>\n\n<pre><code>undefined\n</code></pre>\n\n<p>as the function does not actually return anything. (It would return the return value of the second <code>console.log</code>, if it had one).</p>\n\n<p>As it is, the code is exactly identical to</p>\n\n<pre><code>function add(a, b) {\n    console.log(a + b);\n    console.log(arguments);\n}\n\nconsole.log(add(2, 2));\n</code></pre>\n", "title": "What type is the &#39;return&#39; keyword?", "answer_id": 36716265, "question_id": 36690704, "tags": ["javascript", "return", "keyword"]}, {"body": "<p>I'm kinda shocked that no one here has directly referenced <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-12.9\">the spec</a>:</p>\n\n<blockquote>\n  <p>12.9 The return Statement Syntax ReturnStatement : return ; return [no LineTerminator here] Expression ; </p>\n  \n  <p><strong>Semantics</strong></p>\n  \n  <p>An ECMAScript program is considered syntactically incorrect if it\n  contains a return statement that is not within a FunctionBody. A\n  return statement causes a function to cease execution and return a\n  value to the caller. If Expression is omitted, the return value is\n  undefined. Otherwise, the return value is the value of Expression.</p>\n  \n  <p>A ReturnStatement is evaluated as follows:</p>\n  \n  <p>If the Expression is not present, return <code>(return, undefined, empty)</code>.\n  Let <code>exprRef</code> be the result of evaluating Expression.\n  Return <code>(return, GetValue(exprRef), empty)</code>.</p>\n</blockquote>\n\n<p>So, because of the spec, your example reads:</p>\n\n<p><code>return ( GetValue(exprRef) )</code></p>\n\n<p>where\n<code>exprRef = console.log(a + b), console.log(arguments)</code></p>\n\n<p>Which according to the <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-11.14\">spec on the comma operator</a>...</p>\n\n<blockquote>\n  <p><strong>Semantics</strong></p>\n  \n  <p>The production Expression : Expression , AssignmentExpression is\n  evaluated as follows:</p>\n\n<pre><code>Let lref be the result of evaluating Expression.\nCall GetValue(lref).\nLet rref be the result of evaluating AssignmentExpression.\nReturn GetValue(rref).\n</code></pre>\n</blockquote>\n\n<p>...means that every expression will get evaluated until the last item in the comma list, which becomes the assignment expression.  So your code <code>return (console.log(a + b) , console.log(arguments))</code> is going to</p>\n\n<p>1.)  print the result of <code>a + b</code></p>\n\n<p>2.)  Nothing is left to execute, so execute the next expression which</p>\n\n<p>3.)  prints the <code>arguments</code>, and because <code>console.log()</code> doesn't specify a \nreturn statement </p>\n\n<p>4.)  Evaluates to undefined</p>\n\n<p>5.)  Which is then returned to the caller.  </p>\n\n<p>So the correct answer is, <code>return</code> doesn't have a type, it only returns the result of some expression.  </p>\n\n<p>For the next question:  </p>\n\n<blockquote>\n  <p>So, we can pass comma separated expressions into the return statement.\n  Is this a function?</p>\n</blockquote>\n\n<p>No.  The comma in JavaScript is an operator, defined to allow you to combine multiple expressions into a single line, and is defined by the spec to return the evaluated expression of whatever is last in your list.  </p>\n\n<p>You still don't believe me?  </p>\n\n<pre><code>&lt;script&gt;\nalert(foo());\nfunction foo(){\n    var foo = undefined + undefined;\n    console.log(foo);\n    return undefined, console.log(1), 4;\n}\n&lt;/script&gt;\n</code></pre>\n\n<p>Play with that code <a href=\"https://jsfiddle.net/LLpg025y/\">here</a> and mess with the last value in the list.  It will always return the last value in the list, in your case it just happens to be <code>undefined.</code></p>\n\n<p>For your final question, </p>\n\n<blockquote>\n  <p>And starting with this, can we take a wild guess that every keyword in\n  JavaScript are ultimately a function?</p>\n</blockquote>\n\n<p>Again, no.  <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-13\">Functions have a very specific definition</a> in the language.  I won't reprint it here because this answer is already getting extremely long.</p>\n", "title": "What type is the &#39;return&#39; keyword?", "answer_id": 36720428, "question_id": 36690704, "tags": ["javascript", "return", "keyword"]}, {"body": "<p>An interesting way to understand the <a href=\"https://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Statements/return\" rel=\"nofollow\">return</a> statement is through the <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/void\" rel=\"nofollow\">void</a> operator\nTake a look at this code</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>var console = {\r\n    log: function(s) {\r\n      document.getElementById(\"console\").innerHTML += s + \"&lt;br/&gt;\"\r\n    }\r\n}\r\n\r\nfunction funReturnUndefined(x,y) {\r\n   return ( void(x+y) );\r\n}\r\n\r\nfunction funReturnResult(x,y) {\r\n   return ( (x+y) );\r\n}\r\n\r\nconsole.log( funReturnUndefined(2,3) );\r\nconsole.log( funReturnResult(2,3) );</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;div id=\"console\" /&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>Since the <code>return</code> statement takes one argument that is <code>[[expression]]</code> and return this to the caller on the stack i.e. <code>arguments.callee.caller</code> it will then execute <code>void(expression)</code> and then return <code>undefined</code> that is the evaluation of the void operator.</p>\n", "title": "What type is the &#39;return&#39; keyword?", "answer_id": 37125225, "question_id": 36690704, "tags": ["javascript", "return", "keyword"]}], "question_id": 36690704}, {"body": "<p>The output of this program:</p>\n\n<pre><code>#include &lt;iostream&gt; \nclass c1\n{   \n  public:\n    c1&amp; meth1(int* ar) {\n      std::cout &lt;&lt; \"method 1\" &lt;&lt; std::endl;\n      *ar = 1;\n      return *this;\n    }\n    void meth2(int ar)\n    {\n      std::cout &lt;&lt; \"method 2:\"&lt;&lt; ar &lt;&lt; std::endl;\n    }\n};\n\nint main()\n{\n  c1 c;\n  int nu = 0;\n  c.meth1(&amp;nu).meth2(nu);\n}\n</code></pre>\n\n<p>Is:</p>\n\n<pre><code>method 1\nmethod 2:0\n</code></pre>\n\n<p>Why is <code>nu</code> not 1 when <code>meth2()</code> starts?</p>\n", "title": "C++ execution order in method chaining", "tags": ["c++", "chaining", "order-of-evaluation"], "answer_count": 4, "link": "http://stackoverflow.com/questions/37252328/c-execution-order-in-method-chaining", "answers": [{"body": "<p>Because evaluation order is unspecified.</p>\n\n<p>You are seeing <code>nu</code> in <code>main</code> being evaluated to <code>0</code> before even <code>meth1</code> is called. This is the problem with chaining. I advise not doing it.</p>\n\n<p>Just make a nice, simple, clear, easy-to-read, easy-to-understand program:</p>\n\n<pre><code>int main()\n{\n  c1 c;\n  int nu = 0;\n  c.meth1(&amp;nu);\n  c.meth2(nu);\n}\n</code></pre>\n", "title": "C++ execution order in method chaining", "answer_id": 37252368, "question_id": 37252328, "tags": ["c++", "chaining", "order-of-evaluation"]}, {"body": "<p>I think this part of the <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2014/n4296.pdf\" rel=\"nofollow\">draft standard</a> regarding order of evaluation is relevant:</p>\n\n<blockquote>\n  <p><strong>1.9 Program Execution</strong></p>\n  \n  <p>...</p>\n  \n  <ol start=\"15\">\n  <li>Except where noted, evaluations of operands of individual operators and of subexpressions of individual\n  expressions are unsequenced. The value computations of the operands of an\n  operator are sequenced before the value computation of the result of the operator. <strong>If a side effect on a scalar\n  object is unsequenced relative to either another side effect on the same scalar object or a value computation\n  using the value of the same scalar object, and they are not potentially concurrent, the behavior is\n  undefined</strong></li>\n  </ol>\n</blockquote>\n\n<p>and also:</p>\n\n<blockquote>\n  <p><strong>5.2.2 Function call</strong></p>\n  \n  <p>...</p>\n  \n  <ol start=\"8\">\n  <li>[ Note: <strong>The evaluations of the postfix expression and of the arguments are all unsequenced relative to one\n  another.</strong> All side effects of argument evaluations are sequenced before the function is entered \u2014 end note ]</li>\n  </ol>\n</blockquote>\n\n<p>So for your line <code>c.meth1(&amp;nu).meth2(nu);</code>, consider what is happening in operator in terms of the function call operator for the final call to <code>meth2</code>, so we clearly see the breakdown into the postfix expression and argument <code>nu</code>:</p>\n\n<pre><code>operator()(c.meth1(&amp;nu).meth2, nu);\n</code></pre>\n\n<p>The <em>evaluations of the postfix expression and argument</em> for the final function call (i.e. the postfix expression <code>c.meth1(&amp;nu).meth2</code> and <code>nu</code>) are <em>unsequenced relative to one another</em> as per the <strong>function call</strong> rule above. Therefore, the <em>side-effect</em> of the computation of the postfix expression on the scalar object <code>ar</code> is unsequenced relative to the argument evaluation of <code>nu</code> prior to the <code>meth2</code> function call. By the <strong>program execution</strong> rule above, this is undefined behaviour.</p>\n\n<p>In other words, there is no requirement for the compiler to evaluate the <code>nu</code> argument to the <code>meth2</code> call after the <code>meth1</code> call - it is free to assume no side-effects of <code>meth1</code> affect the <code>nu</code> evaluation.</p>\n\n<p>The assembly code produced by the above contains the following sequence in the <code>main</code> function:</p>\n\n<ol>\n<li>Variable <code>nu</code> is allocated on the stack and initialised with 0.</li>\n<li>A register (<code>ebx</code> in my case) receives a copy of the value of <code>nu</code></li>\n<li>The addresses of <code>nu</code> and <code>c</code> are loaded into parameter registers</li>\n<li><code>meth1</code> is called</li>\n<li>The return value register and the <em>previously cached value</em> of <code>nu</code> in the <code>ebx</code> register are loaded into parameter registers</li>\n<li><code>meth2</code> is called</li>\n</ol>\n\n<p>Critically, in step 5 above the compiler allows the cached value of <code>nu</code> from step 2 to be re-used in the function call to <code>meth2</code>. Here it disregards the possibility that <code>nu</code> may have been changed by the call to <code>meth1</code> - 'undefined behaviour' in action.</p>\n\n<p><strong>NOTE:</strong> This answer has changed in substance from its original form. My initial explanation in terms of side-effects of operand computation not being sequenced before the final function call were incorrect, because they are. The problem is the fact that computation of the operands themselves is indeterminately sequenced.</p>\n", "title": "C++ execution order in method chaining", "answer_id": 37252637, "question_id": 37252328, "tags": ["c++", "chaining", "order-of-evaluation"]}, {"body": "<p>I think when compiling ,before the funtions meth1 and meth2 are really called, the paramaters have been passed to them. I mean when you  use \"c.meth1(&amp;nu).meth2(nu);\" the value nu = 0 have been passed to meth2, so it doesn't matter wether \"nu\" is changed latter.</p>\n\n<p>you can try this: </p>\n\n<pre><code>#include &lt;iostream&gt; \nclass c1\n{\npublic:\n    c1&amp; meth1(int* ar) {\n        std::cout &lt;&lt; \"method 1\" &lt;&lt; std::endl;\n        *ar = 1;\n        return *this;\n    }\n    void meth2(int* ar)\n    {\n        std::cout &lt;&lt; \"method 2:\" &lt;&lt; *ar &lt;&lt; std::endl;\n    }\n};\n\nint main()\n{\n    c1 c;\n    int nu = 0;\n    c.meth1(&amp;nu).meth2(&amp;nu);\n    getchar();\n}\n</code></pre>\n\n<p>it will get the answer you want</p>\n", "title": "C++ execution order in method chaining", "answer_id": 37252707, "question_id": 37252328, "tags": ["c++", "chaining", "order-of-evaluation"]}, {"body": "<p>In the 1998 C++ standard, Section 5, para 4</p>\n\n<blockquote>\n  <p>Except where noted, the order of evaluation of operands of individual operators and subexpressions of individual expressions, and the order in which side effects take place, is unspecified. Between the previous\n  and next sequence point a scalar object shall have its stored value modified at most once by the evaluation of an expression. Furthermore, the prior value shall be accessed only to determine the value to be stored. The requirements of this paragraph shall be met for each allowable ordering of the subexpressions of a full expression; otherwise the behavior is undefined.</p>\n</blockquote>\n\n<p>(I've omitted a reference to footnote #53 which is not relevant to this question).</p>\n\n<p>Essentially, <code>&amp;nu</code> must be evaluated before calling <code>c1::meth1()</code>, and <code>nu</code> must be evaluated before calling <code>c1::meth2()</code>.  There is, however, no requirement that <code>nu</code> be evaluated before <code>&amp;nu</code> (e.g. it is permitted that <code>nu</code> be evaluated first, then <code>&amp;nu</code>, and then <code>c1::meth1()</code> is called - which might be what your compiler is doing).    The expression <code>*ar = 1</code> in <code>c1::meth1()</code> is therefore not guaranteed to be evaluated before <code>nu</code> in <code>main()</code> is evaluated, in order to be passed to <code>c1::meth2()</code>.</p>\n\n<p>Later C++ standards (which I don't currently have on the PC I'm using tonight) have essentially the same clause.</p>\n", "title": "C++ execution order in method chaining", "answer_id": 37253102, "question_id": 37252328, "tags": ["c++", "chaining", "order-of-evaluation"]}], "question_id": 37252328}, {"body": "<p>Almost every C++ resource I've seen that discusses this kind of thing tells me that I should prefer polymorphic approaches to using RTTI (run-time type identification). In general, I take this kind of advice seriously, and will try and understand the rationale -- after all, C++ is a mighty beast and hard to understand in its full depth. However, for this particular question, I'm drawing a blank and would like to see what kind of advice the internet can offer. First, let me summarize what I've learned so far, by listing the common reasons that are quoted why RTTI is \"considered harmful\":</p>\n\n<h2>Some compilers don't use it / RTTI is not always enabled</h2>\n\n<p>I really don't buy this argument. It's like saying I shouldn't use C++14 features, because there are compilers out there that don't support it. And yet, no one would discourage me from using C++14 features. The majority of projects will have influence over the compiler they're using, and how it's configured. Even quoting the gcc manpage:</p>\n\n<blockquote>\n  <p><code>-fno-rtti</code></p>\n  \n  <p>Disable generation of information about every class with virtual functions for use by the C++ run-time type identification features\n  (dynamic_cast and typeid).  If you don't use those parts of the language, you can save some space by using this flag.  Note that exception\n  handling uses the same information, but G++ generates it as needed. The dynamic_cast operator can still be used for casts that do not require\n  run-time type information, i.e. casts to \"void *\" or to unambiguous base classes.</p>\n</blockquote>\n\n<p>What this tells me is that <em>if</em> I'm not using RTTI, I can disable it. That's like saying, if you're not using Boost, you don't have to link to it. I don't have to plan for the case where someone is compiling with <code>-fno-rtti</code>. Plus, the compiler will fail loud and clear in this case.</p>\n\n<h2>It costs extra memory / Can be slow</h2>\n\n<p>Whenever I'm tempted to use RTTI, that means I need to access some kind of type information or trait of my class. If I implement a solution that does not use RTTI, this usually means I will have to add some fields to my classes to store this information, so the memory argument is kind of void (I'll give an example of this further down).</p>\n\n<p>A dynamic_cast can be slow, indeed. There's usually ways to avoid having to use it speed-critical situations, though. And I don't quite see the alternative. <a href=\"http://stackoverflow.com/questions/579887/how-expensive-is-rtti\">This SO answer</a> suggests using an enum, defined in the base class, to store the type. That only works if you know all your derived classes a-priori. That's quite a big \"if\"!</p>\n\n<p>From that answer, it seems also that the cost of RTTI is not clear, either. Different people measure different stuff.</p>\n\n<h2>Elegant polymorphic designs will make RTTI unnecessary</h2>\n\n<p>This is the kind of advice I take seriously. In this case, I simply can't come up with good non-RTTI solutions that cover my RTTI use case. Let me provide an example:</p>\n\n<p>Say I'm writing a library to handle graphs of some kind of objects. I want to allow users to generate their own types when using my library (so the enum method is not available). I have a base class for my node:</p>\n\n<pre><code>class node_base\n{\n  public:\n    node_base();\n    virtual ~node_base();\n\n    std::vector&lt; std::shared_ptr&lt;node_base&gt; &gt; get_adjacent_nodes();\n};\n</code></pre>\n\n<p>Now, my nodes can be of different types. How about these:</p>\n\n<pre><code>class red_node : virtual public node_base\n{\n  public:\n    red_node();\n    virtual ~red_node();\n\n    void get_redness();\n};\n\nclass yellow_node : virtual public node_base\n{\n  public:\n    yellow_node();\n    virtual ~yellow_node();\n\n    void set_yellowness(int);\n};\n</code></pre>\n\n<p>Hell, why not even one of these:</p>\n\n<pre><code>class orange_node : public red_node, public yellow_node\n{\n  public:\n    orange_node();\n    virtual ~orange_node();\n\n    void poke();\n    void poke_adjacent_oranges();\n};\n</code></pre>\n\n<p>The last function is interesting. Here's a way to write it:</p>\n\n<pre><code>void orange_node::poke_adjacent_oranges()\n{\n    auto adj_nodes = get_adjacent_nodes();\n    foreach(auto node, adj_nodes) {\n        // In this case, typeid() and static_cast might be faster\n        std::shared_ptr&lt;orange_node&gt; o_node = dynamic_cast&lt;orange_node&gt;(node);\n        if (o_node) {\n             o_node-&gt;poke();\n        }\n    }\n}\n</code></pre>\n\n<p>This all seems clear and clean. I don't have to define attributes or methods where I don't need them, the base node class can stay lean and mean. Without RTTI, where do I start? Maybe I can add a node_type attribute to the base class:</p>\n\n<pre><code>class node_base\n{\n  public:\n    node_base();\n    virtual ~node_base();\n\n    std::vector&lt; std::shared_ptr&lt;node_base&gt; &gt; get_adjacent_nodes();\n\n  private:\n    std::string my_type;\n};\n</code></pre>\n\n<p>Is std::string a good idea for a type? Maybe not, but what else can I use? Make up a number and hope no one else is using it yet? Also, in the case of my orange_node, what if I want to use the methods from red_node and yellow_node? Would I have to store multiple types per node? That seems complicated.</p>\n\n<h2>Conclusion</h2>\n\n<p>This examples doesn't seem overly complex or unusual (I'm working on something similar in my day job, where the nodes represent actual hardware that gets controlled through the software, and which do very different thing depending on what they are). Yet I wouldn't know a clean way of doing this with templates or other methods.\nPlease note that I'm trying to understand the problem, not defend my example. My reading of pages such as the SO answer I linked above and <a href=\"https://en.wikibooks.org/wiki/C++_Programming/RTTI#Misuses_of_RTTI\">this page on Wikibooks</a> seem to suggest I'm misusing RTTI, but I would like to learn why.</p>\n\n<p>So, back to my original question: <strong>Why is 'pure polymorphism' preferable over using RTTI?</strong></p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "tags": ["c++", "polymorphism", "rtti"], "answer_count": 7, "link": "http://stackoverflow.com/questions/35764905/why-is-pure-polymorphism-preferable-over-using-rtti", "answers": [{"body": "<p>If you call a function, as a rule you don't really care what precise steps it will take, only that some higher-level goal will be achieved within certain constraints (and how the function makes that happen is really it's own problem). </p>\n\n<p>When you use RTTI to make a preselection of special objects that can do a certain job, while others in the same set cannot, you are breaking that comfortable view of the world. All of a sudden the caller is supposed to know who can do what, instead of simply telling his minions to get on with it. Some people are bothered by this, and I suspect this is a large part of the reason why RTTI is considered a little dirty.</p>\n\n<p>Is there a performance issue? Maybe, but I've never experienced it, and it might be wisdom from twenty years ago, or from people who honestly believe that using three assembly instructions instead of two is unacceptable bloat. </p>\n\n<p>So how to deal with it... Depending on your situation it might make sense to have any node-specific properties bundled into separate objects (i.e. the entire 'orange' API could be a separate object). The root object could then have a virtual function to return the 'orange' API, returning nullptr by default for non-orange objects. </p>\n\n<p>While this might be overkill depending on your situation, it would allow you to query on root level whether a specific node supports a specific API, and if it does, execute functions specific to that API. </p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35765577, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}, {"body": "<p>It looks kind of neat in a small example, but in real life you will soon end up with a long set of types that can poke each other, some of them perhaps only in one direction. </p>\n\n<p>What about <code>dark_orange_node</code>, or <code>black_and_orange_striped_node</code>, or <code>dotted_node</code>? Can it have dots of different colors? What if most dots are orange, can it be poked then?</p>\n\n<p>And each time you have to add a new rule, you will have to revisit all the <code>poke_adjacent</code> functions and add more if-statements.</p>\n\n<hr>\n\n<p>As always, it is hard to create generic examples, I'll give you that. </p>\n\n<p>But if I were to do this <em>specific</em> example, I would add a <code>poke()</code> member to all the classes and let some of them ignore the call (<code>void poke() {}</code>) if they are not interested.</p>\n\n<p>Surely that would be even less expensive than comparing the <code>typeid</code>s.</p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35766892, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}, {"body": "<p>The most of the <em>moral suasion</em> against this or that feature are typicality originated from the observation that there are a umber of <em>misconceived</em> uses of that feature.</p>\n\n<p>Where <em>moralists</em> fail is that they presume ALL the usages are misconceived, while <em>in fact</em> features exist for a reason. </p>\n\n<p>They have what I used to call the \"plumber complex\": they think all <em>taps</em> are <em>malfunctioning</em> because all the taps they are called to repair are. The reality is that most taps work well: you simply don't call a plumber for them!</p>\n\n<p>A crazy thing that can happen is when, to avoid using a given feature, programmers write a lot of boilerplate code actually privately re-implementing exactly that feature. (Have you ever met classes that don't use RTTI nor virtual calls, but have a value to track which actual derived type are they? That's no more than <em>RTTI</em> reinvention in disguise.)</p>\n\n<p>There is a general way to think about polymorphism: <code>IF(selection) CALL(something) WITH(parameters)</code>. (Sorry, but programming, when disregarding abstraction, is all about that)</p>\n\n<p>The use of design-time (concepts) compile-time (template-deduction based), run-time (inheritance and virtual function-based) or data-driven (RTTI and switching) polymorphism, depends on how much of the decisions are known at each of the stages of the <em>production</em> and how <em>variable</em> they are at every context.</p>\n\n<p>The idea is that: </p>\n\n<p><strong>the more you can anticipate, the better the chance of catching errors and avoid bugs affecting the end-user.</strong></p>\n\n<p>If everything is constant (including the data) you can do everything with template meta-programming. After compilation occurred on actualized constants, the entire <em>program</em> boils down to just a return statement that spits out the <em>result</em>.</p>\n\n<p>If there are a number of cases that are all known <em>at compile time</em>, but you don't know about the actual data they have to act on, then compile-time polymorphism (mainly CRTP or similar) can be a solution.</p>\n\n<p>If the selection of the cases depends on the data (not compile-time known values) and the switching is mono-dimensional (what to do can be reduced to one value only) then virtual function based dispatch (or in general \"function pointer tables\") is needed.</p>\n\n<p>If the switching is multidimensional, since no native <em>multiple runtime dispatch</em> exist in C++, then you have to either:</p>\n\n<ul>\n<li>Reduce to one dimension by <em>Goedelization</em>: that's where virtual bases and multiple inheritance, with <em>diamonds</em> and <em>stacked parallelograms</em> are, but this requires the number of possible combination to be known and to be relatively small.</li>\n<li>Chain the dimensions one into the other (like in the composite-visitors pattern, but this requires all classes to be aware of their other siblings, thus it cannot \"scale\" out from the place it has been conceived)</li>\n<li>Dispatch calls based on multiple values. <strong>That's exactly what RTTI is for.</strong></li>\n</ul>\n\n<p>If not just the switching, but even the actions are not compile time known, then <em>scripting &amp; parsing</em> is required: the data themselves must describe the action to be taken on them.</p>\n\n<p>Now, since each of the cases I enumerated can be seen as a particular case of what follows it, you can solve every problem by abusing the bottom-most solution also for problems affordable with the top-most.</p>\n\n<p>That's what <em>moralization</em> actually pushes to avoid.\nBut that does not means that problems living in the bottom-most domains don't exist!</p>\n\n<p>Bashing RTTI just to bash it, is like bashing <code>goto</code> just to bash it. Things for parrots, not programmers.</p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35767352, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}, {"body": "<p>An interface describes what one needs to know in order to interact in a given situation in code.  Once you extend the interface with \"your entire type hierarchy\", your interface \"surface area\" becomes huge, which makes reasoning about it <em>harder</em>.</p>\n\n<p>As an example, your \"poke adjacent oranges\" means that me, as a 3rd party, cannot emulate being an orange!  You privately declared an orange type, then use RTTI to make your code behave special when interacting with that type.  If I want to \"be orange\", I <em>must</em> be within your private garden.</p>\n\n<p>Now everyone who couples with \"orangeness\" couples with your entire orange type, and implicitly with your entire private garden, instead of with a defined interface.</p>\n\n<p>While at first glance this looks like a great way to extend the limited interface without having to change all clients (adding <code>am_I_orange</code>), what tends to happen instead is it ossified the code base, and <em>prevents</em> further extension.  The special orange-ness becomes inherent to the functioning of the system, and prevents you from creating a tangerine replacement for orange that is implemented differently and maybe removes a dependency or solves some other problem elegantly.</p>\n\n<p>This does mean your interface has to be sufficient to solve your problem.  From that perspective, why do you need to only poke oranges, and if so why was orangeness unavailable in the interface?  If you need some fuzzy set of tags that can be added ad-hoc, you could add that to your type:</p>\n\n<pre><code>class node_base {\n  public:\n    bool has_tag(tag_name);\n</code></pre>\n\n<p>This provides a similar massive broadening of your interface from narrowly specified to broad tag-based.  Except instead of doing it through RTTI and implementation details (aka, \"how are you implemented?  With the orange type?  Ok you pass.\"), it does so with something easily emulated through a completely different implementation.</p>\n\n<p>This can even be extended to dynamic methods, <em>if</em> you need that.  \"Do you support being Foo'd with arguments Baz, Tom and Alice?  Ok, Fooing you.\"  In a big sense, this is <em>less</em> intrusive than a dynamic cast to get at the fact the other object is a type you know.</p>\n\n<p>Now tangerine objects can have the orange tag and play along, while being implementation-decoupled.</p>\n\n<p>It can still lead to a huge mess, but it is at least a mess of messages and data, not implementation hierarchies.</p>\n\n<p>Abstraction is a game of decoupling and hiding irrelevancies.  It makes code easier to reason about locally.  RTTI is boring a hole straight through the abstraction into implementation details.  This can make solving a problem easier, but it has the cost of locking you into one specific implementation really easily.</p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35767956, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}, {"body": "<p>C++ is built on the idea of static type checking.</p>\n\n<p><sup>[1]</sup>RTTI, that is, <code>dynamic_cast</code> and <code>type_id</code>, is dynamic type checking.</p>\n\n<p>So, essentially you're asking why static type checking is preferable to dynamic type checking. And the simple answer is, <strong>whether static type checking is preferable to dynamic type checking, depends</strong>. On a lot. But C++ is one of the programming languages that are designed around the idea of static type checking. And this means that e.g. the development process, in particular testing, is typically adapted to static type checking, and then fits that best.</p>\n\n<hr>\n\n<p>Re</p>\n\n<blockquote>\n  <p><strong>&rdquo;</strong>  I wouldn't know a clean way of doing this with templates or other methods</p>\n</blockquote>\n\n<p>you can do this process-heterogenous-nodes-of-a-graph with static type checking and no casting whatsoever via the visitor pattern, e.g. like this:</p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;set&gt;\n#include &lt;initializer_list&gt;\n\nnamespace graph {\n    using std::set;\n\n    class Red_thing;\n    class Yellow_thing;\n    class Orange_thing;\n\n    struct Callback\n    {\n        virtual void handle( Red_thing&amp; ) {}\n        virtual void handle( Yellow_thing&amp; ) {}\n        virtual void handle( Orange_thing&amp; ) {}\n    };\n\n    class Node\n    {\n    private:\n        set&lt;Node*&gt; connected_;\n\n    public:\n        virtual void call( Callback&amp; cb ) = 0;\n\n        void connect_to( Node* p_other )\n        {\n            connected_.insert( p_other );\n        }\n\n        void call_on_connected( Callback&amp; cb )\n        {\n            for( auto const p : connected_ ) { p-&gt;call( cb ); }\n        }\n\n        virtual ~Node(){}\n    };\n\n    class Red_thing\n        : public virtual Node\n    {\n    public:\n        void call( Callback&amp; cb ) override { cb.handle( *this ); }\n\n        auto redness() -&gt; int { return 255; }\n    };\n\n    class Yellow_thing\n        : public virtual Node\n    {\n    public:\n        void call( Callback&amp; cb ) override { cb.handle( *this ); }\n    };\n\n    class Orange_thing\n        : public Red_thing\n        , public Yellow_thing\n    {\n    public:\n        void call( Callback&amp; cb ) override { cb.handle( *this ); }\n\n        void poke() { std::cout &lt;&lt; \"Poked!\\n\"; }\n\n        void poke_connected_orange_things()\n        {\n            struct Poker: Callback\n            {\n                void handle( Orange_thing&amp; obj ) override\n                {\n                    obj.poke();\n                }\n            } poker;\n\n            call_on_connected( poker );\n        }\n    };\n}  // namespace graph\n\nauto main() -&gt; int\n{\n    using namespace graph;\n\n    Red_thing   r;\n    Yellow_thing    y1, y2;\n    Orange_thing    o1, o2, o3;\n\n    for( Node* p : std::initializer_list&lt;Node*&gt;{ &amp;y1, &amp;y2, &amp;r, &amp;o2, &amp;o3 } )\n    {\n        o1.connect_to( p );\n    }\n    o1.poke_connected_orange_things();\n}\n</code></pre>\n\n<p>This assumes that the set of node types is known.</p>\n\n<p>When it isn't, the visitor pattern (there are many variations of it) can be expressed with a few centralized casts, or, just a single one.</p>\n\n<hr>\n\n<p>For a template-based approach see the Boost Graph library. Sad to say I am not familiar with it, I haven't used it. So I'm not sure exactly what it does and how, and to what degree it uses static type checking instead of RTTI, but since Boost is generally template-based with static type checking as the central idea, I think you'll find that its Graph sub-library is also based on static type checking.</p>\n\n<hr>\n\n<p><sup>\n<sup>[1]</sup> <em>Run Time Type Information</em>.\n</sup></p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35771194, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}, {"body": "<p>Of course there is a scenario where polymorphism can't help: names. <code>typeid</code> lets you access the name of the type, although the way this name is encoded is implementation-defined. But usually this is not a problem since you can compare two <code>typeid</code>-s:</p>\n\n<pre><code>if ( typeid(5) == \"int\" )\n    // may be false\n\nif ( typeid(5) == typeid(int) )\n   // always true\n</code></pre>\n\n<p>The same holds for hashes.</p>\n\n<blockquote>\n  <p>[...] RTTI is \"considered harmful\"</p>\n</blockquote>\n\n<p><em>harmful</em> is definitely overstating: RTTI has some drawbacks, but it <em>does</em> have advantages too.</p>\n\n<p>You don't truly have to use RTTI. RTTI is a <strong>tool</strong> to solve OOP problems: should you use another paradigm, these would likely disappear. C doesn't have RTTI, but still works. C++ instead fully supports OOP and gives you <strong>multiple</strong> tools to overcome some issue that may require runtime information: one of them <em>is</em> indeed RTTI, which though comes with a price. If you can't afford it, thing you'd better state only after a secure performance analysis, there is still the old-school <code>void*</code>: it's free. Costless. But you get no type safety. So it's all about trades.</p>\n\n<hr>\n\n<blockquote>\n  <ul>\n  <li>Some compilers don't use / RTTI is not always enabled<br>\n  I really don't buy this argument. It's like saying I shouldn't use C++14 features,\n  because there are compilers out there that don't support it. And yet,\n  no one would discourage me from using C++14 features.</li>\n  </ul>\n</blockquote>\n\n<p>If you write (possibly strictly) conforming C++ code, you can expect the same behavior regardless of the implementation. Standard-compliant implementations shall support standard C++ features.</p>\n\n<p>But do consider that in some environments C++ defines (\u00abfreestanding\u00bb ones), RTTI need not be provided and neither do exceptions, <code>virtual</code> and so on. RTTI needs an underlying layer to work correctly that deals with low-level details such as the ABI and the actual type information. </p>\n\n<hr>\n\n<p>I agree  with Yakk regarding RTTI in this case. Yes, it could be used; but is it logically correct? The fact that the language allows you to bypass this check does not mean it should be done.  </p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35775159, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}, {"body": "<blockquote>\n  <p>Some compilers don't use it / RTTI is not always enabled</p>\n</blockquote>\n\n<p>I believe you have misunderstood such arguments.</p>\n\n<p>There are a number of C++ coding places where RTTI is not to be used. Where compiler switches are used to forcibly disable RTTI. If you are coding within such a paradigm... then you almost certainly have already been informed of this restriction.</p>\n\n<p>The problem therefore is with <em>libraries</em>. That is, if you're writing a library that depends on RTTI, then your library <em>cannot</em> be used by users who turn off RTTI. If you want your library to be used by those people, then it cannot use RTTI, even if your library also gets used by people who can use RTTI. Equally importantly, if you can't use RTTI, you have to shop around a little harder for libraries, since RTTI use is a deal-breaker for you.</p>\n\n<blockquote>\n  <p>It costs extra memory / Can be slow</p>\n</blockquote>\n\n<p>There are many things you don't do in hot loops. You don't allocate memory. You don't go iterating through linked lists. And so forth. RTTI certainly can be another one of those \"don't do this here\" things.</p>\n\n<p>However, consider all of your RTTI examples. In all cases, you have one or more objects of an indeterminate type, and you want to perform some operation on them which may not be possible for some of them.</p>\n\n<p>That's something you have to work around at a <em>design</em> level. You can write containers that don't allocate memory which fit into the \"STL\" paradigm. You can avoid linked list data structures, or limit their use. You can reorganize arrays of structs into structs of arrays or whatever. It changes some things, but you can keep it compartmentalized.</p>\n\n<p>Changing a complex RTTI operation into a regular virtual function call? That's a design issue. If you have to change that, then it's something that requires changes to <em>every</em> derived class. It changes how lots of code interacts with various classes. The scope of such a change extends far beyond the performance-critical sections of code.</p>\n\n<p>So... why did you write it the wrong way to begin with?</p>\n\n<blockquote>\n  <p>I don't have to define attributes or methods where I don't need them, the base node class can stay lean and mean.</p>\n</blockquote>\n\n<p>To what end?</p>\n\n<p>You say that the base class is \"lean and mean\". But really... it's <em>nonexistent</em>. It doesn't actually <em>do anything</em>.</p>\n\n<p>Just look at your example: <code>node_base</code>. What is it? It seems to be a thing which has adjacent other things. This is a Java interface (pre-generics Java at that): a class that exists solely to be something that users can cast to the <em>real</em> type. Maybe you add some basic feature like adjacency (Java adds <code>ToString</code>), but that's it.</p>\n\n<p>There's a difference between \"lean and mean\" and \"transparent\".</p>\n\n<p>As Yakk said, such programming styles limit themselves in interoperability, because if all of the functionality is in a derived class, then users outside of that system, with no access to that derived class, cannot interoperate with the system. They can't override virtual functions and add new behaviors. They can't even <em>call</em> those functions.</p>\n\n<p>But what they also do is make it a major pain to actually do new stuff, even within the system. Consider your <code>poke_adjacent_oranges</code> function. What happens if someone wants a <code>lime_node</code> type which can be poked just like <code>orange_node</code>s? Well, we can't derive <code>lime_node</code> from <code>orange_node</code>; that makes no sense.</p>\n\n<p>Instead, we have to add a new <code>lime_node</code> derived from <code>node_base</code>. Then change the name of <code>poke_adjacent_oranges</code> to <code>poke_adjacent_pokables</code>. And then, try casting to <code>orange_node</code> and <code>lime_node</code>; whichever cast works is the one we poke.</p>\n\n<p>However, <code>lime_node</code> needs it's <em>own</em> <code>poke_adjacent_pokables</code>. And this function needs to do the same casting checks.</p>\n\n<p>And if we add a third type, we have to not only add its own function, but we must change the functions in the other two classes.</p>\n\n<p>Obviously, now you make <code>poke_adjacent_pokables</code> a free function, so that it works for all of them. But what do you suppose happens if someone adds a fourth type and forgets to add it to that function?</p>\n\n<p>Hello, <em>silent breakage</em>. The program appears to work more or less OK, but it isn't. Had <code>poke</code> been an <em>actual</em> virtual function, the compiler would have failed when you didn't override the pure virtual function from <code>node_base</code>.</p>\n\n<p>With your way, you have no such compiler checks. Oh sure, the compiler won't check for non-pure virtuals, but at least you have protection in cases where protection is possible (ie: there is no default operation).</p>\n\n<p>The use of transparent base classes with RTTI leads to a maintenance nightmare. Indeed, most uses of RTTI leads to maintenance headaches. That doesn't mean that RTTI isn't <em>useful</em> (it's vital for making <code>boost::any</code> work, for example). But it is a very specialized tool for <em>very</em> specialized needs.</p>\n\n<p>In that way, it is \"harmful\" in the same way as <code>goto</code>. It's a useful tool that shouldn't be done away with. But it's use should be <em>rare</em> within your code.</p>\n\n<hr>\n\n<p>So, if you can't use transparent base classes and dynamic casting, how do you avoid fat interfaces? How do you keep from bubbling every function you might want to call on a type from bubbling up to the base class?</p>\n\n<p>The answer depends on what the base class is for.</p>\n\n<p>Transparent base classes like <code>node_base</code> are just using the wrong tool for the problem. Linked lists are best handled by templates. The node type and adjacency would be provided by a template type. If you want to put a polymorphic type in the list, you can. Just use <code>BaseClass*</code> as <code>T</code> in the template argument. Or your preferred smart pointer.</p>\n\n<p>But there are other scenarios. One is a type that does a lot of things, but has some optional parts. A particular instance might implement certain functions, while another wouldn't. However, the design of such types usually offers a proper answer.</p>\n\n<p>The \"entity\" class is a perfect example of this. This class has long since plagued game developers. Conceptually, it has a gigantic interface, living at the intersection of nearly a dozen, entirely disparate systems. And different entities have different properties. Some entities don't have any visual representation, so their rendering functions do nothing. And this is all determined at runtime.</p>\n\n<p>The modern solution for this is a component-style system. <code>Entity</code> is merely a container of a set of components, with some glue between them. Some components are optional; an entity that has no visual representation does not have the \"graphics\" component. An entity with no AI has no \"controller\" component. And so forth.</p>\n\n<p>Entities in such a system are just pointers to components, with most of their interface being provided by accessing the components directly.</p>\n\n<p>Developing such a component system requires recognizing, at the design stage, that certain functions are conceptually grouped together, such that all types that implement one will implement them all. This allows you to extract the class from the prospective base class and make it a separate component.</p>\n\n<p>This also helps follow the Single Responsibility Principle. Such a componentized class only has the responsibility of being a holder of components.</p>\n\n<hr>\n\n<p>From Matthew Walton:</p>\n\n<blockquote>\n  <p>I note lots of answers don't note the idea that your example suggests node_base is part of a library and users will make their own node types. Then they can't modify node_base to allow another solution, so maybe RTTI becomes their best option then.</p>\n</blockquote>\n\n<p>OK, let's explore that.</p>\n\n<p>For this to make sense, what you would have to have is a situation where some library L provides a container or other structured holder of data. The user gets to add data to this container, iterate over its contents, etc. However, the library doesn't really do anything with this data; it simply manages its existence.</p>\n\n<p>But it doesn't even manage its existence so much as its <em>destruction</em>. The reason being that, if you're expected to use RTTI for such purposes, then you are creating classes that L is ignorant of. This means that <em>your code</em> allocates the object and hands it off to L for management.</p>\n\n<p>Now, there are cases where something like this is a legitimate design. Event signaling/message passing, thread-safe work queues, etc. The general pattern here is this: someone is performing a service between two pieces of code that is appropriate for any type, but the service need not be aware of the specific types involved.</p>\n\n<p>In C, this pattern is spelled <code>void*</code>, and its use requires a great deal of care to avoid being broken. In C++, this pattern is spelled <a href=\"http://en.cppreference.com/w/cpp/experimental/any\" rel=\"nofollow\"><code>std::experimental::any</code></a> (soon to be spelled <code>std::any</code>).</p>\n\n<p>The way this <em>ought</em> to work is that L provides a <code>node_base</code> class that takes an <code>any</code> that represents your actual data. When you receive the message, thread queue work item, or whatever you're doing, you then cast that <code>any</code> to its appropriate type, which both the sender and the receiver know.</p>\n\n<p>So instead of deriving <code>orange_node</code> from <code>node_data</code>, you simply stick an <code>orange</code> inside of <code>node_data</code>'s <code>any</code> member field. The end-user extracts it and uses <code>any_cast</code> to convert it to <code>orange</code>. If the cast fails, then it wasn't <code>orange</code>.</p>\n\n<p>Now, if you're at all familiar with the implementation of <code>any</code>, you'll likely say, \"hey wait a minute: <code>any</code> <em>internally</em> uses RTTI to make <code>any_cast</code> work.\" To which I answer, \"... yes\".</p>\n\n<p>That's the point of an <em>abstraction</em>. Deep down in the details, someone is using RTTI. But at the level you ought to be operating at, direct RTTI is not something you should be doing.</p>\n\n<p>You should be using types that provide you the functionality you want. After all, you don't really want RTTI. What you want is a data structure that can store a value of a given type, hide it from everyone except the desired destination, then be converted back into that type, with verification that the stored value actually is of that type.</p>\n\n<p>That's called <code>any</code>. It <em>uses</em> RTTI, but using <code>any</code> is far superior to using RTTI directly, since it fits the desired semantics more correctly.</p>\n", "title": "Why is &#39;pure polymorphism&#39; preferable over using RTTI?", "answer_id": 35787291, "question_id": 35764905, "tags": ["c++", "polymorphism", "rtti"]}], "question_id": 35764905}, {"body": "<p>I've been playing with Python's <a href=\"https://docs.python.org/3/library/functions.html#hash\">hash function</a>. For small integers, it appears <code>hash(n) == n</code> always. However this does not extend to large numbers:</p>\n\n<pre><code>&gt;&gt;&gt; hash(2**100) == 2**100\nFalse\n</code></pre>\n\n<p>I'm not surprised, I understand hash takes a finite range of values. What is that range?</p>\n\n<p>I tried using <a href=\"http://codejamhelpers.readthedocs.io/en/latest/#codejamhelpers.binary_search\">binary search</a> to find the smallest number <code>hash(n) != n</code></p>\n\n<pre><code>&gt;&gt;&gt; import codejamhelpers # pip install codejamhelpers\n&gt;&gt;&gt; help(codejamhelpers.binary_search)\nHelp on function binary_search in module codejamhelpers.binary_search:\n\nbinary_search(f, t)\n    Given an increasing function :math:`f`, find the greatest non-negative integer :math:`n` such that :math:`f(n) \\le t`. If :math:`f(n) &gt; t` for all :math:`n \\ge 0`, return None.\n\n&gt;&gt;&gt; f = lambda n: int(hash(n) != n)\n&gt;&gt;&gt; n = codejamhelpers.binary_search(f, 0)\n&gt;&gt;&gt; hash(n)\n2305843009213693950\n&gt;&gt;&gt; hash(n+1)\n0\n</code></pre>\n\n<p>What's special about 2305843009213693951? I note it's less than <code>sys.maxsize == 9223372036854775807</code></p>\n\n<p>Edit: I'm using Python 3. I ran the same binary search on Python 2 and got a different result 2147483648, which I note is <code>sys.maxint+1</code></p>\n\n<p>I also played with <code>[hash(random.random()) for i in range(10**6)]</code> to estimate the range of hash function. The max is consistently below n above. Comparing the min, it seems Python 3's hash is always positively valued, whereas Python 2's hash can take negative values.</p>\n", "title": "When is hash(n) == n in Python?", "tags": ["python", "python-2.7", "python-3.x", "hash"], "answer_count": 4, "link": "http://stackoverflow.com/questions/37612524/when-is-hashn-n-in-python", "answers": [{"body": "<p>The <a href=\"https://hg.python.org/cpython/file/c6880edaf6f3/Objects/intobject.c#l448\" rel=\"nofollow\">implementation for the int type in cpython can be found here.</a> </p>\n\n<p>It just returns the value, except for <code>-1</code>, than it returns <code>-2</code>:</p>\n\n<pre class=\"lang-c prettyprint-override\"><code>static long\nint_hash(PyIntObject *v)\n{\n    /* XXX If this is changed, you also need to change the way\n       Python's long, float and complex types are hashed. */\n    long x = v -&gt; ob_ival;\n    if (x == -1)\n        x = -2;\n    return x;\n}\n</code></pre>\n", "title": "When is hash(n) == n in Python?", "answer_id": 37612722, "question_id": 37612524, "tags": ["python", "python-2.7", "python-3.x", "hash"]}, {"body": "<p>Hash function returns <strong>plain int</strong> that means that returned value is greater than <code>-sys.maxint</code> and lower than <code>sys.maxint</code>, which means if you pass <code>sys.maxint + x</code> to it result would be <code>-sys.maxint + (x - 2)</code>.</p>\n\n<pre><code>hash(sys.maxint + 1) == sys.maxint + 1 # False\nhash(sys.maxint + 1) == - sys.maxint -1 # True\nhash(sys.maxint + sys.maxint) == -sys.maxint + sys.maxint - 2 # True\n</code></pre>\n\n<p>Meanwhile <code>2**200</code> is a <code>n</code> times greater than <code>sys.maxint</code> -  my guess is that hash would go over range <code>-sys.maxint..+sys.maxint</code> n times until it stops on plain integer in that range, like in code snippets above..</p>\n\n<p>So generally, for any <strong>n &lt;= sys.maxint</strong>:</p>\n\n<pre><code>hash(sys.maxint*n) == -sys.maxint*(n%2) +  2*(n%2)*sys.maxint - n/2 - (n + 1)%2 ## True\n</code></pre>\n\n<p><strong>Note:</strong> this is true for python 2.</p>\n", "title": "When is hash(n) == n in Python?", "answer_id": 37613404, "question_id": 37612524, "tags": ["python", "python-2.7", "python-3.x", "hash"]}, {"body": "<p><code>2305843009213693951</code> is <code>2^61 - 1</code>. It's the largest Mersenne prime that fits into 64 bits.</p>\n\n<p>If you have to make a hash just by taking the value mod some number, then a large Mersenne prime is a good choice -- it's easy to compute and ensures an even distribution of possibilities. (Although I personally would never make a hash this way)</p>\n\n<p>It's especially convenient to compute the modulus for floating point numbers.  They have an exponential component that multiplies the whole number by <code>2^x</code>.  Since <code>2^61 = 1 mod 2^61-1</code>, you only need to consider the <code>(exponent) mod 61</code>.</p>\n\n<p>See: <a href=\"https://en.wikipedia.org/wiki/Mersenne_prime\">https://en.wikipedia.org/wiki/Mersenne_prime</a></p>\n", "title": "When is hash(n) == n in Python?", "answer_id": 37614051, "question_id": 37612524, "tags": ["python", "python-2.7", "python-3.x", "hash"]}, {"body": "<p>Based on python documentation in <a href=\"https://github.com/python/cpython/blob/master/Python/pyhash.c\"><code>pyhash.c</code></a> file:</p>\n\n<blockquote>\n  <p>For numeric types, the hash of a number x is based on the reduction\n     of x modulo the prime <code>P = 2**_PyHASH_BITS - 1</code>.  It's designed so that\n     <code>hash(x) == hash(y)</code> whenever x and y are numerically equal, even if\n     x and y have different types.</p>\n</blockquote>\n\n<p>So for a 64/32 bit machine, the reduction would be 2 <sup>_PyHASH_BITS </sup> - 1, but what is <code>_PyHASH_BITS</code>?</p>\n\n<p>You can find it in <a href=\"https://github.com/python/cpython/blob/master/Include/pyhash.h\"><code>pyhash.h</code></a> header file which for a 64 bit machine has been defined as 61 (you can read more explanation in <code>pyconfig.h</code> file).</p>\n\n<pre><code>#if SIZEOF_VOID_P &gt;= 8\n#  define _PyHASH_BITS 61\n#else\n#  define _PyHASH_BITS 31\n#endif\n</code></pre>\n\n<p>So first off all it's based on your platform for example in my 64bit Linux platform the reduction is 2<sup>61</sup>-1, which is <code>2305843009213693951</code>:</p>\n\n<pre><code>&gt;&gt;&gt; 2**61 - 1\n2305843009213693951\n</code></pre>\n\n<p>Also You can use <code>math.frexp</code> in order to get the mantissa and exponent of <code>sys.maxint</code> which for a 64 bit machine shows that max int is 2<sup>63</sup>: </p>\n\n<pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; math.frexp(sys.maxint)\n(0.5, 64)\n</code></pre>\n\n<p>And you can see the difference by a simple test:</p>\n\n<pre><code>&gt;&gt;&gt; hash(2**62) == 2**62\nTrue\n&gt;&gt;&gt; hash(2**63) == 2**63\nFalse\n</code></pre>\n\n<p>Read the complete documentation about python hashing algorithm <a href=\"https://github.com/python/cpython/blob/master/Python/pyhash.c#L34\">https://github.com/python/cpython/blob/master/Python/pyhash.c#L34</a></p>\n\n<p>As mentioned in comment you can use <code>sys.hash_info</code> (in python 3.X) which will give you a struct sequence of parameters used for computing\nhashes.</p>\n\n<pre><code>&gt;&gt;&gt; sys.hash_info\nsys.hash_info(width=64, modulus=2305843009213693951, inf=314159, nan=0, imag=1000003, algorithm='siphash24', hash_bits=64, seed_bits=128, cutoff=0)\n&gt;&gt;&gt; \n</code></pre>\n\n<p>Alongside the modulus that I've described in preceding lines, you can also get the <code>inf</code> value as following:</p>\n\n<pre><code>&gt;&gt;&gt; hash(float('inf'))\n314159\n&gt;&gt;&gt; sys.hash_info.inf\n314159\n</code></pre>\n", "title": "When is hash(n) == n in Python?", "answer_id": 37614182, "question_id": 37612524, "tags": ["python", "python-2.7", "python-3.x", "hash"]}], "question_id": 37612524}, {"body": "<p>I'm trying to pass both date strings to <code>new Date(t)</code>.</p>\n\n<p>I expect both strings represent the same time, after all, if I omit the time, shouldn't it be midnight of that day?</p>\n\n<p>But while,</p>\n\n<pre><code>new Date(\"2016-02-16 00:00\")\n</code></pre>\n\n<p>returns 2016-02-16, midnight, local time as expected,</p>\n\n<pre><code>new Date(\"2016-02-16\")\n</code></pre>\n\n<p>returns 2016-02-16, midnight UTC, which is wrong, or at least not what I expected given what the other string parses as.</p>\n\n<p>I would understand it if they would both have the same behavior, whether it is to return the time as local time, or as UTC, but it seems very inconsistent why they return different things like this.</p>\n\n<p>As a workaround, whenever I encounter a date that has no corresponding timestamp I can append \" 00:00\" to get consistent behavior, but it seems like this is rather fragile.</p>\n\n<p>I am getting this value from an INPUT element, of type 'datetime-local' so it seems especially inconsistent that I have to work around a value returned by a page element.</p>\n\n<p>Am I doing something wrong, or should I be doing something differently?</p>\n", "title": "Why isn&#39;t &quot;2016-02-16&quot; equal to &quot;2016-02-16 00:00&quot;?", "tags": ["javascript", "date", "datetime"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35448343/why-isnt-2016-02-16-equal-to-2016-02-16-0000", "answers": [{"body": "<blockquote>\n  <p>returns 2016-02-16, midnight UTC, which is wrong, or at least not what I expected given what the other string parses as.</p>\n</blockquote>\n\n<p>It adds the timezone offset to the <code>00:00</code></p>\n\n<p><code>new Date(\"2016-02-16\")</code> outputs <code>Tue Feb 16 2016 05:30:00 GMT+0530 (India Standard Time)</code></p>\n\n<p>My timezone being IST with an offset value (in minutes) <code>+330</code>, so it added 330 minutes to 00:00.</p>\n\n<p>As per <a href=\"http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf\" rel=\"nofollow\">ecma-262, section 20.3.3.2 Date.parse ( string )</a></p>\n\n<blockquote>\n  <p>If ToString results in an abrupt completion the Completion Record is\n  immediately returned. Otherwise, parse interprets the resulting String\n  as a date and time; it returns a Number, the UTC time value\n  corresponding to the date and time. The String may be interpreted as a\n  local time, a UTC time, or a time in some other time zone, depending\n  on the contents of the String.</p>\n</blockquote>\n\n<p>When you <strong>explicitly</strong> set the time-units <code>new Date(\"2016-02-16 00:00\")</code> it wll use set that as <code>hours</code> and <code>minutes</code>, </p>\n\n<p><strong>Otherwise</strong> as stated here in 2<a href=\"http://www.ecma-international.org/ecma-262/6.0/#sec-date-constructor\" rel=\"nofollow\">0.3.1.16</a></p>\n\n<blockquote>\n  <p>If the time zone offset is absent, the date-time is interpreted as a\n  local time.</p>\n</blockquote>\n", "title": "Why isn&#39;t &quot;2016-02-16&quot; equal to &quot;2016-02-16 00:00&quot;?", "answer_id": 35448372, "question_id": 35448343, "tags": ["javascript", "date", "datetime"]}, {"body": "<p>You are perhaps running into a differences between ES5, ES6 implementations and your expected result. Per <a href=\"https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date/parse\" rel=\"nofollow\">Date.parse</a> at MDN, \"especially across different ECMAScript implementations where strings like \"2015-10-12 12:00:00\" may be parsed to as NaN, UTC or local timezone\" is significant.</p>\n\n<p>Additional testing in Firefox 44 and IE 11 revealed they both return a date object for <code>new Date(\"2016-02-16 00:00\")</code>, which object returns NaN when atttempting to get a date component value, and whose toString value is \n\"Invalid Date\" (not \"NaN\"). Hence appending \" 00:00 to get consistent behavior\" can easily break in different browsers.</p>\n\n<p>As noted in other answers <code>new Date(\"2016-02-16\")</code> uses a timezone offset of zero by default, producing midnight UTC instead of local. </p>\n", "title": "Why isn&#39;t &quot;2016-02-16&quot; equal to &quot;2016-02-16 00:00&quot;?", "answer_id": 35448639, "question_id": 35448343, "tags": ["javascript", "date", "datetime"]}, {"body": "<p>It's what the <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.1.15\" rel=\"nofollow\">ES5.1 specification</a> says to do:</p>\n\n<blockquote>\n  <p>The value of an absent time zone offset is \u201cZ\u201d.</p>\n</blockquote>\n\n<p>It also says:</p>\n\n<blockquote>\n  <p>The function first attempts to parse the format of the String according to the rules called out in Date Time String Format (15.9.1.15). If the String does not conform to that format the function may fall back to any implementation-specific heuristics or implementation-specific date formats. </p>\n</blockquote>\n\n<p>Since the format requires a <code>T</code> separator between date and time, the valid times go to UTC:</p>\n\n<pre><code>&gt; new Date(\"2016-02-16T00:00:00\")\nTue Feb 16 2016 01:00:00 GMT+0100 (CET)\n&gt; new Date(\"2016-02-16\")\nTue Feb 16 2016 01:00:00 GMT+0100 (CET)\n</code></pre>\n\n<p>...while in node.js, an invalid time (without the T separator) seems to go to the implementation specific localtime:</p>\n\n<pre><code>&gt; new Date(\"2016-02-16 00:00:00\")\nTue Feb 16 2016 00:00:00 GMT+0100 (CET)\n</code></pre>\n\n<p>Note that ES6 <em>changed</em> this, in the same <a href=\"http://www.ecma-international.org/ecma-262/6.0/#sec-date-time-string-format\" rel=\"nofollow\">part of the documentation</a> it changes to:</p>\n\n<blockquote>\n  <p>If the time zone offset is absent, the date-time is interpreted as a local time.</p>\n</blockquote>\n\n<p>The joy of <a href=\"https://bugs.ecmascript.org/show_bug.cgi?id=112\" rel=\"nofollow\">breaking changes</a>.</p>\n\n<h3>Edit</h3>\n\n<p>According to <a href=\"https://github.com/tc39/ecma262/issues/87\" rel=\"nofollow\"><em>TC39</em></a>, the specification is meant to be interpreted as date and time strings without a time zone (e.g. \"2016-02-16T00:00:00\") are treated as local (per ISO 8601), but date only strings (e.g. \"2016-02-16\") as UTC (which is inconsistent with ISO 8601).</p>\n", "title": "Why isn&#39;t &quot;2016-02-16&quot; equal to &quot;2016-02-16 00:00&quot;?", "answer_id": 35448777, "question_id": 35448343, "tags": ["javascript", "date", "datetime"]}, {"body": "<p>Per <a href=\"https://github.com/v8/v8/blob/master/src/dateparser-inl.h#L16\"><code>DateParser::Parse()</code></a> of V8 source codes for Chrome.</p>\n\n<blockquote>\n  <p>ES5 ISO 8601 dates:</p>\n  \n  <p><code>[('-'|'+')yy]yyyy[-MM[-DD]][THH:mm[:ss[.sss]][Z|(+|-)hh:mm]]</code></p>\n  \n  <p>An unsigned number followed by ':' is a time value, and is added to the TimeComposer.</p>\n  \n  <p>timezone defaults to Z if missing</p>\n</blockquote>\n\n<pre><code>&gt; new Date(\"2016-02-16 00:00\")\n  Tue Feb 16 2016 00:00:00 GMT+0800 (China Standard Time)\n</code></pre>\n\n<blockquote>\n  <p>A string that matches both formats (e.g. <code>1970-01-01</code>) will be parsed as an ES5 date-time string - which means it will default to <code>UTC time-zon</code>e. That's unavoidable if following the ES5 specification.</p>\n</blockquote>\n\n<pre><code>&gt; new Date(\"2016-02-16\")\nTue Feb 16 2016 08:00:00 GMT+0800 (China Standard Time)\n</code></pre>\n", "title": "Why isn&#39;t &quot;2016-02-16&quot; equal to &quot;2016-02-16 00:00&quot;?", "answer_id": 35449202, "question_id": 35448343, "tags": ["javascript", "date", "datetime"]}, {"body": "<p>According to the <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.4.2\" rel=\"nofollow\">specifications</a>:</p>\n\n<blockquote>\n  <p>The function first attempts to parse the format of the String\n  according to the rules called out in Date Time String Format\n  (15.9.1.15). If the String does not conform to that format the\n  function may fall back to any implementation-specific heuristics or\n  implementation-specific date formats.</p>\n</blockquote>\n\n<p>And <a href=\"http://www.ecma-international.org/ecma-262/5.1/#sec-15.9.1.15\" rel=\"nofollow\">Date Time String Formats</a> accept <code>2016-02-16</code> as a valid date</p>\n\n<blockquote>\n  <p>This format includes date-only forms:</p>\n  \n  <p>YYYY<br>\n  YYYY-MM<br>\n  YYYY-MM-DD</p>\n  \n  <p>[...] If the HH, mm, or ss fields are absent \u201c00\u201d is used as the value\n  and the value of an absent sss field is \u201c000\u201d. The value of an absent\n  time zone offset is \u201cZ\u201d.</p>\n</blockquote>\n\n<p>Thus <code>2016-02-16</code> translates to <code>2016-02-16T00:00:00.000Z</code>.</p>\n\n<p>The other date <code>2016-02-16 00:00</code> does not conform to the format and therefore its parsing is implementation specific. Apparently, such dates are treated as having local time zone and your example date will return different values depending on time zone:</p>\n\n<pre><code>/* tz = +05:00 */ new Date(\"2016-02-16 00:00\").toISOString() // 2016-02-15T19:00:00.000Z\n/* tz = -08:00 */ new Date(\"2016-02-16 00:00\").toISOString() // 2016-02-16T08:00:00.000Z\n</code></pre>\n\n<p>Summary:</p>\n\n<ul>\n<li>For conforming date time formats the behavior is well defined \u2014 in the absence of time zone offset the date string is treated as UTC (ES5) or local (ES6).</li>\n<li>For non-conforming date time formats the behavior is implementation specific \u2014 in the absence of time zone offset the <em>usual</em> behavior is to treat the date as local. </li>\n<li>As a matter of fact, the implementation could choose to return <code>NaN</code> instead of <em>trying</em> to parse non-conforming dates. Just test your code in Internet Explorer 11 ;)</li>\n</ul>\n", "title": "Why isn&#39;t &quot;2016-02-16&quot; equal to &quot;2016-02-16 00:00&quot;?", "answer_id": 35454646, "question_id": 35448343, "tags": ["javascript", "date", "datetime"]}], "question_id": 35448343}, {"body": "<p>I upgraded my build.gradle file from</p>\n\n<pre><code>compile 'com.google.android.gms:play-services:8.4.0'\n</code></pre>\n\n<p>to</p>\n\n<p><code>compile 'com.google.android.gms:play-services:9.0.0'</code></p>\n\n<p>and now I am getting this error that I wasn't getting before.</p>\n\n<p><strong>Error:Failed to resolve: com.google.android.gms:play-services-measurement:9.0.0</strong>\n<a href=\"http://i.stack.imgur.com/oijCq.png\"><img src=\"http://i.stack.imgur.com/oijCq.png\" alt=\"enter image description here\"></a></p>\n\n<p><a href=\"http://i.stack.imgur.com/mg4Ld.png\"><img src=\"http://i.stack.imgur.com/mg4Ld.png\" alt=\"enter image description here\"></a></p>\n", "title": "Upgrade to Google Play Services:9.0.0 Error Failed to resolve: com.google.android.gms:play-services-measurement:9.0.0", "tags": ["android", "google-play-services"], "answer_count": 3, "link": "http://stackoverflow.com/questions/37311794/upgrade-to-google-play-services9-0-0-error-failed-to-resolve-com-google-androi", "answers": [{"body": "<p>This was found to fix the problem.</p>\n\n<p>Update classpath <code>com.google.gms:google-services:2.1.0</code> to classpath <code>com.google.gms:google-services:3.0.0</code></p>\n", "title": "Upgrade to Google Play Services:9.0.0 Error Failed to resolve: com.google.android.gms:play-services-measurement:9.0.0", "answer_id": 37312564, "question_id": 37311794, "tags": ["android", "google-play-services"]}, {"body": "<p>GCM has been <code>rebranded</code> to <code>Firebase</code> Cloud Messaging (FCM), If you want to use <code>com.google.android.gms:play-services:9.0.0</code> read this article <a href=\"http://firebase.google.com/docs/cloud-messaging/\" rel=\"nofollow\">FCM</a>.\nDo this maybe work, modify your <code>build.gradle</code> file to use the plugin. </p>\n\n<pre><code>buildscript {\n  dependencies {\n    // Add this line\n    classpath 'com.google.gms:google-services:3.0.0'\n  }\n}\n</code></pre>\n", "title": "Upgrade to Google Play Services:9.0.0 Error Failed to resolve: com.google.android.gms:play-services-measurement:9.0.0", "answer_id": 37316709, "question_id": 37311794, "tags": ["android", "google-play-services"]}, {"body": "<p><strong><a href=\"https://developers.google.com/analytics/devguides/collection/android/v4/#set-up-your-project\">Required: Latest versions of Android Studio and Google Play Services</a></strong></p>\n\n<p>You can add the plugin to your project by updating your top-level <strong>build.gradle and your app-level build.gradle</strong> files as follows:</p>\n\n<pre><code>classpath 'com.google.gms:google-services:3.0.0'\n</code></pre>\n\n<p><strong>Like</strong></p>\n\n<pre><code>    buildscript {\n    repositories {\n        jcenter()\n        mavenLocal()\n    }\n    dependencies {\n        classpath 'com.android.tools.build:gradle:2.1.0'\n        classpath 'com.google.gms:google-services:3.0.0'\n\n        // NOTE: Do not place your application dependencies here; they belong\n        // in the individual module build.gradle files\n    }\n}\n\nallprojects {\n    repositories {\n        jcenter()\n        mavenLocal()\n    }\n}\n</code></pre>\n\n<p>Now, you need to add a dependency for Google Play Services. Inside your app's <strong>build.gradle</strong> add:</p>\n\n<pre><code>compile 'com.google.android.gms:play-services:9.0.0'\n</code></pre>\n\n<p><strong>Finally</strong></p>\n\n<pre><code>    apply plugin: 'com.android.application'\n\nandroid {\n    compileSdkVersion 23\n    buildToolsVersion \"23.0.3\"\n\n    defaultConfig {\n        applicationId \"// set Yours\"\n        minSdkVersion 15\n        targetSdkVersion 23\n        versionCode 1\n        versionName \"1.0\"\n\n\n    }\n    buildTypes {\n        release {\n            minifyEnabled false\n            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'\n        }\n    }\n    packagingOptions {\n        exclude 'LICENSE.txt'\n    }\n}\n\ndependencies {\n    compile fileTree(dir: 'libs', include: ['*.jar'])\n    compile 'com.google.android.gms:play-services-gcm:9.0.0'\n    compile 'com.android.support:appcompat-v7:23.4.0'\n\n}\n\napply plugin: 'com.google.gms.google-services'\n</code></pre>\n", "title": "Upgrade to Google Play Services:9.0.0 Error Failed to resolve: com.google.android.gms:play-services-measurement:9.0.0", "answer_id": 37406307, "question_id": 37311794, "tags": ["android", "google-play-services"]}], "question_id": 37311794}, {"body": "<p>In general, for <code>int num</code>, <code>num++</code> (or <code>++num</code>), as a read-modify-write operation, is <strong>not atomic</strong>. But I often see compilers, for example <a href=\"https://en.wikipedia.org/wiki/GNU_Compiler_Collection\">GCC</a>, generate the following code for it (<a href=\"https://godbolt.org/g/UFKEvp\">try here</a>):</p>\n\n<p><a href=\"https://i.stack.imgur.com/cZMqP.png\"><img src=\"https://i.stack.imgur.com/cZMqP.png\" alt=\"Enter image description here\"></a></p>\n\n<p>Since line 5, which corresponds to <code>num++</code> is one instruction, can we conclude that <code>num++</code> <strong>is atomic</strong> in this case?</p>\n\n<p>And if so, <strong>does it mean that so-generated <code>num++</code> can be used in concurrent (multi-threaded) scenarios without any danger of data races</strong> (i.e. we don't need to make it, for example, <code>std::atomic&lt;int&gt;</code> and impose the associated costs, since it's atomic anyway)?</p>\n\n<p><strong>UPDATE</strong></p>\n\n<p>Notice that this question is <em>not</em> whether increment <em>is</em> atomic (it's not and that was and is the opening line of the question). It's whether it <em>can</em> be in particular scenarios, i.e. whether one-instruction nature can in certain cases be exploited to avoid the overhead of the <code>lock</code> prefix. And, as the accepted answer mentions in the section about uniprocessor machines, as well as <a href=\"http://stackoverflow.com/a/39414316/4973224\">this answer</a>, the conversation in its comments and others explain, <strong>it can</strong> (although not with C or C++).</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "tags": ["c++", "c", "multithreading", "assembly", "atomic"], "answer_count": 12, "link": "http://stackoverflow.com/questions/39393850/can-num-be-atomic-for-int-num", "answers": [{"body": "<p>Try compiling the same code on a non-x86 machine, and you'll quickly see very different assembly results.</p>\n\n<p>The reason <code>num++</code> <em>appears</em> to be atomic is because on x86 machines, incrementing a 32-bit integer is, in fact, atomic (assuming no memory retrieval takes place). But this is neither guaranteed by the c++ standard, nor is it likely to be the case on a machine that doesn't use the x86 instruction set. So this code is not cross-platform safe from race conditions.</p>\n\n<p>You also don't have a strong guarantee that this code is safe from Race Conditions even on an x86 architecture, because x86 doesn't set up loads and stores to memory unless specifically instructed to do so. So if multiple threads tried to update this variable simultaneously, they may end up incrementing cached (outdated) values</p>\n\n<p>The reason, then, that we have <code>std::atomic&lt;int&gt;</code> and so on is so that when you're working with an architecture where the atomicity of basic computations is not guaranteed, you have a mechanism that will force the compiler to generate atomic code.</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39393945, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>The add instruction is <strong>not</strong> atomic. It references memory, and two processor cores may have different local cache of that memory.</p>\n\n<p>IIRC the atomic variant of the add instruction is called <strong>lock xadd</strong></p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39394156, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<blockquote>\n  <p>Since line 5, which corresponds to num++ is one instruction, can we conclude that num++ is atomic in this case?</p>\n</blockquote>\n\n<p>It is dangerous to make conclusions based on \"reverse engineering\" generated assembly. For example you seem to compile your code with optimization disabled, otherwise compiler would through away that variable or load 1 directly to it without invoking <code>operator++</code>. So generated assembly may significantly change based on optimization flags, target CPU etc and your conclusion is based on sand.</p>\n\n<p>Also your idea that one assembly instruction is equal to atomic is wrong as well. This <code>add</code> will not be atomic on multi-CPU system even on x86 architecture.</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39394159, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>...and now let's enable optimisations:</p>\n\n<pre><code>f():\n        rep ret\n</code></pre>\n\n<p>OK, let's give it a chance:</p>\n\n<pre><code>void f(int&amp; num)\n{\n  num = 0;\n  num++;\n  --num;\n  num += 6;\n  num -=5;\n  --num;\n}\n</code></pre>\n\n<p>result:</p>\n\n<pre><code>f(int&amp;):\n        mov     DWORD PTR [rdi], 0\n        ret\n</code></pre>\n\n<p>another observing thread (even ignoring cache synchronisation delays) has no opportunity to observe the individual changes.</p>\n\n<p>compare to:</p>\n\n<pre><code>#include &lt;atomic&gt;\n\nvoid f(std::atomic&lt;int&gt;&amp; num)\n{\n  num = 0;\n  num++;\n  --num;\n  num += 6;\n  num -=5;\n  --num;\n}\n</code></pre>\n\n<p>where the result is:</p>\n\n<pre><code>f(std::atomic&lt;int&gt;&amp;):\n        mov     DWORD PTR [rdi], 0\n        mfence\n        lock add        DWORD PTR [rdi], 1\n        lock sub        DWORD PTR [rdi], 1\n        lock add        DWORD PTR [rdi], 6\n        lock sub        DWORD PTR [rdi], 5\n        lock sub        DWORD PTR [rdi], 1\n        ret\n</code></pre>\n\n<p>Now, each modification is:-</p>\n\n<ol>\n<li>observable in another thread, and</li>\n<li>respectful of similar modifications happening in other threads.</li>\n</ol>\n\n<p>atomicity is not just at the instruction level, it involves the whole pipeline from processor, through the caches, to memory and back.</p>\n\n<h3>Further info</h3>\n\n<p>Regarding the effect of optimisations of updates of <code>std::atomic</code>s.</p>\n\n<p>The c++ standard has the 'as if' rule, by which it is permissible for the compiler to reorder code, and even rewrite code provided that the outcome has the <em>exact same observable</em> effects (including side-effects) as if it had simply executed your code.</p>\n\n<p>The as-if rule is conservative, particularly involving atomics.</p>\n\n<p>consider:</p>\n\n<pre><code>void incdec(int&amp; num) {\n    ++num;\n    --num;\n}\n</code></pre>\n\n<p>Because there are no mutex locks, atomics or any other constructs that influence inter-thread sequencing, I would argue that the compiler is free to rewrite this function as a NOP, eg:</p>\n\n<pre><code>void incdec(int&amp;) {\n    // nada\n}\n</code></pre>\n\n<p>This is because in the c++ memory model, there is no possibility of another thread observing the result of the increment. It would of course be different if <code>num</code> was <code>volatile</code> (might influence hardware behaviour). But in this case, this function will be the only function modifying this memory (otherwise the program is ill-formed).</p>\n\n<p>However, this is a different ball game:</p>\n\n<pre><code>void incdec(std::atomic&lt;int&gt;&amp; num) {\n    ++num;\n    --num;\n}\n</code></pre>\n\n<p><code>num</code> is an atomic. Changes to it <em>must</em> be observable to other threads that are watching. Changes those threads themselves make (such as setting the value to 100 in between the increment and decrement) will have very far-reaching effects on the eventual value of num.</p>\n\n<p>Here is a demo:</p>\n\n<pre><code>#include &lt;thread&gt;\n#include &lt;atomic&gt;\n\nint main()\n{\n    for (int iter = 0 ; iter &lt; 20 ; ++iter)\n    {\n        std::atomic&lt;int&gt; num = { 0 };\n        std::thread t1([&amp;] {\n            for (int i = 0 ; i &lt; 10000000 ; ++i)\n            {\n                ++num;\n                --num;\n            }\n        });\n        std::thread t2([&amp;] {\n            for (int i = 0 ; i &lt; 10000000 ; ++i)\n            {\n                num = 100;\n            }\n        });\n\n        t2.join();\n        t1.join();\n        std::cout &lt;&lt; num &lt;&lt; std::endl;\n    }\n}\n</code></pre>\n\n<p>sample output:</p>\n\n<pre><code>99\n99\n99\n99\n99\n100\n99\n99\n100\n100\n100\n100\n99\n99\n100\n99\n99\n100\n100\n99\n</code></pre>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39394173, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>Without many complications an instruction like <code>add DWORD PTR [rbp-4], 1</code> is very CISC-style.</p>\n\n<p>It perform three operations: load the operand from memory, increment it, store the operand back to memory.<br>\nDuring these operations the CPU acquire and release the bus twice, in between any other agent can acquire it too and this violates the atomicity.</p>\n\n<pre><code>AGENT 1          AGENT 2\n\nload X              \ninc C\n                 load X\n                 inc C\n                 store X\nstore X\n</code></pre>\n\n<p>X is incremented only once.</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39394630, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>On a single-core x86 machine, an \"add\" instruction with a word-aligned memory destination will generally be atomic; an interrupt can't split a single instruction down the middle.  Out-of-order execution is required to preserve the illusion of instructions executing one at a time in order within a single core, so any instruction running on the same CPU will either happen completely before or completely after the add.</p>\n\n<p>Modern x86 systems are multi-core, so the uniprocessor special case doesn't apply.</p>\n\n<p>If one is targeting a small embedded PC and has no plans to move the code to anything else, the atomic nature of the \"add\" instruction could be exploited.  On the other hand, platforms where operations are inherently atomic are becoming more and more scarce.</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39396781, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>Even if your compiler always emitted this as an atomic operation, accessing <code>num</code> from any other thread concurrently would constitute a data race according to the C++11 and C++14 standards and the program would have undefined behavior.</p>\n\n<p>But it is worse than that. First, as has been mentioned, the instruction generated by the compiler when incrementing a variable may depend on the optimization level. Secondly, the compiler may reorder <em>other</em> memory accesses around <code>++num</code> if <code>num</code> is not atomic, e.g.</p>\n\n<pre><code>int main()\n{\n  std::unique_ptr&lt;std::vector&lt;int&gt;&gt; vec;\n  int ready = 0;\n  std::thread t{[&amp;]\n    {\n       while (!ready);\n       // use \"vec\" here\n    });\n  vec.reset(new std::vector&lt;int&gt;());\n  ++ready;\n  t.join();\n}\n</code></pre>\n\n<p>Even if we assume optimistically that <code>++ready</code> is \"atomic\", and that the compiler generates the checking loop as needed (as I said, it's UB and therefore the compiler is free to remove it, replace it with an infinite loop, etc.), the compiler might still move the pointer assignment, or even worse the initialization of the <code>vector</code> to a point after the increment operation, causing chaos in the new thread. In practice, I would not be surprised at all if an optimizing compiler removed the <code>ready</code> variable and the checking loop completely, as this does not affect observable behavior under language rules (as opposed to your private hopes).</p>\n\n<p>In fact, at last year's Meeting C++ conference, I've heard from <em>two</em> compiler developers that they very gladly implement optimizations that make naively written multi-threaded programs misbehave, as long as language rules allow it, if even a minor performance improvement is seen in correctly written programs.</p>\n\n<p>Lastly, even <em>if</em> you didn't care about portability, and your compiler was magically nice, the CPU you are using is very likely of a superscalar CISC type and will break down instructions into micro-ops, reorder and/or speculatively execute them, to an extent only limited by synchronizing primitives such as (on Intel) the <code>LOCK</code> prefix or memory fences, in order to maximize operations per second.</p>\n\n<p>To make a long story short, the natural responsibilities of thread-safe programming are:</p>\n\n<ol>\n<li>Your duty is to write code that has well-defined behavior under language rules (and in particular the language standard memory model).</li>\n<li>Your compiler's duty is to generate machine code which has the same well-defined (observable) behavior under the target architecture's memory model.</li>\n<li>Your CPU's duty is to execute this code so that the observed behavior is compatible with its own architecture's memory model.</li>\n</ol>\n\n<p>If you want to do it your own way, it might just work in some cases, but understand that the warranty is void, and you will be solely responsible for any <em>unwanted</em> outcomes. :-)</p>\n\n<p>PS: Correctly written example:</p>\n\n<pre><code>int main()\n{\n  std::unique_ptr&lt;std::vector&lt;int&gt;&gt; vec;\n  std::atomic&lt;int&gt; ready{0}; // NOTE the use of the std::atomic template\n  std::thread t{[&amp;]\n    {\n       while (!ready);\n       // use \"vec\" here\n    });\n  vec.reset(new std::vector&lt;int&gt;());\n  ++ready;\n  t.join();\n}\n</code></pre>\n\n<p>This is safe because:</p>\n\n<ol>\n<li>The checks of <code>ready</code> cannot be optimized away according to language rules.</li>\n<li>The <code>++ready</code> <em>happens-before</em> the check that sees <code>ready</code> as not zero, and other operations cannot be reordered around these operations. This is because <code>++ready</code> and the check are <em>sequentially consistent</em>, which is another term described in the C++ memory model and that forbids this specific reordering. Therefore the compiler must not reorder the instructions, and must also tell the CPU that it must not e.g. postpone the write to <code>vec</code> to after the increment of <code>ready</code>. <em>Sequentially consistent</em> is the strongest guarantee regarding atomics in the language standard. Lesser (and theoretically cheaper) guarantees are available e.g. via other methods of <code>std::atomic&lt;T&gt;</code>, but these are definitely for experts only, and may not be optimized much by the compiler developers, because they are rarely used.</li>\n</ol>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39396820, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>This is absolutely a data race as defined by C++. It wouldn't matter if one compiler happened to produce code that does what you hoped on some target machine; it's still undefined behaviour. You need to use <code>std::atomic</code>, but you can use it with <code>memory_order_relaxed</code> if you don't care about reordering.</p>\n\n<p>But first, the assembly language part of the question:</p>\n\n<blockquote>\n  <p>Since num++ is one instruction (<code>add dword [num], 1</code>), can we conclude that num++ is atomic in this case?</p>\n</blockquote>\n\n<p>Memory-destination instructions are read-modify-write operations. No architectural register is modified, but the CPU has to hold the data internally while it sends it through its <a href=\"https://en.wikipedia.org/wiki/Arithmetic_logic_unit\">ALU</a>. The actual register file is only a small part of the data storage inside even the simplest CPU, with latches holding outputs of one stage as inputs for another stage, etc., etc.</p>\n\n<p>Memory operations from other CPUs can become globally visible between the load and store. I.e. two threads running <code>add dword [num], 1</code> in a loop would step on each other's stores. (See <a href=\"http://stackoverflow.com/questions/39393850/can-num-be-atomic-for-an-int-num/39394630#39394630\">@Margaret's answer</a> for a nice diagram). After 40k increments from each of two threads, the counter might have only gone up by ~60k (not 80k) on real multi-core x86 hardware.</p>\n\n<hr>\n\n<p>The <a href=\"http://www.felixcloutier.com/x86/LOCK.html\"><code>lock</code> prefix</a> can be applied to many read-modify-write (memory destination) instructions to make them atomic. That is why it exists.  (See also <a href=\"http://stackoverflow.com/questions/8891067/what-does-the-lock-instruction-mean-in-x86-assembly/8891781#8891781\">this Q&amp;A</a>).</p>\n\n<p><strong>So <code>lock add dword [num], 1</code> <em>is</em> atomic</strong>. A CPU core running that instruction would keep the cache line locked from when the load reads data from cache until the store commits its result back into cache.  Operations by other cores appear to happen either before or after, not during. (<strong>This is basically the definition of atomic: that no observer can <em>see</em> the operation as separate steps, not that it physically / electrically happened simultaneously</strong>). I went into a lot more detail about this point in my answer to <em><a href=\"http://stackoverflow.com/questions/38447226/atomicity-on-x86/38465341#38465341\">Atomicity on x86</a></em>.</p>\n\n<p>Note that the <code>lock</code> prefix also turns an instruction into a full memory barrier, stopping all reordering. (See <a href=\"http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/\">Jeff Preshing's excellent blog post</a>. His other posts are all excellent, too, and clearly explain a <em>lot</em> of good stuff about <a href=\"http://preshing.com/20120612/an-introduction-to-lock-free-programming/\">lock-free programming</a>, from x86 and other hardware details to C++ rules.)</p>\n\n<hr>\n\n<p><strong>On a uniprocessor machine, or in a single-threaded process</strong>, a single <a href=\"https://en.wikipedia.org/wiki/Read-modify-write\">RMW</a> instruction actually <em>is</em> atomic without a <code>lock</code> prefix. The only way for other code to access the shared variable is for the CPU to do a context switch, which can't happen in the middle of an instruction. So a plain <code>dec dword [num]</code> can synchronize between a single-threaded program and its signal handlers, or in a multi-threaded program running on a single-core machine. See <a href=\"http://stackoverflow.com/a/39358907/224132\">the second half of my answer on another question</a>, and the comments under it, where I explain this in more detail.</p>\n\n<hr>\n\n<h3>Back to C++:</h3>\n\n<p>It's totally bogus to use <code>num++</code> without telling the compiler that you need it to compile to a single read-modify-write implementation. It's <a href=\"http://stackoverflow.com/a/39358907/224132\">free to compile it to this if it wants</a>:</p>\n\n<pre><code>;; Valid compiler output for num++\nmov   eax, [num]\ninc   eax\nmov   [num], eax\n</code></pre>\n\n<p>(That used to be more efficient (on some older x86 CPUs like <a href=\"https://en.wikipedia.org/wiki/Pentium_4\">Pentium&nbsp;4</a>, I think; see the <a href=\"/questions/tagged/x86\" class=\"post-tag\" title=\"show questions tagged &#39;x86&#39;\" rel=\"tag\">x86</a> tag wiki and <a href=\"http://agner.org/optimize/\">Agner Fog's instruction tables / guides</a> for performance details), but modern x86 CPUs once again handle RMW operations at least as efficiently as separate simple instructions.)</p>\n\n<h3>Don't confuse the target memory model (x86) with the C++ memory model</h3>\n\n<p><strong><a href=\"http://preshing.com/20120625/memory-ordering-at-compile-time/\">Compile-time reordering</a> is allowed</strong>. The other part of what you get with std::atomic is control over compile-time reordering, to make sure your <code>num++</code> becomes globally visible only after some other operation.</p>\n\n<p>Classic example: Storing some data into a buffer for another thread to look at, then setting a flag. Even though <a href=\"https://en.wikipedia.org/wiki/X86\">x86</a> does acquire loads/release stores for free, you still have to tell the compiler not to reorder by using <code>flag.store(1, std::memory_order_release);</code>.</p>\n\n<p>You might expecting that this code will synchronize with other threads:</p>\n\n<pre><code>// flag is just a plain int global, not std::atomic&lt;int&gt;.\nflag--;       // This isn't a real lock, but pretend it's somehow meaningful.\nmodify_a_data_structure(&amp;foo);\nflag++;\n</code></pre>\n\n<p>But it won't. The compiler is free to move the <code>flag++</code> across the function call (if it inlines the function or knows that it doesn't look at <code>flag</code>). Then it can optimize away the modification entirely, because <code>flag</code> isn't even <code>volatile</code>. (And no, C++ <code>volatile</code> is not a useful substitute for std::atomic.)</p>\n\n<hr>\n\n<p>As I mentioned, the x86 <code>lock</code> prefix is a full memory barrier, so using <code>num.fetch_add(1, std::memory_order_relaxed);</code> generates the same code on x86 as <code>num++</code> (the default is sequential consistency), but it can be much more efficient on other architectures (like <a href=\"http://en.wikipedia.org/wiki/ARM_architecture\">ARM</a>).</p>\n\n<p><strong>This is what GCC actually does on x86, for a few functions that operate on a <code>std::atomic</code> global variable.</strong></p>\n\n<p>See the source + assembly language code formatted nicely on the <a href=\"http://gcc.godbolt.org/#compilers:!((compiler:g62,options:&#39;-std%3Dgnu%2B%2B11+-Wall+-Wextra++-O3&#39;,sourcez:MQSwdgxgNgrgJgUwAQB4CGAXA9gWxBAPgCgiBnDOALks13xXAwKTBhwG4SA3LEOJcBAD6AJwRQ0ADwRwAFAEokAbyJIWbAHQAzBBggALIWjhyAjABok5KpRwIcWEQE8hjxCNHipM%2BZwC%2BJEQ8fAKQQqQIAI5CEOQKyqrqOADUyexqAPQZSNq6BkYmshZWFNR2Ds6uIu7hUTHkvkQBRIxIUFjGQqw48UpIYhgwImBJ6X5qE5NTWUgIXAgjEdGxGG0dcKRIaGJIWmLIWCOSABwAbEG8/OSOCF1ssq1caFDyfd1IALxIT1BjFyHXMR3HCeKAINARB5gVY/V6JGbPdoAdxkSGwSAARshEBInKjwJiEBA6GAAOZIUntDGIpzfECkEAYsFIGDQkBQLZaDAIES7LBQZHgclYAAOpA0iW6GkBCFkP0s1jK9kcLjcPNB4IijQCwSu2CB3VB3jkj2ecMy2VoeAglgxMFWKLQAGsoLS1WI4Bo1LJiTgRezkBgQHYtmB%2BCJWWjgwh5JLNDK5c8FaVbMrKu6jdI4NqSDMnFgYAByfgISQiomrX3%2BsEiTbo30Y8DIHAwKBBkXMiBoSDiKBCpCinmYECHTaMLBbKxC5miogzLFdmARFhYDD6fukmDbbvchB1/SYUMDjERETzXnE1tXBCB/TIRg8uxwECYZA/GAIOfZEAaBBetcHNUPL9kid47GAq7rmShK%2BnuFJUjSdIMkyyBYhgKILGid7LqKY6bIcyAilgpDIcyYikK2GBflYE4AUgVYBrypD6AWUD8P6EBOgIqyHgxzJBnYubZAAQvaK4YlgcBOIWmxIiIIAYNyIx0VWfZdty/DEogWGHkirH8IiWBIlhyB8TyaITnAtEHqspBoDoroSjMSIKSxYlzOZdgiKS/YEl2ESyWB962W5bFgIWqwHvMTnZAAypZhwRYS9EQluHIEvmQwsjhWhbNg1qkJQJm0tsCxJQY4IiqG/BWeFqyigJIAAF7NjABj/PwLZtiAHa3LhtTLHEio0Pl9CsgypJgPi0IEAAZEksYqBaSSfEgAAMnBqN0qSbUgAC0e3dLt7zJF8pzHWw%2B0fAArLtWQHUdTTcJcSBde2YKuGKmYyLIw1WmNYATVN/CMHNC1EEtSAzO8XwbXGOC5HohjGHISDFMN5QqlUNRiBIWaNFtmg6EjBRyHt6Mppj6ZAR4uPGgTSSI/kKOyEgpzJjYVOqjT33ZhdCPE8zhR7ddHNKhU3M414%2BN3RkUqC8jwsU5zaaS%2BqdMy09QAA)),filterAsm:(commentOnly:!t,directives:!t,intel:!t,labels:!t),version:3\">Godbolt compiler explorer</a>. You can select other target architectures, including ARM, <a href=\"https://en.wikipedia.org/wiki/MIPS_instruction_set\">MIPS</a>, and <a href=\"https://en.wikipedia.org/wiki/PowerPC\">PowerPC</a>, to see what kind of assembly language code you get from atomics for those targets.</p>\n\n<pre><code>#include &lt;atomic&gt;\nstd::atomic&lt;int&gt; num;\nvoid inc_relaxed() {\n  num.fetch_add(1, std::memory_order_relaxed);\n}\n\nint load_num() { return num; }            // Even seq_cst loads are free on x86\nvoid store_num(int val){ num = val; }\nvoid store_num_release(int val){\n  num.store(val, std::memory_order_release);\n}\n// Can the compiler collapse multiple atomic operations into one? No, it can't.\n</code></pre>\n\n<p></p>\n\n<pre><code># g++ 6.2 -O3, targeting x86-64 System V calling convention. (First argument in edi/rdi)\ninc_relaxed():\n    lock add        DWORD PTR num[rip], 1      #### Even relaxed RMWs need a lock. There's no way to request just a single-instruction RMW with no lock, for synchronizing between a program and signal handler for example. :/ There is atomic_signal_fence for ordering, but nothing for RMW.\n    ret\ninc_seq_cst():\n    lock add        DWORD PTR num[rip], 1\n    ret\nload_num():\n    mov     eax, DWORD PTR num[rip]\n    ret\nstore_num(int):\n    mov     DWORD PTR num[rip], edi\n    mfence                          ##### seq_cst stores need an mfence\n    ret\nstore_num_release(int):\n    mov     DWORD PTR num[rip], edi\n    ret                             ##### Release and weaker doesn't.\nstore_num_relaxed(int):\n    mov     DWORD PTR num[rip], edi\n    ret\n</code></pre>\n\n<p>Notice how MFENCE (a full barrier) is needed after a sequential-consistency stores. x86 is strongly ordered in general, but StoreLoad reordering is allowed. Having a store buffer is essential for good performance on a pipelined out-of-order CPU. Jeff Preshing's <strong><a href=\"http://preshing.com/20120515/memory-reordering-caught-in-the-act/\">Memory Reordering Caught in the Act</a></strong> shows the consequences of <em>not</em> using MFENCE, with real code to show reordering happening on real hardware.</p>\n\n<hr>\n\n<p>Re: discussion in comments about <strong>merging std::atomic <code>num++; num-=2;</code> operations into one</strong>:</p>\n\n<p>You might hope that compilers would combine multiple cancelling operations into a single operation, because nothing guarantees that an observer could see the intermediate values. I.e. the ordering where nothing becomes globally visible between these operations is a possibility, so the compiler should pick it at compile time (so it always happens that way).</p>\n\n<p>I think this is true, and the compiler is <em>allowed</em> to do that, but nobody's written the complicated code that would allow the compiler to do that safely (without ever getting it wrong). Don't be casual in your use of atomic weapons: they aren't cheap and don't optimize much. It would also potentially violate the <a href=\"https://en.wikipedia.org/wiki/Principle_of_least_astonishment\">principle of least surprise</a>, and lock-free code is hard enough to write correctly in the first place. Lock-free code needs to be carefully written for high performance, so it won't contain redundant atomic operations in the first place.</p>\n\n<p>The only time you'd use atomics for all your variables \"just in case\" is when <a href=\"http://stackoverflow.com/questions/37361145/deoptimizing-a-program-for-the-pipeline-in-intel-sandybridge-family-cpus\">Deoptimizing a program for the pipeline in Intel Sandybridge-family CPUs</a>, or something. :)</p>\n\n<p>Even with mo_relaxed, you still get separate <code>lock</code> operations.</p>\n\n<pre><code>void multiple_ops_relaxed(std::atomic&lt;unsigned int&gt;&amp; num) {\n  num.fetch_add( 1, std::memory_order_relaxed);\n  num.fetch_add(-1, std::memory_order_relaxed);\n  num.fetch_add( 6, std::memory_order_relaxed);\n  num.fetch_add(-5, std::memory_order_relaxed);\n  //num.fetch_add(-1, std::memory_order_relaxed);\n}\n\nmultiple_ops_relaxed(std::atomic&lt;unsigned int&gt;&amp;):\n    lock add        DWORD PTR [rdi], 1\n    lock sub        DWORD PTR [rdi], 1\n    lock add        DWORD PTR [rdi], 6\n    lock sub        DWORD PTR [rdi], 5\n</code></pre>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39396999, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<h2>Yes, but...</h2>\n\n<p><strong>Atomic is not what you meant to say. You're probably asking the wrong thing.</strong></p>\n\n<p>The increment is certainly <em>atomic</em>. Unless the storage is misaligned (and since you left alignment to the compiler, it is not), it is necessarily aligned within a single cache line. Short of special non-caching streaming instructions, each and every write goes through the cache. Complete cache lines are being atomically read and written, never anything different.<br>\nSmaller-than-cacheline data is, of course, also written atomically (since the surrounding cache line is).</p>\n\n<h2>Is it thread-safe?</h2>\n\n<p>This is a different question, and there are at least two good reasons to answer with a definite <strong>\"No!\"</strong>.</p>\n\n<p>First, there is the possibility that another core might have a copy of that cache line in L1 (L2 and upwards is usually shared, but L1 is normally per-core!), and concurrently modifies that value. Of course that happens atomically, too, but now you have two \"correct\" (correctly, atomically, modified) values -- which one is the truly correct one now?<br>\nThe CPU will sort it out somehow, of course. But the result may not be what you expect.</p>\n\n<p>Second, there is memory ordering, or worded differently happens-before guarantees. The most important thing about atomic instructions is not so much that they are <em>atomic</em>. It's ordering.</p>\n\n<p>You have the possibility of enforcing a guarantee that everything that happens memory-wise is realized in some guaranteed, well-defined order where you have a \"happened before\" guarantee. This ordering may be as \"relaxed\" (read as: none at all) or as strict as you need.</p>\n\n<p>For example, you can set a pointer to some block of data (say, the results of some calculation) and then atomically <em>release</em> the \"data is ready\" flag. Now, whoever <em>acquires</em> this flag will be led into thinking that the pointer is valid. And indeed, it will <em>always</em> be a valid pointer, never anything different. That's because the write to the pointer happened-before the atomic operation.</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39397538, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>That a single compiler's output, on a specific CPU architecture, with optimizations disabled (since gcc doesn't even compile <code>++</code> to <code>add</code> when optimizing <a href=\"https://godbolt.org/g/pKDMjb\" rel=\"nofollow\">in a quick&amp;dirty example</a>), seems to imply incrementing this way is atomic doesn't mean this is standard-compliant (you would cause undefined behavior when trying to access <code>num</code> in a thread), and is wrong anyways, because <code>add</code> is <em>not</em> atomic in x86.</p>\n\n<p>Note that atomics (using the <code>lock</code> instruction prefix) are relatively heavy on x86 (<a href=\"http://stackoverflow.com/a/16132551/6261331\">see this relevant answer</a>), but still remarkably less than a mutex, which isn't very appropriate in this use-case.</p>\n\n<p>Following results are taken from clang++ 3.8 when compiling with <code>-Os</code>.</p>\n\n<p>Incrementing an int by reference, the \"regular\" way :</p>\n\n<pre><code>void inc(int&amp; x)\n{\n    ++x;\n}\n</code></pre>\n\n<p>This compiles into :</p>\n\n<pre><code>inc(int&amp;):\n    incl    (%rdi)\n    retq\n</code></pre>\n\n<p>Incrementing an int passed by reference, the atomic way :</p>\n\n<pre><code>#include &lt;atomic&gt;\n\nvoid inc(std::atomic&lt;int&gt;&amp; x)\n{\n    ++x;\n}\n</code></pre>\n\n<p>This example, which is not much more complex than the regular way, just gets the <code>lock</code> prefix added to the <code>incl</code> instruction - but caution, as previously stated this is <em>not</em> cheap. Just because assembly looks short doesn't mean it's fast.</p>\n\n<pre><code>inc(std::atomic&lt;int&gt;&amp;):\n    lock            incl    (%rdi)\n    retq\n</code></pre>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39398808, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>No.\n<a href=\"https://www.youtube.com/watch?v=31g0YE61PLQ\" rel=\"nofollow\">https://www.youtube.com/watch?v=31g0YE61PLQ</a>\n(That's just a link to the \"No\" scene from \"The Office\")</p>\n\n<p>Do you agree that this would be a possible output for the program:</p>\n\n<p>sample output:</p>\n\n<pre><code>100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n100\n</code></pre>\n\n<p>If so, then the compiler is free to make that the <em>only</em> possible output for the program, in whichever way the compiler wants.  ie a main() that just puts out 100s.</p>\n\n<p>This is the \"as-if\" rule.</p>\n\n<p>And regardless of output, you can think of thread synchronization the same way - if thread A does <code>num++; num--;</code> and thread B reads <code>num</code> repeatedly, then a possible valid interleaving is that thread B never reads between <code>num++</code> and <code>num--</code>.  Since that interleaving is valid, the compiler is free to make that the <em>only</em> possible interleaving.  And just remove the incr/decr entirely.</p>\n\n<p>There are some interesting implications here:</p>\n\n<pre><code>while (working())\n    progress++;  // atomic, global\n</code></pre>\n\n<p>(ie imagine some other thread updates a progress bar UI based on <code>progress</code>)</p>\n\n<p>Can the compiler turn this into:</p>\n\n<pre><code>int local = 0;\nwhile (working())\n    local++;\n\nprogress += local;\n</code></pre>\n\n<p>probably that is valid.  But probably not what the programmer was hoping for :-(</p>\n\n<p>The committee is still working on this stuff.  Currently it \"works\" because compilers don't optimize atomics much.  But that is changing.</p>\n\n<p>And even if <code>progress</code> was also volatile, this would still be valid:</p>\n\n<pre><code>int local = 0;\nwhile (working())\n    local++;\n\nwhile (local--)\n    progress++;\n</code></pre>\n\n<p>:-/</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39413350, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}, {"body": "<p>Back in the day when x86 computers had one CPU, the use of a single instruction ensured that interrupts would not split the read/modify/write and if the memory would not be used as a DMA buffer too, it was atomic in fact (and C++ did not mention threads in the standard so this wasn\u2019t addresses).</p>\n\n<p>When it was rare to have a dual core (Pentium Pro) on a customer desktop, I effectively used this to avoid the LOCK prefix on a single core machine and improve performance.  </p>\n\n<p>Today, it would only help against multiple threads that were all set to the same CPU affinity, so the threads you are worried about would only come into play via time slice expiring and running the other thread on the same CPU (core).  That is not realistic.</p>\n\n<p>With modern x86/x64 processors, the single instruction is broken up into several <em>micro ops</em> and furthermore the memory reading and writing is buffered.  So different threads running on different CPUs will not only see this as non-atomic but may see inconsistent results concerning what it reads from memory and what it assumes other threads have read to that point in time: you need to add <strong>memory fenses</strong> to restore sane behavior.</p>\n", "title": "Can num++ be atomic for &#39;int num&#39;?", "answer_id": 39414316, "question_id": 39393850, "tags": ["c++", "c", "multithreading", "assembly", "atomic"]}], "question_id": 39393850}, {"body": "<p>I've installed git on a mac via <code>brew install git</code>. When I try to start gitk I get the following error:</p>\n\n<pre><code>Error in startup script: unknown color name \"lime\"\n    (processing \"-fore\" option)\n    invoked from within\n\"$ctext tag conf m2 -fore [lindex $mergecolors 2]\"\n    (procedure \"makewindow\" line 347)\n    invoked from within\n\"makewindow\"\n    (file \"/usr/local/bin/gitk\" line 12434)\n</code></pre>\n\n<p>It appears that my Mac doesn't have a color named <code>lime</code>. </p>\n\n<p>Can I add a lime color to the environment, or is there a better fix? </p>\n\n<p>The git version is 2.7.0, and the Mac is running Yosemite 10.10.5</p>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "tags": ["gitk"], "answer_count": 9, "link": "http://stackoverflow.com/questions/34637896/gitk-will-not-start-on-mac-unknown-color-name-lime", "answers": [{"body": "<p>I also found the same problem with exactly the same version of git and OSX. As a temporary workaround I suggest you to do the following:</p>\n\n<pre><code>cp /usr/local/bin/gitk /usr/local/bin/gitk.bkp\nvi /usr/local/bin/gitk\n</code></pre>\n\n<p>and replace all occurences of lime to any color you want (I used #99FF00). Here is the vim command:</p>\n\n<pre><code>:%s/lime/\"#99FF00\"/g\n</code></pre>\n\n<p>EDIT:\nPlease follow Keith B answer for permanent fix.</p>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 34649042, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>@Konrad Thanks for the help!! Although the answer is correct, please be aware that your command</p>\n\n<pre><code>:%s/lime/\"#99FF00\u201d/g\n</code></pre>\n\n<p>contains two different quotation marks, which will def. throw an error if copied &amp; pasted.</p>\n\n<p>So in order to go through with it, I had to do:</p>\n\n<pre><code>cp /usr/local/bin/gitk /usr/local/bin/gitk.bkp\nvi /usr/local/bin/gitk\n</code></pre>\n\n<p>And inside vim:</p>\n\n<pre><code>:%s/lime/\"#99FF00\"/g\n</code></pre>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 34682208, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>Follow the instructions above, but additionally do the following:</p>\n\n<p>before running vi:\nchmod +w /usr/local/bin/gitk</p>\n\n<p>after closing vi:\nchmod -w /usr/local/bin/gitk</p>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 34779312, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>You can check your version of Tcl/Tk by running <code>wish</code> and using the command <code>info patchlevel</code>. It appears that git 2.7.0, Tcl/Tk 8.5.9, and OS X 10.11 El Capitan do not work well together.</p>\n\n<p>I solved this problem by doing <code>brew cask install tcl</code>, which installed 8.6.4, and <code>gitk</code> works now.</p>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 34886815, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>If you don't want to use <code>Caskroom</code>, follow this:</p>\n\n<pre class=\"lang-bash prettyprint-override\"><code>$ brew install tcl-tk\n$ brew link tcl-tk --force\n</code></pre>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 36984698, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>Make sure, your path is set correctly when trying Keith B's solution. Running <code>brew doctor</code> showed, that I had /usr/bin before /usr/local/bin.</p>\n\n<pre><code>~ $ brew doctor\n...\nConsider setting your PATH so that /usr/local/bin\noccurs before /usr/bin. Here is a one-liner:\n  echo 'export PATH=\"/usr/local/bin:$PATH\"' &gt;&gt; ~/.bash_profile\n</code></pre>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 37372977, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>Useful addition: the replacement of occurrences of \"lime\" in the file \"gitk\" is sometimes not enough. You also need to process the file <code>~/.config/git/gitk</code> or remove it. </p>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 38062589, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>I was face same issue in ubuntu system. Then i flow below step to resolve issue.</p>\n\n<ol>\n<li><p>open /usr/bin/gitk file in any text editor</p></li>\n<li><p>search 'lime' keyword in file </p></li>\n<li><p>replace 'lime' to 'green' with all in file</p></li>\n<li><p>save file changes</p></li>\n<li><p>try again gitk then it works</p></li>\n</ol>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 38481685, "question_id": 34637896, "tags": ["gitk"]}, {"body": "<p>I am running MAC with Yosemetti and followed the below steps to resolve.</p>\n\n<pre><code>open /usr/local/git/bin/gitk file in any text editor\n\nsearch 'lime' keyword in file\n\nreplace 'lime' to 'green' with all in file\n\nsave file changes\n\ntry again gitk then it works\n</code></pre>\n\n<p>Thanks @kaushal sharma, followed your answer, with a change in the gitk path.</p>\n", "title": "gitk will not start on Mac: unknown color name &quot;lime&quot;", "answer_id": 39362856, "question_id": 34637896, "tags": ["gitk"]}], "question_id": 34637896}, {"body": "<p>I've found that <code>max</code> is slower than the <code>sort</code> function in Python 2 and 3.</p>\n\n<p>Python 2</p>\n\n<pre><code>$ python -m timeit -s 'import random;a=range(10000);random.shuffle(a)' 'a.sort();a[-1]'\n1000 loops, best of 3: 239 usec per loop\n$ python -m timeit -s 'import random;a=range(10000);random.shuffle(a)' 'max(a)'        \n1000 loops, best of 3: 342 usec per loop\n</code></pre>\n\n<p>Python 3</p>\n\n<pre><code>$ python3 -m timeit -s 'import random;a=list(range(10000));random.shuffle(a)' 'a.sort();a[-1]'\n1000 loops, best of 3: 252 usec per loop\n$ python3 -m timeit -s 'import random;a=list(range(10000));random.shuffle(a)' 'max(a)'\n1000 loops, best of 3: 371 usec per loop\n</code></pre>\n\n<p>Why <em>is</em> <code>max</code> (<code>O(n)</code>) slower than the <code>sort</code> function (<code>O(nlogn)</code>)?</p>\n", "title": "Why is max slower than sort in Python?", "tags": ["python", "sorting", "max", "python-internals"], "answer_count": 3, "link": "http://stackoverflow.com/questions/35014951/why-is-max-slower-than-sort-in-python", "answers": [{"body": "<p>You have to be very careful when using the <code>timeit</code> module in Python.</p>\n\n<pre><code>python -m timeit -s 'import random;a=range(10000);random.shuffle(a)' 'a.sort();a[-1]'\n</code></pre>\n\n<p>Here the initialisation code runs once to produce a randomised array <code>a</code>. Then the rest of the code is run several times. The first time it sorts the array, but every other time you are calling the sort method on an already sorted array. Only the fastest time is returned, so you are actually timing how long it takes Python to sort an already sorted array.</p>\n\n<p>Part of Python's sort algorithm is to detect when the array is already partly or completely sorted. When completely sorted it simply has to scan once through the array to detect this and then it stops.</p>\n\n<p>If instead you tried:</p>\n\n<pre><code>python -m timeit -s 'import random;a=range(100000);random.shuffle(a)' 'sorted(a)[-1]'\n</code></pre>\n\n<p>then the sort happens on every timing loop and you can see that the time for sorting an array is indeed much longer than to just find the maximum value.</p>\n\n<p><strong>Edit:</strong> @skyking's <a href=\"http://stackoverflow.com/a/35015156/641833\">answer</a> explains the part I left unexplained: <code>a.sort()</code> knows it is working on a list so can directly access the elements. <code>max(a)</code> works on any arbitrary iterable so has to use generic iteration.</p>\n", "title": "Why is max slower than sort in Python?", "answer_id": 35015128, "question_id": 35014951, "tags": ["python", "sorting", "max", "python-internals"]}, {"body": "<p>This could be because <code>l.sort</code> is a member of <code>list</code> while <code>max</code> is a generic function. This means that <code>l.sort</code> can rely on the internal representation of <code>list</code> while <code>max</code> will have to go through generic iterator protocol.</p>\n\n<p>This makes that each element fetch for <code>l.sort</code> is faster than each element fetch that <code>max</code> does.</p>\n\n<p>I assume that if you instead use <code>sorted(a)</code> you will get the result slower than <code>max(a)</code>.</p>\n", "title": "Why is max slower than sort in Python?", "answer_id": 35015156, "question_id": 35014951, "tags": ["python", "sorting", "max", "python-internals"]}, {"body": "<p>First off, note that <a href=\"https://hg.python.org/cpython/file/fadc4b53b840/Python/bltinmodule.c#l1478\"><code>max()</code> uses the iterator protocol</a>, while <a href=\"https://hg.python.org/cpython/file/fadc4b53b840/Objects/listobject.c#l1903\"><code>list.sort()</code> uses ad-hoc code</a>. Clearly, using an iterator is an important overhead, that's why you are observing that difference in timings.</p>\n\n<p>However, apart from that, your tests are not fair. You are running <code>a.sort()</code> on the same list more than once. The <a href=\"https://en.wikipedia.org/wiki/Timsort\">algorithm used by Python</a> is specifically designed to be fast for already (partially) sorted data. Your tests are saying that the algorithm is doing its job well.</p>\n\n<p>These are fair tests:</p>\n\n<pre><code>$ python3 -m timeit -s 'import random;a=list(range(10000));random.shuffle(a)' 'max(a[:])'\n1000 loops, best of 3: 227 usec per loop\n$ python3 -m timeit -s 'import random;a=list(range(10000));random.shuffle(a)' 'a[:].sort()'\n100 loops, best of 3: 2.28 msec per loop\n</code></pre>\n\n<p>Here I'm creating a copy of the list every time. As you can see, the order of magnitude of the results are different: micro- vs milliseconds, as we would expect.</p>\n\n<p>And remember: big-Oh specifies an upper bound! The lower bound for Python's sorting algorithm is \u03a9(<em>n</em>). Being O(<em>n</em> log <em>n</em>) does not automatically imply that every run takes a time proportional to <em>n</em> log <em>n</em>. It does not even imply that it needs to be slower than a O(<em>n</em>) algorithm, but that's another story. What's important to understand is that in some favorable cases, an O(<em>n</em> log <em>n</em>) algorithm may run in O(<em>n</em>) time or less.</p>\n", "title": "Why is max slower than sort in Python?", "answer_id": 35015364, "question_id": 35014951, "tags": ["python", "sorting", "max", "python-internals"]}], "question_id": 35014951}, {"body": "<p>I am a C# newbie and I just encounter a problem. There is a difference between C# and Java when dealing with the ternary operator (<code>? :</code>).</p>\n\n<p>In the following code segment, why does the 4th line not work? The compiler shows an error message of <code>there is no implicit conversion between 'int' and 'string'</code>. The 5th line does not work as well. Both <code>List</code>s are objects, aren't they?</p>\n\n<pre><code>int two = 2;\ndouble six = 6.0;\nWrite(two &gt; six ? two : six); //param: double\nWrite(two &gt; six ? two : \"6\"); //param: not object\nWrite(two &gt; six ? new List&lt;int&gt;() : new List&lt;string&gt;()); //param: not object\n</code></pre>\n\n<p>However, the same code works in Java:</p>\n\n<pre><code>int two = 2;\ndouble six = 6.0;\nSystem.out.println(two &gt; six ? two : six); //param: double\nSystem.out.println(two &gt; six ? two : \"6\"); //param: Object\nSystem.out.println(two &gt; six ? new ArrayList&lt;Integer&gt;()\n                   : new ArrayList&lt;String&gt;()); //param: Object\n</code></pre>\n\n<p>What language feature in C# is missing? If any, why is it not added?</p>\n", "title": "Difference between C# and Java&#39;s ternary operator (? :)", "tags": ["java", "c#", "ternary-operator", "conditional-operator"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35230846/difference-between-c-and-javas-ternary-operator", "answers": [{"body": "<p>This is pretty straightforward. There is no implicit conversion between string and int. the ternary operator needs the last two operands to have the same type.</p>\n\n<p>Try:</p>\n\n<pre><code>Write(two &gt; six ? two.ToString() : \"6\");\n</code></pre>\n", "title": "Difference between C# and Java&#39;s ternary operator (? :)", "answer_id": 35230883, "question_id": 35230846, "tags": ["java", "c#", "ternary-operator", "conditional-operator"]}, {"body": "<p>In both Java and C# (and most other languages), the result of an expression has a type. In the case of the ternary operator, there are two possible subexpressions evaluated for the result and both must have the same type. In the case of Java, an <code>int</code> variable can be converted to an <code>Integer</code> by autoboxing. Now since both <code>Integer</code> and <code>String</code> inherit from <code>Object</code>, they can be converted to the same type by a simple narrowing conversion.</p>\n\n<p>On the other hand, in C#, an <code>int</code> is a primitive and there is not implicit conversion to <code>string</code> or any other <code>object</code>.</p>\n", "title": "Difference between C# and Java&#39;s ternary operator (? :)", "answer_id": 35230929, "question_id": 35230846, "tags": ["java", "c#", "ternary-operator", "conditional-operator"]}, {"body": "<p>Looking through the <em>C# 5 Language Specification section 7.14: Conditional Operator</em> we can see the following:</p>\n\n<blockquote>\n  <ul>\n  <li><p>If x has type X and y has type Y then</p>\n  \n  <ul>\n  <li><p>If an implicit conversion (\u00a76.1) exists from X to Y, but not from Y to X, then Y is the type of the\n  conditional expression.</p></li>\n  <li><p>If an implicit conversion (\u00a76.1) exists from Y to X, but not from X to Y, then X is the type of the\n  conditional expression.</p></li>\n  <li><p>Otherwise, no expression type can be determined, and a compile-time error occurs</p></li>\n  </ul></li>\n  </ul>\n</blockquote>\n\n<p>In other words: it tries to find whether or not x and y can be converted to <strong>eachother</strong> and if not, a compilation error occurs. In our case <code>int</code> and <code>string</code> have no explicit or implicit conversion so it won't compile.</p>\n\n<p>Contrast this with the <a href=\"https://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.25\"><em>Java 7 Language Specification section 15.25: Conditional Operator</em></a>:</p>\n\n<blockquote>\n  <ul>\n  <li>If the second and third operands have the same type (which may be the null type), then that is the type of the conditional expression. (<strong>NO</strong>)</li>\n  <li>If one of the second and third operands is of primitive type T, and the type of the other is the result of applying boxing conversion (\u00a75.1.7) to T, then the type of the conditional expression is T. (<strong>NO</strong>)</li>\n  <li>If one of the second and third operands is of the null type and the type of the other is a reference type, then the type of the conditional expression is that reference type. (<strong>NO</strong>)</li>\n  <li>Otherwise, if the second and third operands have types that are convertible (\u00a75.1.8) to numeric types, then there are several cases: (<strong>NO</strong>)</li>\n  <li>Otherwise, the second and third operands are of types S1 and S2 respectively. Let T1 be the type that results from applying boxing conversion to S1, and let T2 be the type that results from applying boxing conversion to S2.<br>\n  The type of the conditional expression is the result of applying capture conversion (\u00a75.1.10) to lub(T1, T2) (\u00a715.12.2.7). (<strong>YES</strong>)</li>\n  </ul>\n</blockquote>\n\n<p>And, looking at <a href=\"https://docs.oracle.com/javase/specs/jls/se7/html/jls-15.html#jls-15.12.2.7\">section 15.12.2.7. Inferring Type Arguments Based on Actual Arguments</a> we can see it tries to find a common ancestor that will serve as the type used for the call which lands it with <code>Object</code>. <code>Object</code> <em>is</em> an acceptable argument so the call will work.</p>\n", "title": "Difference between C# and Java&#39;s ternary operator (? :)", "answer_id": 35231172, "question_id": 35230846, "tags": ["java", "c#", "ternary-operator", "conditional-operator"]}, {"body": "<p>Regarding the generics part:</p>\n\n<blockquote>\n<pre><code>two &gt; six ? new List&lt;int&gt;() : new List&lt;string&gt;()\n</code></pre>\n</blockquote>\n\n<p>In C#, the compiler tries to <em>convert</em> the right-hand expression parts to some common type; since <code>List&lt;int&gt;</code> and <code>List&lt;string&gt;</code> are two distinct constructed types, one can't be converted to the other.</p>\n\n<p>In Java, the compiler tries to <em>find a common supertype</em> instead of converting, so the compilation of the code involves the implicit use of <a href=\"http://docs.oracle.com/javase/tutorial/java/generics/wildcards.html\">wildcards</a> and <a href=\"http://docs.oracle.com/javase/tutorial/java/generics/erasure.html\">type erasure</a>;</p>\n\n<blockquote>\n<pre><code>two &gt; six ? new ArrayList&lt;Integer&gt;() : new ArrayList&lt;String&gt;()\n</code></pre>\n</blockquote>\n\n<p>has the compile type of <code>ArrayList&lt;?&gt;</code> (actually, it can be also <code>ArrayList&lt;? extends Serializable&gt;</code> or <code>ArrayList&lt;? extends Comparable&lt;?&gt;&gt;</code>, depending on use context, since they are both common generic supertypes) and runtime type of raw <code>ArrayList</code> (since it's the common raw supertype).</p>\n\n<p>For example <a href=\"http://ideone.com/BGX8I3\">(test it yourself)</a>,</p>\n\n<pre><code>void test( List&lt;?&gt; list ) {\n    System.out.println(\"foo\");\n}\n\nvoid test( ArrayList&lt;Integer&gt; list ) { // note: can't use List&lt;Integer&gt; here\n                                 // since both test() methods would clash after the erasure\n    System.out.println(\"bar\");\n}\n\nvoid test() {\n    test( true ? new ArrayList&lt;Object&gt;() : new ArrayList&lt;Object&gt;() ); // foo\n    test( true ? new ArrayList&lt;Integer&gt;() : new ArrayList&lt;Object&gt;() ); // foo \n    test( true ? new ArrayList&lt;Integer&gt;() : new ArrayList&lt;Integer&gt;() ); // bar\n} // compiler automagically binds the correct generic QED\n</code></pre>\n", "title": "Difference between C# and Java&#39;s ternary operator (? :)", "answer_id": 35231581, "question_id": 35230846, "tags": ["java", "c#", "ternary-operator", "conditional-operator"]}, {"body": "<p>The given answers are good; I would add to them that this rule of C# is a consequence of a more general design guideline. When asked to infer the type of an expression from one of several choices, <strong>C# chooses the unique best of them</strong>.  That is, if you give C# some choices like \"Giraffe, Mammal, Animal\" then it might choose the most general -- Animal -- or it might choose the most specific -- Giraffe -- depending on the circumstances. But it must choose <em>one of the choices it was actually given</em>.  C# never says \"my choices are between Cat and Dog, therefore I will deduce that Animal is the best choice\".  That wasn't a choice given, so C# cannot choose it.</p>\n\n<p>In the case of the ternary operator C# tries to choose the more general type of int and string, but neither is the more general type. Rather than picking a type that was not a choice in the first place, like object, C# decides that no type can be inferred.</p>\n\n<p>I note also that this is in keeping with another design principle of C#: if something looks wrong, tell the developer. The language does not say \"I'm going to guess what you meant and muddle on through if I can\". The language says \"I think you've written something confusing here, and I'm going to tell you about that.\"</p>\n\n<p>Also, I note that C# does not reason from the <em>variable</em> to the <em>assigned value</em>, but rather the other direction.  C# does not say \"you're assigning to an object variable therefore the expression must be convertible to object, therefore I will make sure that it is\".  Rather, C# says \"this expression must have a type, and I must be able to deduce that the type is compatible with object\". Since the expression does not have a type, an error is produced.</p>\n", "title": "Difference between C# and Java&#39;s ternary operator (? :)", "answer_id": 35235473, "question_id": 35230846, "tags": ["java", "c#", "ternary-operator", "conditional-operator"]}], "question_id": 35230846}, {"body": "<p>While I'm doing something on my app, I see that the navigation drawer on my app reduced its size. But I'm not doing anything on that.</p>\n\n<p><a href=\"http://i.stack.imgur.com/Mp6I0.png\"><img src=\"http://i.stack.imgur.com/Mp6I0.png\" alt=\"navigation drawer\"></a></p>\n\n<p>Then, after checking the code, I saw that setDrawerListener is deprecated. Does anyone has a solution to this? Any help will do. Thank you in advance!</p>\n\n<pre><code>drawerLayout.setDrawerListener(actionBarDrawerToggle);\n</code></pre>\n", "title": "Method setDrawerListener is deprecated", "tags": ["android", "drawerlayout", "deprecation-warning"], "answer_count": 4, "link": "http://stackoverflow.com/questions/35639454/method-setdrawerlistener-is-deprecated", "answers": [{"body": "<p>Use <code>addDrawerListener()</code> instead.</p>\n", "title": "Method setDrawerListener is deprecated", "answer_id": 35643506, "question_id": 35639454, "tags": ["android", "drawerlayout", "deprecation-warning"]}, {"body": "<p>I guess I'm gonna answer my question. The latest <code>navigationView</code> produces its default <code>android:layout_height</code> at almost <code>18dp</code> when you choose <code>\"wrap_content\"</code>. So, you must choose the <code>android:layout_height</code> that you want for your <code>navigationView</code> or simply make <code>android:layout_height=\"match_parent\"</code>.</p>\n\n<pre><code>&lt;android.support.design.widget.NavigationView\n    android:layout_width=\"320dp\"\n    android:layout_height=\"550dp\"\n    android:id=\"@+id/navigation_view_admin\"\n    android:layout_gravity=\"start\"&gt;\n\n&lt;/android.support.design.widget.NavigationView&gt;\n</code></pre>\n\n<p>Anyways, it's height automatically increases when you add an item in the navigation drawer.</p>\n\n<p>Lastly, Use <code>addDrawerListener()</code> instead of <code>setDrawerLister()</code> as Luxi Liu said.</p>\n", "title": "Method setDrawerListener is deprecated", "answer_id": 35666847, "question_id": 35639454, "tags": ["android", "drawerlayout", "deprecation-warning"]}, {"body": "<p>Replace: </p>\n\n<pre><code>drawer.setDrawerListener(...);\n</code></pre>\n\n<p>with </p>\n\n<pre><code>drawer.addDrawerListener(...);\n</code></pre>\n\n<blockquote>\n  <p><code>public void setDrawerListener(DrawerLayout.DrawerListener listener)</code> Sets a listener to be notified of drawer events.</p>\n  \n  <p>Note that this method is deprecated and you should use <code>addDrawerListener(DrawerLayout.DrawerListener)</code> to add a listener and <code>removeDrawerListener(DrawerLayout.DrawerListener)</code> to remove a registered listener.</p>\n</blockquote>\n", "title": "Method setDrawerListener is deprecated", "answer_id": 35678450, "question_id": 35639454, "tags": ["android", "drawerlayout", "deprecation-warning"]}, {"body": "<p>Replace this line `</p>\n\n<pre><code>drawerLayout.setDrawerListener(actionBarDrawerToggle);\n</code></pre>\n\n<p>with this </p>\n\n<pre><code>drawerLayout.addDrawerListener(actionBarDrawerToggle);\n</code></pre>\n\n<p><strong>example</strong> </p>\n\n<pre><code>  DrawerLayout drawer = (DrawerLayout) findViewById(R.id.drawer_layout);\n            ActionBarDrawerToggle toggle = new ActionBarDrawerToggle(\n                    this, drawer, toolbar, R.string.navigation_drawer_open, R.string.navigation_drawer_close);\n            assert drawer != null;\n            drawer.addDrawerListener(toggle);\n            toggle.syncState();\n\n            NavigationView navigationView = (NavigationView) findViewById(R.id.nav_view);\n            assert navigationView != null;\n            navigationView.setNavigationItemSelectedListener(this);\n</code></pre>\n", "title": "Method setDrawerListener is deprecated", "answer_id": 37299317, "question_id": 35639454, "tags": ["android", "drawerlayout", "deprecation-warning"]}], "question_id": 35639454}, {"body": "<p>I have updated android SDK to android preview N after updating, I am getting this xml rendering error. After clicking on details it is showing following stack trace How to avoid this</p>\n\n<p><a href=\"http://i.stack.imgur.com/PabMN.jpg\"><img src=\"http://i.stack.imgur.com/PabMN.jpg\" alt=\"?\"></a></p>\n\n<pre><code>org.jetbrains.android.uipreview.RenderingException: Failed to load the LayoutLib: com/android/layoutlib/bridge/Bridge : Unsupported major.minor version 52.0\nat org.jetbrains.android.uipreview.LayoutLibraryLoader.load(LayoutLibraryLoader.java:90)\nat org.jetbrains.android.sdk.AndroidTargetData.getLayoutLibrary(AndroidTargetData.java:180)\nat com.android.tools.idea.rendering.RenderService.createTask(RenderService.java:166)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager.doRender(AndroidLayoutPreviewToolWindowManager.java:649)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager.access$1700(AndroidLayoutPreviewToolWindowManager.java:80)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager$7$1.run(AndroidLayoutPreviewToolWindowManager.java:594)\nat com.intellij.openapi.progress.impl.CoreProgressManager$2.run(CoreProgressManager.java:152)\nat com.intellij.openapi.progress.impl.CoreProgressManager.registerIndicatorAndRun(CoreProgressManager.java:452)\nat com.intellij.openapi.progress.impl.CoreProgressManager.executeProcessUnderProgress(CoreProgressManager.java:402)\nat com.intellij.openapi.progress.impl.ProgressManagerImpl.executeProcessUnderProgress(ProgressManagerImpl.java:54)\nat com.intellij.openapi.progress.impl.CoreProgressManager.runProcess(CoreProgressManager.java:137)\nat org.jetbrains.android.uipreview.AndroidLayoutPreviewToolWindowManager$7.run(AndroidLayoutPreviewToolWindowManager.java:589)\nat com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:320)\nat com.intellij.util.ui.update.MergingUpdateQueue.execute(MergingUpdateQueue.java:310)\nat com.intellij.util.ui.update.MergingUpdateQueue$2.run(MergingUpdateQueue.java:254)\nat com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:269)\nat com.intellij.util.ui.update.MergingUpdateQueue.flush(MergingUpdateQueue.java:227)\nat com.intellij.util.ui.update.MergingUpdateQueue.run(MergingUpdateQueue.java:217)\nat com.intellij.util.concurrency.QueueProcessor.runSafely(QueueProcessor.java:238)\nat com.intellij.util.Alarm$Request$1.run(Alarm.java:351)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\n</code></pre>\n", "title": "XML Rendering errors Android preview N", "tags": ["android", "xml", "android-studio", "android-n-preview"], "answer_count": 4, "link": "http://stackoverflow.com/questions/35914758/xml-rendering-errors-android-preview-n", "answers": [{"body": "<p>This is bug in Android Studio.\nUsually you get error: <strong>Unsupported major.minor version 52.0</strong></p>\n\n<p><strong>WORKAROUND:</strong> If you have installed Android N, change Android rendering version with older one and the problem will disappear.</p>\n\n<p><strong>SOLUTION:</strong> Install Android SDK Tools 25.1.3 (tools)</p>\n\n<p><a href=\"http://i.stack.imgur.com/3HF0n.png\"><img src=\"http://i.stack.imgur.com/3HF0n.png\" alt=\"enter image description here\"></a></p>\n", "title": "XML Rendering errors Android preview N", "answer_id": 35917202, "question_id": 35914758, "tags": ["android", "xml", "android-studio", "android-n-preview"]}, {"body": "<h1>NEW SOLUTION:</h1>\n\n<p>After Updating <strong>Android SDK Tools 25.1.3</strong> It will remove automatically that bug. </p>\n\n<p><strong>Update your SDK Tools.</strong> </p>\n\n<p><a href=\"http://i.stack.imgur.com/XujNT.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/XujNT.png\" alt=\"enter image description here\"></a></p>\n\n<h1>OLD SOLUTION:</h1>\n\n<p>I got same problem after updating sdk with <a href=\"/questions/tagged/android-n-preview\" class=\"post-tag\" title=\"show questions tagged &#39;android-n-preview&#39;\" rel=\"tag\">android-n-preview</a></p>\n\n<p><a href=\"http://i.stack.imgur.com/d4e7p.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/d4e7p.png\" alt=\"enter image description here\"></a></p>\n\n<p>I think there is some bug in <strong>API level N</strong>, I really appreciate the <a href=\"http://stackoverflow.com/a/35917202/1318946\">answer</a> of <a href=\"http://stackoverflow.com/users/2298241/ivo-stoyanov\">Ivo Stoyanov</a> but I found another solution that is:</p>\n\n<p>First of all I have change <strong>Android Version to use when Rendering layout in IDE</strong> with <strong>API 23: Android 6.0</strong></p>\n\n<p><a href=\"http://i.stack.imgur.com/3oZB0.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/3oZB0.png\" alt=\"enter image description here\"></a></p>\n\n<p>Now Error is gone away. I don't know exactly why but Now Its doesn't seems to me.</p>\n\n<p>Thank you :)</p>\n", "title": "XML Rendering errors Android preview N", "answer_id": 35980065, "question_id": 35914758, "tags": ["android", "xml", "android-studio", "android-n-preview"]}, {"body": "<p>It cause while rendering version is  <strong>Andorid API 23: AndroidN(Preview)</strong>. Need to change it <a href=\"/questions/tagged/android-n\" class=\"post-tag\" title=\"show questions tagged &#39;android-n&#39;\" rel=\"tag\">android-n</a>. \nIt is a <a href=\"https://code.google.com/p/android/issues/detail?id=203683\">Issue 203683:  Unsupported major.minor version 52.0</a></p>\n", "title": "XML Rendering errors Android preview N", "answer_id": 36193516, "question_id": 35914758, "tags": ["android", "xml", "android-studio", "android-n-preview"]}, {"body": "<p>I had similar kind of error. I followed the steps mentioned above. I changed the preview version to older version and it worked.</p>\n\n<p><img src=\"http://i.stack.imgur.com/crFsm.png\" alt=\"Screenshot for the same\"></p>\n", "title": "XML Rendering errors Android preview N", "answer_id": 36854836, "question_id": 35914758, "tags": ["android", "xml", "android-studio", "android-n-preview"]}], "question_id": 35914758}, {"body": "<p>Could you tell me why <code>'?\\\\\\?'=='?\\\\\\\\?'</code> gives <code>True</code>? That drives me crazy and I can't find a reasonable answer...</p>\n\n<pre><code>&gt;&gt;&gt; list('?\\\\\\?')\n['?', '\\\\', '\\\\', '?']\n&gt;&gt;&gt; list('?\\\\\\\\?')\n['?', '\\\\', '\\\\', '?']\n</code></pre>\n", "title": "Why do 3 backslashes equal 4 in a Python string?", "tags": ["python", "python-2.7"], "answer_count": 5, "link": "http://stackoverflow.com/questions/35121288/why-do-3-backslashes-equal-4-in-a-python-string", "answers": [{"body": "<p>Basically, because python is slightly lenient in backslash processing. Quoting from <a href=\"https://docs.python.org/2.0/ref/strings.html\">https://docs.python.org/2.0/ref/strings.html</a> :</p>\n\n<blockquote>\n  <p>Unlike Standard C, all unrecognized escape sequences are left in the string unchanged, i.e., <em>the backslash is left in the string</em>.</p>\n</blockquote>\n\n<p>(Emphasis in the original)</p>\n\n<p>Therefore, in python, it isn't that three backslashes are equal to four, it's that when you follow backslash with a character like <code>?</code>, the two together come through as two characters, because <code>\\?</code> is not a recognized escape sequence.</p>\n", "title": "Why do 3 backslashes equal 4 in a Python string?", "answer_id": 35121348, "question_id": 35121288, "tags": ["python", "python-2.7"]}, {"body": "<p>Because <code>\\x</code> in a character string, when <code>x</code> is not one of the special backslashable characters like <code>n</code>, <code>r</code>, <code>t</code>, <code>0</code>, etc, evaluates to a string with a backslash and then an <code>x</code>.</p>\n\n<pre><code>&gt;&gt;&gt; '\\?'\n'\\\\?'\n</code></pre>\n", "title": "Why do 3 backslashes equal 4 in a Python string?", "answer_id": 35121350, "question_id": 35121288, "tags": ["python", "python-2.7"]}, {"body": "<p>From the python lexical analysis page under string literals at:\n<a href=\"https://docs.python.org/2/reference/lexical_analysis.html\" rel=\"nofollow\">https://docs.python.org/2/reference/lexical_analysis.html</a></p>\n\n<p>There is a table that lists all the recognized escape sequences.</p>\n\n<p>\\\\ is an escape sequence that is === \\</p>\n\n<p>\\? is not an escape sequence and is === \\?</p>\n\n<p>so '\\\\\\\\' is '\\\\' followed by '\\\\' which is '\\\\' (two escaped \\)</p>\n\n<p>and '\\\\\\' is '\\\\' followed by '\\' which is also '\\\\' (one escaped \\ and one raw \\)</p>\n\n<p>also, it should be noted that python does not distinguish between single and double quotes surrounding a string literal, unlike some other languages.</p>\n\n<p>So 'String' and \"String\" are the exact same thing in python, they do not affect the interpretation of escape sequences.</p>\n", "title": "Why do 3 backslashes equal 4 in a Python string?", "answer_id": 35121361, "question_id": 35121288, "tags": ["python", "python-2.7"]}, {"body": "<p>This is because backslash acts as an escape character for the character(s) immediately following it, if the combination represents a valid escape sequence. The dozen or so escape sequences are <a href=\"https://docs.python.org/2/reference/lexical_analysis.html?highlight=literal#string-literals\">listed here</a>. They include the obvious ones such as newline <code>\\n</code>, horizontal tab <code>\\t</code>, carriage return <code>\\r</code> and more obscure ones such as named unicode characters using <code>\\N{...}</code>, e.g. <code>\\N{WAVY DASH}</code> which represents unicode character <code>\\u3030</code>. The key point though is that if the escape sequence is not known, the character sequence is left in the string as is.</p>\n\n<p>Part of the problem might also be that the Python interpreter output is misleading you. This is because the backslashes are escaped when displayed. However, if you <em>print</em> those strings, you will see the extra backslashes disappear.</p>\n\n<pre><code>&gt;&gt;&gt; '?\\\\\\?'\n'?\\\\\\\\?'\n&gt;&gt;&gt; print('?\\\\\\?')\n?\\\\?\n&gt;&gt;&gt; '?\\\\\\?' == '?\\\\?'    # I don't know why you think this is True???\nFalse\n&gt;&gt;&gt; '?\\\\\\?' == r'?\\\\?'   # but if you use a raw string for '?\\\\?'\nTrue\n&gt;&gt;&gt; '?\\\\\\\\?' == '?\\\\\\?'  # this is the same string... see below\nTrue\n</code></pre>\n\n<p>For your specific examples, in the first case <code>'?\\\\\\?'</code>, the first <code>\\</code> escapes the second backslash leaving a single backslash, but the third backslash remains as a backslash because <code>\\?</code> is not a valid escape sequence. Hence the resulting string is <code>?\\\\?</code>.</p>\n\n<p>For the second case <code>'?\\\\\\\\?'</code>, the first backslash escapes the second, and the third backslash escapes the fourth which results in the string <code>?\\\\?</code>.</p>\n\n<p>So that's why three backslashes is the same as four:</p>\n\n<pre><code>&gt;&gt;&gt; '?\\\\\\?' == '?\\\\\\\\?'\nTrue\n</code></pre>\n\n<p>If you want to create a string with 3 backslashes you can escape each backslash:</p>\n\n<pre><code>&gt;&gt;&gt; '?\\\\\\\\\\\\?'\n'?\\\\\\\\\\\\?'\n&gt;&gt;&gt; print('?\\\\\\\\\\\\?')\n?\\\\\\?\n</code></pre>\n\n<p>or you might find \"raw\" strings more understandable:</p>\n\n<pre><code>&gt;&gt;&gt; r'?\\\\\\?'\n'?\\\\\\\\\\\\?'\n&gt;&gt;&gt; print(r'?\\\\\\?')\n?\\\\\\?\n</code></pre>\n\n<p>This turns of escape sequence processing for the string literal. See <a href=\"https://docs.python.org/2/reference/lexical_analysis.html?highlight=literal#string-literals\">String Literals</a> for more details.</p>\n", "title": "Why do 3 backslashes equal 4 in a Python string?", "answer_id": 35121689, "question_id": 35121288, "tags": ["python", "python-2.7"]}, {"body": "<p>mhawke's answer pretty much covers it, I just want to restate it in a more concise form and with minimal examples that illustrate this behaviour.</p>\n\n<p>I guess one thing to add is that escape processing moves from left to right, so that <code>\\n</code> first finds the backslash and then looks for a character to escape, then finds <code>n</code> and escapes it; <code>\\\\n</code> finds first backslash, finds second and escapes it, then finds <code>n</code> and sees it as a literal n; <code>\\?</code> finds backslash and looks for a char to escape, finds <code>?</code> which cannot be escaped, and so treats <code>\\</code> as a literal backslash.</p>\n\n<p>As mhawke noted, the key here is that interactive interpreter escapes the backslash when displaying a string. I'm guessing the reason for that is to ensure that text strings copied from interpreter into code editor are valid python strings. However, in this case this allowance for convenience causes confusion.</p>\n\n<pre><code>&gt;&gt;&gt; print('\\?') # \\? is not a valid escape code so backslash is left as-is\n\\?\n&gt;&gt;&gt; print('\\\\?') # \\\\ is a valid escape code, resulting in a single backslash\n'\\?'\n\n&gt;&gt;&gt; '\\?' # same as first example except that interactive interpreter escapes the backslash\n\\\\?\n&gt;&gt;&gt; '\\\\?' # same as second example, backslash is again escaped\n\\\\?\n</code></pre>\n", "title": "Why do 3 backslashes equal 4 in a Python string?", "answer_id": 35387084, "question_id": 35121288, "tags": ["python", "python-2.7"]}], "question_id": 35121288}, {"body": "<p>We are developing a program which receives and forwards \"messages\", while keeping a temporary history of those messages, so that it can tell you the message history if requested. Messages are identified numerically, are typically around 1 kilobyte in size, and we need to keep hundreds of thousands of these messages.</p>\n\n<p>We wish to optimize this program for latency: the time between sending and receiving a message must be below 10 milliseconds.</p>\n\n<p>The program is written in Haskell and compiled with GHC. However, we have found that garbage collection pauses are far too long for our latency requirements: over 100 milliseconds in our real-world program.</p>\n\n<p>The following program is a simplified version of our application. It uses a <code>Data.Map.Strict</code> to store messages. Messages are <code>ByteString</code>s identified by an <code>Int</code>. 1,000,000 messages are inserted in increasing numeric order, and the oldest messages are continually removed to keep the history at a maximum of 200,000 messages.</p>\n\n<pre><code>module Main (main) where\n\nimport qualified Control.Exception as Exception\nimport qualified Control.Monad as Monad\nimport qualified Data.ByteString as ByteString\nimport qualified Data.Map.Strict as Map\n\ndata Msg = Msg !Int !ByteString.ByteString\n\ntype Chan = Map.Map Int ByteString.ByteString\n\nmessage :: Int -&gt; Msg\nmessage n = Msg n (ByteString.replicate 1024 (fromIntegral n))\n\npushMsg :: Chan -&gt; Msg -&gt; IO Chan\npushMsg chan (Msg msgId msgContent) =\n  Exception.evaluate $\n    let\n      inserted = Map.insert msgId msgContent chan\n    in\n      if 200000 &lt; Map.size inserted\n      then Map.deleteMin inserted\n      else inserted\n\nmain :: IO ()\nmain = Monad.foldM_ pushMsg Map.empty (map message [1..1000000])\n</code></pre>\n\n<p>We compiled and ran this program using:</p>\n\n<pre><code>$ ghc --version\nThe Glorious Glasgow Haskell Compilation System, version 7.10.3\n$ ghc -O2 -optc-O3 Main.hs\n$ ./Main +RTS -s\n   3,116,460,096 bytes allocated in the heap\n     385,101,600 bytes copied during GC\n     235,234,800 bytes maximum residency (14 sample(s))\n     124,137,808 bytes maximum slop\n             600 MB total memory in use (0 MB lost due to fragmentation)\n\n                                     Tot time (elapsed)  Avg pause  Max pause\n  Gen  0      6558 colls,     0 par    0.238s   0.280s     0.0000s    0.0012s\n  Gen  1        14 colls,     0 par    0.179s   0.250s     0.0179s    0.0515s\n\n  INIT    time    0.000s  (  0.000s elapsed)\n  MUT     time    0.652s  (  0.745s elapsed)\n  GC      time    0.417s  (  0.530s elapsed)\n  EXIT    time    0.010s  (  0.052s elapsed)\n  Total   time    1.079s  (  1.326s elapsed)\n\n  %GC     time      38.6%  (40.0% elapsed)\n\n  Alloc rate    4,780,213,353 bytes per MUT second\n\n  Productivity  61.4% of total user, 49.9% of total elapsed\n</code></pre>\n\n<p>The important metric here is the \"max pause\" of 0.0515s, or 51 milliseconds. We wish to reduce this by at least an order of magnitude.</p>\n\n<p>Experimentation shows that the length of a GC pause is determined by the number of messages in the history. The relationship is roughly linear, or perhaps super-linear. The following table shows this relationship. (<a href=\"https://github.com/jameshfisher/ghc_gc_benchmarks\">You can see our benchmarking tests here</a>, and <a href=\"https://docs.google.com/spreadsheets/d/1mDXrk8bqPb_nsR__pzHtSxwO8a_6bPVL8hd3Y9lCIm0/edit?usp=sharing\">some charts here</a>.)</p>\n\n<pre><code>msgs history length  max GC pause (ms)\n===================  =================\n12500                                3\n25000                                6\n50000                               13\n100000                              30\n200000                              56\n400000                             104\n800000                             199\n1600000                            487\n3200000                           1957\n6400000                           5378\n</code></pre>\n\n<p>We have experimented with several other variables to find whether they can reduce this latency, none of which make a big difference. Among these unimportant variables are: optimization (<code>-O</code>, <code>-O2</code>); RTS GC options (<code>-G</code>, <code>-H</code>, <code>-A</code>, <code>-c</code>), number of cores (<code>-N</code>), different data structures (<code>Data.Sequence</code>), the size of messages, and the amount of generated short-lived garbage. The overwhelming determining factor is the number of messages in the history.</p>\n\n<p>Our working theory is that the pauses are linear in the number of messages because each GC cycle has to walk over all the working accessible memory and copy it, which are clearly linear operations.</p>\n\n<p>Questions:</p>\n\n<ul>\n<li>Is this linear-time theory correct? Can the length of GC pauses be expressed in this simple way, or is the reality more complex?</li>\n<li>If GC pause is linear in the working memory, is there any way to reduce the constant factors involved?</li>\n<li>Are there any options for incremental GC, or anything like it? We can only see research papers. We are very willing to trade throughput for lower latency.</li>\n<li>Are there any ways to \"partition\" memory for smaller, GC cycles, other than splitting into multiple processes?</li>\n</ul>\n", "title": "Reducing garbage-collection pause time in a Haskell program", "tags": ["performance", "haskell", "garbage-collection", "ghc", "latency"], "answer_count": 4, "link": "http://stackoverflow.com/questions/36772017/reducing-garbage-collection-pause-time-in-a-haskell-program", "answers": [{"body": "<p>You're actually doing pretty well to have a 51ms pause time with over 200Mb of live data.  The system I work on has a larger max pause time with half that amount of live data.</p>\n\n<p>Your assumption is correct, the major GC pause time is directly proportional to the amount of live data, and unfortunately there's no way around that with GHC as it stands.  We experimented with incremental GC in the past, but it was a research project and didn't reach the level of maturity needed to fold it into the released GHC.</p>\n\n<p>One thing that we're hoping will help with this in the future is compact regions: <a href=\"https://phabricator.haskell.org/D1264\">https://phabricator.haskell.org/D1264</a>.  It's a kind of manual memory management where you compact a structure in the heap, and the GC doesn't have to traverse it.  It works best for long-lived data, but perhaps it will be good enough to use for individual messages in your setting.  We're aiming to have it in GHC 8.2.0.</p>\n\n<p>If you're in a distributed setting and have a load-balancer of some kind there are tricks you can play to avoid taking the pause hit, you basically make sure that the load-balancer doesn't send requests to machines that are about to do a major GC, and of course make sure that the machine still completes the GC even though it isn't getting requests.</p>\n", "title": "Reducing garbage-collection pause time in a Haskell program", "answer_id": 36779227, "question_id": 36772017, "tags": ["performance", "haskell", "garbage-collection", "ghc", "latency"]}, {"body": "<p>Well you found the limitation of languages with GC: They aren't fit for hardcore real-time systems.</p>\n\n<p>You have 2 options:</p>\n\n<p>1st Increase heap size and use a 2 level caching system, oldest messages are sent to disk and you keep newest messages on memory, you can do this by using OS paging. The problem, though with this solution is that paging can be expensive depending on the reading capabilities of the secondary memory unit used.</p>\n\n<p>2nd Program that solution using 'C' and interface it with FFI to haskell. That way you can do your own memory management. This would be the best option as you can control the memory you need by yourself.</p>\n", "title": "Reducing garbage-collection pause time in a Haskell program", "answer_id": 36790800, "question_id": 36772017, "tags": ["performance", "haskell", "garbage-collection", "ghc", "latency"]}, {"body": "<p>I have to agree with the others - if you have hard real-time constraints, then using a GC language is not ideal.</p>\n\n<p>However, you might consider experimenting with other available data structures rather than just Data.Map.</p>\n\n<p>I rewrote it using Data.Sequence and got some promising improvements:</p>\n\n<pre><code>msgs history length  max GC pause (ms)\n===================  =================\n12500                              0.7\n25000                              1.4\n50000                              2.8\n100000                             5.4\n200000                            10.9\n400000                            21.8\n800000                            46\n1600000                           87\n3200000                          175\n6400000                          350\n</code></pre>\n\n<p>Even though you're optimising for latency, I noticed other metrics improving too.  In the 200000 case, execution time drops from 1.5s to 0.2s, and total memory usage drops from 600MB to 27MB.</p>\n\n<p>I should note that I cheated by tweaking the design:</p>\n\n<ul>\n<li>I removed the <code>Int</code> from the <code>Msg</code>, so it's not in two places.</li>\n<li>Instead of using a Map from <code>Int</code>s to <code>ByteString</code>s, I used a <code>Sequence</code> of <code>ByteString</code>s, and instead of one <code>Int</code> per message, I think it can be done with one <code>Int</code> for the whole <code>Sequence</code>.  Assuming messages can't get reordered, you can use a single offset to translate which message you want to where it sits in the queue.  </li>\n</ul>\n\n<p>(I included an additional function <code>getMsg</code> to demonstrate that.) </p>\n\n<pre><code>{-# LANGUAGE BangPatterns #-}\n\nimport qualified Control.Exception as Exception\nimport qualified Control.Monad as Monad\nimport qualified Data.ByteString as ByteString\nimport Data.Sequence as S\n\nnewtype Msg = Msg ByteString.ByteString\n\ndata Chan = Chan Int (Seq ByteString.ByteString)\n\nmessage :: Int -&gt; Msg\nmessage n = Msg (ByteString.replicate 1024 (fromIntegral n))\n\nmaxSize :: Int\nmaxSize = 200000\n\npushMsg :: Chan -&gt; Msg -&gt; IO Chan\npushMsg (Chan !offset sq) (Msg msgContent) =\n    Exception.evaluate $\n        let newSize = 1 + S.length sq\n            newSq = sq |&gt; msgContent\n        in\n        if newSize &lt;= maxSize\n            then Chan offset newSq\n            else\n                case S.viewl newSq of\n                    (_ :&lt; newSq') -&gt; Chan (offset+1) newSq'\n                    S.EmptyL -&gt; error \"Can't happen\"\n\ngetMsg :: Chan -&gt; Int -&gt; Maybe Msg\ngetMsg (Chan offset sq) i_ = getMsg' (i_ - offset)\n    where\n    getMsg' i\n        | i &lt; 0            = Nothing\n        | i &gt;= S.length sq = Nothing\n        | otherwise        = Just (Msg (S.index sq i))\n\nmain :: IO ()\nmain = Monad.foldM_ pushMsg (Chan 0 S.empty) (map message [1..5 * maxSize])\n</code></pre>\n", "title": "Reducing garbage-collection pause time in a Haskell program", "answer_id": 36800153, "question_id": 36772017, "tags": ["performance", "haskell", "garbage-collection", "ghc", "latency"]}, {"body": "<p>I've tried your code snippet with a ringbuffer approach using <code>IOVector</code> as the underlying data structure. On my system (GHC 7.10.3, same compilation options) this resulted in a reduction of max time (the metric you mentioned in your OP) by ~22%.</p>\n\n<p>NB. I made two assumptions here:</p>\n\n<ol>\n<li>A mutable data structure is an okay fit for the problem (I guess message passing implies IO anyhow)</li>\n<li>Your messageId's are continuous</li>\n</ol>\n\n<p>With some additional <code>Int</code> parameter and arithmetic (like when messageId's are reset back to 0 or <code>minBound</code>) it should then be straightforward to determine whether a certain message is still in the history and retrieve it form the corresponding index in the ringbuffer.</p>\n\n<p>For your testing pleasure:</p>\n\n<pre><code>import qualified Control.Exception as Exception\nimport qualified Control.Monad as Monad\nimport qualified Data.ByteString as ByteString\nimport qualified Data.Map.Strict as Map\n\nimport qualified Data.Vector.Mutable as Vector\n\ndata Msg = Msg !Int !ByteString.ByteString\n\ntype Chan = Map.Map Int ByteString.ByteString\n\ndata Chan2 = Chan2\n    { next          :: !Int\n    , maxId         :: !Int\n    , ringBuffer    :: !(Vector.IOVector ByteString.ByteString)\n    }\n\nchanSize :: Int\nchanSize = 200000\n\nmessage :: Int -&gt; Msg\nmessage n = Msg n (ByteString.replicate 1024 (fromIntegral n))\n\n\nnewChan2 :: IO Chan2\nnewChan2 = Chan2 0 0 &lt;$&gt; Vector.unsafeNew chanSize\n\npushMsg2 :: Chan2 -&gt; Msg -&gt; IO Chan2\npushMsg2 (Chan2 ix _ store) (Msg msgId msgContent) =\n    let ix' = if ix == chanSize then 0 else ix + 1\n    in Vector.unsafeWrite store ix' msgContent &gt;&gt; return (Chan2 ix' msgId store)\n\npushMsg :: Chan -&gt; Msg -&gt; IO Chan\npushMsg chan (Msg msgId msgContent) =\n  Exception.evaluate $\n    let\n      inserted = Map.insert msgId msgContent chan\n    in\n      if chanSize &lt; Map.size inserted\n      then Map.deleteMin inserted\n      else inserted\n\nmain, main1, main2 :: IO ()\n\nmain = main2\n\nmain1 = Monad.foldM_ pushMsg Map.empty (map message [1..1000000])\n\nmain2 = newChan2 &gt;&gt;= \\c -&gt; Monad.foldM_ pushMsg2 c (map message [1..1000000])\n</code></pre>\n", "title": "Reducing garbage-collection pause time in a Haskell program", "answer_id": 36800350, "question_id": 36772017, "tags": ["performance", "haskell", "garbage-collection", "ghc", "latency"]}], "question_id": 36772017}, {"body": "<p>The following code compiles without problems:</p>\n\n<pre><code>int main() {\n    printf(\"Hi\" \"Bye\");\n}\n</code></pre>\n\n<p>However, this does not compile:</p>\n\n<pre><code>int main() {\n    int test = 0;\n    printf(\"Hi\" (test ? \"Bye\" : \"Goodbye\"));\n}\n</code></pre>\n\n<p>What is the reason for that?</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "tags": ["c", "string", "concatenation"], "answer_count": 8, "link": "http://stackoverflow.com/questions/37259472/why-does-c-not-allow-concatenating-strings-when-using-the-ternary-operator", "answers": [{"body": "<p>According to the C Standard (5.1.1.2 Translation phases)</p>\n\n<blockquote>\n  <p>1 The precedence among the syntax rules of translation is specified by\n  the following phases.6)</p>\n  \n  <ol start=\"6\">\n  <li>Adjacent string literal tokens are concatenated.</li>\n  </ol>\n</blockquote>\n\n<p>And only after that</p>\n\n<blockquote>\n  <ol start=\"7\">\n  <li>White-space characters separating tokens are no longer significant. Each \n  preprocessing token is converted into a token. <strong>The resulting\n  tokens are syntactically and semantically analyzed and translated as a\n  translation unit</strong>.</li>\n  </ol>\n</blockquote>\n\n<p>In this construction</p>\n\n<pre><code>\"Hi\" (test ? \"Bye\" : \"Goodbye\")\n</code></pre>\n\n<p>there are no adjacent string literal tokens. So this construction is invalid.</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37259811, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<p>As per the C11 standard, chapter \u00a75.1.1.2, concatenation of adjacent string literals:</p>\n\n<blockquote>\n  <p>Adjacent string literal tokens are concatenated.</p>\n</blockquote>\n\n<p>happens in <strong>translation phase</strong>. On the other hand:</p>\n\n<pre><code>printf(\"Hi\" (test ? \"Bye\" : \"Goodbye\"));\n</code></pre>\n\n<p>involves the conditional operator, which is evaluated at <strong>run-time</strong>. So, at compile time, during the translation phase, there are no adjacent string literals present, hence the concatenation is not possible. The syntax is invalid and thus reported by your compiler.</p>\n\n<hr>\n\n<p>To elaborate a bit on the <strong><em>why</em></strong> part, during the preprocessing phase, the adjacent string literals are concatenated and represented as a single <em>string literal</em> (token). The storage is allocated accordingly and the concatenated string literal is considered as a <em>single entity</em> (one string literal).</p>\n\n<p>On the other hand, in case of run-time concatenation, the destination should have enough memory to hold the concatenated <em>string literal</em> otherwise, there will be no way that the <em>expected</em> concatenated output can be accessed. Now, in case of <em>string literals</em>, they are already allocated memory at compile-time and cannot be <em>extended</em> to fit in any more incoming input <em>into</em> or <em>appended to</em> the original content. In other words, there will be no way that the concatenated result can be accessed (presented) as a single <em>string literal</em>. So, this construct in inherently incorrect.</p>\n\n<p>Just FYI, for run-time <strong>string</strong> (<em>not literals</em>) concatenation, we have the library function <a href=\"http://linux.die.net/man/3/strcat\" rel=\"nofollow\"><code>strcat()</code></a> which concatenates two <em>strings</em>. Notice, the description mentions:</p>\n\n<blockquote>\n  <p><code>char *strcat(char * restrict s1,const char * restrict s2);</code></p>\n  \n  <p>The <code>strcat()</code> function appends a copy of the string pointed to by <code>s2</code> (including the\n  terminating null character) to the end of the <strong>string pointed to by <code>s1</code></strong>. The initial character\n  of <code>s2</code> overwrites the null character at the end of <code>s1</code>. [...]</p>\n</blockquote>\n\n<p>So, we can see, the <code>s1</code> is a <strong>string</strong>, not a <strong>string literal</strong>. However, as the content of <code>s2</code> is not altered in any way, it can very well be a <em>string literal</em>.</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37259911, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<blockquote>\n  <p>What is the reason for that?</p>\n</blockquote>\n\n<p>Your code using ternary operator conditionally chooses between two string literals. No matter condition known or unknown, this can't be evaluated at compile time, so it can't compile. Even this statement <code>printf(\"Hi\" (1 ? \"Bye\" : \"Goodbye\"));</code> wouldn't compile. The reason is in depth explained in the answers above. Another possibility of <em>making such a statement using ternary operator valid to compile</em>, would also involve a <em>format tag</em> and the result of the ternary operator statement formatted as <em>additional argument</em> to <code>printf</code>. Even then, <code>printf()</code> printout would give an impression of \"having concatenated\" those strings only at, and as early as <em>runtime</em>.</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    int test = 0;\n    printf(\"Hi %s\\n\", (test ? \"Bye\" : \"Goodbye\")); //specify format and print as result\n}\n</code></pre>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37259946, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<p>String literal concatenation is performed by the preprocessor at compile-time. There is no way for this concatenation to be aware of the value of <code>test</code>, which is not known until the program actually executes. Therefore, these string literals cannot be concatenated.</p>\n\n<p>Because the general case is that you wouldn't have a construction like this for values known at compile-time, the C standard was designed to restrict the auto-concatenation feature to the most basic case: when the literals are literally right alongside each other.</p>\n\n<p>But even if it did not word this restriction in that way, or if the restriction were differently-constructed, your example would still be impossible to realise without making the concatenation a runtime process. And, for that, we have the library functions such as <code>strcat</code>.</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37264592, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<p>Because C has no <code>string</code> type. String literals are compiled to <code>char</code> arrays, referenced by a <code>char*</code> pointer.</p>\n\n<p>C allows adjacent <em>literals</em> to be combined <em>at compile-time</em>, as in your first example. The C compiler itself has some knowledge about strings. But this information is <em>not present at runtime,</em> and thus concatenation cannot happen.</p>\n\n<p>During the compilation process, your first example is \"translated\" to:</p>\n\n<pre><code>int main() {\n    static const char char_ptr_1[] = {'H', 'i', 'B', 'y', 'e', '\\0'};\n    printf(char_ptr_1);\n}\n</code></pre>\n\n<p>Note how the two strings are combined to a single static array by the compiler, before the program ever executes.</p>\n\n<p>However, your second example is \"translated\" to something like this:</p>\n\n<pre><code>int main() {\n    static const char char_ptr_1[] = {'H', 'i', '\\0'};\n    static const char char_ptr_2[] = {'B', 'y', 'e', '\\0'};\n    static const char char_ptr_3[] = {'G', 'o', 'o', 'd', 'b', 'y', 'e', '\\0'};\n    int test = 0;\n    printf(char_ptr_1 (test ? char_ptr_2 : char_ptr_3));\n}\n</code></pre>\n\n<p>It should be clear why this does not compile. The ternary operator <code>?</code> is evaluated at runtime, not compile-time, when the \"strings\" no longer exist as such, but only as simple <code>char</code> arrays, referenced by <code>char*</code> pointers. Unlike adjacent <em>string literals</em>, adjacent <em>char pointers</em> are simply a syntax error.</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37264779, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<p>If you really want to have both branches produce compile-time string constants to be chosen at runtime, you'll need a macro.</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n#define ccat(s, t, a, b) ((t)?(s a):(s b))\n\nint\nmain ( int argc, char **argv){\n  printf(\"%s\\n\", ccat(\"hello \", argc &gt; 2 , \"y'all\", \"you\"));\n  return 0;\n}\n</code></pre>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37267341, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<p>In <code>printf(\"Hi\" \"Bye\");</code> you have two consecutive arrays of char which the compiler can make into a single array.</p>\n\n<p>In <code>printf(\"Hi\" (test ? \"Bye\" : \"Goodbye\"));</code> you have one array followed by a pointer to char (an array converted to a pointer to its first element). The compiler cannot <em>merge</em> an array and a pointer.</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37272438, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}, {"body": "<p>This does not compile because the parameter list for the printf function is</p>\n\n<pre><code>(const char *format, ...)\n</code></pre>\n\n<p>and</p>\n\n<pre><code>(\"Hi\" (test ? \"Bye\" : \"Goodbye\"))\n</code></pre>\n\n<p>does not fit the parameter list.</p>\n\n<p>gcc tries to make sense of it by imagining that</p>\n\n<pre><code>(test ? \"Bye\" : \"Goodbye\")\n</code></pre>\n\n<p>is a parameter list, and complains that \"Hi\" is not a function.</p>\n", "title": "Why does C not allow concatenating strings when using the ternary operator?", "answer_id": 37283626, "question_id": 37259472, "tags": ["c", "string", "concatenation"]}], "question_id": 37259472}, {"body": "<p>I have recently applied <a href=\"http://stackoverflow.com/questions/30379311/fast-way-to-take-average-of-every-n-rows-in-a-npy-array\">this</a> solution for averaging every N rows of matrix.\nAlthough the solution works in general I had problems when applied to a 7x1 array. I have noticed that the problem is when using the <code>-=</code> operator.\nTo make a small example:</p>\n\n<pre><code>import numpy as np\n\na = np.array([1,2,3])\nb = np.copy(a)\n\na[1:] -= a[:-1]\nb[1:] = b[1:] - b[:-1]\n\nprint a\nprint b\n</code></pre>\n\n<p>which outputs:</p>\n\n<pre><code>[1 1 2]\n[1 1 1]\n</code></pre>\n\n<p>So, in the case of an array <code>a -= b</code> produces a different result than <code>a = a - b</code>. I thought until now that these two ways are exactly the same. What is the difference?</p>\n\n<p>How come the method I am mentioning for summing every N rows in a matrix is working e.g. for a 7x4 matrix but not for a 7x1 array?</p>\n", "title": "Difference between a -= b and a = a - b in Python", "tags": ["python", "arrays", "numpy", "in-place"], "answer_count": 3, "link": "http://stackoverflow.com/questions/35036126/difference-between-a-b-and-a-a-b-in-python", "answers": [{"body": "<p>Internally, the difference is that this:</p>\n\n<pre><code>a[1:] -= a[:-1]\n</code></pre>\n\n<p>is equivalent to this:</p>\n\n<pre><code>a[1:] = a[1:].__isub__(a[:-1])\na.__setitem__(slice(1, None, None), a.__getitem__(slice(1, None, None)).__isub__(a.__getitem__(slice(1, None, None)))\n</code></pre>\n\n<p>while this:</p>\n\n<pre><code>b[1:] = b[1:] - b[:-1]\n</code></pre>\n\n<p>maps to this:</p>\n\n<pre><code>b[1:] = b[1:].__sub__(b[:-1])\nb.__setitem__(slice(1, None, None), b.__getitem__(slice(1, None, None)).__sub__(b.__getitem__(slice(1, None, None)))\n</code></pre>\n\n<p>In some cases, <code>__sub__()</code> and <code>__isub__()</code> work in a similar way. But mutable objects should mutate and return themselves when using <code>__isub__()</code>, while they should return a new object with <code>__sub__()</code>.</p>\n\n<p>Applying slice operations on numpy objects creates views on them, so using them directly accesses the memory of the \"original\" object.</p>\n", "title": "Difference between a -= b and a = a - b in Python", "answer_id": 35036306, "question_id": 35036126, "tags": ["python", "arrays", "numpy", "in-place"]}, {"body": "<p>Mutating arrays while they're being used in computations can lead to unexpected results!</p>\n\n<p>In the example in the question, subtraction with <code>-=</code> modifies the second element of <code>a</code> and then immediately uses that <em>modified</em> second element in the operation on the third element of <code>a</code>.</p>\n\n<p>Here is what happens with <code>a[1:] -= a[:-1]</code> step by step:</p>\n\n<ul>\n<li><p><code>a</code> is the array with the data <code>[1, 2, 3]</code>.</p></li>\n<li><p>We have two views onto this data: <code>a[1:]</code> is <code>[2, 3]</code>, and <code>a[:-1]</code> is <code>[1, 2]</code>.</p></li>\n<li><p>The inplace subtraction <code>-=</code> begins. The first element of <code>a[:-1]</code>, 1, is subtracted from the first element of <code>a[1:]</code>. This has modified <code>a</code> to be <code>[1, 1, 3]</code>. Now we have that <code>a[1:]</code> is a view of the data <code>[1, 3]</code>, and <code>a[:-1]</code> is a view of the data <code>[1, 1]</code> (the second element of array <code>a</code> has been changed).</p></li>\n<li><p><code>a[:-1]</code> is now <code>[1, 1]</code> and NumPy must now subtract its second element <em>which is 1</em> (not 2 anymore!) from the second element of <code>a[1:]</code>. This makes <code>a[1:]</code> a view of the values <code>[1, 2]</code>.</p></li>\n<li><p><code>a</code> is now an array with the values <code>[1, 1, 2]</code>.</p></li>\n</ul>\n\n<p><code>b[1:] = b[1:] - b[:-1]</code> does not have this problem because <code>b[1:] - b[:-1]</code> creates a <em>new</em> array first and then assigns the values in this array to <code>b[1:]</code>. It does not modify <code>b</code> itself during the subtraction, so the views <code>b[1:]</code> and <code>b[:-1]</code> do not change.</p>\n\n<hr>\n\n<p>The general advice is to avoid modifying one view inplace with another if they overlap. This includes the operators <code>-=</code>, <code>*=</code>, etc. and using the <code>out</code> parameter in universal functions (like <code>np.subtract</code> and <code>np.multiply</code>) to write back to one of the arrays.</p>\n", "title": "Difference between a -= b and a = a - b in Python", "answer_id": 35036528, "question_id": 35036126, "tags": ["python", "arrays", "numpy", "in-place"]}, {"body": "<p><a href=\"http://legacy.python.org/dev/peps/pep-0203/\">The docs</a> say :</p>\n\n<blockquote>\n  <p>The idea behind augmented assignment in Python is that it isn't\n      just an easier way to write the common practice of storing the\n      result of a binary operation in its left-hand operand, but also a\n      way for the left-hand operand in question to know that it should\n      operate `on itself', rather than creating a modified copy of\n      itself.</p>\n</blockquote>\n\n<p>As a thumb rule, augmented substraction (<code>x-=y</code>) is <code>x.__isub__(y)</code>,  for <strong>IN</strong>-place operation  <strong>IF</strong> possible, when normal substraction (<code>x = x-y</code>) is <code>x=x.__sub__(y)</code> .  On non mutable objects like integers it's equivalent.  But for mutable ones like arrays or lists, as in your example, they can be very different things. </p>\n", "title": "Difference between a -= b and a = a - b in Python", "answer_id": 35036836, "question_id": 35036126, "tags": ["python", "arrays", "numpy", "in-place"]}], "question_id": 35036126}, {"body": "<p>Recently I've made a switch to Node v.6, and It started creating more and more problems with running normal builds grunt/gulp/webpack</p>\n\n<p>For example:</p>\n\n<pre><code>$ gulp\n[14:02:20] Local gulp not found in ~/_Other/angular-2-ts/angular2-seed\n[14:02:20] Try running: npm install gulp\n</code></pre>\n\n<p>while gulp and all other plugins and modules are installed (and even re-installed via <code>npm -rf node_modules</code>) in /node_modules folder.</p>\n\n<p>Most of those errors have line like</p>\n\n<pre><code>(node:42) fs: re-evaluating native module sources is not supported. \nIf you are using the graceful-fs module, \nplease update it to a more recent version.\n</code></pre>\n\n<p>with 42 as arbitrary number</p>\n\n<p>Like in that issue I've submitted - in angular2-seed repo <a href=\"https://github.com/mgechev/angular2-seed/issues/902\">https://github.com/mgechev/angular2-seed/issues/902</a></p>\n\n<p>What I've tried to do is downgrade to Node v.5 via n (<a href=\"https://www.npmjs.com/package/n\">https://www.npmjs.com/package/n</a>) - it worked. Then remove all <code>node_modules</code> folders, then do </p>\n\n<pre><code>npm info graceful-fs -v\n3.3.6\n</code></pre>\n\n<p>ok, lets upgrade or remove and install new:</p>\n\n<pre><code>npm i graceful-fs@latest\nnpm i graceful-fs@4.1.4\nsudo npm i graceful-fs@4.1.4 -g\n</code></pre>\n\n<p>all results in </p>\n\n<pre><code>npm info graceful-fs -v\n3.3.6\n</code></pre>\n\n<p>So now I am currenlty stuck with <strong>graceful-fs 3.3.6</strong> or even worse in some modules dependances, like</p>\n\n<pre><code>$ angular2-seed\n$ npm install\n\n//other lines..\nnpm WARN deprecated graceful-fs@1.2.3: graceful-fs v3.0.0 and before \nwill fail on node releases &gt;= v7.0. Please update to graceful-fs@^4.0.0 \nas soon as possible. Use 'npm ls graceful-fs' to find it in the tree.\n</code></pre>\n\n<p>What could be the strategy here:</p>\n\n<ul>\n<li>Manually patch all the deps that contain graceful-fs &lt; 4.0.0?</li>\n<li>There's some global switch to use specific package version?</li>\n<li>Reinstall everything? </li>\n</ul>\n", "title": "How to fix &#39;fs: re-evaluating native module sources is not supported&#39; - graceful-fs", "tags": ["node.js", "npm", "node-modules", "npm-install"], "answer_count": 3, "link": "http://stackoverflow.com/questions/37346512/how-to-fix-fs-re-evaluating-native-module-sources-is-not-supported-graceful", "answers": [{"body": "<p>I had this problem and I was able to fix this by updating npm</p>\n\n<pre><code>sudo npm update -g npm\n</code></pre>\n\n<p>Before the update, the result of <code>npm info graceful-fs -v</code> was</p>\n\n<pre><code>3.3.12\n</code></pre>\n\n<p>After the update the result is</p>\n\n<pre><code>3.9.3\n</code></pre>\n\n<p>Hope this helps</p>\n", "title": "How to fix &#39;fs: re-evaluating native module sources is not supported&#39; - graceful-fs", "answer_id": 37466305, "question_id": 37346512, "tags": ["node.js", "npm", "node-modules", "npm-install"]}, {"body": "<p>Type <code>npm list graceful-fs</code> and you will see which versions of graceful-fs are currently installed.</p>\n\n<p>In my case I got:</p>\n\n<pre><code>npm list graceful-fs\n\n@request/promise-core@0.0.1 /projects/request/promise-core\n+-- gulp@3.9.1\n| `-- vinyl-fs@0.3.14\n|   +-- glob-watcher@0.0.6\n|   | `-- gaze@0.5.2\n|   |   `-- globule@0.1.0\n|   |     `-- glob@3.1.21\n|   |       `-- graceful-fs@1.2.3        &lt;==== !!!\n|   `-- graceful-fs@3.0.8 \n`-- publish-please@2.1.3\n  +-- nsp@2.4.0\n  | `-- nodesecurity-npm-utils@4.0.1\n  |   `-- silent-npm-registry-client@2.0.0\n  |     `-- npm-registry-client@7.1.0\n  |       `-- graceful-fs@4.1.3 \n  `-- read-pkg@1.1.0\n    `-- load-json-file@1.1.0\n      `-- graceful-fs@4.1.4\n</code></pre>\n\n<p>As you can see <code>gulp</code> deep down depends on a very old version. Unfortunately, I can't update that myself using <code>npm update graceful-fs</code>. <code>gulp</code> would need to update their dependencies. So if you have a case like this you are out of luck. But you may open an issue for the project with the old dependency - i.e. <code>gulp</code>.</p>\n", "title": "How to fix &#39;fs: re-evaluating native module sources is not supported&#39; - graceful-fs", "answer_id": 37470253, "question_id": 37346512, "tags": ["node.js", "npm", "node-modules", "npm-install"]}, {"body": "<p>if you are running nvm you might want to run <code>nvm use &lt;desired-node-version&gt;</code> This keeps node consistent with npm</p>\n", "title": "How to fix &#39;fs: re-evaluating native module sources is not supported&#39; - graceful-fs", "answer_id": 37838135, "question_id": 37346512, "tags": ["node.js", "npm", "node-modules", "npm-install"]}], "question_id": 37346512}, {"body": "<p>The statement <code>printf(\"%f\\n\",0.0f);</code> prints 0.<br>\nHowever, the statement <code>printf(\"%f\\n\",0);</code> prints random values.  </p>\n\n<p>I realize I'm exhibiting some kind of undefined behaviour, but I can't figure out why specifically.  </p>\n\n<p>A floating point value in which all the bits are 0 is still a valid <code>float</code> with value of 0.<br>\n<code>float</code> and <code>int</code> are the same size on my machine (if that is even relevant).  </p>\n\n<p>Why does using an integer literal instead of a floating point literal in <code>printf</code> cause this behavior?</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"], "answer_count": 10, "link": "http://stackoverflow.com/questions/38597274/why-does-printff-0-give-undefined-behavior", "answers": [{"body": "<p>I'm not sure what's confusing.</p>\n\n<p>Your format string expects a <code>double</code>; you provide instead an <code>int</code>.</p>\n\n<p>Whether the two types have the same bit width is utterly irrelevant, except that it may help you avoid getting hard memory violation exceptions from broken code like this.</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38597315, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>The <code>\"%f\"</code> format requires an argument of type <code>double</code>. You're giving it an argument of type <code>int</code>. That's why the behavior is undefined.</p>\n\n<p>The standard does not guarantee that all-bits-zero is a valid representation of <code>0.0</code> (though it often is), or of any <code>double</code> value, or that <code>int</code> and <code>double</code> are the same size (remember it's <code>double</code>, not <code>float</code>), or, even if they are the same size, that they're passed as arguments to a variadic function in the same way.</p>\n\n<p>It might happen to \"work\" on your system. That's the worst possible symptom of undefined behavior, because it makes it difficult to diagnose the error.</p>\n\n<p><a href=\"http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf\">N1570</a> 7.21.6.1 paragraph 9:</p>\n\n<blockquote>\n  <p>... If any argument is not the correct type for the corresponding\n  conversion specification, the behavior is undefined.</p>\n</blockquote>\n\n<p>Arguments of type <code>float</code> are promoted to <code>double</code>, which is why <code>printf(\"%f\\n\",0.0f)</code> works. Arguments of integer types narrower than <code>int</code> are promoted to <code>int</code> or to <code>unsigned int</code>. These promotion rules (specified by N1570 6.5.2.2 paragraph 6) do not help in the case of <code>printf(\"%f\\n\", 0)</code>.</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38597324, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<blockquote>\n  <p>Why does using an integer literal instead of a float literal cause this behavior?</p>\n</blockquote>\n\n<p>Because <code>printf()</code> doesn't have typed parameters besides the <code>const char* formatstring</code> as the 1st one. It uses a c-style ellipsis (<code>...</code>) for all the rest. </p>\n\n<p>It's just decides how to interpret the values passed there according to the formatting types given in the format string.</p>\n\n<p>You would have the same kind of undefined behavior as when trying</p>\n\n<pre><code> int i = 0;\n const double* pf = (const double*)(&amp;i);\n printf(\"%f\\n\",*pf); // dereferencing the pointer is UB\n</code></pre>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38597340, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>Ordinarily when you call a function that expects a <code>double</code>, but you provide an <code>int</code>, the compiler will automatically convert to a <code>double</code> for you. That doesn't happen with <code>printf</code>, because the types of the arguments aren't specified in the function prototype - the compiler doesn't know that a conversion should be applied.</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38597358, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>First off, as touched on in several other answers but not, to my mind, spelled out clearly enough: It <em>does</em> work to provide an integer in <em>most</em> contexts where a library function takes a <code>double</code> or <code>float</code> argument.  The compiler will automatically insert a conversion.  For instance, <code>sqrt(0)</code> is well-defined and will behave exactly as <code>sqrt((double)0)</code>, and the same is true for any other integer-type expression used there.</p>\n\n<p><code>printf</code> is different.  It's different because it takes a variable number of arguments.  Its function prototype is</p>\n\n<pre><code>extern int printf(const char *fmt, ...);\n</code></pre>\n\n<p>Therefore, when you write</p>\n\n<pre><code>printf(message, 0);\n</code></pre>\n\n<p>the compiler does not have any information about what type <code>printf</code> <em>expects</em> that second argument to be.  It has only the type of the argument expression, which is <code>int</code>, to go by.  Therefore, unlike most library functions, it is on you, the programmer, to make sure the argument list matches the expectations of the format string.</p>\n\n<p>(Modern compilers <em>can</em> look into a format string and tell you that you've got a type mismatch, but they're not going to start inserting conversions to accomplish what you meant, because better your code should break now, when you'll notice, than years later when rebuilt with a less helpful compiler.)</p>\n\n<p>Now, the other half of the question was: Given that (int)0 and (float)0.0 are, on most modern systems, both represented as 32 bits all of which are zero, why doesn't it work anyway, by accident?  The C standard just says \"this isn't required to work, you're on your own\", but let me spell out the two most common reasons why it wouldn't work; that will probably help you understand <em>why</em> it's not required.</p>\n\n<p>First, for historical reasons, when you pass a <code>float</code> through a variable argument list it gets <em>promoted</em> to <code>double</code>, which, on most modern systems, is <em>64</em> bits wide.  So <code>printf(\"%f\", 0)</code> passes only 32 zero bits to a callee expecting 64 of them.</p>\n\n<p>The second, equally significant reason is that floating-point function arguments may be passed in a different <em>place</em> than integer arguments.  For instance, most CPUs have separate register files for integers and floating-point values, so it might be a rule that arguments 0 through 4 go in registers r0 through r4 if they are integers, but f0 through f4 if they are floating-point.  So <code>printf(\"%f\", 0)</code> looks in register f1 for that zero, but it's not there at all.</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38597778, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>Using a mis-matched <code>printf()</code> specifier <code>\"%f\"</code>and type <code>(int) 0</code> leads to undefined behavior.</p>\n\n<blockquote>\n  <p>If a conversion specification is invalid, the behavior is undefined. C11dr \u00a77.21.6.1 9</p>\n</blockquote>\n\n<p>Candidate causes of UB.</p>\n\n<ol>\n<li><p>It is UB per spec and the compile is ornery - 'nuf said.</p></li>\n<li><p><code>double</code> and <code>int</code> are of different sizes.  </p></li>\n<li><p><code>double</code> and <code>int</code> may pass their values using different stacks  (general vs. <a href=\"https://en.wikipedia.org/wiki/Floating-point_unit\">FPU</a> stack.)</p></li>\n<li><p>A <code>double 0.0</code> <em>might</em> not be defined by an all zero bit pattern. (rare)</p></li>\n</ol>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38597827, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p><code>\"%f\\n\"</code> guarantees predictable result only when the second <code>printf()</code> parameter has type of <code>double</code>. Next, an extra arguments of variadic functions are subject of default argument promotion. Integer arguments fall under integer promotion, which never results in floating-point typed values. And <code>float</code> parameters are promoted to <code>double</code>.</p>\n\n<p>To top it off: standard allows the second argument to be or <code>float</code> or <code>double</code> and nothing else.</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38598781, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>Why it is formally UB has now been discussed in several answers.</p>\n\n<p>The reason why you get specifically this behaviour is platform-dependent, but probably is the following:</p>\n\n<ul>\n<li><code>printf</code> expects its arguments according to standard vararg propagation. That means a <code>float</code> will be a <code>double</code> and anything smaller than an <code>int</code> will be an <code>int</code>.</li>\n<li>You are passing an <code>int</code> where the function expects a <code>double</code>. Your <code>int</code> is probably 32 bit, your <code>double</code> 64 bit. That means that the four stack bytes starting at the place where the argument is supposed to sit are <code>0</code>, but the following four bytes have arbitrary content. That's what is used for constructing the value which is displayed.</li>\n</ul>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38606192, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>This is one of those great opportunities to learn from your compiler warnings.</p>\n\n<pre><code>$ gcc -Wall -Wextra -pedantic fnord.c \nfnord.c: In function \u2018main\u2019:\nfnord.c:8:2: warning: format \u2018%f\u2019 expects argument of type \u2018double\u2019, but argument 2 has type \u2018int\u2019 [-Wformat=]\n  printf(\"%f\\n\",0);\n  ^\n</code></pre>\n\n<p>or</p>\n\n<pre><code>$ clang -Weverything -pedantic fnord.c \nfnord.c:8:16: warning: format specifies type 'double' but the argument has type 'int' [-Wformat]\n        printf(\"%f\\n\",0);\n                ~~    ^\n                %d\n1 warning generated.\n</code></pre>\n\n<p>So, <code>printf</code> is producing undefined behavior because you are passing it an incompatible type of argument.</p>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38617737, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}, {"body": "<p>The main cause of this \"undetermined value\" issue stands in the cast of the pointer at the <code>int</code> value passed to the <code>printf</code> variable parameters section to a pointer at <code>double</code> types that <code>va_arg</code> macro carries out. </p>\n\n<p>This causes a referencing to a memory area that was not completely initialized with value passed as parameter to the printf, because <code>double</code> size memory buffer area is greater than <code>int</code> size. </p>\n\n<p>Therefore, when this pointer is dereferenced, it is returned an undetermined value, or better a \"value\" that contains in part the value passed as parameter to <code>printf</code>, and for the remaining part could came from another stack buffer area or even a code area (raising a memory fault exception),  <strong>a real buffer overflow</strong>.\n<br><br><br>\n<em>It can consider these specific portions of semplificated code implementations of \"printf\" and \"va_arg\"...</em>\n<br><br>\n<strong>printf</strong> </p>\n\n<pre><code>va_list arg;\n....\ncase('%f')\n      va_arg ( arg, double ); //va_arg is a macro, and so you can pass it the \"type\" that will be used for casting the int pointer argument of printf..\n.... \n</code></pre>\n\n<p><br></p>\n\n<blockquote>\n  <p><em>the real implementation in vprintf (considering gnu impl.) of double value parameters code case\n  management is:</em></p>\n\n<pre><code>if (__ldbl_is_dbl)\n{\n   args_value[cnt].pa_double = va_arg (ap_save, double);\n   ...\n}\n</code></pre>\n</blockquote>\n\n<p><br><br>\n<strong>va_arg</strong></p>\n\n<pre><code>char *p = (double *) &amp;arg + sizeof arg;  //printf parameters area pointer\n\ndouble i2 = *((double *)p); //casting to double because va_arg(arg, double)\n   p += sizeof (double);\n</code></pre>\n\n<p><br><br></p>\n\n<blockquote>\n  <p><strong><em>references</em></strong></p>\n</blockquote>\n\n<ol>\n<li><a href=\"https://sourceware.org/git/?p=glibc.git;a=blob;f=stdio-common/vfprintf.c;h=fc370e8cbc4e9652a2ed377b1c6f2324f15b1bf9;hb=3321010338384ecdc6633a8b032bb0ed6aa9b19a\" rel=\"nofollow\">gnu project glibc implementation of \"printf\"(vprintf))</a></li>\n<li><a href=\"http://www.firmcodes.com/write-printf-function-c/\" rel=\"nofollow\">example of semplification code of printf</a></li>\n<li><a href=\"http://stackoverflow.com/questions/12371450/how-are-variable-arguments-implemented-in-gcc\">example of semplification code of va_arg</a></li>\n</ol>\n", "title": "Why does printf(&quot;%f&quot;,0); give undefined behavior?", "answer_id": 38741563, "question_id": 38597274, "tags": ["c++", "c", "printf", "implicit-conversion", "undefined-behavior"]}], "question_id": 38597274}, {"body": "<p>I came across some code with a line similar to</p>\n\n<pre><code>x[x&lt;2]=0\n</code></pre>\n\n<p>Playing around with variations, I am still stuck on what this syntax does.</p>\n\n<p>Examples:</p>\n\n<pre><code>&gt;&gt;&gt; x = [1,2,3,4,5]\n&gt;&gt;&gt; x[x&lt;2]\n1\n&gt;&gt;&gt; x[x&lt;3]\n1\n&gt;&gt;&gt; x[x&gt;2]\n2\n&gt;&gt;&gt; x[x&lt;2]=0\n&gt;&gt;&gt; x\n[0, 2, 3, 4, 5]\n</code></pre>\n", "title": "What does x[x &lt; 2] = 0 mean in Python?", "tags": ["python", "python-2.7", "numpy"], "answer_count": 5, "link": "http://stackoverflow.com/questions/36603042/what-does-xx-2-0-mean-in-python", "answers": [{"body": "<pre><code>&gt;&gt;&gt; x = [1,2,3,4,5]\n&gt;&gt;&gt; x&lt;2\nFalse\n&gt;&gt;&gt; x[False]\n1\n&gt;&gt;&gt; x[True]\n2\n</code></pre>\n\n<p>The bool is simply converted to an integer. The index is either 0 or 1.</p>\n", "title": "What does x[x &lt; 2] = 0 mean in Python?", "answer_id": 36603120, "question_id": 36603042, "tags": ["python", "python-2.7", "numpy"]}, {"body": "<p>This only makes sense with <strong><a href=\"http://en.wikipedia.org/wiki/NumPy\">NumPy</a> arrays</strong>. The behavior with lists is useless, and specific to Python 2 (not Python 3). You may want to double-check if the original object was indeed a NumPy array (see further below) and not a list.</p>\n\n<p>But in your code here, x is a simple list.</p>\n\n<p>Since</p>\n\n<pre><code>x &lt; 2\n</code></pre>\n\n<p>is False\ni.e 0, therefore</p>\n\n<p><code>x[x&lt;2]</code> is <code>x[0]</code></p>\n\n<p><code>x[0]</code> gets changed.</p>\n\n<p>Conversely, <code>x[x&gt;2]</code> is <code>x[True]</code> or <code>x[1]</code></p>\n\n<p>So, <code>x[1]</code> gets changed.</p>\n\n<p><strong>Why does this happen?</strong></p>\n\n<p>The rules for comparison are:</p>\n\n<ol>\n<li><p>When you order two strings or two numeric types the ordering is done in the expected way (lexicographic ordering for string, numeric ordering for integers).</p></li>\n<li><p>When you order a numeric and a non-numeric type, the numeric type comes first.</p></li>\n<li><p>When you order two incompatible types where neither is numeric, they are ordered by the alphabetical order of their typenames:</p></li>\n</ol>\n\n<p>So, we have the following order</p>\n\n<p>numeric &lt; list &lt; string &lt; tuple</p>\n\n<p>See the accepted answer for <em><a href=\"http://stackoverflow.com/questions/3270680/how-does-python-compare-string-and-int\">How does Python compare string and int?</a></em>.</p>\n\n<p><strong>If x is a NumPy array</strong>, then the syntax makes more sense because of <strong>boolean array indexing</strong>. In that case, <code>x &lt; 2</code> isn't a boolean at all; it's an array of booleans representing whether each element of <code>x</code> was less than 2. <code>x[x &lt; 2] = 0</code> then selects the elements of <code>x</code> that were less than 2 and sets those cells to 0. See <em><a href=\"http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html\">Indexing</a></em>.</p>\n\n<pre><code>&gt;&gt;&gt; x = np.array([1., -1., -2., 3])\n&gt;&gt;&gt; x &lt; 0\narray([False,  True,  True, False], dtype=bool)\n&gt;&gt;&gt; x[x &lt; 0] += 20   # All elements &lt; 0 get increased by 20\n&gt;&gt;&gt; x\narray([  1.,  19.,  18.,   3.]) # Only elements &lt; 0 are affected\n</code></pre>\n", "title": "What does x[x &lt; 2] = 0 mean in Python?", "answer_id": 36603274, "question_id": 36603042, "tags": ["python", "python-2.7", "numpy"]}, {"body": "<p>The original code in your question works only in Python 2. If <code>x</code> is a <code>list</code> in Python 2, the comparison <code>x &lt; y</code> is <code>False</code> if <code>y</code> is an <code>int</code>eger. This is because it does not make sense to compare a list with an integer. However in Python 2, if the operands are not comparable, the comparison is based in CPython on the <a href=\"https://docs.python.org/2/library/stdtypes.html#comparisons\" rel=\"nofollow\"><strong>alphabetical ordering of the names of the types</strong></a>; additionally <strong>all numbers come first in mixed-type comparisons</strong>. This is not even spelled out in the documentation of CPython 2, and different Python 2 implementations could give different results. That is <code>[1, 2, 3, 4, 5] &lt; 2</code> evaluates to <code>False</code> because <code>2</code> is a number and thus \"smaller\" than a <code>list</code> in CPython. This mixed comparison was eventually <a href=\"http://stackoverflow.com/a/2384139/918959\">deemed to be too obscure a feature</a>, and was removed in Python 3.0.</p>\n\n<hr>\n\n<p>Now, the result of <code>&lt;</code> is a <code>bool</code>; and <a href=\"https://www.python.org/dev/peps/pep-0285/\" rel=\"nofollow\"><code>bool</code> is a <em>subclass</em> of <code>int</code></a>:</p>\n\n<pre><code>&gt;&gt;&gt; isinstance(False, int)\nTrue\n&gt;&gt;&gt; isinstance(True, int)\nTrue\n&gt;&gt;&gt; False == 0\nTrue\n&gt;&gt;&gt; True == 1\nTrue\n&gt;&gt;&gt; False + 5\n5\n&gt;&gt;&gt; True + 5\n6\n</code></pre>\n\n<p>So basically you're taking the element 0 or 1 depending on whether the comparison is true or false.</p>\n\n<hr>\n\n<p>If you try the code above in Python 3, you will get <code>TypeError: unorderable types: list() &lt; int()</code> due to <a href=\"https://docs.python.org/3.0/whatsnew/3.0.html#ordering-comparisons\" rel=\"nofollow\">a change in Python 3.0</a>:</p>\n\n<blockquote>\n  <p><strong>Ordering Comparisons</strong></p>\n  \n  <p>Python 3.0 has simplified the rules for ordering comparisons:</p>\n  \n  <p>The ordering comparison operators (<code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>&gt;</code>) raise a <code>TypeError</code> exception when the operands don\u2019t have a meaningful natural ordering. Thus, expressions like <code>1 &lt; ''</code>, <code>0 &gt; None</code> or <code>len &lt;= len</code> are no longer valid, and e.g. <code>None &lt; None</code> raises <code>TypeError</code> instead of returning <code>False</code>. A corollary is that sorting a heterogeneous list no longer makes sense \u2013 all the elements must be comparable to each other. Note that this does not apply to the <code>==</code> and <code>!=</code> operators: objects of different incomparable types always compare unequal to each other.</p>\n</blockquote>\n\n<hr>\n\n<p>There are many datatypes that <em>overload</em> the comparison operators to do something <em>different</em> (dataframes from pandas, numpy's arrays). If the code that you were using did something else, it was because <code>x</code> was <em>not a <code>list</code></em>, but an instance of some other class with operator <code>&lt;</code> overridden to return a value that is not a <code>bool</code>; and this value was then handled specially by <code>x[]</code> (aka <code>__getitem__</code>/<code>__setitem__</code>)</p>\n", "title": "What does x[x &lt; 2] = 0 mean in Python?", "answer_id": 36606250, "question_id": 36603042, "tags": ["python", "python-2.7", "numpy"]}, {"body": "<p>This has one more use: code golf. Code golf is the art of writing programs that solve some problem in as few source code bytes as possible. </p>\n\n<pre><code>return(a,b)[c&lt;d]\n</code></pre>\n\n<p>is roughly equivalent to</p>\n\n<pre><code>if c &lt; d:\n    return b\nelse:\n    return a\n</code></pre>\n\n<p>except that both a and b are evaluated in the first version, but not in the second version.</p>\n\n<p><code>c&lt;d</code> evaluates to <code>True</code> or <code>False</code>.<br>\n<code>(a, b)</code> is a tuple.<br>\nIndexing on a tuple works like indexing on a list: <code>(3,5)[1]</code> == <code>5</code>.<br>\n<code>True</code> is equal to <code>1</code> and <code>False</code> is equal to <code>0</code>.  </p>\n\n<ol>\n<li><code>(a,b)[c&lt;d]</code></li>\n<li><code>(a,b)[True]</code></li>\n<li><code>(a,b)[1]</code></li>\n<li><code>b</code></li>\n</ol>\n\n<p>or for <code>False</code>:</p>\n\n<ol>\n<li><code>(a,b)[c&lt;d]</code></li>\n<li><code>(a,b)[False]</code></li>\n<li><code>(a,b)[0]</code></li>\n<li><code>a</code></li>\n</ol>\n\n<p>There's a good list on the stack exchange network of many nasty things you can do to python in order to save a few bytes. <a href=\"http://codegolf.stackexchange.com/questions/54/tips-for-golfing-in-python\">http://codegolf.stackexchange.com/questions/54/tips-for-golfing-in-python</a></p>\n\n<p>Although in normal code this should never be used, and in your case it would mean that <code>x</code> acts both as something that can be compared to an integer and as a container that supports slicing, which is a very unusual combination. It's probably Numpy code, as others have pointed out.</p>\n", "title": "What does x[x &lt; 2] = 0 mean in Python?", "answer_id": 36619440, "question_id": 36603042, "tags": ["python", "python-2.7", "numpy"]}, {"body": "<p>In general it could mean <strong>anything</strong>. It was already explained what it means if <code>x</code> is a <code>list</code> or <code>numpy.ndarray</code> but in general it only depends on how the comparison operators (<code>&lt;</code>, <code>&gt;</code>, ...) and also how the get/set-item (<code>[...]</code>-syntax) are implemented.</p>\n\n<pre><code>x.__getitem__(x.__lt__(2))      # this is what x[x &lt; 2] means!\nx.__setitem__(x.__lt__(2), 0)   # this is what x[x &lt; 2] = 0 means!\n</code></pre>\n\n<p>Because:</p>\n\n<ul>\n<li><code>x &lt; value</code> is equivalent to <code>x.__lt__(value)</code></li>\n<li><code>x[value]</code> is (roughly) equivalent to <code>x.__getitem__(value)</code> </li>\n<li><code>x[value] = othervalue</code> is (also roughly) equivalent to <code>x.__setitem__(value, othervalue)</code>.</li>\n</ul>\n\n<hr>\n\n<p>This can be customized to do <strong>anything</strong> you want. Just as an example (mimics a bit numpys-boolean indexing):</p>\n\n<pre><code>class Test:\n    def __init__(self, value):\n        self.value = value\n\n    def __lt__(self, other):\n        # You could do anything in here. For example create a new list indicating if that \n        # element is less than the other value\n        res = [item &lt; other for item in self.value]\n        return self.__class__(res)\n\n    def __repr__(self):\n        return '{0} ({1})'.format(self.__class__.__name__, self.value)\n\n    def __getitem__(self, item):\n        # If you index with an instance of this class use \"boolean-indexing\"\n        if isinstance(item, Test):\n            res = self.__class__([i for i, index in zip(self.value, item) if index])\n            return res\n        # Something else was given just try to use it on the value\n        return self.value[item]\n\n    def __setitem__(self, item, value):\n        if isinstance(item, Test):\n            self.value = [i if not index else value for i, index in zip(self.value, item)]\n        else:\n            self.value[item] = value\n</code></pre>\n\n<p>So now let's see what happens if you use it:</p>\n\n<pre><code>&gt;&gt;&gt; a = Test([1,2,3])\n&gt;&gt;&gt; a\nTest ([1, 2, 3])\n&gt;&gt;&gt; a &lt; 2  # calls __lt__\nTest ([True, False, False])\n&gt;&gt;&gt; a[Test([True, False, False])] # calls __getitem__\nTest ([1])\n&gt;&gt;&gt; a[a &lt; 2] # or short form\nTest ([1])\n\n&gt;&gt;&gt; a[a &lt; 2] = 0  # calls __setitem__\n&gt;&gt;&gt; a\nTest ([0, 2, 3])\n</code></pre>\n\n<p>Notice this is just one possibility. You are free to implement almost everything you want.</p>\n", "title": "What does x[x &lt; 2] = 0 mean in Python?", "answer_id": 36630265, "question_id": 36603042, "tags": ["python", "python-2.7", "numpy"]}], "question_id": 36603042}, {"body": "<p>I am working on a scenario in which some JavaScript files are to be hosted on a CDN. I want to have some mechanism so that when these file are downloaded on user side, I can ensure that the files were not tampered with and are indeed coming from the specified CDN. </p>\n\n<p>I understand that the task is very easy if I am using SSL, but still, I want to ensure that the right files are served even on HTTP without SSL.</p>\n\n<p>As far as I could search, there is no existing mechanism like digital signature for JavaScript files which is supported across platforms. Perhaps it's not needed?</p>\n\n<p>Is there some method built in to browsers to verify the author of the JavaScript files? Is there anything I can do to do this in a secure way?</p>\n", "title": "How can I make sure that my JavaScript files delivered over a CDN are not altered?", "tags": ["javascript", "code-signing"], "answer_count": 5, "link": "http://stackoverflow.com/questions/38700923/how-can-i-make-sure-that-my-javascript-files-delivered-over-a-cdn-are-not-altere", "answers": [{"body": "<p>You're looking for <a href=\"https://www.w3.org/TR/SRI/\">subresource integrity</a> checks.</p>\n\n<p>For example, here's the jQuery CDN snippet:</p>\n\n<pre><code>&lt;script src=\"https://code.jquery.com/jquery-3.1.0.js\"\n        integrity=\"sha256-slogkvB1K3VOkzAI8QITxV3VzpOnkeNVsKvtkYLMjfk=\"\n        crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n</code></pre>\n", "title": "How can I make sure that my JavaScript files delivered over a CDN are not altered?", "answer_id": 38701082, "question_id": 38700923, "tags": ["javascript", "code-signing"]}, {"body": "<p>As a matter of fact, a feature like this is <a href=\"https://w3c.github.io/webappsec-subresource-integrity/\">currently being drafted</a> under the name of <em>Subresource Integrity</em>. <strong>Look into the <code>integrity</code> attribute of the <code>&lt;script&gt;</code> tag.</strong> <a href=\"https://developer.mozilla.org/en-US/docs/Web/Security/Subresource_Integrity#Browser_compatibility\">While not yet fully adopted across the board</a>, it fulfills just this purpose.</p>\n\n<blockquote>\n  <p><strong><code>integrity</code></strong></p>\n  \n  <p>Contains inline metadata that a user agent can use to verify that a fetched resource has been delivered free of unexpected manipulation. See Subresource Integrity.</p>\n</blockquote>\n\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Element/script#Attributes\">Source</a></p>\n\n<blockquote>\n  <p>Subresource Integrity (SRI) is a security feature that enables browsers to verify that files they fetch (for example, from a CDN) are delivered without unexpected manipulation. It works by allowing you to provide a cryptographic hash that a fetched file must match.</p>\n</blockquote>\n\n<p><a href=\"https://developer.mozilla.org/en-US/docs/Web/Security/Subresource_Integrity\">Source</a></p>\n\n<hr>\n\n<p><strong>Example:</strong></p>\n\n<pre><code>&lt;script src=\"https://example.com/example-framework.js\"\n    integrity=\"sha384-oqVuAfXRKap7fdgcCY5uykM6+R9GqQ8K/uxy9rx7HNQlGYl1kPzQho1wx4JwY8wC\"\n    crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n</code></pre>\n\n<hr>\n\n<p>Note however that this <strong>will not protect you against <a href=\"https://en.wikipedia.org/wiki/Man-in-the-middle_attack\">Man in the Middle attacks</a></strong> if you are transferring your resources via plain HTTP. In this case, the hash code can be spoofed by the attacker, rendering the defense against manipulated script files useless.</p>\n\n<p>For this reason, you should always use secure HTTPS connections instead of plain HTTP in addition to the security measures described above.</p>\n", "title": "How can I make sure that my JavaScript files delivered over a CDN are not altered?", "answer_id": 38701090, "question_id": 38700923, "tags": ["javascript", "code-signing"]}, {"body": "<p><em>Disclaimer: As always, you should only consider these mechanisms to be of any use when using https, as they can easily be disabled via MitM with http</em></p>\n\n<p>In addition to the mechanism in the above answers, you can also use the <strong>content-security policy</strong> http response headers on the parent page.</p>\n\n<p><a href=\"http://www.html5rocks.com/en/tutorials/security/content-security-policy/\" rel=\"nofollow\">http://www.html5rocks.com/en/tutorials/security/content-security-policy/</a></p>\n\n<blockquote>\n  <p>Content-Security-Policy: script-src 'sha256-qznLcsROx4GACP2dm0UCKCzCG-HiZ1guq6ZZDob_Tng='</p>\n  \n  <p>There are a few things to note here. The sha*- prefix specifies the algorithm used to generate the hash. In the example above, sha256- is used. CSP also supports sha384- and sha512-. When generating the hash do not include the  tags. Also capitalization and whitespace matter, including leading or trailing whitespace.</p>\n  \n  <p>Using Chrome 40 or later you can open DevTools then reload your page. The Console tab will contain error messages with the correct sha256 hash for each of your inline scripts.</p>\n</blockquote>\n\n<p>This mechanism has been around for quite some time, so the browser support is likely pretty good, just be sure to check. </p>\n\n<p>Additionally, if you want to ensure that older non-compliant browsers are not insecure, you can include a synchronous redirect script at the top of the page that is not allowed by the policy.</p>\n", "title": "How can I make sure that my JavaScript files delivered over a CDN are not altered?", "answer_id": 38709665, "question_id": 38700923, "tags": ["javascript", "code-signing"]}, {"body": "<p>If your adversary model permits an attacker to modify JavaScript files as they are delivered from a CDN, then your adversary model permits an attacker to modify the referring source as it is delivered to remove any attempt at verification, to alter the source address to other than the CDN, and/or to remove the reference to the JavaScript entirely.</p>\n\n<p>And lets not open the can of worms of how your application can determine whether the user's resolver is or is not correctly resolving to the CDN via HTTP requests (or any other mechanism that doesn't have a verified chain of trust).</p>\n\n<p>/etc/hosts:</p>\n\n<pre><code>#  ...\n1.2.3.4    vile-pirates.org    trustworthy.cdn\n#  ...\n</code></pre>\n", "title": "How can I make sure that my JavaScript files delivered over a CDN are not altered?", "answer_id": 38710939, "question_id": 38700923, "tags": ["javascript", "code-signing"]}, {"body": "<p>There's an important point about what this kind of signing can and cannot do.  It <em>can</em> protect the user from hypothetical attacks in which someone modifies your code.  It <em>cannot</em> assure your site that your code is the code being executed.  In other words, you still can't trust anything that comes to your site from the client.</p>\n", "title": "How can I make sure that my JavaScript files delivered over a CDN are not altered?", "answer_id": 38727638, "question_id": 38700923, "tags": ["javascript", "code-signing"]}], "question_id": 38700923}, {"body": "<p>I deleted all my provisioning profiles from my Mac by mistake (<code>Library/Mobile Device/Provisioning Profiles</code>). Now, no matter what I do, Xcode gives me this error:</p>\n\n<blockquote>\n  <p>Missing iOS Distribution signing identity</p>\n</blockquote>\n\n<p>How can I fix this?</p>\n", "title": "Missing iOS Distribution signing identity", "tags": ["xcode", "code-signing-certificate"], "answer_count": 7, "link": "http://stackoverflow.com/questions/35398083/missing-ios-distribution-signing-identity", "answers": [{"body": "<p>Apparently, I was having the same issue as <a href=\"http://stackoverflow.com/questions/13820680/this-certificate-was-signed-by-an-unknown-authority\">this</a> question, the answer there solved my problem too!! Even though I had my WWDR certificate, re adding to the keychain solved my problem!</p>\n\n<p><strong>EDIT:</strong> Before doing this, here is what I did:</p>\n\n<ol>\n<li><p>Exit xCode</p></li>\n<li><p>Go to Keychain Access. Delete every certificate which has iOS in it's name. Do the same for keys as well.</p></li>\n<li><p>Open up finder, press Go and press ALT, this will bring up the option to go to your Library. Click on that, find Mobile Devices -> Provisioning Profiles -> and delete everything from there. </p></li>\n<li><p>Go to Member Center and revoke every certificate you have, delete all existing provisioning profiles. </p></li>\n<li><p>Open xCode, open your project and in General select your team, under it you will get a warning and a button which says fix issue, click on that. After it is done, build your app to a real device(not simulator) and exit xCode.</p></li>\n<li><p>Go to Member Center and go to Certificates, you will see that xCode has created a new one. Download that, and add it to your Keychain Access. </p></li>\n<li><p>Add WWDR to Keychain Access. </p></li>\n<li><p>Try archiving, it should work now!</p></li>\n</ol>\n\n<p><strong>Note:</strong> In Build Settings set code signing to automatic.</p>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35399459, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}, {"body": "<p>This is the same exact error for an expired Apple cert - though I know it's not your issue, you're the #1 answer on Bing at the moment.</p>\n\n<p><a href=\"https://forums.developer.apple.com/thread/37208\">According to the Apple Developer Forums</a>, the Apple Worldwide Developer Relations Intermediate Certificate Expiration expired 2/14/2016.</p>\n\n<blockquote>\n  <p>... This issue stems from having a copy of the expired WWDR Intermediate certificate in both your System and Login keychains. To resolve the issue, you should first download and install the new WWDR intermediate certificate (by double-clicking on the file). Next, in the Keychain Access application, select the System keychain. Make sure to select \u201cShow Expired Certificates\u201d in the View menu and then delete the expired version of the Apple Worldwide Developer Relations Certificate Authority Intermediate certificate (expired on February 14, 2016). Your certificates should now appear as valid in Keychain Access and be available to Xcode for submissions to the App Store.</p>\n</blockquote>\n\n<p><a href=\"http://i.stack.imgur.com/HDw9t.png\"><img src=\"http://i.stack.imgur.com/HDw9t.png\" alt=\"enter image description here\"></a></p>\n\n<p>You can get a new cert (which expires February 7, 2023). The AppleWWDRCA.cer is available on the explanation page: <a href=\"https://developer.apple.com/support/certificates/expiration/\">https://developer.apple.com/support/certificates/expiration/</a></p>\n\n<p>BTW: it appears this new cert is installed with Xcode 7.2.1</p>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35400707, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}, {"body": "<p>Check here: <a href=\"https://forums.developer.apple.com/thread/37208\">https://forums.developer.apple.com/thread/37208</a></p>\n\n<p>Download <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a> and double-click to install to Keychain.</p>\n\n<p>Select \"View\" -> \"Show Expired Certificates\" in Keychain app.</p>\n\n<p>Remove Apple Worldwide Developer Relations Certificate Authority certificates from \"login\" tab and \"System\" tab in Keychain app.</p>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35402192, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}, {"body": "<p>From Apple - </p>\n\n<blockquote>\n  <p>Thanks for bringing this to the attention of the community and\n  apologies for the issues you\u2019ve been having. This issue stems from\n  having a copy of the expired WWDR Intermediate certificate in both\n  your System and Login keychains. To resolve the issue, you should\n  first download and install the new <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">WWDR intermediate certificate</a> (by\n  double-clicking on the file). Next, in the Keychain Access\n  application, select the System keychain. Make sure to select \u201cShow\n  Expired Certificates\u201d in the View menu and then delete the expired\n  version of the Apple Worldwide Developer Relations Certificate\n  Authority Intermediate certificate (expired on February 14, 2016).\n  Your certificates should now appear as valid in Keychain Access and be\n  available to Xcode for submissions to the App Store.</p>\n</blockquote>\n\n<p>Also answered at <a href=\"http://stackoverflow.com/a/35401483/921358\">http://stackoverflow.com/a/35401483/921358</a></p>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35409208, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}, {"body": "<p>It says very clearly. I just had to delete expired certificate as described below and download new one from <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">this link</a>. It is all described on <a href=\"https://developer.apple.com/support/certificates/expiration/\">this page</a>.</p>\n\n<blockquote>\n  <p>What should I do if Xcode doesn\u2019t recognize my distribution\n  certificate? If you have a copy of the expired WWDR Intermediate\n  certificate in both your System and Login keychains within the\n  Keychain Access application, your certificates may appear as invalid\n  and not be recognized by Xcode. To resolve the issue, you should first\n  download and install the renewed certificate. Next, in the Keychain\n  Access application, select the System keychain. Select 'Show Expired\n  Certificates' in the View menu and then delete the expired version of\n  the Apple Worldwide Developer Relations Certificate Authority\n  Intermediate certificate. Your certificates should now appear as valid\n  in Keychain Access and be available to Xcode.</p>\n</blockquote>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35435084, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}, {"body": "<p>First: down and install WWDR in here <a href=\"https://developer.apple.com/support/certificates/expiration/\" rel=\"nofollow\">https://developer.apple.com/support/certificates/expiration/</a>\nThen: In Keychain Access click View and select show expired Certificates and delete them on login and System tab\nFinally: Clear and Build project on Xcode</p>\n\n<p>I uesed above ways to create ipa file. Hope help you.</p>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35545562, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}, {"body": "<p>This worked for me:\nKeychain Access -> View -> Show Expired Certificates</p>\n\n<p>then</p>\n\n<p>Keychain Access -> System tab</p>\n\n<p>Locate the <strong>EXPIRED</strong> \"Apple Worldwide Developer Relations Certificate Authority\" certificate and delete it.</p>\n", "title": "Missing iOS Distribution signing identity", "answer_id": 35641203, "question_id": 35398083, "tags": ["xcode", "code-signing-certificate"]}], "question_id": 35398083}, {"body": "<p>I need a function that (like SecureZeroMemory from the WinAPI) always zeros memory and doesn't get optimized away, even if the compiler thinks the memory is never going to accessed again after that. Seems like a perfect candidate for volatile. But I'm having some problems actually getting this to work with GCC. Here is an example function:</p>\n\n<pre><code>void volatileZeroMemory(volatile void* ptr, unsigned long long size)\n{\n    volatile unsigned char* bytePtr = (volatile unsigned char*)ptr;\n\n    while (size--)\n    {\n        *bytePtr++ = 0;\n    }\n}\n</code></pre>\n\n<p>Simple enough. But the code that GCC actually generates if you call it varies wildly with the compiler version and the amount of bytes you're actually trying to zero. <a href=\"https://godbolt.org/g/cMaQm2\">https://godbolt.org/g/cMaQm2</a></p>\n\n<ul>\n<li>GCC 4.4.7 and 4.5.3 never ignore the volatile.</li>\n<li>GCC 4.6.4 and 4.7.3 ignore volatile for array sizes 1, 2, and 4.</li>\n<li>GCC 4.8.1 until 4.9.2 ignore volatile for array sizes 1 and 2.</li>\n<li>GCC 5.1 until 5.3 ignore volatile for array sizes 1, 2, 4, 8.</li>\n<li>GCC 6.1 just ignores it for any array size (bonus points for consistency).</li>\n</ul>\n\n<p>Any other compiler I have tested (clang, icc, vc) generates the stores one would expect, with any compiler version and any array size. So at this point I'm wondering, is this a (pretty old and severe?) GCC compiler bug, or is the definition of volatile in the standard that imprecise that this is actually conforming behavior, making it essentially impossible to write a portable \"SecureZeroMemory\" function?</p>\n\n<p>Edit: Some interesting observations.</p>\n\n<pre><code>#include &lt;cstddef&gt;\n#include &lt;cstdint&gt;\n#include &lt;cstring&gt;\n#include &lt;atomic&gt;\n\nvoid callMeMaybe(char* buf);\n\nvoid volatileZeroMemory(volatile void* ptr, std::size_t size)\n{\n    for (auto bytePtr = static_cast&lt;volatile std::uint8_t*&gt;(ptr); size-- &gt; 0; )\n    {\n        *bytePtr++ = 0;\n    }\n\n    //std::atomic_thread_fence(std::memory_order_release);\n}\n\nstd::size_t foo()\n{\n    char arr[8];\n    callMeMaybe(arr);\n    volatileZeroMemory(arr, sizeof arr);\n    return sizeof arr;\n}\n</code></pre>\n\n<p><a href=\"https://godbolt.org/g/v8lMjG\">The possible write from callMeMaybe() will make all GCC versions except 6.1 generate the expected stores.</a> Commenting in the memory fence will also make GCC 6.1 generate the stores, although only in combination with the possible write from callMeMaybe().</p>\n\n<p>Someone has also suggested to flush the caches. <a href=\"http://codepad.org/1yC7a8J8\">Microsoft does <strong>not</strong> try to flush the cache at all in \"SecureZeroMemory\".</a> The cache is likely going to be invalidated pretty fast anyway, so this is probably not be a big deal. Also, if another program was trying to probe the data, or if it was going to be written to the page file, it would always be the zeroed version.</p>\n\n<p>There are also some concerns about GCC 6.1 using memset() in the standalone function. The GCC 6.1 compiler on godbolt might a broken build, as GCC 6.1 seems to generate a normal loop (like 5.3 does on godbolt) for the standalone function for some people. (Read comments of zwol's answer.)</p>\n", "title": "Is the definition of &quot;volatile&quot; this volatile, or is GCC having some standard compliancy problems?", "tags": ["c++", "c", "gcc", "standards"], "answer_count": 3, "link": "http://stackoverflow.com/questions/38230856/is-the-definition-of-volatile-this-volatile-or-is-gcc-having-some-standard-co", "answers": [{"body": "<blockquote>\n  <p>I need a function that (like SecureZeroMemory from the WinAPI) always zeros memory and doesn't get optimized away,</p>\n</blockquote>\n\n<p>This is what the standard function <code>memset_s</code> is for.</p>\n\n<hr>\n\n<p>As to whether this behavior with volatile is conforming or not, that's a bit hard to say, and volatile has been <a href=\"http://blog.regehr.org/archives/503\">said</a> to have long been plagued with bugs.</p>\n\n<p>One issue is that the specs say that \"Accesses to volatile objects are evaluated strictly according to the rules of the abstract machine.\" But that only refers to 'volatile objects', not accessing a non-volatile object via a pointer that has had volatile added. So apparently if a compiler can tell that you're not really accessing a volatile object then it's not required to treat the object as volatile after all.</p>\n", "title": "Is the definition of &quot;volatile&quot; this volatile, or is GCC having some standard compliancy problems?", "answer_id": 38230939, "question_id": 38230856, "tags": ["c++", "c", "gcc", "standards"]}, {"body": "<p>GCC's behavior <em>may</em> be conforming, and even if it isn't, you should not rely on <code>volatile</code> to do what you want in cases like these.  The C committee designed <code>volatile</code> for memory-mapped hardware registers and for variables modified during abnormal control flow (e.g. signal handlers and <code>setjmp</code>).  <strong>Those are the only things it is reliable for.</strong>  It is not safe to use as a general \"don't optimize this out\" annotation.</p>\n\n<p>In particular, the standard is unclear on a key point.  (I've converted your code to C; there <em>shouldn't</em> be any divergence between C and C++ here. I've also manually done the inlining that would happen before the questionable optimization, to show what the compiler \"sees\" at that point.)</p>\n\n<pre><code>extern void use_arr(void *, size_t);\nvoid foo(void)\n{\n    char arr[8];\n    use_arr(arr, sizeof arr);\n\n    for (volatile char *p = (volatile char *)arr;\n         p &lt; (volatile char *)(arr + 8);\n         p++)\n      *p = 0;\n}\n</code></pre>\n\n<p>The memory-clearing loop accesses <code>arr</code> through a volatile-qualified lvalue, but <code>arr</code> itself is <em>not</em> declared <code>volatile</code>.  It is, therefore, at least arguably allowed for the C compiler to infer that the stores made by the loop are \"dead\", and delete the loop altogether.  There's text in the C Rationale that implies that the committee <strong>meant</strong> to require those stores to be preserved, but the standard itself does not actually make that requirement, as I read it.</p>\n\n<p>For more discussion of what the standard does or does not require, see <a href=\"https://stackoverflow.com/questions/38235112/why-is-a-volatile-local-variable-optimised-differently-from-a-volatile-argument\">Why is a volatile local variable optimised differently from a volatile argument, and why does the optimiser generate a no-op loop from the latter?</a>, <a href=\"https://stackoverflow.com/questions/38243501/does-accessing-a-declared-non-volatile-object-through-a-volatile-reference-point\">Does accessing a declared non-volatile object through a volatile reference/pointer confer volatile rules upon said accesses?</a>, and <a href=\"https://gcc.gnu.org/bugzilla/show_bug.cgi?id=71793\">GCC bug 71793</a>.</p>\n\n<p>For more on what the committee <em>thought</em> <code>volatile</code> was for, search the <a href=\"http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf\">C99 Rationale</a> for the word \"volatile\". John Regehr's paper \"<a href=\"https://www.cs.utah.edu/~regehr/papers/emsoft08-preprint.pdf\">Volatiles are Miscompiled</a>\" illustrates in detail how programmer expectations for <code>volatile</code> may not be satisfied by production compilers. The LLVM team's series of essays \"<a href=\"http://blog.llvm.org/2011/05/what-every-c-programmer-should-know.html\">What Every C Programmer Should Know About Undefined Behavior</a>\" does not touch specifically on <code>volatile</code> but will help you understand how and why modern C compilers are <em>not</em> \"portable assemblers\".</p>\n\n<hr>\n\n<p>To the <strong>practical</strong> question of how to implement a function that does what you wanted <code>volatileZeroMemory</code> to do:  Regardless of what the standard requires or was meant to require, it would be wisest to assume that you can't use <code>volatile</code> for this.  There <em>is</em> an alternative that can be relied on to work, because it would break far too much other stuff if it didn't work:</p>\n\n<pre><code>extern void memory_optimization_fence(void *ptr, size_t size);\ninline void\nexplicit_bzero(void *ptr, size_t size)\n{\n   memset(ptr, 0, size);\n   memory_optimization_fence(ptr, size);\n}\n\n/* in a separate source file */\nvoid memory_optimization_fence(void *unused1, size_t unused2) {}\n</code></pre>\n\n<p>However, you must make absolutely sure that <code>memory_optimization_fence</code> is not inlined under any circumstances.  It must be in its own source file and it must not be subjected to link-time optimization.</p>\n\n<p>There are other options, relying on compiler extensions, that may be usable under some circumstances and can generate tighter code (one of them appeared in a previous edition of this answer), but none are universal.</p>\n\n<p>(I recommend calling the function <code>explicit_bzero</code>, because it is available under that name in <em>three</em> C libraries: OpenBSD, FreeBSD, and musl libc (I think).  No other name for this function \u2014 there are at least four other contenders \u2014 is as widespread.)</p>\n\n<p>You should also know that, even if you can get this to work, it may not be enough.  In particular, consider</p>\n\n<pre><code>struct aes_expanded_key { __uint128_t rndk[16]; };\n\nvoid encrypt(const char *key, const char *iv,\n             const char *in, char *out, size_t size)\n{\n    aes_expanded_key ek;\n    expand_key(key, ek);\n    encrypt_with_ek(ek, iv, in, out, size);\n    explicit_bzero(&amp;ek, sizeof ek);\n}\n</code></pre>\n\n<p>Assuming hardware with AES acceleration instructions, if <code>expand_key</code> and <code>encrypt_with_ek</code> are inline, the compiler may be able to keep <code>ek</code> entirely in the vector register file -- until the call to <code>explicit_bzero</code>, which forces it to <em>copy the sensitive data onto the stack</em> just to erase it, and, worse, doesn't do a darn thing about the keys that are still sitting in the vector registers!</p>\n", "title": "Is the definition of &quot;volatile&quot; this volatile, or is GCC having some standard compliancy problems?", "answer_id": 38231071, "question_id": 38230856, "tags": ["c++", "c", "gcc", "standards"]}, {"body": "<p>It should be possible to write a portable version of the function by using a volatile object on the right-hand side and forcing the compiler to preserve the stores to the array.</p>\n\n<pre><code>void volatileZeroMemory(void* ptr, unsigned long long size)\n{\n    volatile unsigned char zero = 0;\n    unsigned char* bytePtr = static_cast&lt;unsigned char*&gt;(ptr);\n\n    while (size--)\n    {\n        *bytePtr++ = zero;\n    }\n\n    zero = static_cast&lt;unsigned char*&gt;(ptr)[zero];\n}\n</code></pre>\n\n<p>The <code>zero</code> object is declared <code>volatile</code> that ensures the compiler can make no assumptions about its value even though it always evaluates as zero. </p>\n\n<p>The final assignment expression reads from a volatile index in the array and stores the value in a volatile object. Since this read cannot be optimized, it ensures that the compiler must generate the stores specified in the loop.</p>\n", "title": "Is the definition of &quot;volatile&quot; this volatile, or is GCC having some standard compliancy problems?", "answer_id": 38233312, "question_id": 38230856, "tags": ["c++", "c", "gcc", "standards"]}], "question_id": 38230856}, {"body": "<p>Instead of <code>2*i</code>, I carelessly wrote <code>2i</code>:</p>\n\n<pre><code>int foo(int i)\n{\n    2i;\n    return 2i;\n}\n</code></pre>\n\n<p>I expected the compiler to catch the error. But it did not. So is <code>2i</code> a valid statement in C? If so what does it do? Puzzled!</p>\n\n<p>I compiled using gcc version 5.3.0 and here is the assembly output:</p>\n\n<pre><code>    .file   \"strange.c\"\n    .text\n    .globl  foo\n    .type   foo, @function\nfoo:\n.LFB0:\n    .cfi_startproc\n    pushq   %rbp\n    .cfi_def_cfa_offset 16\n    .cfi_offset 6, -16\n    movq    %rsp, %rbp\n    .cfi_def_cfa_register 6\n    movl    %edi, -4(%rbp)\n    nop\n    popq    %rbp\n    .cfi_def_cfa 7, 8\n    ret\n    .cfi_endproc\n.LFE0:\n    .size   foo, .-foo\n    .ident  \"GCC: (GNU) 5.3.0\"\n    .section    .note.GNU-stack,\"\",@progbits\n</code></pre>\n", "title": "Why does the statement &quot;2i;&quot; NOT cause a compiler error?", "tags": ["c", "syntax"], "answer_count": 2, "link": "http://stackoverflow.com/questions/35183722/why-does-the-statement-2i-not-cause-a-compiler-error", "answers": [{"body": "<p>This is a <a href=\"https://gcc.gnu.org/onlinedocs/gcc/Complex.html\" rel=\"nofollow\"><em>gcc</em> extension</a>, and <code>2i</code> is the imaginary constant <a href=\"http://i.stack.imgur.com/efus9.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/efus9.png\" alt=\"enter image description here\"></a>. So you can write a complex number like so:</p>\n\n<pre><code>#include &lt;complex.h&gt;\n\n_Complex x = 4 + 5i;\n</code></pre>\n", "title": "Why does the statement &quot;2i;&quot; NOT cause a compiler error?", "answer_id": 35183761, "question_id": 35183722, "tags": ["c", "syntax"]}, {"body": "<p><code>2i</code> is a <code>gcc</code> extension for a complex integer literal, a pure imaginary number twice the square root of <code>-1</code>.  This extension is supported by <code>clang</code> as well.</p>\n\n<p>It is somewhat surprising that your compiling with <code>gcc 5.4.0</code> produces the posted assembly output:</p>\n\n<ul>\n<li>Compiling on <a href=\"http://gcc.godbolt.org/#\">http://gcc.godbolt.org/#</a> I get a compilation error from <code>gcc</code> 5.3.0: <code>http://gcc.godbolt.org/#</code>: <code>error: cannot convert '__complex__ int' to 'int' in return</code>.</li>\n<li>The posted assembly code for function <code>foo</code> is incorrect: it does not return <code>0</code>.  Converting the complex integer constant <code>2i</code> to <code>int</code> should return its real part <code>0</code>.</li>\n</ul>\n\n<p>Conversely, with <code>clang</code> 3.7, it compiles without a warning and generates optimum code, but of course not what you expect:</p>\n\n<pre><code>foo(int):                       # @foo(int)\n    xorl    %eax, %eax\n    retq\n</code></pre>\n\n<p>This syntax can be combined with other suffixes in any order. Compiling the code below with <code>clang -Weverything</code> gives me appropriate warnings <code>warning: imaginary constants are a GNU extension [-Wgnu-imaginary-constant]</code>:</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\nint main() {\n    /* complex integer literals */\n    printf(\"sizeof(2i) = %zd\\n\", sizeof(2i));\n    printf(\"sizeof(2ui) = %zd\\n\", sizeof(2ui));\n    printf(\"sizeof(2li) = %zd\\n\", sizeof(2li));\n    printf(\"sizeof(2lli) = %zd\\n\", sizeof(2lli));\n    /* complex floating point literals */\n    printf(\"sizeof(2.i) = %zd\\n\", sizeof(2.i));\n    printf(\"sizeof(2.fi) = %zd\\n\", sizeof(2.fi));\n    printf(\"sizeof(2e0fi) = %zd\\n\", sizeof(2e0fi));\n    printf(\"sizeof(2e0i) = %zd\\n\", sizeof(2e0i));\n    /* alternate order */\n    printf(\"sizeof(2il) = %zd\\n\", sizeof(2il));\n    printf(\"sizeof(2ill) = %zd\\n\", sizeof(2ill));\n    printf(\"sizeof(2.if) = %zd\\n\", sizeof(2.if));\n\n    return 0;\n}\n</code></pre>\n\n<p>It produces this output in my environment:</p>\n\n<pre><code>sizeof(2i) = 8\nsizeof(2ui) = 8\nsizeof(2li) = 16\nsizeof(2lli) = 16\nsizeof(2.i) = 16\nsizeof(2.fi) = 8\nsizeof(2e0fi) = 8\nsizeof(2e0i) = 16\nsizeof(2il) = 16\nsizeof(2ill) = 16\nsizeof(2.if) = 8\n</code></pre>\n\n<p>Try the last one with your syntax coloring editor <code>;-)</code></p>\n", "title": "Why does the statement &quot;2i;&quot; NOT cause a compiler error?", "answer_id": 35190356, "question_id": 35183722, "tags": ["c", "syntax"]}], "question_id": 35183722}, {"body": "<p>It's clearly stated in the <a href=\"https://docs.python.org/3.5/library/functions.html#int\">docs</a> that int(number) is a flooring type conversion:</p>\n\n<pre><code>int(1.23)\n1\n</code></pre>\n\n<p>and int(string) returns an int if and only if the string is an integer literal.</p>\n\n<pre><code>int('1.23')\nValueError\n\nint('1')\n1\n</code></pre>\n\n<p>Is there any special reason for that? I find it counterintuitive that the function floors in one case, but not the other.</p>\n", "title": "Counterintuitive behaviour of int() in python", "tags": ["python"], "answer_count": 4, "link": "http://stackoverflow.com/questions/36085185/counterintuitive-behaviour-of-int-in-python", "answers": [{"body": "<p>This is almost certainly a case of applying three of the principles from the <a href=\"https://www.python.org/dev/peps/pep-0020/\">Zen of Python</a>: </p>\n\n<blockquote>\n  <p>Explicit is better implicit. </p>\n  \n  <p>[...] practicality beats purity</p>\n  \n  <p>Errors should never pass silently </p>\n</blockquote>\n\n<p>Some percentage of the time, someone doing <code>int('1.23')</code> is calling the wrong conversion for their use case, and wants something like <code>float</code> or <code>decimal.Decimal</code> instead. In these cases, it's clearly better for them to get an immediate error that they can fix, rather than silently giving the wrong value. </p>\n\n<p>In the case that you <em>do</em> want to truncate that to an int, it is trivial to explicitly do so by passing it through <code>float</code> first, and then calling one of <code>int</code>, <code>round</code>, <code>trunc</code>, <code>floor</code> or <code>ceil</code> as appropriate. This also makes your code more self-documenting, guarding against a later modification \"correcting\" a hypothetical silently-truncating <code>int</code> call to <code>float</code> by making it clear that the rounded value <em>is</em> what you want.</p>\n", "title": "Counterintuitive behaviour of int() in python", "answer_id": 36085574, "question_id": 36085185, "tags": ["python"]}, {"body": "<p>There is no <em>special</em> reason. Python is simply applying its general principle of not performing implicit conversions, which are well-known causes of problems, particularly for newcomers, in languages such as Perl and Javascript.</p>\n\n<p><code>int(some_string)</code> is an explicit request to convert a string to integer format; the rules for this conversion specify that the string must contain a valid integer literal representation. <code>int(float)</code> is an explicit request to convert a float to an integer; the rules for this conversion specify that the float's fractional portion will be truncated.</p>\n\n<p>In order for <code>int(\"3.1459\")</code> to return <code>3</code> the interpreter would have to implicitly convert the string to a float. Since Python doesn't support implicit conversions, it chooses to raise an exception instead.</p>\n", "title": "Counterintuitive behaviour of int() in python", "answer_id": 36085637, "question_id": 36085185, "tags": ["python"]}, {"body": "<p>In simple words - they're not the same function. int( decimal ) and int( string ) are 2 different functions with the <em>same name</em> that return an integer.</p>\n\n<p>One is a string-integer-conversion, one is performing floor on a decimal, and they're both called 'int' because it's short and makes sense for each, but there's no implication they are providing the same or combined functionality</p>\n", "title": "Counterintuitive behaviour of int() in python", "answer_id": 36090233, "question_id": 36085185, "tags": ["python"]}, {"body": "<p>Sometimes a thought experiment can be useful.</p>\n\n<ul>\n<li>Behavior A: <code>int('1.23')</code> fails with an error.  This is the existing behavior.</li>\n<li>Behavior B: <code>int('1.23')</code> produces <code>1</code> without error.  This is what you're proposing.</li>\n</ul>\n\n<p>With behavior A, it's straightforward and trivial to get the effect of behavior B: use <code>int(float('1.23'))</code> instead.</p>\n\n<p>On the other hand, with behavior B, getting the effect of behavior A is significantly more complicated:</p>\n\n<pre><code>def parse_pure_int(s):\n    if \".\" in s:\n        raise ValueError(\"invalid literal for integer with base 10: \" + s)\n    return int(s)\n</code></pre>\n\n<p>(and even with the code above, I don't have complete confidence that there isn't some corner case that it mishandles.)</p>\n\n<p>Behavior A therefore is more expressive than behavior B.</p>\n\n<p>Another thing to consider: <code>'1.23'</code> is a string representation of a floating-point value.  Converting <code>'1.23'</code> to an integer conceptually involves two conversions (string to float to integer), but <code>int(1.23)</code> and <code>int('1')</code> each involve only one conversion.</p>\n\n<hr>\n\n<p>Edit:</p>\n\n<p>And indeed, there are corner cases that the above code would not handle: <code>1e-2</code> and <code>1E-2</code> are both floating point values too.</p>\n", "title": "Counterintuitive behaviour of int() in python", "answer_id": 36098510, "question_id": 36085185, "tags": ["python"]}], "question_id": 36085185}, {"body": "<pre><code>#include &lt;stdio.h&gt;\n#define decode(s,t,u,m,p,e,d) m##s##u##t\n#define begin decode(a,n,i,m,a,t,e)\n\nint begin()\n{\n    printf(\"Ha HA see how it is?? \");\n}\n</code></pre>\n\n<p>Does this indirectly call <code>main</code>? If yes, then how?</p>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "tags": ["c", "c-preprocessor", "obfuscation"], "answer_count": 6, "link": "http://stackoverflow.com/questions/36449358/this-obfuscated-c-code-claims-to-run-without-a-main-but-what-does-it-really-d", "answers": [{"body": "<p>C define execution environment in two categories: <em>freestanding</em> and <em>hosted</em>. In both execution environment a function is called by the environment for program startup.<br>\nIn a <em>freestanding</em> environment program startup function can be implementation defined while in <em>hosted</em> environment it should be <code>main</code>. No program in C can run without program startup function on the defined environments. </p>\n\n<p>In your case, <code>main</code> is hidden by the preprocessor definitions. <code>begin()</code> will expand to  <code>decode(a,n,i,m,a,t,e)</code> which further will be expanded to <code>main</code>.  </p>\n\n<pre><code>int begin() -&gt; int decode(a,n,i,m,a,t,e)() -&gt; int m##a##i##n() -&gt; int main() \n</code></pre>\n\n<hr>\n\n<p><code>decode(s,t,u,m,p,e,d)</code> is a parameterized macro with 7 parameters. Replacement list for this macro is <code>m##s##u##t</code>. <code>m, s, u</code> and <code>t</code> are 4<sup>th</sup>, 1<sup>st</sup>, 3<sup>rd</sup> and 2<sup>nd</sup> parameter used in the replacement list.  </p>\n\n<pre><code>s, t, u, m, p, e, d\n1  2  3  4  5  6  7\n</code></pre>\n\n<p>Rest are of no use (<em>just to confuse</em>). Argument passed to <code>decode</code> is \"<strong>a</strong>,<strong>n</strong>,<strong>i</strong>,<strong>m</strong>,a,t,e\" so, the identifiers <code>m, s, u</code> and <code>t</code> are replaced with arguments <code>m, a, i</code> and <code>n</code>, respectively.</p>\n\n<pre><code> m --&gt; m  \n s --&gt; a \n u --&gt; i \n t --&gt; n\n</code></pre>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "answer_id": 36449421, "question_id": 36449358, "tags": ["c", "c-preprocessor", "obfuscation"]}, {"body": "<p>Try using <code>gcc -E source.c</code>, output ends with:</p>\n\n<pre><code>int main()\n{\n    printf(\"Ha HA see how it is?? \");\n}\n</code></pre>\n\n<p>So a <code>main()</code> function is actually generated by preprocessor.</p>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "answer_id": 36449462, "question_id": 36449358, "tags": ["c", "c-preprocessor", "obfuscation"]}, {"body": "<p><code>decode(a,b,c,d,[...])</code> shuffles the first four arguments and joins them to get a new identifier, in the order <code>dacb</code>. (The remaining three arguments are ignored.) For instance, <code>decode(a,n,i,m,[...])</code> gives the identifier <code>main</code>. Note that this is what the <code>begin</code> macro is defined as.</p>\n\n<p>Therefore, the <code>begin</code> macro is simply defined as <code>main</code>.</p>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "answer_id": 36449510, "question_id": 36449358, "tags": ["c", "c-preprocessor", "obfuscation"]}, {"body": "<p>Someone is trying to act like Magician.\nHe thinks he can trick us. But we all know, c program execution begins with <code>main()</code>. </p>\n\n<p>The <code>int begin()</code> will be replaced with <code>decode(a,n,i,m,a,t,e)</code> by one pass of preprocessor stage. Then again, <code>decode(a,n,i,m,a,t,e)</code> will be replaced with m##a##i##n. As by positional association of macro call,  <code>s</code> will has a value of character <code>a</code>. Likewise, <code>u</code> will be replaced by 'i' and <code>t</code> will be replaced by 'n'. And, that's how, <code>m##s##u##t</code> will become <code>main</code></p>\n\n<p>Regarding, <code>##</code> symbol in macro expansion, it is the preprocessing operator and it performs token pasting. When a macro is expanded, the two tokens on either side of each \u2018##\u2019 operator are combined into a single token, which then replaces the \u2018##\u2019 and the two original tokens in the macro expansion.</p>\n\n<p>If you don't believe me, you can compile your code with <code>-E</code> flag. It will stop compilation process after preprocessing and you can see the result of token pasting.</p>\n\n<pre><code>gcc -E FILENAME.c\n</code></pre>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "answer_id": 36452822, "question_id": 36449358, "tags": ["c", "c-preprocessor", "obfuscation"]}, {"body": "<p>The program in question <em>does</em> call <code>main()</code> due to macro expansion, but your assumption is flawed - it <em>doesn't</em> have to call <code>main()</code> at all! </p>\n\n<p>Strictly speaking, you can have a C program and be able to compile it without having a <code>main</code> symbol. <code>main</code> is something that the <code>c library</code> expects to jump in to, after it has finished its own initialization. Usually you jump into <code>main</code> from the libc symbol known as <code>_start</code>. It is always possible to have a very valid program, that simply executes assembly, without having a main. Take a look at this:</p>\n\n<pre><code>/* This must be compiled with the flag -nostdlib because otherwise the\n * linker will complain about multiple definitions of the symbol _start\n * (one here and one in glibc) and a missing reference to symbol main\n * (that the libc expects to be linked against).\n */\n\nvoid\n_start ()\n{\n    /* calling the write system call, with the arguments in this order:\n     * 1. the stdout file descriptor\n     * 2. the buffer we want to print (Here it's just a string literal).\n     * 3. the amount of bytes we want to write.\n     */\n    asm (\"int $0x80\"::\"a\"(4), \"b\"(1), \"c\"(\"Hello world!\\n\"), \"d\"(13));\n    asm (\"int $0x80\"::\"a\"(1), \"b\"(0)); /* calling exit syscall, with the argument to be 0 */\n}\n</code></pre>\n\n<p>Compile the above with <code>gcc -nostdlib without_main.c</code>, and see it printing <code>Hello World!</code> on the screen just by issuing system calls (interrupts) in inline assembly.</p>\n\n<p>For more information about this particular issue, check out the <a href=\"https://blogs.oracle.com/ksplice/entry/hello_from_a_libc_free\">ksplice blog</a></p>\n\n<p>Another interesting issue, is that you can also have a program that compiles without having the <code>main</code> symbol correspond to a C function. For instance you can have the following as a very valid C program, that only makes the compiler whine when you up the Warnings level.</p>\n\n<pre><code>/* These values are extracted from the decimal representation of the instructions\n * of a hello world program written in asm, that gdb provides.\n */\nconst int main[] = {\n    -443987883, 440, 113408, -1922629632,\n    4149, 899584, 84869120, 15544,\n    266023168, 1818576901, 1461743468, 1684828783,\n    -1017312735\n};\n</code></pre>\n\n<p>The values in the array are bytes that correspond to the instructions needed to print Hello World on the screen. For a more detailed account of how this specific program works, take a look at this <a href=\"https://jroweboy.github.io/c/asm/2015/01/26/when-is-main-not-a-function.html\">blog post</a>, which is where I also read it first.</p>\n\n<p>I want to make one final notice about these programs. I do not know if they register as valid C programs according to the C language specification, but compiling these and running them is certainly very possible, even if they violate the specification itself.</p>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "answer_id": 36476439, "question_id": 36449358, "tags": ["c", "c-preprocessor", "obfuscation"]}, {"body": "<p>In your example, <code>main()</code> function is actually present, because <code>begin</code> is a macro which the compiler replaces with <code>decode</code> macro which in turn replaced by the expression m##s##u##t. Using macro expansion <code>##</code>, you will reach the word <code>main</code> from <code>decode</code>. This is a trace:</p>\n\n<pre><code>begin --&gt; decode(a,n,i,m,a,t,e) --&gt; m##parameter1##parameter3##parameter2 ---&gt; main\n</code></pre>\n\n<p>It's just a trick to have <code>main()</code>, but using the name <code>main()</code> for the program's entry function is not necessary in C programming language. It depends on your operating systems and the linker as one of its tools.</p>\n\n<p>In Windows, you don't always use <code>main()</code>, but <a href=\"https://msdn.microsoft.com/en-us/library/windows/desktop/ff381406%28v=vs.85%29.aspx\" rel=\"nofollow\">rather <code>WinMain</code> or <code>wWinMain</code></a>, although <a href=\"http://stackoverflow.com/a/13872211/2316442\">you can use <code>main()</code>, even with Microsoft's toolchain</a>. In Linux, one can use <code>_start</code>.</p>\n\n<p>It's up to the linker as an operating system tool to set the entry point, and not the language itself. You can even <a href=\"http://unix.stackexchange.com/questions/223385\">set our own entry point, and you can make a library that is also executable</a>!</p>\n", "title": "This obfuscated C code claims to run without a main(), but what does it really do?", "answer_id": 36695058, "question_id": 36449358, "tags": ["c", "c-preprocessor", "obfuscation"]}], "question_id": 36449358}, {"body": "<p>Suppose I have a number of statements that I want to execute in\na fixed order. I want to use g++ with optimization level 2, so some\nstatements could be reordered. What tools does one have to enforce a certain ordering of statements?</p>\n\n<p>Consider the following example.</p>\n\n<pre><code>using Clock = std::chrono::high_resolution_clock;\n\nauto t1 = Clock::now(); // Statement 1\nfoo();                  // Statement 2\nauto t2 = Clock::now(); // Statement 3\n\nauto elapsedTime = t2 - t1;\n</code></pre>\n\n<p>In this example it is important that the statements 1-3 are executed in\nthe given order. However, can't the compiler think statement 2 is\nindependent of 1 and 3 and execute the code as follows?</p>\n\n<pre><code>using Clock=std::chrono::high_resolution_clock;\n\nfoo();                  // Statement 2\nauto t1 = Clock::now(); // Statement 1\nauto t2 = Clock::now(); // Statement 3\n\nauto elapsedTime = t2 - t1;\n</code></pre>\n", "title": "Enforcing statement order in C++", "tags": ["c++", "c++11", "order-of-evaluation"], "answer_count": 6, "link": "http://stackoverflow.com/questions/37786547/enforcing-statement-order-in-c", "answers": [{"body": "<p>No it can't. According to the C++ standard [intro.execution]: </p>\n\n<blockquote>\n  <p>14 Every value computation and side effect associated with a\n  full-expression is sequenced before every value computation and side\n  effect associated with the next full-expression to be evaluated.</p>\n</blockquote>\n\n<p>A full-expression is basically a statement terminated by a semicolon. As you can see the above rule stipulates statements must be executed in order. It is <em>within</em> statements that the compiler is allowed more free rein (i.e. it is under some circumstance allowed to evaluate expressions that make up a statement in orders other than left-to-right or anything else specific).</p>\n\n<p>Note the conditions for the as-if rule to apply are not met here. It is unreasonable to think that any compiler would be able to <em>prove</em> that reordering calls to get the system time would not affect observable program behaviour. If there was a circumstance in which two calls to get the time could be reordered without changing observed behaviour, it would be extremely inefficient to actually produce a compiler that analyses a program with enough understanding to be able to infer this with certainty.</p>\n", "title": "Enforcing statement order in C++", "answer_id": 37786733, "question_id": 37786547, "tags": ["c++", "c++11", "order-of-evaluation"]}, {"body": "<p><strong>No.</strong></p>\n\n<p>Sometimes, by the \"as-if\" rule, statements may be re-ordered. This is not because they are logically independent of each other, but because that independence allows such a re-ordering to occur without changing the semantics of the program.</p>\n\n<p>Moving a system call that obtains the current time obviously does not satisfy that condition. A compiler that knowingly or unknowingly does so is non-compliant and really silly.</p>\n\n<p>In general, I wouldn't expect any expression that results in a system call to be \"second-guessed\" by even an aggressively optimizing compiler. It just doesn't know enough about what that system call does.</p>\n", "title": "Enforcing statement order in C++", "answer_id": 37786918, "question_id": 37786547, "tags": ["c++", "c++11", "order-of-evaluation"]}, {"body": "<p>Reordering may be done by the compiler, or by the processor. </p>\n\n<p>Most compilers offer a platform-specific method to prevent reordering of read-write instructions. On gcc, this is </p>\n\n<pre><code>asm volatile(\"\" ::: \"memory\");\n</code></pre>\n\n<p>(<a href=\"https://en.wikipedia.org/wiki/Memory_ordering#Compile-time_memory_barrier_implementation\">More information here</a>)</p>\n\n<p>Note that this only indirectly prevents reordering operations, as long as they depend on the reads / writes. </p>\n\n<p><strong>In practice</strong> I haven't yet seen a system where the system call in <code>Clock::now()</code> does have the same effect as such a barrier. You could inspect the resulting assembly to be sure. </p>\n\n<p>It is not uncommon, however, that the function under test gets evaluated during compile time. To enforce \"realistic\" execution, you may need to derive input for <code>foo()</code> from I/O or a <code>volatile</code> read. </p>\n\n<hr>\n\n<p>Another option would be to disable inlining for <code>foo()</code> - again, this is compiler  specific and usually not portable, but would have the same effect.</p>\n\n<p>On gcc, this would be <code>__attribute__ ((noinline))</code></p>\n\n<hr>\n\n<p>@Ruslan brings up a fundamental issue: How realistic is this measurement? </p>\n\n<p>Execution time is affected by many factors: one is the actual hardware we are running on, the other is concurrent access to shared resources like cache, memory, disk and CPU cores. </p>\n\n<p>So what we usually do to get <em>comparable</em> timings: make sure they are <em>reproducible</em> with a low error margin. This makes them somewhat artificial. </p>\n\n<p>\"hot cache\" vs. \"cold cache\" execution performance can easily differ by an order of magnitude - but in reality, it will be something inbetween (\"lukewarm\"?)</p>\n", "title": "Enforcing statement order in C++", "answer_id": 37787147, "question_id": 37786547, "tags": ["c++", "c++11", "order-of-evaluation"]}, {"body": "<p><strong>Summary:</strong></p>\n\n<p>There seems to be no guaranteed way to prevent reordering, but as long as link-time/full-program optimisation is not enabled, <strong>locating the called function in a separate compilation unit seems a fairly good bet</strong>.  (At least with GCC, although logic would suggest that this is likely with other compilers too.)  This comes at the cost of the function call - inlined code is by definition in the same compilation unit and open to reordering.</p>\n\n<p><strong>Original answer:</strong></p>\n\n<p>GCC reorders the calls under -O2 optimisation:</p>\n\n<pre><code>#include &lt;chrono&gt;\nstatic int foo(int x)    // 'static' or not here doesn't affect ordering.\n{\n    return x*2;\n}\nint fred(int x)\n{\n    auto t1 = std::chrono::high_resolution_clock::now();\n    int y = foo(x);\n    auto t2 = std::chrono::high_resolution_clock::now();\n    return y;\n}\n</code></pre>\n\n<p>GCC 5.3.0:</p>\n\n<p><code>g++ -S --std=c++11 -O0 fred.cpp</code> :</p>\n\n<pre><code>_ZL3fooi:\n        pushq   %rbp\n        movq    %rsp, %rbp\n        movl    %ecx, 16(%rbp)\n        movl    16(%rbp), %eax\n        addl    %eax, %eax\n        popq    %rbp\n        ret\n_Z4fredi:\n        pushq   %rbp\n        movq    %rsp, %rbp\n        subq    $64, %rsp\n        movl    %ecx, 16(%rbp)\n        call    _ZNSt6chrono3_V212system_clock3nowEv\n        movq    %rax, -16(%rbp)\n        movl    16(%rbp), %ecx\n        call    _ZL3fooi\n        movl    %eax, -4(%rbp)\n        call    _ZNSt6chrono3_V212system_clock3nowEv\n        movq    %rax, -32(%rbp)\n        movl    -4(%rbp), %eax\n        addq    $64, %rsp\n        popq    %rbp\n        ret\n</code></pre>\n\n<p>But:</p>\n\n<p><code>g++ -S --std=c++11 -O2 fred.cpp</code> : </p>\n\n<pre><code>_Z4fredi:\n        pushq   %rbx\n        subq    $32, %rsp\n        movl    %ecx, %ebx\n        call    _ZNSt6chrono3_V212system_clock3nowEv\n        call    _ZNSt6chrono3_V212system_clock3nowEv\n        leal    (%rbx,%rbx), %eax\n        addq    $32, %rsp\n        popq    %rbx\n        ret\n</code></pre>\n\n<p>Now, with foo() as an extern function:</p>\n\n<pre><code>#include &lt;chrono&gt;\nint foo(int x);\nint fred(int x)\n{\n    auto t1 = std::chrono::high_resolution_clock::now();\n    int y = foo(x);\n    auto t2 = std::chrono::high_resolution_clock::now();\n    return y;\n}\n</code></pre>\n\n<p><code>g++ -S --std=c++11 -O2 fred.cpp</code> : </p>\n\n<pre><code>_Z4fredi:\n        pushq   %rbx\n        subq    $32, %rsp\n        movl    %ecx, %ebx\n        call    _ZNSt6chrono3_V212system_clock3nowEv\n        movl    %ebx, %ecx\n        call    _Z3fooi\n        movl    %eax, %ebx\n        call    _ZNSt6chrono3_V212system_clock3nowEv\n        movl    %ebx, %eax\n        addq    $32, %rsp\n        popq    %rbx\n        ret\n</code></pre>\n\n<p>BUT, if this is linked with -flto (link-time optimisation):</p>\n\n<pre><code>0000000100401710 &lt;main&gt;:\n   100401710:   53                      push   %rbx\n   100401711:   48 83 ec 20             sub    $0x20,%rsp\n   100401715:   89 cb                   mov    %ecx,%ebx\n   100401717:   e8 e4 ff ff ff          callq  100401700 &lt;__main&gt;\n   10040171c:   e8 bf f9 ff ff          callq  1004010e0 &lt;_ZNSt6chrono3_V212system_clock3nowEv&gt;\n   100401721:   e8 ba f9 ff ff          callq  1004010e0 &lt;_ZNSt6chrono3_V212system_clock3nowEv&gt;\n   100401726:   8d 04 1b                lea    (%rbx,%rbx,1),%eax\n   100401729:   48 83 c4 20             add    $0x20,%rsp\n   10040172d:   5b                      pop    %rbx\n   10040172e:   c3                      retq\n</code></pre>\n", "title": "Enforcing statement order in C++", "answer_id": 37789799, "question_id": 37786547, "tags": ["c++", "c++11", "order-of-evaluation"]}, {"body": "<p>The C++ language defines what is observable in a number of ways.</p>\n\n<p>If <code>foo()</code> does nothing observable, then it can be eliminated completely.  If <code>foo()</code> only does a computation that stores values in \"local\" state (be it on the stack or in an object somewhere), <em>and</em> the compiler can prove that no safely-derived pointer can get into the <code>Clock::now()</code> code, then there are no observable consequences to moving the <code>Clock::now()</code> calls.</p>\n\n<p>If <code>foo()</code> interacted with a file or the display, and the compiler cannot prove that <code>Clock::now()</code> does <em>not</em> interact with the file or the display, then reordering cannot be done, because interaction with a file or display is observable behavior.</p>\n\n<p>While you can use compiler-specific hacks to force code not to move around (like inline assembly), another approach is to attempt to outsmart your compiler.</p>\n\n<p>Create a dynamically loaded library.  Load it prior to the code in question.</p>\n\n<p>That library exposes one thing:</p>\n\n<pre><code>namespace details {\n  void execute( void(*)(void*), void *);\n}\n</code></pre>\n\n<p>and wraps it like this:</p>\n\n<pre><code>template&lt;class F&gt;\nvoid execute( F f ) {\n  struct bundle_t {\n    F f;\n  } bundle = {std::forward&lt;F&gt;(f)};\n\n  auto tmp_f = [](void* ptr)-&gt;void {\n    auto* pb = static_cast&lt;bundle_t*&gt;(ptr);\n    (pb-&gt;f)();\n  };\n  details::execute( tmp_f, &amp;bundle );\n}\n</code></pre>\n\n<p>which packs up a nullary lambda and uses the dynamic library to run it in a context that the compiler cannot understand.</p>\n\n<p>Inside the dynamic library, we do:</p>\n\n<pre><code>void details::execute( void(*f)(void*), void *p) {\n  f(p);\n}\n</code></pre>\n\n<p>which is pretty simple.</p>\n\n<p>Now to reorder the calls to <code>execute</code>, it must understand the dynamic library, which it cannot while compiling your test code.</p>\n\n<p>It can still eliminate <code>foo()</code>s with zero side effects, but you win some, you lose some.</p>\n", "title": "Enforcing statement order in C++", "answer_id": 37792745, "question_id": 37786547, "tags": ["c++", "c++11", "order-of-evaluation"]}, {"body": "<p>I'd like to try to provide a somewhat more comprehensive answer after this was discussed with the C++ standards committee. In addition to being a member of the C++ committee, I'm also a developer on the LLVM and Clang compilers.</p>\n\n<p>Fundamentally, there is no way to use a barrier or some operation in the sequence to achieve these transformations. The fundamental problem is that the operational semantics of something like an integer addition are <em>totally known</em> to the implementation. It can simulate them, it knows they cannot be observed by correct programs, and is always free to move them around.</p>\n\n<p>We could try to prevent this, but it would have extremely negative results and would ultimately fail.</p>\n\n<p>First, the only way to prevent this in the compiler is to tell it that all of these basic operations are observable. The problem is that this then would preclude the overwhelming majority of compiler optimizations. Inside the compiler, we have essentially no good mechanisms to model that the <em>timing</em> is observable but nothing else. We don't even have a good model of <em>what operations take time</em>. As an example, does converting a 32-bit unsigned integer to a 64-bit unsigned integer take time? It takes zero time on x86-64, but on other architectures it takes non-zero time. There is no generically correct answer here.</p>\n\n<p>But even if we succeed through some heroics at preventing the compiler from reordering these operations, there is no guarantee this will be enough. Consider a valid and conforming way to execute your C++ program on an x86 machine: DynamoRIO. This is a system that dynamically evaluates the machine code of the program. One thing it can do is online optimizations, and it is even capable of speculatively executing the entire range of basic arithmetic instructions outside of the timing. And this behavior isn't unique to dynamic evaluators, the actual x86 CPU will also speculate (a much smaller number of) instructions and reorder them dynamically.</p>\n\n<p>The essential realization is that the fact that arithmetic isn't observable (even at the timing level) is something that permeates the layers of the computer. It is true for the compiler, the runtime, and often even the hardware. Forcing it to be observable would both dramatically constrain the compiler, but it would also dramatically constrain the hardware.</p>\n\n<p>But all of this should not cause you to lose hope. When you want to time the execution of basic mathematical operations, we have well studied techniques that work reliably. Typically these are used when doing <em>micro-benchmarking</em>. I gave a talk about this at CppCon2015: <a href=\"https://youtu.be/nXaxk27zwlk\">https://youtu.be/nXaxk27zwlk</a></p>\n\n<p>The techniques shown there are also provided by various micro-benchmark libraries such as Google's: <a href=\"https://github.com/google/benchmark#preventing-optimisation\">https://github.com/google/benchmark#preventing-optimisation</a></p>\n\n<p>The key to these techniques is to focus on the data. You make the input to the computation opaque to the optimizer and the result of the computation opaque to the optimizer. Once you've done that, you can time it reliably. Let's look at a realistic version of the example in the original question, but with the definition of <code>foo</code> fully visible to the implementation. I've also extracted a (non-portable) version of <code>DoNotOptimize</code> from the Google Benchmark library which you can find here: <a href=\"https://github.com/google/benchmark/blob/master/include/benchmark/benchmark_api.h#L208\">https://github.com/google/benchmark/blob/master/include/benchmark/benchmark_api.h#L208</a></p>\n\n<pre><code>#include &lt;chrono&gt;\n\ntemplate &lt;class T&gt;\n__attribute__((always_inline)) inline void DoNotOptimize(const T &amp;value) {\n  asm volatile(\"\" : \"+m\"(const_cast&lt;T &amp;&gt;(value)));\n}\n\n// The compiler has full knowledge of the implementation.\nstatic int foo(int x) { return x * 2; }\n\nauto time_foo() {\n  using Clock = std::chrono::high_resolution_clock;\n\n  auto input = 42;\n\n  auto t1 = Clock::now();         // Statement 1\n  DoNotOptimize(input);\n  auto output = foo(input);       // Statement 2\n  DoNotOptimize(output);\n  auto t2 = Clock::now();         // Statement 3\n\n  return t2 - t1;\n}\n</code></pre>\n\n<p>Here we ensure that the input data and the output data are marked as un-optimizable around the computation <code>foo</code>, and only around those markers are the timings computed. Because you are using data to pincer the computation, it is guaranteed to stay between the two timings and yet the computation itself is allowed to be optimized. The resulting x86-64 assembly generated by a recent build of Clang/LLVM is:</p>\n\n<pre><code>% ./bin/clang++ -std=c++14 -c -S -o - so.cpp -O3\n        .text\n        .file   \"so.cpp\"\n        .globl  _Z8time_foov\n        .p2align        4, 0x90\n        .type   _Z8time_foov,@function\n_Z8time_foov:                           # @_Z8time_foov\n        .cfi_startproc\n# BB#0:                                 # %entry\n        pushq   %rbx\n.Ltmp0:\n        .cfi_def_cfa_offset 16\n        subq    $16, %rsp\n.Ltmp1:\n        .cfi_def_cfa_offset 32\n.Ltmp2:\n        .cfi_offset %rbx, -16\n        movl    $42, 8(%rsp)\n        callq   _ZNSt6chrono3_V212system_clock3nowEv\n        movq    %rax, %rbx\n        #APP\n        #NO_APP\n        movl    8(%rsp), %eax\n        addl    %eax, %eax              # This is \"foo\"!\n        movl    %eax, 12(%rsp)\n        #APP\n        #NO_APP\n        callq   _ZNSt6chrono3_V212system_clock3nowEv\n        subq    %rbx, %rax\n        addq    $16, %rsp\n        popq    %rbx\n        retq\n.Lfunc_end0:\n        .size   _Z8time_foov, .Lfunc_end0-_Z8time_foov\n        .cfi_endproc\n\n\n        .ident  \"clang version 3.9.0 (trunk 273389) (llvm/trunk 273380)\"\n        .section        \".note.GNU-stack\",\"\",@progbits\n</code></pre>\n\n<p>Here you can see the compiler optimizing the call to <code>foo(input)</code> down to a single instruction, <code>addl %eax, %eax</code>, but without moving it outside of the timing or eliminating it entirely despite the constant input.</p>\n\n<p>Hope this helps, and the C++ standards committee is looking at the possibility of standardizing APIs similar to <code>DoNotOptimize</code> here.</p>\n", "title": "Enforcing statement order in C++", "answer_id": 38025837, "question_id": 37786547, "tags": ["c++", "c++11", "order-of-evaluation"]}], "question_id": 37786547}, {"body": "<p>Using <code>import numpy as np</code> I've noticed that </p>\n\n<pre><code>np.tan(np.pi/2)\n</code></pre>\n\n<p>gives the number in the title and not <code>np.inf</code></p>\n\n<pre><code>16331239353195370.0\n</code></pre>\n\n<p>I'm curious about this number. Is it related to some system machine precision parameter? Could I have calculated it from something? (I'm thinking along the lines of something similar to <code>sys.float_info</code>)</p>\n\n<p><strong>EDIT:</strong> The same result is indeed reproducible  in other environments such as Java, octace, matlab... The suggested dupe does not explain why, though.</p>\n", "title": "Is there special significance to 16331239353195370.0?", "tags": ["python", "numpy", "numerical-methods"], "answer_count": 1, "link": "http://stackoverflow.com/questions/38295501/is-there-special-significance-to-16331239353195370-0", "answers": [{"body": "<p><code>pi</code> isn't exactly representable as Python float (same as the platform C's <code>double</code> type).  The closest representable approximation is used.</p>\n\n<p>Here's the exact approximation in use on my box (probably the same as on your box):</p>\n\n<pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; (math.pi / 2).as_integer_ratio()\n(884279719003555, 562949953421312)\n</code></pre>\n\n<p>To find the tangent of that ratio, I'm going to switch to wxMaxima now:</p>\n\n<pre><code>(%i1) fpprec: 32;\n(%o1) 32\n(%i2) tan(bfloat(884279719003555) / 562949953421312);\n(%o2) 1.6331239353195369755967737041529b16\n</code></pre>\n\n<p>So essentially identical to what you got.  The binary approximation to <code>pi/2</code> used is a little bit less than the mathematical (\"infinite precision\") value of <code>pi/2</code>.  So you get a very large tangent instead of <code>infinity</code>.  The computed <code>tan()</code> is appropriate for the actual input!</p>\n\n<p>For exactly the same kinds of reasons, e.g.,</p>\n\n<pre><code>&gt;&gt;&gt; math.sin(math.pi)\n1.2246467991473532e-16\n</code></pre>\n\n<p>doesn't return 0.  The approximation <code>math.pi</code> is a little bit less than <code>pi</code>, and the displayed result is correct <em>given</em> that truth.</p>\n\n<h2>OTHER WAYS OF SEEING math.pi</h2>\n\n<p>There are several ways to see the exact approximation in use:</p>\n\n<pre><code>&gt;&gt;&gt; import math\n&gt;&gt;&gt; math.pi.as_integer_ratio()\n(884279719003555, 281474976710656)\n</code></pre>\n\n<p><code>math.pi</code> is exactly equal to the mathematical (\"infinite precision\") value of that ratio.</p>\n\n<p>Or as an exact float in hex notation:</p>\n\n<pre><code>&gt;&gt;&gt; math.pi.hex()\n'0x1.921fb54442d18p+1'\n</code></pre>\n\n<p>Or in a way most easily understood by just about everyone:</p>\n\n<pre><code>&gt;&gt;&gt; import decimal\n&gt;&gt;&gt; decimal.Decimal(math.pi)\nDecimal('3.141592653589793115997963468544185161590576171875')\n</code></pre>\n\n<p>While it may not be immediately obvious, every finite binary float is exactly representable as a finite decimal float (the reverse is <em>not</em> true; e.g. the decimal <code>0.1</code> is not exactly representable as a finite binary float), and the <code>Decimal(some_float)</code> constructor produces the exact equivalent.</p>\n\n<p>Here's the true value of <code>pi</code> followed by the exact decimal value of <code>math.pi</code>, and a caret on the third line points to the first digit where they differ:</p>\n\n<pre><code>true    3.14159265358979323846264338327950288419716939937510...\nmath.pi 3.141592653589793115997963468544185161590576171875\n                         ^\n</code></pre>\n\n<p><code>math.pi</code> is the same across \"almost all\" boxes now, because almost all boxes now use the same binary floating-point format (IEEE 754 double precision).  You can use any of the ways above to confirm that on <em>your</em> box, or to find the precise approximation in use if your box is an exception.</p>\n", "title": "Is there special significance to 16331239353195370.0?", "answer_id": 38295695, "question_id": 38295501, "tags": ["python", "numpy", "numerical-methods"]}], "question_id": 38295501}, {"body": "<p>There is a new autocomplete in Xcode. Probably might be useful because it checks not only beginning of names etc. But I found that very often it doesn't find a class name or a const name at all etc. I need to type in entire name by myself. Over all I found it makes my life harder and coding more time consuming. Is there a way to switch to the old way it used to work?</p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "tags": ["autocomplete", "xcode7.3"], "answer_count": 9, "link": "http://stackoverflow.com/questions/36169099/xcode-7-3-autocomplete-is-so-frustrating", "answers": [{"body": "<p>I was searching, hoping that I was alone in this so it would mean I just have to reinstall or figure out what's wrong with my Mac.<br>\nIt happens to me too.<br>\nI have a CustomClass.h and CustomClass.m with their own alloc/init methods, public methods and delegates.<br>\nI use:  </p>\n\n<pre><code>#import \"CustomClass.h\"\n</code></pre>\n\n<p>And later, when trying to add the CustomClassDelegate to my interface, it won't autocomplete. I have to type it out perfectly.<br>\nIn my init, it won't autocomplete the class declaration so I have to perfectly type out:  </p>\n\n<pre><code>CustomClass *customClass = [[CustomClass alloc] init];\n</code></pre>\n\n<p>It won't even auto correct it inside of the alloc bracket. I have to type it perfectly for both the variable declaration and the alloc bracket. </p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 36188300, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p>It seems that clearing the checkbox \"Enable type-over completions\" in XCode -> Preferences -> Text Editing does the trick. At least in my case autocompletion fell back to a sort of old way, so it could autocomplete the class name that I had to type in manually before that.</p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 36218021, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<blockquote>\n  <p>it doesn't find a class name</p>\n</blockquote>\n\n<p>As a work-around, you can try to press\n<strong><em>Command + Shift + K</em></strong> and  <strong><em>Command + B</em></strong> \nSeveral times\uff0cit works temporarily.</p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 36300631, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p>This is by no means an adequate solution, BUT it has allowed me to (barely) maintain my sanity the past few days:\nAfter every build, you need to trash your Derived Data folder. You can find this folder in <code>Xcode &gt; Preferences &gt; Locations &gt; Derived Data</code>. Just trash the whole thing and it'll kick off a re-indexing step that should restore proper autocomplete functionality.\nUnfortunately, I've found that once I build, the autocomplete behavior reverts to its broken state.</p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 36900891, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p>Xcode 7.3.1</p>\n\n<ol>\n<li><p>In Xcode > Preferences > Text Editing </p>\n\n<p>uncheck <code>Enable type-over completions</code></p></li>\n<li><p>restart Xcode</p></li>\n</ol>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 38651972, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p>I have the impression that some 'parts' of autocompletion simply fail after a while. I use to restart the Mac to get it back working. But sometimes it fails quite soon again.\nMaybe the answer of @Alex Bykov combined with a restart will do the trick.</p>\n\n<p>Anyway: auto-completion of Xcode always was crap. AppCode used to get it much better. Unfortunately not yet with swift.</p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 38685829, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p>Xcode 7.3.1</p>\n\n<p>I will share another posible reason, that after couple of days we found out. We have multiple schemes, and in one of them, the <strong>bridging header</strong> was importing a file that didn't exist anymore. So, it didn't break while compiling and running (the header belongs to another scheme) but it caused the autocompletion to break (couldn't find any objective-c class).</p>\n\n<p>Hope it would help someone!</p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 38724121, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p><strong>Control + SpaceBar</strong> triggers XCode\u2019s autocomplete functionality</p>\n\n<p>or</p>\n\n<p>Press <strong>Esc</strong> Key </p>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 39181391, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}, {"body": "<p>I ran into this issue while trying to make an OS X app and I was able to fix this issue by making sure that the Xcode 7.3 documentation and the OS X 10.11.4 documentation was actually downloaded. Doing this fully restored my autocomplete functionality. My full instructions are below as well as in my answer to a similar question: <a href=\"http://stackoverflow.com/a/39420664/3444925\">http://stackoverflow.com/a/39420664/3444925</a></p>\n\n<blockquote>\n  <p>I had this problem myself and after looking through all the other similar questions &amp; answers about this, I couldn't find a solution. However, I finally found what worked for me.</p>\n  \n  <p>Go to <code>Xcode -&gt; Preferences -&gt; Components</code>. There you will probably find a screen that looks like the following:</p>\n  \n  <p><a href=\"http://i.stack.imgur.com/5vDt5.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/5vDt5.png\" alt=\"enter image description here\"></a></p>\n  \n  <p>This shows that the documentation has not has not been downloaded and therefore, any attempts to re-index or re-build the application without downloading the documentation would prevent you from being able to use the autocompletion functionality.</p>\n  \n  <p>Once I downloaded the Xcode 7.3 Documentation and the OSX 10.11.4 Documentation, this was enough for me to get the autocomplete functionality back (I was trying to build an OS X app, so feel free to download as much documentation as is relevant for you).</p>\n</blockquote>\n", "title": "Xcode 7.3 autocomplete is so frustrating", "answer_id": 39420715, "question_id": 36169099, "tags": ["autocomplete", "xcode7.3"]}], "question_id": 36169099}, {"body": "<p>The Akka Streams library already comes with quite a <a href=\"http://doc.akka.io/docs/akka/2.4.2-RC1/scala/stream/index.html\">wealth of documentation</a>. However, the main problem for me is that it provides too much material - I feel quite overwhelmed by the number of concepts that I have to learn. Lots of examples shown there feel very heavyweight and can't easily be translated to real world use cases and are therefore quite esoteric. I think it gives way too much details without explaining how to build all the building blocks together and how exactly it helps to solve specific problems.</p>\n\n<p>There are sources, sinks, flows, graph stages, partial graphs, materialization, a graph DSL and a lot more and I just don't know where to start. The <a href=\"http://doc.akka.io/docs/akka/2.4.2-RC1/scala/stream/stream-quickstart.html#stream-quickstart-scala\">quick start guide</a> is meant to be a starting place but I don't understand it. It just throws in the concepts mentioned above without explaining them. Furthermore the code examples can't be executed - there are missing parts which makes it more or less impossible for me to follow the text.</p>\n\n<p>Can anyone explain the concepts sources, sinks, flows, graph stages, partial graphs, materialization and maybe some other things that I missed in simple words and with easy examples that don't explain every single detail (and which are probably not needed anyway at the beginning)?</p>\n", "title": "How to get started with Akka Streams?", "tags": ["scala", "akka-stream"], "answer_count": 1, "link": "http://stackoverflow.com/questions/35120082/how-to-get-started-with-akka-streams", "answers": [{"body": "<p>This answer is based on <code>akka-stream</code> version <code>2.4.2</code>. The API can be slightly different in other versions. The dependency can be consumed by <a href=\"http://www.scala-sbt.org/\">sbt</a>:</p>\n\n<pre><code>libraryDependencies += \"com.typesafe.akka\" %% \"akka-stream\" % \"2.4.2\"\n</code></pre>\n\n<hr>\n\n<p>Alright, lets get started. The API of Akka Streams consists of three main types. In contrast to <a href=\"https://github.com/reactive-streams/reactive-streams-jvm/tree/v1.0.0\">Reactive Streams</a>, these types are a lot more powerful and therefore more complex. It is assumed that for all the code examples the following definitions already exist:</p>\n\n<pre><code>import scala.concurrent._\nimport akka._\nimport akka.actor._\nimport akka.stream._\nimport akka.stream.scaladsl._\nimport akka.util._\n\nimplicit val system = ActorSystem(\"TestSystem\")\nimplicit val materializer = ActorMaterializer()\nimport system.dispatcher\n</code></pre>\n\n<p>The <code>import</code> statements are needed for the type declarations. <code>system</code> represents the actor system of Akka and <code>materializer</code> represents the evaluation context of the stream. In our case we use a <code>ActorMaterializer</code>, which means that the streams are evaluated on top of actors. Both values are marked as <code>implicit</code>, which gives the Scala compiler the possibility to inject these two dependencies automatically whenever they are needed. We also import <code>system.dispatcher</code>, which is a execution context for <a href=\"http://docs.scala-lang.org/overviews/core/futures.html\"><code>Futures</code></a>.</p>\n\n<h1>A New API</h1>\n\n<p>Akka Streams have these key properties:</p>\n\n<ul>\n<li>They implement the <a href=\"https://github.com/reactive-streams/reactive-streams-jvm/tree/v1.0.0\">Reactive Streams specification</a>, whose three main goals backpressure, async and non-blocking boundaries and interoperability between different implementations do fully apply for Akka Streams too.</li>\n<li>They provide an abstraction for an evaluation engine for the streams, which is called <code>Materializer</code>.</li>\n<li>Programs are formulated as reusable building blocks, which are represented as the three main types <code>Source</code>, <code>Sink</code> and <code>Flow</code>. The building blocks form a graph whose evaluation is based on the <code>Materializer</code> and needs to be explicitly triggered.</li>\n</ul>\n\n<p>In the following a deeper introduction in how to use the three main types shall be given.</p>\n\n<h2>Source</h2>\n\n<p>A <code>Source</code> is a data creator, it serves as an input source to the stream. Each <code>Source</code> has a single output channel and no input channel. All the data flows through the output channel to whatever is connected to the <code>Source</code>.</p>\n\n<p><img src=\"https://prismic-io.s3.amazonaws.com/boldradius/f11b4a37597064e93856d4d0880b9e74e5507635_akka-streams-source.png\" alt=\"Source\"></p>\n\n<p><sub>Image taken from <a href=\"http://boldradius.com/blog-post/VS0NpTAAADAACs_E/introduction-to-akka-streams\">boldradius.com</a>.</sub></p>\n\n<p>A <code>Source</code> can be created in multiple ways:</p>\n\n<pre><code>scala&gt; val s = Source.empty\ns: akka.stream.scaladsl.Source[Nothing,akka.NotUsed] = ...\n\nscala&gt; val s = Source.single(\"single element\")\ns: akka.stream.scaladsl.Source[String,akka.NotUsed] = ...\n\nscala&gt; val s = Source(1 to 3)\ns: akka.stream.scaladsl.Source[Int,akka.NotUsed] = ...\n\nscala&gt; val s = Source(Future(\"single value from a Future\"))\ns: akka.stream.scaladsl.Source[String,akka.NotUsed] = ...\n\nscala&gt; s runForeach println\nres0: scala.concurrent.Future[akka.Done] = ...\nsingle value from a Future\n</code></pre>\n\n<p>In the above cases we fed the <code>Source</code> with finite data, which means they will terminate eventually. One should not forget, that Reactive Streams are lazy and asynchronous by default. This means one explicitly has to request the evaluation of the stream. In Akka Streams this can be done through the <code>run*</code> methods. The <code>runForeach</code> would be no different to the well known <code>foreach</code> function - through the <code>run</code> addition it makes explicit that we ask for an evaluation of the stream. Since finite data is boring, we continue with infinite one:</p>\n\n<pre><code>scala&gt; val s = Source.repeat(5)\ns: akka.stream.scaladsl.Source[Int,akka.NotUsed] = ...\n\nscala&gt; s take 3 runForeach println\nres1: scala.concurrent.Future[akka.Done] = ...\n5\n5\n5\n</code></pre>\n\n<p>With the <code>take</code> method we can create an artificial stop point that prevents us from evaluating indefinitely. Since actor support is built-in, we can also easily feed the stream with messages that are sent to an actor:</p>\n\n<pre><code>def run(actor: ActorRef) = {\n  Future { Thread.sleep(300); actor ! 1 }\n  Future { Thread.sleep(200); actor ! 2 }\n  Future { Thread.sleep(100); actor ! 3 }\n}\nval s = Source\n  .actorRef[Int](bufferSize = 0, OverflowStrategy.fail)\n  .mapMaterializedValue(run)\n\nscala&gt; s runForeach println\nres1: scala.concurrent.Future[akka.Done] = ...\n3\n2\n1\n</code></pre>\n\n<p>We can see that the <code>Futures</code> are executed asynchronously on different threads, which explains the result. In the above example a buffer for the incoming elements is not necessary and therefore with <code>OverflowStrategy.fail</code> we can configure that the stream should fail on a buffer overflow. Especially through this actor interface, we can feed the stream through any data source. It doesn't matter if the data is created by the same thread, by a different one, by another process or if they come from a remote system over the Internet.</p>\n\n<h2>Sink</h2>\n\n<p>A <code>Sink</code> is basically the opposite of a <code>Source</code>. It is the endpoint of a stream and therefore consumes data. A <code>Sink</code> has a single input channel and no output channel. <code>Sinks</code> are especially needed when we want to specify the behavior of the data collector in a reusable way and without evaluating the stream. The already known <code>run*</code> methods do not allow us these properties, therefore it is preferred to use <code>Sink</code> instead.</p>\n\n<p><img src=\"https://prismic-io.s3.amazonaws.com/boldradius/41943c3155b7d7ce99faba52b846272e99a41fa9_akka-streams-sink.png\" alt=\"Sink\"></p>\n\n<p><sub>Image taken from <a href=\"http://boldradius.com/blog-post/VS0NpTAAADAACs_E/introduction-to-akka-streams\">boldradius.com</a>.</sub></p>\n\n<p>A short example of a <code>Sink</code> in action:</p>\n\n<pre><code>scala&gt; val source = Source(1 to 3)\nsource: akka.stream.scaladsl.Source[Int,akka.NotUsed] = ...\n\nscala&gt; val sink = Sink.foreach[Int](elem =&gt; println(s\"sink received: $elem\"))\nsink: akka.stream.scaladsl.Sink[Int,scala.concurrent.Future[akka.Done]] = ...\n\nscala&gt; val flow = source to sink\nflow: akka.stream.scaladsl.RunnableGraph[akka.NotUsed] = ...\n\nscala&gt; flow.run()\nres3: akka.NotUsed = NotUsed\nsink received: 1\nsink received: 2\nsink received: 3\n</code></pre>\n\n<p>Connecting a <code>Source</code> to a <code>Sink</code> can be done with the <code>to</code> method. It returns a so called <code>RunnableFlow</code>, which is as we will later see a special form of a <code>Flow</code> - a stream that can be executed by just calling its <code>run()</code> method.</p>\n\n<p><img src=\"https://prismic-io.s3.amazonaws.com/boldradius/82c03ac92626e43f8e3a530289ad5c1174e1881c_akka-streams-runaable-flow.png\" alt=\"Runnable Flow\"></p>\n\n<p><sub>Image taken from <a href=\"http://boldradius.com/blog-post/VS0NpTAAADAACs_E/introduction-to-akka-streams\">boldradius.com</a>.</sub></p>\n\n<p>It is of course possible to forward all values that arrive at a sink to an actor:</p>\n\n<pre><code>val actor = system.actorOf(Props(new Actor {\n  override def receive = {\n    case msg =&gt; println(s\"actor received: $msg\")\n  }\n}))\n\nscala&gt; val sink = Sink.actorRef[Int](actor, onCompleteMessage = \"stream completed\")\nsink: akka.stream.scaladsl.Sink[Int,akka.NotUsed] = ...\n\nscala&gt; val runnable = Source(1 to 3) to sink\nrunnable: akka.stream.scaladsl.RunnableGraph[akka.NotUsed] = ...\n\nscala&gt; runnable.run()\nres3: akka.NotUsed = NotUsed\nactor received: 1\nactor received: 2\nactor received: 3\nactor received: stream completed\n</code></pre>\n\n<h2>Flow</h2>\n\n<p>Data sources and sinks are great if you need a connection between Akka streams and an existing system but one can not really do anything with them. Flows are the last missing piece in the Akka Streams base abstraction. They act as a connector between different streams and can be used to transform its elements.</p>\n\n<p><img src=\"https://prismic-io.s3.amazonaws.com/boldradius/7ecc45e11734dedba4bda97601c105e5ecf1f93e_akka-streams-flow.png\" alt=\"Flow\"></p>\n\n<p><sub>Image taken from <a href=\"http://boldradius.com/blog-post/VS0NpTAAADAACs_E/introduction-to-akka-streams\">boldradius.com</a>.</sub></p>\n\n<p>If a <code>Flow</code> is connected to a <code>Source</code> a new <code>Source</code> is the result. Likewise, a <code>Flow</code> connected to a <code>Sink</code> creates a new <code>Sink</code>. And a <code>Flow</code> connected with both a <code>Source</code> and a <code>Sink</code> results in a <code>RunnableFlow</code>. Therefore, they sit between the input and the output channel but by themselves do not correspond to one of the flavors as long as they are not connected to either a <code>Source</code> or a <code>Sink</code>.</p>\n\n<p><img src=\"https://prismic-io.s3.amazonaws.com/boldradius/ba5a6cf66c3116b9b8b20dff87e767b4687da6dd_akka-streams-source-flow-sink.png\" alt=\"Full Stream\"></p>\n\n<p><sub>Image taken from <a href=\"http://boldradius.com/blog-post/VS0NpTAAADAACs_E/introduction-to-akka-streams\">boldradius.com</a>.</sub></p>\n\n<p>In order to get a better understanding of <code>Flows</code>, we will have a look at some examples:</p>\n\n<pre><code>scala&gt; val source = Source(1 to 3)\nsource: akka.stream.scaladsl.Source[Int,akka.NotUsed] = ...\n\nscala&gt; val sink = Sink.foreach[Int](println)\nsink: akka.stream.scaladsl.Sink[Int,scala.concurrent.Future[akka.Done]] = ...\n\nscala&gt; val invert = Flow[Int].map(elem =&gt; elem * -1)\ninvert: akka.stream.scaladsl.Flow[Int,Int,akka.NotUsed] = ...\n\nscala&gt; val doubler = Flow[Int].map(elem =&gt; elem * 2)\ndoubler: akka.stream.scaladsl.Flow[Int,Int,akka.NotUsed] = ...\n\nscala&gt; val runnable = source via invert via doubler to sink\nrunnable: akka.stream.scaladsl.RunnableGraph[akka.NotUsed] = ...\n\nscala&gt; runnable.run()\nres10: akka.NotUsed = NotUsed\n-2\n-4\n-6\n</code></pre>\n\n<p>Via the <code>via</code> method we can connect a <code>Source</code> with a <code>Flow</code>. We need to specify the input type because the compiler can't infer it for us. As we can already see in this simple example, the flows <code>invert</code> and <code>double</code> are completely independent from any data producers and consumers. They only transform the data and forward it to the output channel. This means that we can reuse a flow among multiple streams:</p>\n\n<pre><code>scala&gt; val s1 = Source(1 to 3) via invert to sink\ns1: akka.stream.scaladsl.RunnableGraph[akka.NotUsed] = ...\n\nscala&gt; val s2 = Source(-3 to -1) via invert to sink\ns2: akka.stream.scaladsl.RunnableGraph[akka.NotUsed] = ...\n\nscala&gt; s1.run()\nres10: akka.NotUsed = NotUsed\n-1\n-2\n-3\n\nscala&gt; s2.run()\nres11: akka.NotUsed = NotUsed\n3\n2\n1\n</code></pre>\n\n<p><code>s1</code> and <code>s2</code> represent completely new streams - they do not share any data through their building blocks.</p>\n\n<h1>Unbounded Data Streams</h1>\n\n<p>Before we move on we should first revisit some of the key aspects of Reactive Streams. An unbounded number of elements can arrive at any point and can put a stream in different states. Beside from a runnable stream, which is the usual state, a stream may get stopped either through an error or through a signal that denotes that no further data will arrive. A stream can be modeled in a graphical way by marking events on a timeline as it is the case here:</p>\n\n<p><a href=\"http://i.stack.imgur.com/rcmwZ.png\"><img src=\"http://i.stack.imgur.com/rcmwZ.png\" alt=\"Shows that a stream is a sequence of ongoing events ordered in time\"></a></p>\n\n<p><sub>Image taken from <a href=\"https://gist.github.com/staltz/868e7e9bc2a7b8c1f754/\">The introduction to Reactive Programming you've been missing</a>.</sub></p>\n\n<p>We have already seen runnable flows in the examples of the previous section. We get a <code>RunnableGraph</code> whenever a stream can actually be materialized, which means that a <code>Sink</code> is connected to a <code>Source</code>. So far we always materialized to the value <code>Unit</code>, which can be seen in the types:</p>\n\n<pre><code>val source: Source[Int, NotUsed] = Source(1 to 3)\nval sink: Sink[Int, Future[Done]] = Sink.foreach[Int](println)\nval flow: Flow[Int, Int, NotUsed] = Flow[Int].map(x =&gt; x)\n</code></pre>\n\n<p>For <code>Source</code> and <code>Sink</code> the second type parameter and for <code>Flow</code> the third type parameter denote the materialized value. Throughout this answer, the full meaning of materialization shall not be explained. However, further details about materialization can be found at the <a href=\"http://doc.akka.io/docs/akka/2.4.2-RC1/scala/stream/index.html\">official documentation</a>. For now the only thing we need to know is that the materialized value is what we get when we run a stream. Since we were only interested in side effects so far, we got <code>Unit</code> as the materialized value. The exception to this was a materialization of a sink, which resulted in a <code>Future</code>. It gave us back a <code>Future</code>, since this value can denote when the stream that is connected to the sink has been ended.  So far, the previous code examples were nice to explain the concept but they were also boring because we only dealt with finite streams or with very simple infinite ones. To make it more interesting, in the following a full asynchronous and unbounded stream shall be explained.</p>\n\n<h2>ClickStream Example</h2>\n\n<p>As an example, we want to have a stream that captures click events. To make it more challenging, let's say we also want to group click events that happen in a short time after each other. This way we could easily discover double, triple or tenfold clicks. Furthermore, we want to filter out all single clicks. Take a deep breath and imagine how you would solve that problem in an imperative manner. I bet no one would be able to implement a solution that works correctly on the first try. In a reactive fashion this problem is trivial to solve. In fact, the solution is so simple and straightforward to implement that we can even express it in a diagram that directly describes the behavior of the code:</p>\n\n<p><a href=\"http://i.stack.imgur.com/3tH8R.png\"><img src=\"http://i.stack.imgur.com/3tH8R.png\" alt=\"The logic of the click stream example\"></a></p>\n\n<p><sub>Image taken from <a href=\"https://gist.github.com/staltz/868e7e9bc2a7b8c1f754/\">The introduction to Reactive Programming you've been missing</a>.</sub></p>\n\n<p>The gray boxes are functions that describe how one stream is transformed into another. With the <code>throttle</code> function we accumulate clicks within 250 milliseconds, the <code>map</code> and <code>filter</code> functions should be self-explanatory. The color orbs represent an event and the arrows depict how they flow through our functions. Later in the processing steps, we get less and less elements that flow through our stream, since we group them together and filter them out. The code for this image would look something like this:</p>\n\n<pre><code>val multiClickStream = clickStream\n    .throttle(250.millis)\n    .map(clickEvents =&gt; clickEvents.length)\n    .filter(numberOfClicks =&gt; numberOfClicks &gt;= 2)\n</code></pre>\n\n<p>The whole logic can be represented in only four lines of code! In Scala, we could write it even shorter:</p>\n\n<pre><code>val multiClickStream = clickStream.throttle(250.millis).map(_.length).filter(_ &gt;= 2)\n</code></pre>\n\n<p>The definition of <code>clickStream</code> is a little bit more complex but this is only the case because the example program runs on the JVM, where capturing of click events is not easily possible. Another complication is that Akka by default doesn't provide the <code>throttle</code> function. Instead we had to write it by ourselves. Since this function is (as it is the case for the <code>map</code> or <code>filter</code> functions) reusable across different use cases I don't count these lines to the number of lines we needed to implement the logic. In imperative languages however, it is normal that logic can't be reused that easily and that the different logical steps happen all at one place instead of being applied sequentially, which means that we probably would have misshaped our code with the throttling logic. The full code example is available as a <a href=\"https://gist.github.com/sschaef/8533e23f5856bd459e27#file-clickstreamexample-scala\">gist</a> and shall not be discussed here any further.</p>\n\n<h2>SimpleWebServer Example</h2>\n\n<p>What should be discussed instead is another example. While the click stream is a nice example to let Akka Streams handle a real world example, it lacks the power to show parallel execution in action. The next example shall represent a small web server that can handle multiple requests in parallel. The web sever shall be able to accept incoming connections and receive byte sequences from them that represent printable ASCII signs. These byte sequences or strings should be split at all newline-characters into smaller parts. After that, the server shall respond to the client with each of the split lines. Alternatively, it could do something else with the lines and give a special answer token, but we want to keep it simple in this example and therefore don't introduce any fancy features. Remember, the server needs to be able to handle multiple requests at the same time, which basically means that no request is allowed to block any other request from further execution. Solving all of these requirements can be hard in an imperative way - with Akka Streams however, we shouldn't need more than a few lines to solve any of these. First, let's have an overview over the server itself:</p>\n\n<p><a href=\"http://i.stack.imgur.com/Ji6AG.png\"><img src=\"http://i.stack.imgur.com/Ji6AG.png\" alt=\"server\"></a></p>\n\n<p>Basically, there are only three main building blocks. The first one needs to accept incoming connections. The second one needs to handle incoming requests and the third one needs to send a response. Implementing all of these three building blocks is only a little bit more complicated than implementing the click stream:</p>\n\n<pre><code>def mkServer(address: String, port: Int)(implicit system: ActorSystem, materializer: Materializer): Unit = {\n  import system.dispatcher\n\n  val connectionHandler: Sink[Tcp.IncomingConnection, Future[Unit]] =\n    Sink.foreach[Tcp.IncomingConnection] { conn =>\n      println(s\"Incoming connection from: ${conn.remoteAddress}\")\n      conn.handleWith(serverLogic)\n    }\n\n  val incomingCnnections: Source[Tcp.IncomingConnection, Future[Tcp.ServerBinding]] =\n    Tcp().bind(address, port)\n\n  val binding: Future[Tcp.ServerBinding] =\n    incomingCnnections.to(connectionHandler).run()\n\n  binding onComplete {\n    case Success(b) =>\n      println(s\"Server started, listening on: ${b.localAddress}\")\n    case Failure(e) =>\n      println(s\"Server could not be bound to $address:$port: ${e.getMessage}\")\n  }\n}</code></pre>\n\n<p>The function <code>mkServer</code> takes (besides from the address and the port of the server) also an actor system and a materializer as implicit parameters. The control flow of the server is represented by <code>binding</code>, which takes a source of incoming connections and forwards them to a sink of incoming connections. Inside of <code>connectionHandler</code>, which is our sink, we handle every connection by the flow <code>serverLogic</code>, which will be described later. <code>binding</code> returns a <code>Future</code>, which completes when the server has been started or the start failed, which could be the case when the port is already taken by another process. The code however, doesn't completely reflect the graphic as we can't see a building block that handles responses. The reason for this is that the connection already provides this logic by itself. It is a bidirectional flow and not just a unidirectional one as the flows we have seen in the previous examples. As it was the case for materialization, such complex flows shall not be explained here. The <a href=\"http://doc.akka.io/docs/akka/2.4.2-RC1/scala/stream/index.html\">official documentation</a> has plenty of material to cover more complex flow graphs. For now it is enough to know that  <code>Tcp.IncomingConnection</code> represents a connection that knows how to receive requests and how to send responses. The part that is still missing is the <code>serverLogic</code> building block. It can look like this:</p>\n\n<p><a href=\"http://i.stack.imgur.com/xq4p8.png\"><img src=\"http://i.stack.imgur.com/xq4p8.png\" alt=\"server logic\"></a></p>\n\n<p>Once again, we are able to split the logic in several simple building blocks that all together form the flow of our program. First we want to split our sequence of bytes in lines, which we have to do whenever we find a newline character. After that, the bytes of each line need to be converted to a string because working with raw bytes is cumbersome. Overall we could receive a binary stream of a complicated protocol, which would make working with the incoming raw data extremely challenging. Once we have a readable string, we can create an answer. For simplicity reasons the answer can be anything in our case. In the end, we have to convert back our answer to a sequence of bytes that can be sent over the wire. The code for the entire logic may look like this:</p>\n\n<pre><code>val serverLogic: Flow[ByteString, ByteString, Unit] = {\n  val delimiter = Framing.delimiter(\n    ByteString(\"\\n\"),\n    maximumFrameLength = 256,\n    allowTruncation = true)\n\n  val receiver = Flow[ByteString].map { bytes =>\n    val message = bytes.utf8String\n    println(s\"Server received: $message\")\n    message\n  }\n\n  val responder = Flow[String].map { message =>\n    val answer = s\"Server hereby responds to message: $message\\n\"\n    ByteString(answer)\n  }\n\n  Flow[ByteString]\n    .via(delimiter)\n    .via(receiver)\n    .via(responder)\n}</code></pre>\n\n<p>We already know that <code>serverLogic</code> is a flow that takes a <code>ByteString</code> and has to produce a <code>ByteString</code>. With <code>delimiter</code> we can split a <code>ByteString</code> in smaller parts - in our case it needs to happen whenever a newline character occurs. <code>receiver</code> is the flow that takes all of the split byte sequences and converts them to a string. This is of course a dangerous conversion, since only printable ASCII characters should be converted to a string but for our needs it is good enough. <code>responder</code> is the last component and is responsible for creating an answer and converting the answer back to a sequence of bytes. As opposed to the graphic we didn't split this last component in two, since the logic is trivial. At the end, we connect all of the flows through the <code>via</code> function. At this point one may ask whether we took care of the multi-user property that was mentioned at the beginning. And indeed we did even though it may not be obvious immediately. By looking at this graphic it should get more clear:</p>\n\n<p><a href=\"http://i.stack.imgur.com/0a3Ey.png\"><img src=\"http://i.stack.imgur.com/0a3Ey.png\" width=\"500\" alt=\"server and server logic combined\"></a></p>\n\n<p>The <code>serverLogic</code> component is nothing but a flow that contains smaller flows. This component takes an input, which is a request, and produces an output, which is the response. Since flows can be constructed multiple times and they all work independently to each other, we achieve through this nesting our multi-user property. Every request is handled within its own request and therefore a short running request can overrun a previously started long running request. In case you wondered, the definition of <code>serverLogic</code> that was shown previously can of course be written a lot shorter by inlining most of its inner definitions:</p>\n\n<pre><code>val serverLogic = Flow[ByteString]\n  .via(Framing.delimiter(\n      ByteString(\"\\n\"),\n      maximumFrameLength = 256,\n      allowTruncation = true))\n  .map(_.utf8String)\n  .map(msg =&gt; s\"Server hereby responds to message: $msg\\n\")\n  .map(ByteString(_))\n</code></pre>\n\n<p>A test of the web server may look like this:</p>\n\n<pre><code>$ # Client\n$ echo \"Hello World\\nHow are you?\" | netcat 127.0.0.1 6666\nServer hereby responds to message: Hello World\nServer hereby responds to message: How are you?\n</code></pre>\n\n<p>In order for the above code example to function correctly, we first need to start the server, which is depicted by the <code>startServer</code> script:</p>\n\n<pre><code>$ # Server\n$ ./startServer 127.0.0.1 6666\n[DEBUG] Server started, listening on: /127.0.0.1:6666\n[DEBUG] Incoming connection from: /127.0.0.1:37972\n[DEBUG] Server received: Hello World\n[DEBUG] Server received: How are you?\n</code></pre>\n\n<p>The full code example of this simple TCP server can be found <a href=\"https://gist.github.com/sschaef/bd5ee6273ddaa7b015af#file-simpletcpserver-scala\">here</a>. We are not only able to write a server with Akka Streams but also the client. It may look like this:</p>\n\n<pre><code>val connection = Tcp().outgoingConnection(address, port)\nval flow = Flow[ByteString]\n  .via(Framing.delimiter(\n      ByteString(\"\\n\"),\n      maximumFrameLength = 256,\n      allowTruncation = true))\n  .map(_.utf8String)\n  .map(println)\n  .map(_ \u21d2 StdIn.readLine(\"> \"))\n  .map(_+\"\\n\")\n  .map(ByteString(_))\n\nconnection.join(flow).run()</code></pre>\n\n<p>The full code TCP client can be found <a href=\"https://gist.github.com/sschaef/6b2661a0df512eb321e9\">here</a>. The code looks quite similar but in contrast to the server we don't have to manage the incoming connections anymore. </p>\n\n<h1>Complex Graphs</h1>\n\n<p>In the previous sections we have seen how we can construct simple programs out of flows. However, in reality it is often not enough to just rely on already built-in functions to construct more complex streams. If we want to be able to use Akka Streams for arbitrary programs we need to know how to build our own custom control structures and combinable flows that allow us to tackle the complexity of our applications. The good news is that Akka Streams was designed to scale with the needs of the users and in order to give you a short introduction into the more complex parts of Akka Streams, we add some more features to our client/server example.</p>\n\n<p>One thing we can't do yet is closing a connection. At this point it starts to get a little bit more complicated because the stream API we have seen so far doesn't allow us to stop a stream at an arbitrary point. However, there is the <code>GraphStage</code> abstraction, which can be used to create arbitrary graph processing stages with any number of input or output ports. Let's first have a look at the server side, where we introduce a new component, called <code>closeConnection</code>:</p>\n\n<pre><code>val closeConnection = new GraphStage[FlowShape[String, String]] {\n  val in = Inlet[String](\"closeConnection.in\")\n  val out = Outlet[String](\"closeConnection.out\")\n\n  override val shape = FlowShape(in, out)\n\n  override def createLogic(inheritedAttributes: Attributes) = new GraphStageLogic(shape) {\n    setHandler(in, new InHandler {\n      override def onPush() = grab(in) match {\n        case \"q\" \u21d2\n          push(out, \"BYE\")\n          completeStage()\n        case msg \u21d2\n          push(out, s\"Server hereby responds to message: $msg\\n\")\n      }\n    })\n    setHandler(out, new OutHandler {\n      override def onPull() = pull(in)\n    })\n  }\n}</code></pre>\n\n<p>This API looks a lot more cumbersome than the flow API. No wonder, we have to do a lot of imperative steps here. In exchange, we have more control over the behavior of our streams. In the above example, we only specify one input and one output port and make them available to the system by overriding the <code>shape</code> value. Furthermore we defined a so called <code>InHandler</code> and a <code>OutHandler</code>, which are in this order responsible for receiving and emitting elements. If you looked closely to the full click stream example you should recognize these components already. In the <code>InHandler</code> we grab an element and if it is a string with a single character <code>'q'</code>, we want to close the stream. In order to give the client a chance to find out that the stream will get closed soon, we emit the string <code>\"BYE\"</code> and then we immediately close the stage afterwards. The <code>closeConnection</code> component can be combined with a stream via the <code>via</code> method, which was introduced in the section about flows.</p>\n\n<p>Beside from being able to close connections, it would also be nice if we could show a welcome message to a newly created connection. In order to do this we once again have to go a little bit further:</p>\n\n<pre><code>def serverLogic\n    (conn: Tcp.IncomingConnection)\n    (implicit system: ActorSystem)\n    : Flow[ByteString, ByteString, NotUsed]\n    = Flow.fromGraph(GraphDSL.create() { implicit b \u21d2\n  import GraphDSL.Implicits._\n  val welcome = Source.single(ByteString(s\"Welcome port ${conn.remoteAddress}!\\n\"))\n  val logic = b.add(internalLogic)\n  val concat = b.add(Concat[ByteString]())\n  welcome ~> concat.in(0)\n  logic.outlet ~> concat.in(1)\n\n  FlowShape(logic.in, concat.out)\n})</code></pre>\n\n<p>The function <code>serverLogic</code>  now takes the incoming connection as a parameter. Inside of its body we use a DSL that allows us to describe complex stream behavior. With <code>welcome</code> we create a stream that can only emit one element - the welcome message. <code>logic</code> is what was described as <code>serverLogic</code> in the previous section. The only notable difference is that we added <code>closeConnection</code> to it. Now actually comes the interesting part of the DSL.  The <code>GraphDSL.create</code> function makes a builder <code>b</code> available, which is used to express the stream as a graph. With the <code>~&gt;</code> function it is possible to connect input and output ports with each other. The <code>Concat</code> component that is used in the example can concatenate elements and is here used to prepend the welcome message in front of the other elements that come out of <code>internalLogic</code>. In the last line, we only make the input port of the server logic and the output port of the concatenated stream available because all the other ports shall remain an implementation detail of the <code>serverLogic</code> component. For an in-depth introduction to the graph DSL of Akka Streams, visit the corresponding section in the <a href=\"http://doc.akka.io/docs/akka/2.4.2-RC1/scala/stream/stream-graphs.html\">official documentation</a>. The full code example of the complex TCP server and of a client that can communicate with it can be found <a href=\"https://gist.github.com/sschaef/df6315c87140d1d5951e\">here</a>. Whenever you open a new connection from the client you should see a welcoming message and by typing <code>\"q\"</code> on the client you should see a message that tells you that the connection has been canceled.</p>\n\n<p>There are still some topics which weren't covered by this answer. Especially materialization may scare one reader or another but I'm sure with the material that is covered here everyone should be able to go the next steps by themselves. As already said, the <a href=\"http://doc.akka.io/docs/akka/2.4.2-RC1/scala/stream/index.html\">official documentation</a> is a good place to continue learning about Akka Streams.</p>\n", "title": "How to get started with Akka Streams?", "answer_id": 35120136, "question_id": 35120082, "tags": ["scala", "akka-stream"]}], "question_id": 35120082}, {"body": "<p>Started studying about complexity, I'm struggling with this one:</p>\n\n<pre><code>void what(int n) {\n    int i;\n    for (i = 1; i &lt;= n; i++) {\n        int x = n;\n        while (x &gt; 0)\n            x -= i;\n    }\n}\n</code></pre>\n\n<p>Well, the first for loop is clearly <code>O(n)</code>. The first iteration is <code>O(n)</code>, second one is <code>O(n/2)</code>.. and like that <code>log(n)</code> times I guess?\nWhich means <strong><code>O(n) * O(log(n)) = O(n * log(n)) complexity</code></strong>. Did I get this right?</p>\n\n<p>Edit:(not a duplicate) I know what Big O is. I've asked the correct evaluation in a specific case. </p>\n", "title": "What is the time complexity of my function?", "tags": ["c", "algorithm", "time-complexity"], "answer_count": 4, "link": "http://stackoverflow.com/questions/35326140/what-is-the-time-complexity-of-my-function", "answers": [{"body": "<p>The first inner loop runs <code>n</code> times<br>\nThe second inner loop runs <code>n/2</code> times<br>\nThe third inner loop runs <code>n/3</code> times<br>\n.. \nThe last one runs once</p>\n\n<p>So <code>n + n/2 + n/3 + ... + 1 = n(1+1/2+1/3+1/4 + ... +1/n)</code>.</p>\n\n<p>This is n multiplied by the sum of harmonic series, which does not have a nice closed form representation. But as shown <a href=\"http://stackoverflow.com/questions/25905118/finding-big-o-of-the-harmonic-series\">here</a> it is <code>O(log(n))</code>. So overall the algorithm is <code>O(n log(n))</code></p>\n", "title": "What is the time complexity of my function?", "answer_id": 35326392, "question_id": 35326140, "tags": ["c", "algorithm", "time-complexity"]}, {"body": "<p>The outer loop runs <code>n</code> times.</p>\n\n<p>For each iteration, the inner loops runs <code>n / i</code> times.</p>\n\n<p>The total number of runs is</p>\n\n<pre><code>n + n/2 + n/3 + ... + n/n\n</code></pre>\n\n<p>Asymptotically (ignoring integer arithmetic rounding), this simplifies as</p>\n\n<pre><code>n (1 + 1/2 + 1/3 + ... + 1/n)\n</code></pre>\n\n<p>This series loosely converges towards <code>n * log(n)</code>.</p>\n\n<p>Hence the complexity is <code>O(N.log(N))</code> as you expected.</p>\n", "title": "What is the time complexity of my function?", "answer_id": 35326405, "question_id": 35326140, "tags": ["c", "algorithm", "time-complexity"]}, {"body": "<p>Attempting this by testing and graphics. The log2 vs log2 plot looks fairly linear.  Something between more than linear O(n) and less than O(n*log(n)). \"Draw\" your own conclusion. </p>\n\n<p>[Edit]</p>\n\n<p>Using the mathematical derived formulas, the O() calculated is an upper bound of O(n * log(n)).  That uses \"fractions of loop iterations\" that increase the count by a fraction and not 1. E.g.  When <code>n</code> is 6, iteration count is  6 + 3 + 2 + 1.5 + 1.2 + 1 = 14.7 loops rather than actual 6 + 3 + 2 + 2 + 2 + 1 = 16.  This difference is relatively less significant as <code>n</code> increases, thus the slightly less than O(n * log(n)) growth.  So by not ignoring code uses integer math, we have a much more challenging question.</p>\n\n<hr>\n\n<p><a href=\"http://i.stack.imgur.com/2HbVS.png\" rel=\"nofollow\"><img src=\"http://i.stack.imgur.com/2HbVS.png\" alt=\"enter image description here\"></a></p>\n\n<pre><code>unsigned long long what(int n) {\n  unsigned long long cnt = 0;\n  int i;\n  for (i = 1; i &lt;= n; i++) {\n    int x = n;\n    while (x &gt; 0) {\n      x -= i;\n      cnt++;\n    }\n  }\n  return cnt;\n}\n\nvoid wtest(int n) {\n  unsigned long long cnt = what(n);\n  printf(\"%d %llu\\n\", n, cnt);\n  fflush(stdout);\n}\n\nvoid wtests(void) {\n  int i = INT_MAX/2 + 1;\n  while (i &gt; 0) {\n    wtest(i);\n    i /= 2;\n  }\n}\n\nint main(void) {\n  wtests();\n  return 0;\n}\n</code></pre>\n\n<p>Output</p>\n\n<pre><code>1073741824 23567395117\n536870912 11411566988\n268435456 5519718329\n134217728 2666826555\n67108864 1286897093\n33554432 620190504\n16777216 298466265\n8388608 143418602\n4194304 68802063\n2097152 32947406\n1048576 15746897\n524288 7510048\n262144 3573331\n131072 1695816\n65536 802493\n32768 378537\n16384 177921\n8192 83286\n4096 38803\n2048 17973\n1024 8275\n512 3782\n256 1713\n128 765\n64 337\n32 145\n16 61\n8 24\n4 9\n2 3\n1 1\n</code></pre>\n", "title": "What is the time complexity of my function?", "answer_id": 35326915, "question_id": 35326140, "tags": ["c", "algorithm", "time-complexity"]}, {"body": "<p>As an alternative, use a variable substitution <code>y = n - x</code> followed by Sigma notation to analyse the number of iterations of the inner <code>while</code> loop of your algorithm.</p>\n\n<p><a href=\"http://i.stack.imgur.com/38I3K.png\"><img src=\"http://i.stack.imgur.com/38I3K.png\" alt=\"enter image description here\"></a></p>\n\n<p>The above overestimates, for each inner while loop, the number of iterations by <code>1</code> for cases where <code>n-1</code> is not a multiple of <code>i</code>, i.e. where <code>(n-1) % i != 0</code>. As we proceed, we'll assume that <code>(n-1)/i</code> is an integer for all values of <code>i</code>, hence overestimating the total number of iterations in the inner <code>while</code> loop, subsequently including the less or equal sign at <code>(i)</code> below. We proceed with the Sigma notation analysis:</p>\n\n<p><a href=\"http://i.stack.imgur.com/UBc6E.png\"><img src=\"http://i.stack.imgur.com/UBc6E.png\" alt=\"enter image description here\"></a></p>\n\n<p>where we, at <code>(ii)</code>, have approximated the <code>n</code>:th <a href=\"https://en.wikipedia.org/wiki/Harmonic_number\">harmonic number</a> by the associated integral. Hence, you algorithm runs in <code>O(n\u00b7ln(n))</code>, asymptotically.</p>\n\n<hr>\n\n<p>Leaving asymptotic behaviour and studying actual growth of the algorithm, we can use the nice sample data of exact <code>(n,cnt)</code> (where <code>cnt</code> is number of inner iterations) pairs by @chux (refer to his answer), and compare with the estimated number of iterations from above, i.e., <code>n(1+ln(n))-ln(n)</code>. It's apparent that the estimation harmonize neatly with the actual count, see the plots below or <a href=\"https://gist.github.com/dfrib/58cf872ded63a5985924\">this snippet for the actual numbers</a>.</p>\n\n<p><a href=\"http://i.stack.imgur.com/K4SNk.png\"><img src=\"http://i.stack.imgur.com/K4SNk.png\" alt=\"enter image description here\"></a></p>\n\n<hr>\n\n<p>Finally note that if we let <code>n-&gt;\u221e</code> in the sum over <code>1/i</code> above, the resulting series is the <a href=\"https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)\">infinite harmonic series</a>, which is, curiously enough, divergent. The proof for the latter makes use of the fact that in infinite sum of non-zero terms naturally is infinite itself. In practice, however, for a <em>sufficiently large but not infinite n</em>, <code>ln(n)</code> is an appropriate approximation of the sum, and this divergence is not relevant for our asymptotic analysis here.</p>\n\n<hr>\n", "title": "What is the time complexity of my function?", "answer_id": 35342203, "question_id": 35326140, "tags": ["c", "algorithm", "time-complexity"]}], "question_id": 35326140}, {"body": "<p>I'm trying to upload my iOS app archive to iTunes Connect using Xcode, but when I click \"Upload to App Store\" I get the error:</p>\n\n<blockquote>\n  <p>Xcode attempted to locate or generate matching signing assets and\n  failed to do so because of the following issues. Missing iOS\n  Distribution signing identity for _____. Xcode can request one for\n  you.</p>\n</blockquote>\n\n<p>I'm really confused, because I had just successfully uploaded an archive for the same app about 2 hours earlier, and then I needed to make one simple fix and then needed to reupload the archive, and now I'm getting this error. I did not touch anything else.</p>\n\n<p>I've done everything that I can think of to try fixing the problem, I restarted Xcode, my computer, went to Xcode preferences to click 'Download All' on the provisioning profiles, clicked \"Reset\" on the iOS Distribution on the same page, downloaded again the iOS distribution profile from the member center, went to the code signing settings in my app's build settings and manually set the release signing identity to the iOS distribution profile, and rearchived several times. </p>\n\n<p>Update: Turns out it's due to the WWDRCA expiring today: <a href=\"https://developer.apple.com/support/certificates/expiration/\">https://developer.apple.com/support/certificates/expiration/</a>\nThere are a lot of other people experiencing the same issue. I was able to temporarily bypass the issue by changing the date on my computer back 1 day.</p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "tags": ["ios", "xcode", "provisioning", "signing"], "answer_count": 9, "link": "http://stackoverflow.com/questions/35397453/uploading-archive-error-missing-ios-distribution-signing-identity-for", "answers": [{"body": "<p>As odd as it may seem, setting the computer's date back one day works.</p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35399233, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>I had the same problem and was able to fix it by installing this \"Apple World Wide Developer Relations Certificate Authority\" certificate in my keychain: <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\" rel=\"nofollow\">link</a></p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35399366, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>I faced the same issue and the only thing that worked for me was setting the computer's date backwards as suggested by other responders, I tried downloading the \"Apple World Wide Developer Relations Certificate Authority\" but no luck. </p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35400914, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>The issue is because the Apple WWDRCA expired today (Feb 14, 2016): <a href=\"https://developer.apple.com/support/certificates/expiration/\">https://developer.apple.com/support/certificates/expiration/</a></p>\n\n<p>Open <strong>Keychain Access</strong>, and in the menu, click <strong>View</strong> -> <strong>Show Expired Certificates</strong>. Then, delete the expired <em>Apple Worldwide Developer Relations Certificate Authority</em> from both the <strong>login</strong> and <strong>System</strong> Keychains. Install the renewed certificate from Apple by downloading from <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a> and then opening it.</p>\n\n<p>Also, you might need to restart Xcode to reflect the changes. Then, clean and build your project then re-try generating your archive.</p>\n\n<p><a href=\"http://i.stack.imgur.com/9h4js.png\"><img src=\"http://i.stack.imgur.com/9h4js.png\" alt=\"Your keychain should show like that\"></a></p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35401088, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>Here's statement from Apple.</p>\n\n<blockquote>\n  <p>Thanks for bringing this to the attention of the community and apologies for the issues you\u2019ve been having. This issue stems from having a copy of the expired WWDR Intermediate certificate in both your System and Login keychains. To resolve the issue, you should first download and install the new WWDR intermediate certificate (by double-clicking on the file). Next, in the Keychain Access application, select the System keychain. Make sure to select \u201cShow Expired Certificates\u201d in the View menu and then delete the expired version of the Apple Worldwide Developer Relations Certificate Authority Intermediate certificate (expired on February 14, 2016). Your certificates should now appear as valid in Keychain Access and be available to Xcode for submissions to the App Store.</p>\n</blockquote>\n\n<p><a href=\"https://forums.developer.apple.com/thread/37208\">https://forums.developer.apple.com/thread/37208</a></p>\n\n<ol>\n<li>Download <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a>\nand double-click to install to <strong>Keychain</strong>.</li>\n<li>Select <strong>\"View\" -> \"Show Expired Certificates\"</strong> in <strong>Keychain</strong> app.</li>\n<li><strong>Delete</strong> Apple Worldwide Developer Relations Certificate Authority certificates from <strong>\"login\"</strong> tab and <strong>\"System\"</strong> tab in Keychain app.</li>\n</ol>\n\n<p><code>Edited by :Jeremy Molayem</code> <strong>Ensure the new downloaded cert is in both login and system</strong> Default to only system tab will still give error.**</p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35403041, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>This answer below is the only one that works. Please mark it as the solution</p>\n\n<p>The issue is because the Apple WWDRCA expired today (Feb 14, 2016): <a href=\"https://developer.apple.com/support/certificates/expiration/\" rel=\"nofollow\">https://developer.apple.com/support/certificates/expiration/</a></p>\n\n<p>Open Keychain Access, and in the menu, click View -> Show Expired Certificates. Then, delete the expired Apple Worldwide Developer Relations Certificate Authority from both the login and System Keychains. Install the renewed certificate from Apple by downloading from <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\" rel=\"nofollow\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a> and then opening it.</p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35417171, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<ol>\n<li>Download <a href=\"https://developer.apple.com/certificationauthority/AppleWWDRCA.cer\">https://developer.apple.com/certificationauthority/AppleWWDRCA.cer</a></li>\n<li>Double-click to install it in your Keychain.</li>\n<li>Then from Keychain, Select View -> \"Show Expired Certificates\" in Keychain app.</li>\n<li>It will list all the expired certificates.</li>\n<li>Delete \"Apple Worldwide Developer Relations Certificate Authority\ncertificates\" from \"login\" tab</li>\n<li>And also delete it from \"System\" tab.</li>\n<li>Restart your Mac</li>\n</ol>\n\n<p>Now your system is ready to upload the App</p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35456247, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>I was having this exact same problem and I tried literally everything, I deleted expired certificates on keychain, downloaded the new WWDRCA as others have posted a link to. Edited and re-download my provisioning profile. and then quit xcode opened it back up and cleaned but nothing worked. The only thing that did work for me was changing the date on my Mac to Jan 1st 2016 and it uploaded perfectly! This obviously isn't a permanent fix but I'm on a deadline and it needed to get uploaded asap so hopefully this helps someone in a similar situation. </p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 35568912, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}, {"body": "<p>If you have renewed your account developer, go to <a href=\"https://developer.apple.com/account/ios/profile/limited\" rel=\"nofollow\">Provisioning Profiles</a> site and renew in developer profile.</p>\n", "title": "Uploading archive error: &quot;Missing iOS Distribution signing identity for ...&quot;", "answer_id": 36053585, "question_id": 35397453, "tags": ["ios", "xcode", "provisioning", "signing"]}], "question_id": 35397453}], "quota_max": 300, "page_size": 100, "quota_remaining": 236, "page": 1}