Question-33147339
I was reading Java's ArrayList source code and noticed some comparisons in if-statements. In Java 7, the method grow(int) uses if(args) newCapacity = minCapacity; In Java 6, grow didn't exist. The method ensureCapacity(int) however uses if(args) newCapacity = minCapacity; What was the reason behind the change? Was it a performance issue or just a style? I could imagine that comparing against zero is faster, but performing a complete subtraction just to check whether it's negative seems a bit overkill to me. Also in terms of bytecode, this would involve two instructions ( ISUB and IF_ICMPGE ) instead of one ( IFGE ).

Question-31170459
I was testing boundary conditions on some code involving a BigDecimal , and I noticed that when a BigDecimal is initialized with the String StringLiteral it behaves unexpectedly. It seems to have a value between 0 and 1e-2147483647 . When I try calling intValue(args) , I get a NegativeArraySizeException . I should note that 2147483647 is the max value of an integer on my system. Am I doing something wrong, or is this a problem with BigDecimal ? #pre

Question-32356762
Is there a good reason that the Collections.list() method in the java.utils package returns an ArrayList<T> instead of List<T> ? Obviously an ArrayList is a List , but I'm under the impression that it's generally good practice to return the interface type instead of implementation type.

Question-31922866
I've read on many Web sites Optional should be used as a return type only, and not used in method arguments. I'm struggling to find a logical reason why. For example I have a piece of logic which has 2 optional parameters. Therefore I think it would make sense to write my method signature like this (solution 1): #pre Many web pages specify Optional should not be used as method arguments. With this in mind I could use the following method signature and add a clear Javadoc comment specifying the arguments maybe null hoping future future maintainers will read the javadoc and therefore always carry out null checks prior to using the arguments (solution 2): #pre Alternatively I could replace my method with 4 public methods to provide a nicer interface and make it more obvious p1 and p2 are optional (solution 3): #pre Now I try writing the code of the class which invokes this piece of logic for each approach. I first retrieve the 2 input parameters from another object which returns optionals and then I invoke calculateSomething. Therefore if solution 1 is used the calling code would look like this: #pre if solution 2 is used the calling code would look like this: #pre if solution 3 is used I could use the code above or I could use the following (but its a significant amount more code): #pre So my question is why is it considered bad practice to use Optionals as method arguments as in solution 1? It looks like the most readable solution to me and makes it most obvious that the parameters could be empty/null to future maintainers. (I'm aware the designers of Optional intended it to only be used as a return type, but I can't find any logical reasons not to use it in this scenario)

Question-32693704
I noticed something strange in the implementation of HashMap.clear(args) . This is how it looked in OpenJDK 7u40 : #pre And this is how it looks as of OpenJDK 8u40 : #pre I understand that now the table can be null for empty an map, thus the additional check and caching in a local variable is required. But why was Arrays.fill(args) replaced with a for-loop? It seems that the change was introduced in this commit . Unfortunately I found no explanation for why a plain for loop might be better than Arrays.fill(args) . Is it faster? Or safer?

Question-31202946
Why was the Thread class implemented as a regular class and not an abstract class with run(args) method being abstract. Will it possibly introduce any problems? Or does it have any use in being this way? Also, the Thread.start(args) method is supposed to be a very specific method whose functionality cannot be implemented by any other class (If I am not wrong). And hence I guess the final keyword would be apt for this more than any other method. But I am able to override this method and use it as I like, #pre It obviously only printed, #blockquote Is there any use in overriding other than confusing the engineer replacing you? If not, why was the method not declared final in Thread class?

Question-31993377
Question How is it that for a scanner object the hasNextLine(args) method returns true while the hasNext(args) method returns false? Note: Based on the input file, the hasNext(args) method is returning the result as expected; the hasNextLine(args) does not seem to be returning the correct result. Code Here's the code I'm running that's creating the results below: #pre Input File The following is the actual content of the file that I'm passing to this scanner: #pre Result The following is the end of what's printed in the console when I run my code, and includes the portion I can't make sense of: #pre

Question-34250207
As I recall, before Java 8, the default capacity of ArrayList was 10. Surprisingly, the comment on the default (void) constructor still says: Constructs an empty list with an initial capacity of ten. From ArrayList.java : #pre

Question-31188231
It has generally been the case the Java source code has been forward compatible. Until Java 8, as far as I know, both compiled classes and source have been forward compatible with later JDK/JVM releases. [Update: this is not correct, see comments re 'enum', etc, below.] However, with the addition of default methods in Java 8 this appears to no longer be the case. For example, a library I have been using has an implementation of java.util.List which includes a List<T> sort(args) . This method returns a copy of the contents of the list sorted. This library, deployed as a jar file dependency, worked fine in a project being built using JDK 1.8. However, later I had occasion to recompile the library itself using JDK 1.8 and I found the library no longer compiles: the List -implementing class with its own sort(args) method now conflicts with the Java 8 java.util.List.sort(args) default method. The Java 8 sort(args) default method sorts the list in place (returns void ); my library's sort(args) method - since it returns a new sorted list - has an incompatible signature. So my basic question is: #li Doesn't JDK 1.8 introduce a forward incompatibility for Java source code due to default methods? Also: #li Is this the first such forward incompatible change? #li Was this considered or discussed when default methods where designed and implemented? Is it documented anywhere? #li Was the (admittedly small) inconvenience discounted versus the benefits? The following is an example of some code that compiles and runs under 1.7 and runs under 1.8 - but does not compile under 1.8: #pre The following shows this code being compiled (or failing to) and being run. #pre

Question-32334319
I am using JDK-8 (x64). For Arrays.sort I found the following in the Java documentation: #blockquote For Collections.sort I found this: #blockquote If Collections.sort uses an array, why doesn't it just call Arrays.sort or use dual-pivot QuickSort ? Why use Mergesort ?

Question-31270759
Exceptions, especially checked ones, can severely interrupt the flow of program logic when the FP idiom is used in Java 8. Here is an arbitrary example: #pre The above code breaks when there's an exception for an unparseable string. But say I just want to replace that with a default value, much like I can with Optional : Stream.of(args).forEach(args); Of course, this still fails because Optional only handles null s. I would like something as follows: Stream.of(args).forEach(args); Note: this is a self-answered question.

Question-31419029
I've tried to build my own Map to increase the performance for a special environment, and I realized something pretty interesting: Creating a new Hashmap<T>(2000) is faster than new Object[2000] - no matter in which order I execute these commands. That's pretty confusing to me, esp. because the Hashmap constructor contains a table = new Entry[capacity] , according to this . Is there something wrong with my testbench? #pre I'd love to see the results of testing on another computer. I've got no clue why creating a HashMap is 10 times faster than creating a Object[] .

Question-31370403
I'm currently in the process of writing a paint program in java, designed to have flexible and comprehensive functionalities. It stemmed from my final project, that I wrote overnight the day before. Because of that, it's got tons and tons of bugs, which I've been tackling one by one (e.g. I can only save files that will be empty, my rectangles don't draw right but my circles do...). This time, I've been trying to add undo/redo functionality to my program. However, I can't "undo" something that I have done. Therefore, I got an idea to save copies of my BufferedImage each time a mouseReleased event was fired. However, with some of the images going to 1920x1080 resolution, I figured that this wouldn't be efficient: storing them would probably take gigabytes of memory. The reason for why I can't simply paint the same thing with the background colour to undo is because I have many different brushes, which paint based on Math.random(args) , and because there are many different layers (in a single layer). Then, I've considered cloning the Graphics objects that I use to paint to the BufferedImage . Like this: #pre I haven't done this before, so I have a couple questions: #li Would I still be wasting pointless memory by doing this, like cloning my BufferedImages ? #li Is there necessarily a different way I can do this?

Question-31445024
I'm trying to determine whether the following statements are guaranteed to be true: #pre I've always assumed that autoboxing was equivalent to calling valueOf(args) on the corresponding type. Every discussion that I've seen on the topic seems to support my assumption. But all I could find in the JLS was the following ( §5.1.7 ): #blockquote That describes behavior identical to that of valueOf(args) . But there doesn't seem to be any guarantee that valueOf(args) is actually invoked, meaning there could theoretically be an implementation that keeps a separate, dedicated cache for autoboxed values. In such a case, there might not be identity equality between cached autoboxed values and regular cached boxed values. Oracle's autoboxing tutorial states matter-of-factly that li.add(args) is compiled to li.add(args) , where i is an int . But I don't know whether the tutorial should be considered an authoritative source.

Question-32046078
I have a server side implemented in Scala and React/Flux based front end. My services return Futures and they are handled within Scalatra's AsyncResult for JSON responses. For isomorphic/server side rendering setup I did not want to change services to be blocking so I started with Scala Future-> java.util.function.Function conversion shown here . But the dispatcher in Flux would like to have JS Promise. So far I found only rather complicated sounding way around this Slides 68-81 Is there any recommended way to deal with this Scala Future -> JS Promise conversion?

Question-31696485
When using the Java 8 Optional class, there are two ways in which a value can be wrapped in an optional. #pre I understand Optional.ofNullable is the only safe way of using Optional , but why does Optional.of exist at all? Why not just use Optional.ofNullable and be on the safe side at all times?

Question-32323081
I have a question regarding Java 8 inference with respect to lambdas and their related exception signatures. If I define some method foo: #pre then I get the nice and concise semantic of being able to write foo(args); in most cases for a given T . However, in this example, if my getTheT operation declares that it throws Exception , my foo method which takes a Supplier no longer compiles: the Supplier method signature for get doesn't throw exceptions. It seems like a decent way to get around this would be to overload foo to accept either option, with the overloaded definition being: #pre where ThrowingSupplier is defined as #pre In this way, we have one Supplier type which throws exceptions and one which doesn't. The desired syntax would be something like this: #pre However, this causes issues due to the lambda type being ambiguous (presumably unable to resolve between Supplier and ThrowingSupplier). Doing an explicit cast a la foo(args); would work, but it gets rid of most of the conciseness of the desired syntax. I guess the underlying question is: if the Java compiler is able to resolve the fact that one of my lambdas is incompatible due to it throwing an exception in the Supplier-only case, why isn't it able to use that same information to derive the type of the lambda in the secondary, type-inference case? Any information or resources which anyone could point me to would likewise be much appreciated, as I'm just not too sure where to look for more information on the matter. Thanks!

Question-32714194
String is a special case in Java. It's a class, which I can examine in the source code , but it also has its own infix operator + , which seems to be syntactic sugar for StringBuilder . For example, StringLiteral + yourName; could become new StringBuilder(args).append(args).append(args).toString(args); There are no user-defined operators in Java, so where is + specified for String ? Could the same mechanism be used to make additional operators, such as for vectors?

Question-33358248
Currently the BlendModes (Subtract, Exclusion etc) use the LauncherImage as the mask. Can I apply these BlendModes to a ColorMatrix? I'm using the GPUImageLibrary colorMatrix[ 0.393, 0.7689999, 0.18899999, 0, 0, 0.349, 0.6859999, 0.16799999, 0, 0, 0.272, 0.5339999, 0.13099999, 0, 0, 0, 0, 0, 1, 0]; SubtractBlendFilter.java #pre GPUIMageTwoInputFilter.java #pre My guess it involves changing something with String SUBTRACT_BLEND_GRAGMENT_SHADER & String VERTEX_SHADER .

Question-34172978
#pre Counter to my expectation, the collect call never returns. Setting limit before filter produces the expected result. Why?

Question-32859038
I have a List<T> collection. I need to convert it into Map<T> The key of the map must be the index of the item in the collection. I can not figure it out how to do this with streams. Something like: items.stream(args).collect(args); Any help? As this question is identified as possible duplicate I need to add that my concrete problem was - how to get the position of the item in the list and put it as a key value

Question-33635717
I'm reading up about Java streams and discovering new things as I go along. One of the new things I found was the peek(args) function. Almost everything I've read on peek says it should be used to debug your Streams. What if I had a Stream where each Account has a username, password field and a login() and loggedIn() method. I also have Consumer<T> login = account -> account.login(args); and Predicate<T> loggedIn = account -> account.loggedIn(args); Why would this be so bad? #pre Now as far as I can tell this does exactly what it's intended to do. It; #li Takes a list of accounts #li Tries to log in to each account #li Filters out any account which aren't logged in #li Collects the logged in accounts into a new list What is the downside of doing something like this? Any reason I shouldn't proceed? Lastly, if not this solution then what? The original version of this used the .filter() method as follows; #pre

Question-33147339, answer-33147610
a < b and a - b < 0 can mean two different things. Consider the following code: #pre When run, this will only print a - b < 0 . What happens is that a < b is clearly false, but a - b overflows and becomes -1 , which is negative. Now, having said that, consider that the array has a length that is really close to Integer.MAX_VALUE . The code in ArrayList goes like this: #pre oldCapacity is really close to Integer.MAX_VALUE so newCapacity (which is oldCapacity + 0.5 * oldCapacity ) might overflow and become Integer.MIN_VALUE (i.e. negative). Then, subtracting minCapacity underflows back into a positive number. This check ensures that the if is not executed. If the code were written as if(args) , it would be true in this case (since newCapacity is negative) so the newCapacity would be forced to minCapacity regardless of the oldCapacity . This overflow case is handled by the next if. When newCapacity has overflowed, this will be true : MAX_ARRAY_SIZE is defined as Integer.MAX_VALUE - 8 and Integer.MIN_VALUE - (Integer.MAX_VALUE - 8) > 0 is true . The newCapacity is therefore rightly handled: hugeCapacity method returns MAX_ARRAY_SIZE or Integer.MAX_VALUE . NB: this is what the // overflow-conscious code comment in this method is saying.

Question-33147339, answer-33147762
I found this explanation : #blockquote In Java 6, if you use the API as: #pre And newCount overflows (this becomes negative), if(args) will return false and you may mistakenly assume that the ArrayList was increased by len .

Question-33147339, answer-33148103
Looking at the code: int newCapacity = oldCapacity + (oldCapacity >> 1); If oldCapacity is quite large, this will overflow, and newCapacity will be a negative number. A comparison like newCapacity < oldCapacity will incorrectly evaluate true and the ArrayList will fail to grow. Instead, the code as written ( newCapacity - minCapacity < 0 returns false) will allow the negative value of newCapacity to be further evaluated in the next line, resulting in recalculating newCapacity by invoking hugeCapacity ( newCapacity = hugeCapacity(args); ) to allow for the ArrayList to grow up to MAX_ARRAY_SIZE . This is what the // overflow-conscious code comment is trying to communicate, though rather obliquely. So, bottom line, the new comparison protects against allocating an ArrayList larger than the predefined MAX_ARRAY_SIZE while allowing it to grow right up to that limit if needed.

Question-32323081, answer-32323305
Any lambda which could be accepted as a Supplier<T> can also be accepted as a ThrowingSupplier<T> . The following compiles: #pre Given the above, you probably don't need this, but if foo must accept a non-throwing Supplier<T> , you can always wrap the Exception-throwing method in a method which launders it into an unchecked Exception: #pre

Question-32323081, answer-32323641
First of all, you don' have to overload :D - overloading is never a necessity; use 2 different method names, e.g. foo and fooX Secondly, I don't see why you need 2 methods here. If you want to handle checked and unchecked exceptions differently, it can be done at runtime. To achieve "exception transparency", you can do #pre Finally, disambiguity can be achieved throw lambda return type; the compiler uses the return type as if using an argument type for choosing the most specific method. This gives us the idea to wrap the value together with the exception type, as Result<T> , a "monad" as they say. #pre

Question-32475166, answer-32475290
The chief difference is that an if else block is a statement whereas the ternary (more often known as the conditional operator in Java) is an expression . A statement can do things like return to the caller on some of the control paths. An expression can be used in an assignment: int n = condition ? 3 : 2; So the two expressions in the ternary after the condition need to be coercable to the same type. This can cause some odd effects in Java particularly with auto-boxing and automatic reference casting - this is what the comment in your posted code is referring to. The coercion of the expressions in your case would be to a java.lang.reflect.Executable type (as that's the most specialised type ) and that does not exist in older versions of Java. Stylistically you should use an if else block if the code is statement-like, and a ternary if it's expression-like. Of course, you can make an if else block behave like an expression if you use a lambda function.

Question-32475166, answer-32476850
When you think about the type of the operands, the problem becomes more apparent: this.method != null ? this.method : this.constructor has as type the most specialized common type of both the operands, i.e. the most specialized type common to both this.method and this.constructor . In Java 7 this is java.lang.reflect.Member , however the Java 8 class library introduces a new type java.lang.reflect.Executable which is more specialized than the generic Member . Hence with a Java 8 class library the result type of the ternary expression is Executable rather than Member . Some (pre-release) versions of the Java 8 compiler seem to have produced an explicit reference to Executable inside generated code when compiling the ternary operator. This would trigger a class load, and thus in turn a ClassNotFoundException at runtime when running with a class library < JDK 8, because Executable only exists for JDK ≥ 8. As noted by Tagir Valeev in this answer , this is actually a bug in pre-release versions of JDK 8 and has since been fixed, so both the if-else workaround and the explanatory comment are now obsolete. Additional note: One might come to the conclusion that this compiler bug was present before Java 8. However, the byte code generated for the ternary by OpenJDK 7 is the same as the byte code generated by OpenJDK 8. In fact, the type of the expression goes completely unmentioned at runtime, the code is really only test, branch, load, return without any additional checks going on. So rest assured that this is not a problem (anymore) and indeed seems to have been a temporary problem during development of Java 8.

Question-32820722, answer-32820940
An IntFunction<T> is a function int -> R . An IntUnaryOperator is a function int -> int . Thus an IntFunction<T> is a function that takes an int as parameter and return a function that takes an int as parameter and return an int . #pre Maybe it is more clear if you use anonymous classes to "decompose" the lambda: #pre

Question-32820722, answer-32820984
Let's rewrite that lambda expression with parentheses to make it more clear: IntFunction<T> curriedAdd = a -> (b -> (a + b)); So we are declaring a function taking an int which returns a Function . More specifically, the function returned takes an int and returns an int (the sum of the two elements): this can be represented as an IntUnaryOperator . Therefore, curriedAdd is a function taking an int and returning an IntUnaryOperator , so it can be represented as IntFunction<IntUnaryOperator> .

Question-32820722, answer-32821016
If you look at IntFunction it might become clearer: IntFunction<T> is a FunctionalInterface . It represents a function that takes an int and returns a value of type R . In this case, the return type R is also a FunctionalInterface , namely an IntUnaryOperator . So the first (outer) function itself returns a function. In this case: When applied to an int , curriedAdd is supposed to return a function that again takes an int (and returns again int , because that's what IntUnaryOperator does). In functional programming it is common to write the type of a function as param -> return_value and you see exactly that here. So the type of curriedAdd is int -> int -> int (or int -> (int -> int) if you like that better). Java 8's lambda syntax goes along with this. To define such a function, you write a -> b -> a + b which is very much similar to actual lambda calculus: λa λb a + b λb a + b is a function that takes a single parameter b and returns a value (the sum). λa λb a + b is a function that accepts a single parameter a and returns another function of a single parameter. λa λb a + b returns λb a + b with a set to the parameter value.

Question-32312876, answer-32313069
This is possible using the mergeFunction parameter of Collectors.toMap(keyMapper, valueMapper, mergeFunction) : #pre mergeFunction is a function that operates on two values associated with the same key. adress1 corresponds to the first address that was encountered when collecting elements and adress2 corresponds to the second address encountered: this lambda just tells to keep the first address and ignores the second.

Question-31965179, answer-35427268
There's a long list of things that have been changed in Hibernate 5: #li New bootstrap API so we can bootstrap a JPA environment programmatically without the need of a persistence.xml file . #li Starting in 5.0 Hibernate Spatial is part of the Hibernate project so we can handle GIS data too. #li The Java 8 Date and Time types are supported in domain model mappings. The mapping between the standard SQL Date/Time types and the supported Java 8 Date/Time class types looks as follows; #li DATE: java.time.LocalDate #li TIME: java.time.LocalTime , java.time.OffsetTime #li TIMESTAMP: java.time.Instant , java.time.LocalDateTime , java.time.OffsetDateTime and java.time.ZonedDateTime #li The bytecode enhancement mechanism was redesigned from scratch, and Hibernate features both a Maven and a Gradle plugin. There are three main aspects which we can enhance with bytecode instrumentation: #li Lazy initialization: Fields can be declared as LAZY and they will be fetched only when being accessed for the first time. #li Dirty checking: Entities are enhanced so that they can keep track of all the properties that get changed after being loaded in a Persistence Context. #li Bidirectional associations: It's possible to synchronize both sides of a bidirectional association automatically, even if the developer only updates a single side. #li Hibernate's native APIs ( Session , etc) have been updated to use generic typed. No need to cast when fetching entities. #li Hibernate 5.0 expends this to a broader set of types (e.g. UUID ). #li Second-level cache by reference. This feature enables direct storage of entity references into the second level cache for immutable entities. #li Starting with Hibernate 5.0, we have a completely new User Guide that was written from scratch.

Question-32714194, answer-32714222
+ is implemented in java compilers . The compiler replaces String + String with either compile time constants or StringBuilder code. Note that this applies to primitives too. i.e, int i=1+2 could get directly replaced to int i=3 during compilation itself.

Question-32714194, answer-32714537
While currently most of Java compilers using StringBuilder chain, it's not specified that it should be always in this way. In particular there's a proposal to change this drastically in Java-9 replacing with single invokedynamic call and introduce new metafactory which will generate an appropriate MethodHandle in runtime to perform concatenation.

Question-32175145, answer-32175175
Note that a . b and c are instances of primitive wrapper classes (such as Integer, Double, etc...). These classes are final and cannot be extended, so you can't override their equals implementation. Therefore a.equals(args) will always return true, since those classes implement equals properly.

Question-32175145, answer-32175224
Since equals(...) is not a final method of Object , yes, it is very well be possible in a different situation . #pre This question, however, specifically says that these are primitive wrappers (e.g. Integer , Boolean , etc.) and since these classes are final, you cannot extend them, thus a.equals(args) will always return true .

Question-32175145, answer-32177563
The other answers have already answered your question - no, this is not possible with the Java's primitive wrapper classes. I'll try to address the "question behind the question": Is this possible with other classes? #blockquote This is actually a good question, and the answer is: Yes, it is possible to create such a situation, and yes, it would make Aristotle angry. Actually, I don't know if it would make Aristotle angry, not having known him, but it will certainly cause a lot of grief for whoever has to work with the code. The thing is: There is a contract associated with Object.equals(args) : #blockquote Javadocs for Object.equals Yes, when creating your own class, you can violate this contract. There is (unfortunately) nothing in the compiler or runtime stopping you. However, a lot of code relies on this contract, so if you violate it, any code that uses equals will probably fail in mysterious ways. One example: Java's own Collection classes ( java.util.Collection and friends) rely on equals . If an instance of a class that does not correctly implement equals is put into a collection, weird things happen, such as the collection sometimes containing the instance and sometimes not.

Question-32175145, answer-32182250
Integer a.equals( a ) can return false But you have to be really evil and use reflections and Multithreading: If you run this code, there is a chance that a racing condition can change the internal Value of myInt while the comparision takes place. If you want to simulate this condition, just set a breakpoint inside of Integer.intValue(args) run the code in debug and hit continue. This will create a delay which creates the race condition artificially and the console will return false. #pre

Question-32623834, answer-32624143
You just changed the String of String constant pool Mario to Luigi which was referenced by multiple String s, so every referencing literal Mario is now Luigi . Field stringValue = String.class.getDeclaredField(args); You have fetched the char[] named value field from class String stringValue.setAccessible(args); Make it accessible. stringValue.set(args); You changed original String field to Luigi . But original is Mario the String literal and literal belongs to the String pool and all are interned . Which means all the literals which has same content refers to the same memory address. #pre Basically you have changed the Mario of String pool which got reflected in all the referencing fields. If you create String Object (i.e. new String(args) ) instead of literal you will not face this behavior because than you will have two different Mario s .

Question-32623834, answer-32652449
Another related point: you can make use of the constant pool to improve the performance of string comparisons in some circumstances, by using the String.intern() method. That method returns the instance of String with the same contents as the String on which it is invoked from the String constants pool, adding it it if is not yet present. In other words, after using intern(args) , all Strings with the same contents are guaranteed to be the same String instance as each other and as any String constants with those contents, meaning you can then use the equals operator ( == ) on them. This is just an example which is not very useful on its own, but it illustrates the point: #pre This little trick isn't worth designing your code around, but it is worth keeping in mind for the day when you notice a little more speed could be eked out of some bit of performance sensitive code by using the == operator on a string with judicious use of intern(args) .

Question-31170459, answer-31170506
No, you appear to have a legit bug. The bug presents in JDK7 but fixed in JDK8. Your values are correctly representable as BigDecimal s, and should behave correctly, but don't. Tracing through the source code of BigDecimal , on line 2585, this.precision(args) is 1, and this.scale is -2147483647 . this.precision(args) - this.scale therefore overflows, and the following overflow is not handled correctly. This bug has been fixed in JDK8 by doing the subtraction in long arithmetic .

Question-33804394, answer-33814549
In this sort of situation I would write #pre Interestingly you say that the "try statement will never fail", but you still took the trouble to write a statement e.printStackTrace(args); that you claim will never be executed. Why? Perhaps your belief is not that firmly held. That is good (in my opinion), since your belief is not based on the code you wrote, but rather on the expectation that your client will not violate the precondition. Better to program public methods defensively. By the way, your code won't compile for me. You can't call a.clone(args) even if the type of a is Cloneable . At least Eclipse's compiler says so. Expression a.clone(args) gives error #blockquote What I would do for your specific case is #pre Where PubliclyCloneable is defined by #pre Or, if you absolutely need the parameter type to be Cloneable , the following at least compiles. #pre

Question-32356762, answer-32357142
When returning List , you'll be promoting program to an interface , which is a very good practice. However, this approach has its limitation. For example, you cannot use some methods that are defined for ArrayList and don't exist in the List interface - See this answer for details. I'm quoting the API Design from the The Java™ Tutorials: #blockquote Since ArrayList is essentially an array, they are my first choice when I need to have a "collection-array". So if I want to convert enumeration to a list, my choice would be an array list. In any other cases, it's still valid to write: List<T> list = Collections.list(args);

Question-32356762, answer-32357149
Disclaimer: I am not a JDK author. I agree that it's correct to write your own code to interfaces, but if you're going return a mutable collection to a third party, it's important to let the third party know what sort of List they're getting back. LinkedList and ArrayList are very different, performance wise, for various operations. For example, removing the first element of an ArrayList is O(args) , but removing the first element of a LinkedList is O(args) . By fully specifying the return type, the JDK authors are communicating extra information , in unambiguous code, about what sort of object they're giving back to you, so you can write your code to use this method properly. If you really need a LinkedList , you know that you have to specify one here. Finally, the main reason to code to an interface over an implementation is if you think that the implementation will change. The JDK authors probably figure they are never going to change this method; it's never going to return a LinkedList or a Collections.UnmodifiableList . However, in most cases, you would probably still do: List<T> list = Collections.list(args);

Question-32356762, answer-32370677
There is a small overhead in calling methods var an interface rather than directly on an object. This overhead is often no more than 1 or 2 processor instructions. The overhead of calling a method is even lower if the JIT knows that the method is final. This is not measurable for most code you and me right, but for the low level methods in java.utils may be used in some code where it is an issue. Also as has been pointed out in other answers, the concrete type of the object that is return (even when hidden behind an interface) effects the performance of code that uses it. This change in performance can be very great, so such an extent that the calling software fails to work. Clearly the authors of java.utils have no way to know what all the software that calls Collections.list() does with the result and no way to re-test this software if they change the implantation of Collections.list(). Therefore they are not going to change the implantation of Collections.list() to return a different type of List, even if the type system allowed it! When writing your own software, you (hopefully) have automated test that cover all your code and a good understand of how your code interrelates include know where performance is an issue. Being able to make a change to a method, without having to change the callers is of great value while the design of the software is changing. Therefore the two set of trade offs are very different.

Question-31922866, answer-31922994
I think that is because you usually write your functions to manipulate data, and then lift it to Optional using map and similar functions. This adds the default Optional behavior to it. Of course, there might be cases, when it is necessary to write your own auxilary function that works on Optional .

Question-31922866, answer-31923015
I believe the reson of being is you have to first check whether or not Optional is null itself and then try to evaluate value it wraps. Too many unnecessary validations.

Question-31922866, answer-31923042
The pattern with Optional is for one to avoid returning null . It's still perfectly possible to pass in null to a method. While these aren't really official yet, you can use JSR-308 style annotations to indicate whether or not you accept null values into the function. Note that you'd have to have the right tooling to actually identify it, and it'd provide more of a static check than an enforceable runtime policy, but it would help. public int calculateSomething(args) {}

Question-31922866, answer-31923105
Oh, those coding styles are to be taken with a bit of salt. #li (+) Passing an Option result to another method, without any semantic analysis; leaving that to the method, is quite alright. #li (-) Using Optional parameters causing conditional logic inside the methods is literally contra-productive. #li (-) Needing to pack an argument in an Optional, is suboptimal for the compiler, and does an unnecessary wrapping. #li (-) In comparison to nullable parameters Optional is more costly. In general: Optional unifies two states, which have to be unraveled. Hence better suited for result than input, for the complexity of the data flow.

Question-31922866, answer-31923211
This advice is a variant of the "be as unspecific as possible regarding inputs and as specific as possible regarding outputs" rule of thumb. Usually if you have a method that takes a plain non-null value, you can map it over the Optional , so the plain version is strictly more unspecific regarding inputs. However there are a bunch of possible reasons why you would want to require an Optional argument nonetheless: #li you want your function to be used in conjunction with another API that returns an Optional #li Your function should return something other than an empty Optional if the given value is empty #li You think Optional is so awesome that whoever uses your API should be required to learn about it ;-)

Question-31922866, answer-31923214
This seems a bit silly to me, but the only reason I can think of is that object arguments in method parameters already are optional in a way - they can be null. Therefore forcing someone to take an existing object and wrap it in an optional is sort of pointless. That being said, chaining methods together that take/return optionals is a reasonable thing to do, e.g. Maybe monad.

Question-31922866, answer-31923227
Optionals aren't designed for this purpose, as explained nicely by Brian Goetz . You can always use @Nullable to denote that a method argument can be null. Using an optional does not really enable you to write your method logic more neatly.

Question-31922866, answer-31924845
There are almost no good reasons for not using Optional as parameters. The arguments against this rely on arguments from authority (see Brian Goetz - his argument is we can't enforce non null optionals) or that the Optional arguments may be null (essentially the same argument). Of course, any reference in Java can be null, we need to encourage rules being enforced by the compiler, not programmers memory (which is problematic and does not scale). Functional programming languages encourage Optional parameters. One of the best ways of using this is to have multiple optional parameters and using liftM2 to use a function assuming the parameters are not empty and returning an optional (see #a ). Java 8 has unfortunately implemented a very limited library supporting optional. As Java programmers we should only be using null to interact with legacy libraries.

Question-31922866, answer-39005452
The best post I've seen on the topic was written by Daniel Olszewski and can be found at #a . While others mention when you should or should not use Optional, this post actually explains why . Cross-posting here in case the link goes down: #blockquote

Question-32693704, answer-32744190
There is no actual difference in the functionality between the 2 version's loop. Arrays.fill does the exact same thing. So the choice to use it or not may not necessarily be considered a mistake. It is left up to the developer to decide when it comes to this kind of micromanagement. There are 2 separate concerns for each approach: #li using the Arrays.fill makes the code less verbose and more readable. #li looping directly in the HashMap code (like version 8) peformance wise is actually a better option. While the overhead that inserting the Arrays class is negligible it may become less so when it comes to something as widespread as HashMap where every bit of performance enhancement has a large effect(imagine the tiniest footprint reduce of a HashMap in fullblown webapp). Take into consideration the fact that the Arrays class was used only for this one loop. The change is small enough that it doesn't make the clear method less readable. The precise reason can't be found out without asking the developer who actually did this, however i suspect it's either a mistake or a small enhancement. better option. My opinion is it can be considered an enhancement, even if only by accident.

Question-32693704, answer-32745612
For me, the reason is a likely performance inprovement, at a negligible cost in terms of code clarity. Note that the implementation of the fill method is trivial, a simple for-loop setting each array element to null. So, replacing a call to it with the actual implementation does not cause any significant degradation in the clarity/conciseness of the caller method. The potential performance benefits are not so insignificant, if you consider everything that is involved: #li There will be no need for the JVM to resolve the Arrays class, plus loading and initializing it if needed. This is a non-trivial process where the JVM performs several steps. Firstly, it checks the class loader to see if the class is already loaded, and this happens every time a method is called; there are optimizations involved here, of course, but it still takes some effort. If the class is not loaded, the JVM will need to go through the expensive process of loading it, verifying the bytecode, resolving other necessary dependencies, and finally performing static initialization of the class (which can be arbitrarily expensive). Given that HashMap is such a core class, and that Arrays is such a huge class (3600+ lines), avoiding these costs may add up to noticeable savings. #li Since there is no Arrays.fill(args) method call, the JVM won't have to decide whether/when to inline the method into the caller's body. Since HashMap#clear(args) tends to get called a lot, the JVM will eventually perform the inlining, which requires JIT recompilation of the clear method. With no method calls, clear will always run at top-speed (once initially JITed). Another benefit of no longer calling methods in Arrays is that it simplifies the dependency graph inside the java.util package, since one dependency is removed.

Question-32693704, answer-32749756
I'm going to shoot in the dark here... My guess is that it might have been changed in order to prepare the ground for Specialization (aka generics over primitive types). Maybe (and I insist on maybe ), this change is meant to make transition to Java 10 easier, in the event of specialization being part of the JDK. If you look at the State of the Specialization document , Language restrictions section, it says the following: #blockquote (Emphasis is mine). And ahead in the Specializer transformations section, it says: #blockquote Later on, near the end of the document, in the Further investigation section, it says: #blockquote Now, regarding the change... If the Arrays.fill(args) method is going to be specialized, then its signature should change to Arrays.fill(args) . However this case is specifically listed in the (already mentioned) Language restrictions section (it would violate the emphasized items). So maybe someone decided that it would be better to not use it from the HashMap.clear(args) method, especially if value is null .

Question-32693704, answer-32752970
I will try to summarize three moreless reasonable versions which were proposed in comments. @Holger says : #blockquote This is the most easy thing to test. Let's compile such program: #pre Run it with java -verbose:class HashMapTest . This will print the class loading events as they occur. With JDK 1.8.0_60 I see more than 400 classes loaded: #pre As you can see, HashMap is loaded long before application code and Arrays is loaded only 14 classes after HashMap . The HashMap load is triggered by sun.reflect.Reflection initialization as it has HashMap static fields. The Arrays load is likely to be triggered by WeakHashMap load which actually has Arrays.fill in the clear(args) method. The WeakHashMap load is triggered by java.lang.ClassValue$ClassValueMap which extends WeakHashMap . The ClassValueMap is present in every java.lang.Class instance. So to me seems that without Arrays class the JDK cannot be initialized at all. Also the Arrays static initializer is very short, it only initializes the assertion mechanism. This mechanism is used in many other classes (including, for example, java.lang.Throwable which is loaded very early). No other static initialization steps are performed in java.util.Arrays . Thus @Holger version seems incorrect to me. Here we also found very interesting thing. The WeakHashMap.clear(args) still uses Arrays.fill . It's interesting when it appeared there, but unfortunately this goes to prehistoric times (it was already there in the very first public OpenJDK repository). Next, @MarcoTopolnik says : #blockquote It was actually surprising for me that Arrays.fill is not directly intrinsified (see intrinsic list generated by @apangin ). Seems that such loop can be recognized and vectorized by JVM without explicit intrinsic handling. So it's true that extra call can be not inlined in very specific cases (for example if MaxInlineLevel limit is reached). On the other hand it's very rare situation and it's only a single call, it's not a call inside loop, and it's a static, not virtual/interface call, thus the performance improvement could be only marginal and only in some specific scenarios. Not the thing the JVM developers usually care. Also it should be noted that even C1 'client' compiler (tier 1-3) is capable to inline Arrays.fill called, for example, in WeakHashMap.clear(args) , as inlining log ( -XX:+UnlockDiagnosticVMOptions -XX:+PrintCompilation -XX:+PrintInlining ) says: #pre Of course, it's also easily inlined by smart and powerful C2 'server' compiler. Thus I see no problems here. Seems that @Marco version is incorrect either. Finally we have a couple of comments from @StuartMarks (who is JDK developer, thus some official voice): #blockquote Indeed the HashMap.clear(args) contained the loop many years, was replaced with Arrays.fill on Apr 10th, 2013 and stayed less one half-a-year until Sept 4th when the discussed commit was introduced. The discussed commit was actually a major rewrite of the HashMap internals to fix JDK-8023463 issue. It was a long story about possibility to poison the HashMap with keys having duplicating hashcodes reducing HashMap search speed to linear making it vulnerable to DoS-attacks. The attempts to solve this were performed in JDK-7 including some randomization of String hashCode. So seems that the HashMap implementation was forked from the earlier commit, developed independently, then merged into the master branch overwriting several changes introduced in-between. We may support this hypothesis performing a diff. Take the version where Arrays.fill was removed (2013-09-04) and compare it with previous version (2013-07-30). The diff -U0 output has 4341 lines. Now let's diff against the version prior to one when Arrays.fill was added (2013-04-01). Now diff -U0 contains only 2680 lines. Thus the newer version actually more similar to the older than to immediate parent. Conclusion So to conclude I would agree with Stuart Marks. There were no concrete reason to remove Arrays.fill , it's just because the in-between change was overwritten by mistake. Using Arrays.fill is perfectly fine both in JDK code and in user applications and used, for example, in WeakHashMap . The Arrays class is loaded anyways pretty early during the JDK initialization, has very simple static initializer and Arrays.fill method can be easily inlined even by client compiler, so no performance drawback should be noted.

Question-32693704, answer-32852619
Because it's much faster! I ran some thorough benchmarking tests on cut down versions of the two methods: #pre operating on arrays of various sizes containing random values. Here are the (typical) results: #pre And here are the results when operating over an array filled with nulls (so garbage collection issues are eradicated): #pre The numbers are in nanoseconds, (sd) is 1 standard deviation expressed as a percentage of the result (fyi, a "normally distributed" population has an SD of 68), vs is the JDK 8 timing relative to JDK 7. It is interesting that not only is it significantly faster, but the deviation is also slightly narrower, which means that the JDK 8 implementation gives slightly more consistent performance. The tests were run on jdk 1.8.0_45 over a large (millions) number of times on arrays populated with random Integer objects. To remove out-lying numbers, on each set of results the fastest and slowest 3% of timings were discarded. Garbage collection was requested and the thread yielded and slept just prior to running each invocation of the method. JVM warm up was done on the first 20% of work and those results were discarded.

Question-31202946, answer-31203218
You can of course choose to shoot yourself in the foot, but that doesn't mean you must. #blockquote Because the recommended way to create a start a thread is not to subclass Thread. The recommended way is to define a Runnable , and pass it as argument to the Thread constructor: #pre #blockquote Yes and no. You can't replace the implementation of start() by your own implementation, but you can do additional things in start() if you want: #pre That said, if Java was redesigned from scratch today, there is a good chance the design would be different. Remember that this class dates from Java 1.0, and is still backward-compatible.

Question-31202946, answer-31203252
#blockquote This question actually boils down to the fact that you should always prefer composition over inheritance. If the Thread class was declared as abstract , the language would have to provide another class that extended from it which programmers could use to create a Thread . Your question would then be about why this class that extends from Thread is not abstract . If the language did not provide another class that extends from Thread , programmers would have to create their own class that extend s from Thread and override the run(args) method. #blockquote The only possible explanation I can give is that the developers of the language saw some use-cases for overriding start when the class was introduced to the JDK. The first version of Java that I used was 1.5 and I personally have not come across a use-case where I found the need to override start . As JB Nizet stated in his answer #blockquote

Question-34172978, answer-34173040
Since there are only 4 elements that pass the filter, limit(args) never reaches 10 elements, so the Stream pipeline keeps generating new elements and feeding them to the filter, trying to reach 10 elements that pass the filter, but since only the first 4 elements pass the filter, the processing never ends (at least until i overflows). The Stream pipeline is not smart enough to know that no more elements can pass the filter, so it keeps processing new elements.

Question-34172978, answer-34173110
Flipping the limit and the filter clauses has different behaviors. If you put the limit first, the stream will first generate 10 integers [1..10], and then filter them leaving only those smaller than 5. In the original ordering, with the filter applied first, integers are generated and filtered until you reach 10 elements. This isn't an infinite operator, as i in the supplier will eventually overflow, but it will take a while, especially on a slow computer, to reach MAX_INT .

Question-34172978, answer-34173658
If you want to stop either if number 5 is reached or 10 elements are collected, there's Stream.takeWhile(args) method added in Java-9: #pre

Question-32859038, answer-32859065
You can create a Stream of the indices using an IntStream and then convert them to a Map : Map<T> map = IntStream.range(args) .boxed(args) .collect(args);

Question-32859038, answer-32863263
Don't feel like you have to do everything in/with the stream. I would just do: #pre As long as you don't parallelise the stream this will work and it avoids potentially expensive and/or problematic (in the case of duplicates) get(args) and indexOf(args) operations. (You cannot use a regular int variable in place of the AtomicInteger because variables used from outside a lambda expression must be effectively final. Note that when uncontested (as in this case), AtomicInteger is very fast and won't pose a performance problem. But if it worries you you can use a non-thread-safe counter.)

Question-32859038, answer-32918035
Eran's answer is usually the best approach for random-access lists. If your List isn't random access, or if you have a Stream instead of a List , you can use forEachOrdered : #pre This is safe, if the stream is parallel, even though the destination map is thread-unsafe and is operated upon as a side effect. The forEachOrdered guarantees that items are processed one-at-a-time, in order. For this reason it's unlikely that any speedup will result from running in parallel. (There might be some speedup if there are expensive operations in the pipeline before the forEachOrdered .)

Question-31471444, answer-31471520
org.apache.commons.lang.math.NumberUtils.INTEGER_ONE it gives you a final static Integer object rather than primitive int 1, and as it is final static it acts as a constant and can be used in comparison of Integer objects because will always return same instance. So in the above scenario it might not look fit but somewhere if you are using it while comparison, it for sure has impact. Moreover, as much as we can, should prefer the use of constants over hardcoded beacuse: #li It can make your code easily maintainable. If any situation occurs in future for change, you can change only at a single place. #li The code looks cleaner & more readable.

Question-31471444, answer-31471530
You should not. The INTEGER_ONE name is no more meaningful than 1. If however this value has some other meaning (for example, month in the year), then using a constant (like Calendar.FEBRUARY ) will make your code clearer. I can guess that this constant in Commons Math library was created in Java 1.4 when there were no Integer cache and autoboxing, so it had sense in terms that you may reuse the same Integer object (not primitive int ) in different places to save memory. So it was added for performance reasons, not for code clarity. Now it's obsolete: even if you need an Integer object, you can use Integer.valueOf(args) or implicit autoboxing and get the cached one.

Question-31471444, answer-31476657
From Class NumberUtils you'll see it's defined as: #pre So, you'll see that INTEGER_ONE is not the same as 1. It's an object that's already been constructed for you. So, if we're needing an instance of Integer(args) , rather than create your own, you can reuse the one from the library saving time and memory. It really depends on your application, if you indeed what the int version of 1, then, you'd probably be better of using that instead of this Integer class.

Question-32632364, answer-32633199
It shouldn't have compiled, according to JLS 7 . int can be compared to boxed numeric types, i.e. Byte, Short, Character, Integer, Long, Float, Double. But that is all. And if comparison is between int and say Float , Float will be unboxed first, so that the comparison is between float and int . It'll make no sense to do the other way around - box int then examine the identity of the Integer (with a Float no less).

Question-33477695, answer-33477803
The java.time API in general does have nanosecond precision. For example: #pre Output: 2015-11-02T12:38:00,123456789+0000 However, it's the clock value returned by OffsetDateTime.now(args) which is returning a value which only has milliseconds. From Clock implementation in Java 8: #blockquote So there's nothing inherently imprecise here - just the default implementation of Clock using System.currentTimeMillis(args) . You could potentially create your own more precise subclass. However, you should note that adding more precision without adding more accuracy probably isn't terribly useful. (There are times when it might be, admittedly...)

Question-31316581, answer-31327420
If type inference produces a single upper bound for a type variable, typically the upper bound is chosen as the solution. For example, if T<<Number , the solution is T=Number . Although Integer , Float etc. could also satisfy the constraint, there's no good reason to choose them over Number . That was also the case for throws T in java 5-7: T<<Throwable => T=Throwable . (Sneaky throw solutions all had explicit <RuntimeException> type arguments, otherwise <Throwable> is inferred.) In java8, with the introduction of lambda, this becomes problematic. Consider this case #pre If we invoke with an empty lambda, what would T be inferred as? invoke(args); The only constraint on T is an upper bound Throwable . In earlier stage of java8, T=Throwable would be inferred. See this report I filed. But that is pretty silly, to infer Throwable , a checked exception, out of an empty block. A solution was proposed in the report (which is apparently adopted by JLS) - #pre i.e. if the upper bound is Exception or Throwable , choose RuntimeException as the solution. In this case, there is a good reason to choose a particular subtype of the upper bound.

Question-31993377, answer-31993478
You are consuming the value of next(args) , but asking for hasNext(args) and hasNextLine(args) . next(args) , per default, returns everything to the next whitespace(args) . So you are iterating through all whitespace seperated strings, and after each of them you are asking about the nextLine(args) . i 1 1 -> hasNextLine(args) ? True. hasNext(args) ? Also true. 1 1 -> hasNextLine(args) ? True. hasNext(args) ? Also true (still a whitespace left) 1 -> hasNextLine(args) ? True (Line Seperator, probably). haxNext? False, no whitespace anymore.

Question-31993377, answer-31993533
The reason is that hasNext(args) checks if there are any more non-whitespace characters available. hasNextLine(args) checks to see if there is another line of text available. Your text file probably has a newline at the end of it so it has another line but no more characters that are not whitespace. Many text editors automatically add a newline to the end of a file if there isn't one already. In other words, your input file is not this (the numbers are line numbers): #pre It is actually this: #pre

Question-31993377, answer-31993534
You have a single extra newline at the end of your file. #li hasNextLine() checks to see if there is another linePattern in the buffer. #li hasNext() checks to see if there is a parseable token in the buffer, as separated by the scanner's delimiter. Since the scanner's delimiter is whitespace, and the linePattern is also white space, it is possible for there to be a linePattern in the buffer but no parseable tokens. Typically, the most common way to deal with this issue by always calling nextLine(args) after parsing all the tokens (e.g. numbers) in each line of your text. You need to do this when using Scanner when reading a user's input too from System.in . To advance the scanner past this whitespace delimiter, you must use scanner.nextLine(args) to clear the line delimiter. See: Using scanner.nextLine() Appendix: LinePattern is defined to be a Pattern that matches this: #pre The default token delimiter is this Pattern : private static Pattern WHITESPACE_PATTERN = Pattern.compile(args);

Question-34250207, answer-34250231
Technically, it's 10 , not zero, if you admit for a lazy initialisation of the backing array. See: #pre where /** * Default initial capacity. */ private static final int DEFAULT_CAPACITY = 10; What you're referring to is just the zero-sized initial array object that is shared among all initially empty ArrayList objects. I.e. the capacity of 10 is guaranteed lazily , an optimisation that is present also in Java 7. Admittedly, the constructor contract is not entirely accurate. Perhaps this is the source of confusion here. Background Here's an E-Mail by Mike Duigou #blockquote

Question-32676771, answer-32677400
Despite of the fact that there is no AtomicDouble or AtomicDoubleArray in java, you can easily create your own based on AtomicLongArray . #pre As you can see, I use Double.doubleToLongBits and Double.longBitsToDouble to store Doubles as Longs in AtomicLongArray . They both have the same size in bits, so precision is not lost (except for -NaN, but I don't think it is important). In Java 8 the implementation of add can be even easier, as you can use accumulateAndGet method of AtomicLongArray that was added in java 1.8. Upd : It appears that I virtually re-implemented guava's AtomicDoubleArray .

Question-31188231, answer-31188818
Ironically default methods in interfaces were introduced to allow existing libraries using those interfaces not to break, while introducing massive new functionality in the interfaces. (backward compatibility.) Conflicts like that sort method might arise. Something to pay for the extra functionality. In your case also something to investigate (should new functionality be used instead?). Java forward compatibility breaks are little, more in its typing system, which was constantly enlarged. First with generic types and now with inferred types from functional interfaces. From version to version and from compiler to compiler there were slight differences.

Question-31188231, answer-31190722
We can draw a parallel with abstract class. An abstract class is intended to be subclassed so that the abstract methods can be implemented. The abstract class itself contains concrete methods that invoke the abstract methods. The abstract class is free to evolve by adding more concrete methods; and this practice may break subclasses. Therefore the exact problem you described existed even before Java8. The problem is much more manifested on Collection APIs because there are a lot of subclasses out in the wild. While the leading motivation of default method was to add some useful methods to existing Collection APIs without breaking subclasses, they had to exercise great self-control of doing it too much, for fear of breaking subclasses. A default method is added only if it's absolutely necessary. The real question here is, why List.sort is considered absolutely necessary. I think that is debatable. Regardless of why default method was introduced in the 1st place, it is now a great tool for API designers, and we ought to treat it the same as concrete methods in abstract classes - they need to be designed carefully up front; and new ones must be introduced with great caution.

Question-34509566, answer-34509655
As you said internally the last concatenation is done to something similar to String e = new StringBuilder(args).append(args).append(args).toString(args); the implementation of toString(args) of StringBuilder creates a new String . Here is the implementation. #pre Comparing strings using == instead of .equals(args) returns true only if both strings are the same . In this case they are not the same because the second string is created as a new object of type String . The other concatenations are performed directly by the compiler so no new String is created.

Question-34509566, answer-34509659
Four things are going on: #li (You clearly know this, but for lurkers) == tests to see if the variables point to the same String object , not equivalent strings. So even if x is StringLiteral and y is also StringLiteral , x == y may be true or false, depending on whether x and y refer to the same String object or different ones. That's why we use equals , not == , to compare strings for equivalence. All of the following is just meant to explain why == is sometimes true, it's not a suggestion to use == to compare strings. :-) #li Equivalent string constants (strings the compiler knows are constants according to various rules in the JLS) within the same class are made to refer to the same string by the compiler (which also lists them in the class's "constant pool" ). That's why a == b is true. #li When the class is loaded, each of its string constants is automatically interned — the JVM's string pool is checked for an equivalent string and if one is found, that String object is used (if not, the new String object for the new constant is added to the pool). So even if x is a string constant initialized in class Foo and y is a string constant initialized in class Bar , they'll be == each other. Points 2 and 3 above are covered in part by JLS§3.10.5 . (The bit about the class constant pool is a bit of an implementation detail, hence the link to the JVM spec earlier; the JLS just speaks of interning.) #li The compiler does string concatenation if it's dealing with constant values, so String d = StringLiteral + StringLiteral; is compiled to String d = StringLiteral; and StringLiteral is a string constant the compiler and JVM apply points 2 and 3 above to. E.g., no StringBuilder is used, the concatenation happens at compile-time , not runtime. This is covered in JLS§15.28 - Constant Expressions . So a == d is true for the same reason a == b is true: They refer to the same constant string, so the compiler ensured they were referring to the same string in the class's constant pool. The compiler can't do that when any of the operands is not a constant, so it can't do that with: String e = c + StringLiteral; ...even though code analysis could easily show that the value of c will definitely be StringLiteral and thus e will definitely be StringLiteral . The specification only has the compiler do the concatenation with constant values, specifically. So since the compiler can't do it, it outputs the StringBuilder code you referred to and that work is done at runtime, creating a new String object. That string isn't automatically interned, so e ends up referring to a different String object than a does, and so a == e is false. Note that as Vinod said , if you declared c as final : final String c = StringLiteral; Then it would be a constant variable (yes, they're really called that) and so §15.28 would apply and the compiler would turn String e = c + StringLiteral; into String e = StringLiteral; and a == e would also be true. Just to reiterate: None of which means we should use == to compare strings for equivalence. :-) That's what equals is for.

Question-34509566, answer-34509669
The difference between d and e is that when you concatenate string literals , the concatenation is performed at compile time. Java compiler treats StringLiteral + StringLiteral expression in the same way as StringLiteral expression, producing the same literal at compile time. Since all String literals get interned, d , which is a result of StringLiteral + StringLiteral , also ends up referencing the same object as a and b 's StringLiteral . The expression for e , which is c + StringLiteral , is evaluated at runtime . Even though it produces the same string, this fact is not used by the compiler. That's why a different String object is produced, resulting in failed comparison on == .

Question-34509566, answer-34529067
Keep in mind that Java holds a pool of all string literals found in program, used for matching purposes among others, so any different string literal concatenation above will lead to the same object, to the same string literal. You can check out this useful article for more. On the other hand, the concatenation of a String object and a literal (case c + StringLiteral ) will lead to the creation of a as StringBuilder object at runtime, different to the literals found in pool.

Question-32334319, answer-32334651
The API guarantees a stable sorting which Quicksort doesn’t offer. However, when sorting primitive values by their natural order you won’t notice a difference as primitive values have no identity. Therefore, Quicksort is used for primitive arrays as it is slightly more efficient. For objects you may notice, when objects which are deemed equal according to their equals implementation or the provided Comparator change their order. Therefore, Quicksort is not an option. So a variant of MergeSort is used, the current Java versions use TimSort . This applies to both, Arrays.sort and Collections.sort , though with Java 8, the List itself may override the sort algorithms.

Question-32334319, answer-39380442
As stated across many of the answers. The Quicksort is used by Arrays.sort for sorting primitive collections because stability isn't required (you won't know or care if two identical ints were swapped in the sort) MergeSort or more specifically Timsort is used by Arrays.sort for sorting collections of objects. Stability is required. Quicksort does not provide for stability, Timsort does. Collections.sort delegates to Arrays.sort which is why you see the javadoc referencing the MergeSort.

Question-31270759, answer-31270760
Presented below is the full code of the Exceptional class. It has a quite large API which is a pure extension of the Optional API so it can be a drop-in replacement for it in any existing code—except that it isn't a subtype of the final Optional class. The class can be seen as being in the same relationship with the Try monad as Optional is with the Maybe monad: it draws inspiration from it, but is adapted to the Java idiom (such as actually throwing exceptions, even from non-terminal operations). These are some key guidelines followed by the class: #li as opposed to the monadic approach, doesn't ignore Java's exception mechanism; #li instead it relieves the impedance mismatch between exceptions and higher-order functions; #li exception handling not statically typesafe (due to sneaky throwing), but always safe at runtime (never swallows an exception except on explicit request). The class tries to cover all the typical ways to handle an exception: #li recover with some handling code which provides a substitute value; #li flatRecover which, analogous to flatMap , allows to return a new Exceptional instance which will be unwrapped and the state of the current instance suitably updated; #li propagate an exception, throwing it from the Exceptional expression and making the propagate call declare this exception type; #li propagate it after wrapping into another exception ( translate it); #li handle it, resulting in an empty Exceptional ; #li as a special case of handling, swallow it with an empty handler block. The propagate approach allows one to selectively pick which checked exceptions he wants to expose from his code. Exceptions which remain unhandled at the time a terminal operation is called (like get ) will be sneakily thrown without declaration. This is often considered as an advanced and dangerous approach, but is nevertheless often employed as a way to somewhat alleviate the nuisance of checked exceptions in combination with lambda shapes which do not declare them. The Exceptional class hopes to offer a cleaner and more selective alternative to sneaky throw. #pre #pre #pre

Question-31270759, answer-31273726
What if every functional interface provided by java.util.function was allowed to throw an exception? #pre We could use some default methods to provide the behavior you want. #li You could fallback to some default value or action #li Or you could try to perform another action which may throw an exception I've written a library which redefines most of the interfaces in java.util.function this way. I even provide a ThrowingStream which let's you use these new interfaces with the same API as a regular Stream . #pre ( Nothing is a RuntimeException that can never be thrown.) Your original example would become #pre

Question-31419029, answer-31419047
An empty HashMap object is much smaller than an array of 2000 Object references. Even though you pass 2000 to the initialCapacity parameter of the HashMap constructor, it's not actually creating 2000 spaces for objects yet.

Question-31419029, answer-31419077
If you look at the implementation of HashMap , the constructor looks like: #pre And init(args) looks like: #pre So initialCapacity doesn't actually get used to create an array. Where does it get used? Look at the put(args) method. #pre When doing a put, the array is actually created. I didn't show inflateTable(args) but it does some math and initializes the array.

Question-32294367, answer-32294898
#blockquote Let me ask a counterquestion: why do you think Thread.stop(args) has been deprecated for more than a decade? The reason why is precisely the negation of your statement above. To give two specific examples: you stop(args) a thread while it's executing something as innocuous-sounding as System.out.println(args) or Math.random(args) . Result: those two features are now broken for the entire JVM. The same pertains to any other synchronized code your application may execute. #blockquote The application may theoretically be coded such that absolutely no shared resource protected by locks is ever used; however that will only help to point out the exact extent to which Java threads are codependent. And the "independence" achieved will only pertain to the request-processing threads, not to all threads in such an application.

Question-32294367, answer-32378895
#blockquote Yes and No. I explain: #li Referring to shared memory: Different threads in a Java process share the whole heap, therefore threads can interact in a huge number of planned and unplanned ways. However objects in the stack (e.g. a context you pass down to called method) or a ThreadLocal are their own thread's (unless they start sharing references). #li Crashing: If a thread crashes in Java (a Throwable is propagated into Thread.run(args) , or something gets looped or blocked), that mishap might not affect other threads (e.g. a pool of connections in a server will continue to operate). However as different threads interact. Other threads will easily get stranded if one of them ends abnormally (e.g. one thread trying to read from an empty pipe from another thread which did not close its end). So unless the developers are highly paranoid careful, it is very likely that side effects will occur. I doubt that any other paradigm intends threads to operate as totally independent islands. They must share information and coordinate somehow. And then there will be the chance to mess things up. It is just they will take a more defensive approach that "gives you less rope to hang yourself" (same idiom as with pointers).

Question-32164385, answer-32164423
#blockquote The only way is to read the code unfortunately. #blockquote The common convention is to pass an object which cannot be modified, using a wrapper if needed. This ensure the class cannot modify the object. List<T> readOnly = Collections.unmodifiableList(args); If the object is Cloneable, you can also use clone(args) but another common approach is to use a copy. List<T> readOnly = new ArrayList<T>(list); If you care about such behaviour, unit tests can show whether a method modifies an object or not. If you have unit tests already, it is usually one or two lines extra to check for this.

Question-32164385, answer-32167166
#blockquote I must agree with other answers that there is no direct way to determine that method will modify your object or not and yes to make sure that method can not modify your Object you all have to do it is from your side. #blockquote Here the method name comes to the scene. Moving ahead with the naming convention of method we have to take a look at some method declarations which clearly convince you that your Object will not be changed at all. For example, You know that Arrays.copyOf will not change your actual array, System.out.println(args) will not change your boo Method names are real weapons to provide as much information as possible to the method user. (Yes! it's always not possible but quite a good practice to follow.) Let's consider it in your case that say printBoo will only print, copyBoo will only copy, clearBoo will reset all attributes, checkAndCreateNewBoo will check your boo Object and create new if required. So, ultimately if we can use them in a proper way caller can be assured with the fact that Object will remain the same after calling the method.

Question-31370403, answer-31466466
Idea #1, storing the Graphics objects simply wouldn't work. The Graphics should not be considered as "holding" some display memory, but rather as a handle to access an area of display memory. In the case of BufferedImage , each Graphics object will be always the handle to the same given image memory buffer, so they all will represent the same image. Even more importantly, you can't actually do anything with the stored Graphics : As they do not store anything, there is no way whatsoever they could "re-store" anything. Idea #2, cloning the BufferedImage s is a much better idea, but you'll indeed be wasting memory, and quickly run out of it. It helps only to store those parts of the image affected by the draw, for example using rectangular areas, but it still costs a lot of memory. Buffering those undo images to disk could help, but it will make your UI slow and unresponsive, and that's bad ; furthermore, it makes you application more complex and error-prone . My alternative would be to store store the image modifications in a list, rendered from first to last on top of the image. An undo operation then simply consists of removing the modification from the list. This requires you to "reify" the image modifications , i.e. create a class that implements a single modification, by providing a void draw(args) method which performs the actual drawing. As you said, random modifications pose an additional problem. However, the key problem is your use of Math.random(args) to create random numbers. Instead, perform each random modification with a Random created from a fixed seed value, so that the (pseudo-)random number sequences are the same on each invocation of draw(args) , i.e., each draw has exactly the same effects. (That's why they are called "pseudo-random" -- the generated numbers look random, but they are just as deterministic as any other function.) In contrast to the image storing technique, which has memory problems, the problem with this technique is that many modifications may make the GUI slow, especially if the modifications are computationally intensive. To prevent this, the simplest way would be to fix an appropriate maximum size of the list of undoable modifications . If this limit would be exceeded by adding a new modification, remove the oldest modification the list and apply it to the backing BufferedImage itself. The following simple demo application shows that (and how) this all works together. It also includes a nice "redo" feature for redoing undone actions. #pre

Question-31445024, answer-31445150
I first tought your question was a dupe of What code does the compiler generate for autoboxing? However, after your comment on @ElliottFrisch I realized it was different : #blockquote For other readers, assume that "behaves that way" means using valueOf . Remember that there are multiples compilers for Java. To be "legal" they must follow the contract given in the JLS . Therefore, as long as all the rules here are respected, there is no guarantee of how autoboxing is internally implemented. But I don't see any reason to not use valueOf , specially that it uses the cached values and is the recommended way as per this article by Joseph D. Darcy.

Question-31445024, answer-31760309
Until the language specification mentions it, it is not guaranteed that autoboxing is equivalent to a call to the static valueOf methods. It is an implementation aspect, not part of the boxing conversion specification. An implementation is theoretically free to use another mechanism as long as it conforms to the rule you mentioned from the JLS. In practice, there are many Sun JDK bug reports (e.g. JDK-4990346 and JDK-6628737 ) that clearly imply that when autoboxing was introduced in Java 5, the intention was having the compiler to rely on valueOf as stated in JDK-6628737 : #blockquote But that's only for javac, not necessarily all compilers.

Question-31445024, answer-31871515
Autoboxing is absolutely implemented using valueOf(args) ...in the OpenJDK. If that's your implementation, read on... if not, skip to below. #pre Java documentation states that Boolean.valueOf(args) always returns Boolean.TRUE or Boolean.FALSE , therefore your reference comparisons in these cases will succeed. ((Integer)1) == Integer.valueOf(args) For this particular example, under the OpenJDK implementation with default settings, it will probably work by virtue of the fact that you picked a value < 128 which is cached at startup (although this can be overridden as a commandline arg). It may also work for larger values if it's frequently used enough to be cached. Unless you're working under "safe" assumptions about the Integer cache, don't expect the reference comparison to be an equality. Long , Short , Character and Byte incidentally implement this caching too, but unlike Integer , it's not tunable. Byte will always work if you're comparing autobox/ valueOf(args) references since obviously, you can't go out of range. Float and Double will unsurprisingly always create a new instance. Now, in purely generic terms? See this section of the JLS - you MUST be given equal references for boolean and any int or char within the -128 to 127 range. There are no guarantees for anything else.

Question-31696485, answer-31696584
Your question is based on assumption that the code which may throw NullPointerException is worse than the code which may not. This assumption is wrong. If you expect that your foobar is never null due to the program logic, it's much better to use Optional.of(args) as you will see a NullPointerException which will indicate that your program has a bug. If you use Optional.ofNullable(args) and the foobar happens to be null due to the bug, then your program will silently continue working incorrectly, which may be a bigger disaster. This way an error may occur much later and it would be much harder to understand at which point it went wrong.
