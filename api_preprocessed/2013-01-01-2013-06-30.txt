Question-15182496
The following print statement would print "hello world". Could anyone explain this? System.out.println(_exprs); And randomString(_exprs) looks like this: #pre

Question-15621083
I am working on some Java code which needs to be highly optimized as it will run in hot functions that are invoked at many points in my main program logic. Part of this code involves multiplying double variables by 10 raised to arbitrary non-negative int exponent s. One fast way (edit: but not the fastest possible, see Update 2 below) to get the multiplied value is to switch on the exponent : #pre The commented ellipses above indicate that the case int constants continue incrementing by 1, so there are really 19 case s in the above code snippet. Since I wasn't sure whether I would actually need all the powers of 10 in case statements 10 thru 18 , I ran some microbenchmarks comparing the time to complete 10 million operations with this switch statement versus a switch with only case s 0 thru 9 (with the exponent limited to 9 or less to avoid breaking the pared-down switch ). I got the rather surprising (to me, at least!) result that the longer switch with more case statements actually ran faster. On a lark, I tried adding even more case s which just returned dummy values, and found that I could get the switch to run even faster with around 22-27 declared case s (even though those dummy cases are never actually hit while the code is running). (Again, case s were added in a contiguous fashion by incrementing the prior case constant by 1 .) These execution time differences are not very significant: for a random exponent between 0 and 10 , the dummy padded switch statement finishes 10 million executions in 1.49 secs versus 1.54 secs for the unpadded version, for a grand total savings of 5ns per execution. So, not the kind of thing that makes obsessing over padding out a switch statement worth the effort from an optimization standpoint. But I still just find it curious and counter-intuitive that a switch doesn't become slower (or perhaps at best maintain constant O(1) time) to execute as more case s are added to it. #img These are the results I obtained from running with various limits on the randomly-generated exponent values. I didn't include the results all the way down to 1 for the exponent limit, but the general shape of the curve remains the same, with a ridge around the 12-17 case mark, and a valley between 18-28. All tests were run in JUnitBenchmarks using shared containers for the random values to ensure identical testing inputs. I also ran the tests both in order from longest switch statement to shortest, and vice-versa, to try and eliminate the possibility of ordering-related test problems. I've put my testing code up on a github repo if anyone wants to try to reproduce these results. So, what's going on here? Some vagaries of my architecture or micro-benchmark construction? Or is the Java switch really a little faster to execute in the 18 to 28 case range than it is from 11 up to 17 ? github test repo "switch-experiment" UPDATE: I cleaned up the benchmarking library quite a bit and added a text file in /results with some output across a wider range of possible exponent values. I also added an option in the testing code not to throw an Exception from default , but this doesn't appear to affect the results. UPDATE 2: Found some pretty good discussion of this issue from back in 2009 on the xkcd forum here: #a . The OP's discussion of using Array.binarySearch(_exprs) gave me the idea for a simple array-based implementation of the exponentiation pattern above. There's no need for the binary search since I know what the entries in the array are. It appears to run about 3 times faster than using switch , obviously at the expense of some of the control flow that switch affords. That code has been added to the github repo also.

Question-16635398
Which of the following is better practice in Java 8? Java 8: joins.forEach(_exprs); Java 7: #pre I have lots of for loops that could be "simplified" with lambdas, but is there really any advantage of using them including performance and readability? EDIT I'll also extend this question to longer methods - I know that you cant return or break the parent function from a lambda and this should be mentioned to if they are compared, but is there anything else to be considered?

Question-14636178
I am trying to use the org.springframework.orm.jdo.TransactionAwarePersistenceManagerFactoryProxy in my Spring project, but I am not sure how to use it or whether it's exactly what I am looking for. I realize it can help make my DAOs work with a plain JDO PersistenceManagerFactory . Another question is: what happens if the proxy doesn't get made properly? Can I still use it to access my factory to create a transaction aware persistence manager? If the object managed by the factory is a singleton, does this change things? Why not just access the PersistenceManagerFactory directly? Perhaps PersistenceManagerFactoryUtils.getPersistenceManager would be more suited to my needs? Can getObject return null?

Question-14491966
I made a class called QuickRandom , and its job is to produce random numbers quickly. It's really simple: just take the old value, multiply by a double , and take the decimal part. Here is my QuickRandom class in its entirety: #pre And here is the code I wrote to test it: #pre It is a very simple algorithm that simply multiplies the previous double by a "magic number" double. I threw it together pretty quickly, so I could probably make it better, but strangely, it seems to be working fine. This is sample output of the commented-out lines in the main method: #pre Hm. Pretty random. In fact, that would work for a random number generator in a game. Here is sample output of the non-commented out part: #pre Wow! It performs almost 4 times faster than Math.random . I remember reading somewhere that Math.random used System.nanoTime(_exprs) and tons of crazy modulus and division stuff. Is that really necessary? My algorithm performs a lot faster and it seems pretty random. I have two questions: #li Is my algorithm "good enough" (for, say, a game, where really random numbers aren't too important)? #li Why does Math.random do so much when it seems just simple multiplication and cutting out the decimal will suffice?

Question-15156857
I have a method with a void return type. It can also throw a number of exceptions so I'd like to test those exceptions being thrown. All attempts have failed with the same reason: #blockquote Any ideas how I can get the method to throw a specified exception? doThrow(_exprs).when(_exprs);

Question-16232833
I'm using Spring MVC for a simple JSON API, with @ResponseBody based approach like the following. (I already have a service layer producing JSON directly.) #pre Question is, in the given scenario, what is the simplest, cleanest way to respond with a HTTP 400 error ? I did come across approaches like: return new ResponseEntity(_exprs); ...but I can't use it here since my method's return type is String, not ResponseEntity.

Question-15430247
The usual constructor of ArrayList is: ArrayList<?> list = new ArrayList<>(_exprs); But there is also an overloaded constructor with a parameter for its initial capacity: ArrayList<?> list = new ArrayList<>(_exprs); Why is it useful to create an ArrayList with an initial capacity when we can append to it as we please?

Question-14846920
What are some of the situations where I can use Collections.emptyMap(_exprs) ? The Documentation says I can use this method if I want my collection to be immutable. Why would I want an immutable empty collection? What is the point?

Question-14534767
I have a StringBuilder object, #pre Now I want to append a newline character to the StringBuilder . How can I do it? result.append(_exprs); Does not work. So, I was thinking about writing a newline using Unicode. Will this help? If so, how can I add one?

Question-15336477
All I'm trying to do is download some JSON and deserialize it into an object. I haven't got as far as downloading the JSON yet. Almost every single HttpClient example I can find, including those on the apache site looks something like... #pre However, Netbeans tells me that DefaultHttpClient is deprecated. I've tried googling for DefaultHttpClient deprecated and as many other variations as I can think of and can't find any useful results, so I'm obviously missing something. What is the correct Java7 way to download the contents of a webpage? Is there really no decent Http Client as part of the language? I find that hard to believe. My Maven dependency for this is... #pre

Question-16324831
Why do Double.parseDouble(null) and Integer.parseInt(null) throw different exceptions? Is this a historical accident or intentional? The documentation clearly states two types of exceptions for Double.parseDouble(...) and one for Integer.parseInt() , but it seems inconsistent: Integer.parseInt(_exprs); // throws java.lang.NumberFormatException: null However Double.parseDouble(_exprs); // throws java.lang.NullPointerException

Question-15940234
I'm relative new to the webservices world and my research seems to have confused me more than enlighten me, my problem is that I was given a library(jar) which I have to extend with some webservice functionality. This library will be shared to other developers, and among the classes in the jar will be classes that have a method which calls a webservice (that essentially sets an attribute of the class, does some business logic, like storing the object in a db, etc and sends back the object with those modifications). I want to make the call to this service as simple as possible, hopefully as simple so that the developer using the class only need to do. #pre I have been studying JAX-WS to use on the server but seems to me that I don't need to create a wsimport in the server nor the wsimport on the client, since I know that both have the classes, I just need some interaction between classes shared in both the server and the client. How do you think makes sense to do the webservice and the call in the class?

Question-16148575
I am trying to build a HashMap which will have integer as keys and objects as values. My syntax is: HashMap<int, myObject> myMap = new HashMap<int, myObject>(_exprs); However, the error returned is - Syntax error on token "int", Dimensions expected after this token - I don't understand why I should add a dimension (ie: making the int into an array) since I only need to store a digit as key. What could I do? Thanks in advance! :)

Question-15182496, answer-15182578
The other answers explain why, but here is how: new Random(_exprs).nextInt(_exprs) The first 6 numbers that the above random generates are: #pre and the first 6 numbers that new Random(_exprs).nextInt(_exprs) generates are: #pre Then just add those numbers to the integer representation of the character ` (which is 96): #pre

Question-15202997, answer-15203417
If you're unsure about something, try writing a test first. I did this: #pre Prints: #pre There's an empty line in the last block where getSimpleName returns an empty string. The upshot looking at this is: #li the name is the name that you'd use to dynamically load the class with, for example, a call to Class.forName with the default ClassLoader . #li the canonical name is the name that would be used in an import statement and uniquely identifies the class. Might be useful during toString or logging operations. #li the simple name loosely identifies the class, again might be useful during toString or logging operations but is not guaranteed to be unique.

Question-15202997, answer-29527145
Adding local classes, lambdas and the toString(_exprs) method to complete the previous two answers. Further, I add arrays of lambdas and arrays of anonymous classes (which do not make any sense in practice though): #pre This is the full output: #pre So, here are the rules. First, lets start with primitive types and void : #li If the class object represents a primitive type or void , all the four methods simply returns its name. Now the rules for the getName(_exprs) method: #li Every non-lambda and non-array class or interface (i.e, top-level, nested, inner, local and anonymous) has a name (which is returned by getName(_exprs) ) that is the package name followed by a dot (if there is a package), followed by the name of its class-file as generated by the compiler (whithout the suffix .class ). If there is no package, it is simply the name of the class-file. If the class is an inner, nested, local or anonymous class, the compiler should generate at least one $ in its class-file name. Note that for anonymous classes, the class name would end with a dollar-sign followed by a number. #li Lambda class names are generally unpredictable, and you shouldn't care about they anyway. Exactly, their name is the name of the enclosing class, followed by $$Lambda$ , followed by a number, followed by a slash, followed by another number. #li The class descriptor of the primitives are Z for boolean , B for byte , S for short , C for char , I for int , J for long , F for float and D for double . For non-array classes and interfaces the class descriptor is L followed by what is given by getName(_exprs) followed by ; . For array classes, the class descriptor is [ followed by the class descriptor of the component type (which may be itself another array class). #li For array classes, the getName(_exprs) method returns its class descriptor. This rule seems to fail only for array classes whose the component type is a lambda (which possibly is a bug), but hopefully this should not matter anyway because there is no point even on the existence of array classes whose component type is a lambda. Now, the toString(_exprs) method: #li If the class instance represents an interface (or an annotation, which is a special type of interface), the toString(_exprs) returns _str + getName(_exprs) . If it is a primitive, it returns simply getName(_exprs) . If it is something else (a class type, even if it is a pretty weird one), it returns _str + getName(_exprs) . The getCanonicalName(_exprs) method: #li For top-level classes and interfaces, the getCanonicalName(_exprs) method returns just what the getName(_exprs) method returns. #li The getCanonicalName(_exprs) method returns null for anonymous or local classes and for array classes of those. #li For inner and nested classes and interfaces, the getCanonicalName(_exprs) method returns what the getName(_exprs) method would replacing the compiler-introduced dollar-signs by dots. #li For array classes, the getCanonicalName(_exprs) method returns null if the canonical name of the component type is null . Otherwise, it returns the canonical name of the component type followed by [] . The getSimpleName(_exprs) method: #li For top-level, nested, inner and local classes, the getSimpleName(_exprs) returns the name of the class as written in the source file. #li For anonymous classes the getSimpleName(_exprs) returns an empty String . #li For lambda classes the getSimpleName(_exprs) just returns what the getName(_exprs) would return without the package name. This do not makes much sense and looks like a bug for me, but there is no point in calling getSimpleName(_exprs) on a lambda class to start with. #li For array classes the getSimpleName(_exprs) method returns the simple name of the component class followed by [] . This have the funny/weird side-effect that array classes whose component type is an anonymous class have just [] as their simple names.

Question-15202997, answer-36978943
I've been confused by the wide range of different naming schemes as well, and was just about to ask and answer my own question on this when I found this question here. I think my findings fit it well enough, and complement what's already here. My focus is looking for documentation on the various terms, and adding some more related terms that might crop up in other places. Consider the following example: #pre #li The simple name of D is D . That's just the part you wrote when declaring the class. Anonymous classes have no simple name. Class.getSimpleName() returns this name or the empty string. It is possible for the simple name to contain a $ if you write it like this, since $ is a valid part of an identifier. #li According to the JLS section 6.7 , both a.b.C.D and a.b.C.D.D.D would be fully qualified names , but only a.b.C.D would be the canonical name of D . So every canonical name is a fully qualified name, but the converes is not always true. Class.getCanonicalName() will return the canonical name or null . #li Class.getName() is documented to return the binary name , as specified in JLS section 13.1 . In this case it returns a.b.C$D for D and [La.b.C$D; for D[] . #li This answer demonstrates that it is possible for two classes loaded by the same class loader to have the same canonical name but distinct binary names . Neither name is sufficient to reliably deduce the other: if you have the canonical name, you don't know which parts of the name are packages and which are containing classes. If you have the binary name, you don't know which $ were introduced as separators and which were part of some simple name. #li Anonymous classes and local classes have no fully qualified names but still have a binary name . The same holds for classes nested inside such classes. Every class has a binary name. #li Running javap -v -private on a/b/C.class shows that the bytecode refers to the type of d as La/b/C$D; and that of the array ds as [La/b/C$D; . These are called descriptors , and they are specified in JVMS section 4.3 . #li The class name a/b/C$D used in both of these descriptors is what you get by replacing . by / in the binary name. The JVM spec apparently calls this the internal form of the binary name . JVMS section 4.2.1 describes it, and states that the difference from the binary name were for historical reasons. #li The file name of a class in one of the typical filename-based class loaders is what you get if you interpret the / in the internal form of the binary name as a directory separator, and append the file name extension .class to it. It's resolved relative to the class path used by the class loader in question.

Question-15202997, answer-37603567
this is best document I found describing getName(), getSimpleName(), getCanonicalName() #a #pre

Question-16635398, answer-16635489
The advantage comes into account when the operations can be executed in parallel. (See #a - the section about internal and external iteration) #li The main advantage from my point of view is that the implementation of what is to be done within the loop can be defined without having to decide if it will be executed in parallel or sequential #li If you want your loop to be executed in parallel you could simply write joins.parallelStream(_exprs).forEach(_exprs); You will have to write some extra code for thread handling etc. Note : for my answer I assumed joins implementing the java.util.Stream interface. If joins implements only the java.util.Iterable interface this is no longer true.

Question-16635398, answer-16636899
I feel that I need to extend my comment a bit... About paradigm\style That's probably the most notable aspect. FP became popular due to what you can get avoiding side-effects. I won't delve deep into what pros\cons you can get from this, since this is not related to the question. However, I will say that the iteration using Iterable.forEach is inspired by FP and rather result of bringing more FP to Java (ironically, I'd say that there is no much use for forEach in pure FP, since it does nothing except introducing side-effects). In the end I would say that it is rather a matter of taste\style\paradigm you are currently writing in. About parallelism. From performance point of view there is no promised notable benefits from using Iterable.forEach over foreach(...). According to official docs on Iterable.forEach : #blockquote ... i.e. docs pretty much clear that there will be no implicit parallelism. Adding one would be LSP violation. Now, there are "parallell collections" that are promised in Java 8, but to work with those you need to me more explicit and put some extra care to use them (see mschenk74's answer for example). BTW: in this case Stream.forEach will be used, and it doesn't guarantee that actual work will be done in parallell (depends on underlying collection). UPDATE: might be not that obvious and a little stretched at a glance but there is another facet of style and readability perspective. First of all - plain old forloops are plain and old. Everybody already knows them. Second, and more important - you probably want to use Iterable.forEach only with one-liner lambdas. If "body" gets heavier - they tend to be not-that readable. You have 2 options from here - use inner classes (yuck) or use plain old forloop. People often gets annoyed when they see the same things (iteratins over collections) being done various vays/styles in the same codebase, and this seems to be the case. Again, this might or might not be an issue. Depends on people working on code.

Question-16635398, answer-16637740
forEach(_exprs) can be implemented to be faster than for-each loop, because the iterable knows the best way to iterate its elements, as opposed to the standard iterator way. So the difference is loop internally or loop externally. For example ArrayList.forEach(_exprs) may be simply implemented as #pre as opposed to the for-each loop which requires a lot of scaffolding #pre However, we also need to account for two overhead costs by using forEach(_exprs) , one is making the lambda object, the other is invoking the lambda method. They are probably not significant. see also #a for comparing internal/external iterations for different use cases.

Question-16635398, answer-20177092
The better practice is to use for-each . Besides violating the Keep It Simple, Stupid principle, the new-fangled forEach(_exprs) has at least the following deficiencies: #li Can't use non-final variables . So, code like the following can't be turned into a forEach lambda: #blockquote #li Can't handle checked exceptions . Lambdas aren't actually forbidden from throwing checked exceptions, but common functional interfaces like Consumer don't declare any. Therefore, any code that throws checked exceptions must wrap them in try-catch or Throwables.propagate(_exprs) . But even if you do that, it's not always clear what happens to the thrown exception. It could get swallowed somewhere in the guts of forEach(_exprs) #li Limited flow-control . A return in a lambda equals a continue in a for-each, but there is no equivalent to a break . It's also difficult to do things like return values, short circuit, or set flags (which would have alleviated things a bit, if it wasn't a violation of the no non-final variables rule). "This is not just an optimization, but critical when you consider that some sequences (like reading the lines in a file) may have side-effects, or you may have an infinite sequence." #li Might execute in parallel , which is a horrible, horrible thing for all but the 0.1% of your code that needs to be optimized. Any parallel code has to be thought through (even if it doesn't use locks, volatiles, and other particularly nasty aspects of traditional multi-threaded execution). Any bug will be tough to find. #li Might hurt performance , because the JIT can't optimize forEach()+lambda to the same extent as plain loops, especially now that lambdas are new. By "optimization" I do not mean the overhead of calling lambdas (which is small), but to the sophisticated analysis and transformation that the modern JIT compiler performs on running code. #li If you do need parallelism, it is probably much faster and not much more difficult to use an ExecutorService . Streams are both automagical (read: don't know much about your problem) and use a specialized (read: inefficient for the general case) parallelization strategy ( fork-join recursive decomposition ). #li Makes debugging more confusing , because of the nested call hierarchy and, god forbid, parallel execution. The debugger may have issues displaying variables from the surrounding code, and things like step-through may not work as expected. #li Streams in general are more difficult to code, read, and debug . Actually, this is true of complex " fluent " APIs in general. The combination of complex single statements, heavy use of generics, and lack of intermediate variables conspire to produce confusing error messages and frustrate debugging. Instead of "this method doesn't have an overload for type X" you get an error message closer to "somewhere you messed up the types, but we don't know where or how." Similarly, you can't step through and examine things in a debugger as easily as when the code is broken into multiple statements, and intermediate values are saved to variables. Finally, reading the code and understanding the types and behavior at each stage of execution may be non-trivial. #li Sticks out like a sore thumb . The Java language already has the for-each statement. Why replace it with a function call? Why encourage hiding side-effects somewhere in expressions? Why encourage unwieldy one-liners? Mixing regular for-each and new forEach willy-nilly is bad style. Code should speak in idioms (patterns that are quick to comprehend due to their repetition), and the fewer idioms are used the clearer the code is and less time is spent deciding which idiom to use (a big time-drain for perfectionists like myself!). As you can see, I'm not a big fan of the forEach() except in cases when it makes sense. Particularly offensive to me is the fact that Stream does not implement Iterable (despite actually having method iterator ) and cannot be used in a for-each, only with a forEach(). I recommend casting Streams into Iterables with (_exprs)stream::iterator . A better alternative is to use StreamEx which fixes a number of Stream API problems, including implementing Iterable . That said, forEach(_exprs) is useful for the following: #li Atomically iterating over a synchronized list . Prior to this, a list generated with Collections.synchronizedList(_exprs) was atomic with respect to things like get or set, but was not thread-safe when iterating. #li Parallel execution (using an appropriate parallel stream) . This saves you a few lines of code vs using an ExecutorService, if your problem matches the performance assumptions built into Streams and Spliterators. #li Specific containers which , like the synchronized list, benefit from being in control of iteration (although this is largely theoretical unless people can bring up more examples) #li Calling a single function more cleanly by using forEach(_exprs) and a method reference argument (ie, list.forEach (_exprs) ). However, keep in mind the points on checked exceptions, more difficult debugging, and reducing the number of idioms you use when writing code. Articles I used for reference: #li Everything about Java 8 #li Iteration Inside and Out (as pointed out by another poster) EDIT: Looks like some of the original proposals for lambdas (such as #a ) solved some of the issues I mentioned (while adding their own complications, of course).

Question-16635398, answer-22502206
When reading this question one can get the impression, that Iterable#forEach in combination with lambda expressions is a shortcut/replacement for writing a traditional for-each loop. This is simply not true. This code from the OP: joins.forEach(_exprs); is not intended as a shortcut for writing #pre and should certainly not be used in this way. Instead it is intended as a shortcut (although it is not exactly the same) for writing #pre And it is as a replacement for the following Java 7 code: #pre Replacing the body of a loop with a functional interface, as in the examples above, makes your code more explicit: You are saying that (1) the body of the loop does not affect the surrounding code and control flow, and (2) the body of the loop may be replaced with a different implementation of the function, without affecting the surrounding code. Not being able to access non final variables of the outer scope is not a deficit of functions/lambdas, it is a feature that distinguishes the semantics of Iterable#forEach from the semantics of a traditional for-each loop. Once one gets used to the syntax of Iterable#forEach , it makes the code more readable, because you immediately get this additional information about the code. Traditional for-each loops will certainly stay good practice (to avoid the overused term " best practice ") in Java. But this doesn't mean, that Iterable#forEach should be considered bad practice or bad style. It is always good practice, to use the right tool for doing the job, and this includes mixing traditional for-each loops with Iterable#forEach , where it makes sense. Since the downsides of Iterable#forEach have already been discussed in this thread, here are some reasons, why you might probably want to use Iterable#forEach : #li To make your code more explicit: As described above, Iterable#forEach can make your code more explicit and readable in some situations. #li To make your code more extensible and maintainable: Using a function as the body of a loop allows you to replace this function with different implementations (see Strategy Pattern ). You could e.g. easily replace the lambda expression with a method call, that may be overwritten by sub-classes: joins.forEach(_exprs); Then you could provide default strategies using an enum, that implements the functional interface. This not only makes your code more extensible, it also increases maintainability because it decouples the loop implementation from the loop declaration. #li To make your code more debuggable: Seperating the loop implementation from the declaration can also make debugging more easy, because you could have a specialized debug implementation, that prints out debug messages, without the need to clutter your main code with if(_exprs)System.out.println(_exprs) . The debug implementation could e.g. be a delegate , that decorates the actual function implementation. #li To optimize performance-critical code: Contrary to some of the assertions in this thread, Iterable#forEach does already provide better performance than a traditional for-each loop, at least when using ArrayList and running Hotspot in "-client" mode. While this performance boost is small and negligible for most use cases, there are situations, where this extra performance can make a difference. E.g. library maintainers will certainly want to evaluate, if some of their existing loop implementations should be replaced with Iterable#forEach . To back this statement up with facts, I have done some micro-benchmarks with Caliper . Here is the test code (latest Caliper from git is needed): #pre And here are the results: #li Results for -client #li Results for -server When running with "-client", Iterable#forEach outperforms the traditional for loop over an ArrayList, but is still slower than directly iterating over an array. When running with "-server", the performance of all approaches is about the same. #li To provide optional support for parallel execution: It has already been said here, that the possibility to execute the functional interface of Iterable#forEach in parallel using streams , is certainly an important aspect. Since Collection#parallelStream(_exprs) does not guarantee, that the loop is actually executed in parallel, one must consider this an optional feature. By iterating over your list with list.parallelStream(_exprs).forEach(_exprs); , you explicitly say: This loop supports parallel execution, but it does not depend on it. Again, this is a feature and not a deficit! By moving the decision for parallel execution away from your actual loop implementation, you allow optional optimization of your code, without affecting the code itself, which is a good thing. Also, if the default parallel stream implementation does not fit your needs, no one is preventing you from providing your own implementation. You could e.g. provide an optimized collection depending on the underlying operating system, on the size of the collection, on the number of cores, and on some preference settings: #pre The nice thing here is, that your loop implementation doesn't need to know or care about these details.

Question-16635398, answer-25855691
TL;DR : List.stream(_exprs).forEach(_exprs) was the fastest. I felt I should add my results from benchmarking iteration. I took a very simple approach (no benchmarking frameworks) and benchmarked 5 different methods: #li classic for #li classic foreach #li List.forEach(_exprs) #li List.stream(_exprs).forEach(_exprs) #li List.parallelStream(_exprs).forEach the testing procedure and parameters #pre The list in this class shall be iterated over and have some doIt(_exprs) applied to all it's members, each time via a different method. in the Main class I run the tested method three times to warm up the JVM. I then run the test method 1000 times summing the time it takes for each iteration method (using System.nanoTime(_exprs) ). After that's done i divide that sum by 1000 and that's the result, average time. example: #pre I ran this on a i5 4 core CPU, with java version 1.8.0_05 classic for #pre execution time: 4.21 ms classic foreach #pre execution time: 5.95 ms List.forEach(_exprs) list.forEach(_exprs); execution time: 3.11 ms List.stream(_exprs).forEach(_exprs) list.stream(_exprs).forEach(_exprs); execution time: 2.79 ms List.parallelStream(_exprs).forEach list.parallelStream(_exprs).forEach(_exprs); execution time: 3.6 ms

Question-16635398, answer-31707746
One of most upleasing functional forEach 's limitations is lack of checked exceptions support. One possible workaround is to replace terminal forEach with plain old foreach loop: #pre Here is list of most popular questions with other workarounds on checked exception handling within lambdas and streams: Java 8 Lambda function that throws exception? Java 8: Lambda-Streams, Filter by Method with Exception How can I throw CHECKED exceptions from inside Java 8 streams? Java 8: Mandatory checked exceptions handling in lambda expressions. Why mandatory, not optional?

Question-15655012, answer-15655071
If you make foo static, you must initialize it in the class constructor (or inline where you define it) like the following examples. Class constructor (not instance): #pre Inline: private static final List foo = new ArrayList(_exprs); The problem here is not how the final modifier works, but rather how the static modifier works. The final modifier enforces an initialization of your reference by the time the call to your constructor completes (i.e. you must initialize it in the constructor). When you initialize an attribute in-line, it gets initialized before the code you have defined for the constructor is run, so you get the following outcomes: #li if foo is static , foo = new ArrayList(_exprs) will be executed before the static{} constructor you have defined for your class is executed #li if foo is not static , foo = new ArrayList(_exprs) will be executed before your constructor is run When you do not initilize an attribute in-line, the final modifier enforces that you initialize it and that you must do so in the constructor. If you also have a static modifier, the constructor you will have to initialize the attribute in is the class' initialization block : static{} . The error you get in your code is from the fact that static{} is run when the class is loaded, before the time you instantiate an object of that class. Thus, you will have not initialized foo when the class is created. Think of the static{} block as a constructor for an object of type Class . This is where you must do the initialization of your static final class attributes (if not done inline). Side note: The final modifier assures const-ness only for primitive types and references. When you declare a final object, what you get is a final reference to that object, but the object itself is not constant. What you are really achieving when declaring a final attribute is that, once you declare an object for your specific purpose (like the final List that you have declared), that and only that object will be used for that purpose: you will not be able to change List foo to another List , but you can still alter your List by adding/removing items (the List you are using will be the same, only with its contents altered).

Question-15655012, answer-15656208
This is favorite interview question . Interviewer tries to find out here, how much you understand about behavior of objects with respect to constructors, methods, class variables(static variables), instance variables. #pre In above case we have defined Test constructor and setFoo method. About constructor: Constructor can be invoked only one time per object creation by using new keyword. Programmer cannot invoke constructor many times because constructor are designed so. About method: Method can be invoked as many time as programmer wants and compiler knows it programmer may invoke method zero or multiple time. Scenario 1 private final List foo; // 1 foo is an instance variable. When we create Test class' object then instance variable foo will be copied inside object of Test class. If we assign foo inside constructor then compiler know that Constructor will be invoked only one time. so there is no problem to assign it inside constructor. If we assign foo inside method then compiler knows that method may be invoked multiple time. So value will have to be changed multiple time which is not allowed for final variable. You can assign value to the final variable only one time. So compiler decides constructor is good choice! Scenario 2 private static final List foo = new ArrayList(_exprs); foo is now static variable. When we create object of Test class then foo will not be copied to the object of Test class because foo is static. Now foo is not the property of each object. This is the property of Test class. But foo can be seen by many objects and if every object which is created by using new keyword which ultimately invoke Test constructor then value will be changed at the time of multiple object creation (Remember static foo is not copied in every object now but is shared between multiple objects.) Scenario 3 t.foo.add(_exprs); // Modification-2 Above Modification-2 is from your question. In above case you are not changing first referenced object but you are adding contents inside foo which is allowed. Compiler complains if you try to assign new ArrayList(_exprs) object to the foo reference variable. Rule is that if you have initialized object to final reference variable then you cannot change it to refer different ArrayList object. (in this case ArrayList) final class cannot be subclassed final methods cannot be overridden. (This method is in superclass) final methods can override. (Read this in grammatical way. This method is in subclass)

Question-15655012, answer-24895623
First of all, the place in your code where you are initializing (i.e. assigning for the first time) foo is here: foo = new ArrayList(_exprs); foo is an object (with type List) so it is a reference type, not a value type (like int). As such, it holds a reference to a memory location (e.g. 0xA7D2A834) where your List elements are stored. Lines like this foo.add(_exprs); // Modification-1 do not change the value of foo (which, again, is just a reference to a memory location). Instead, they just add elements into that referenced memory location. To violate the final keyword, you would have to try to re-assign foo as follows again: foo = new ArrayList(_exprs); That would give you a compilation error. Now, with that out of the way, think about what happens when you add the static keyword. When you do NOT have the static keyword, each object that instantiates the class has its own copy of foo. Therefore, the constructor assigns a value to a blank, fresh copy of the foo variable, which is perfectly fine. However, when you DO have the static keyword, only one foo exists in memory that is associated with the class. If you were to create two or more objects, the constructor would be attempting to re-assign that one foo each time, violating the final keyword.

Question-16159203, answer-16323196
#blockquote currentPos = new Point(_exprs); does a few things, including writing default values to x and y (0) and then writing their initial values in the constructor. Since your object is not safely published those 4 write operations can be freely reordered by the compiler / JVM. So from the perspective of the reading thread, it is a legal execution to read x with its new value but y with its default value of 0 for example. By the time you reach the println statement (which by the way is synchronized and therefore does influence the read operations), the variables have their initial values and the program prints the expected values. Marking currentPos as volatile will ensure safe publication since your object is effectively immutable - if in your real use case the object is mutated after construction, volatile guarantees won't be enough and you could see an inconsistent object again. Alternatively, you can make the Point immutable which will also ensure safe publication, even without using volatile . To achieve immutability, you simply need to mark x and y final. As a side note and as already mentioned, synchronized(_exprs) {} can be treated as a no-op by the JVM (I understand you included it to reproduce the behaviour).

Question-16159203, answer-18258659
You have ordinary memory, the 'currentpos' reference and the Point object and its fields behind it, shared between 2 threads, without synchronisation. Thus, there is no defined ordering between the writes that happen to this memory in the main thread and the reads in the created thread (call it T). Main thread is doing the following writes (ignoring the initial setup of point, will result in p.x and p.y having default values): #li to p.x #li to p.y #li to currentpos Because there is nothing special about these writes in terms of synchronisation/barriers, the runtime is free to allow the T thread see them occur in any order (the main thread of course always sees writes and reads ordered according to programme order), and occur at any point between the reads in T. So T is doing: #li reads currentpos to p #li read p.x and p.y (in either order) #li compare, and take the branch #li read p.x and p.y (either order) and call System.out.println Given there's no ordering relationships between the writes in main, and the reads in T, there are clearly several ways this can produce your result, as T may see main's write to currentpos before the writes to currentpos.y or currentpos.x: #li It reads currentpos.x first, before the x write has occurred - gets 0, then reads currentpos.y before the y write has occurred - gets 0. Compare evals to true. The writes become visible to T. System.out.println is called. #li It reads currentpos.x first, after the x write has occurred, then reads currentpos.y before the y write has occurred - gets 0. Compare evals to true. Writes become visible to T... etc. #li It reads currentpos.y first, before the y write has occurred (0), then reads currentpos.x after the x write, evals to true. etc. and so on... There are a number of data races here. I suspect the flawed assumption here is thinking that the writes that result from this line are made visible across all the threads in the programme order of the thread executing it: currentPos = new Point(_exprs); Java makes no such guarantee (it'd be terrible for performance). Something more must be added if your programme needs a guaranteed ordering of the writes relative to reads in other threads. Others have suggested making the x,y fields final, or alternatively making currentpos volatile. #li If you make the x,y fields final, then Java guarantees that the writes of their values will be seen to occur before the constructor returns, in all threads. Thus, as the assignment to currentpos is after the constructor, the T thread is guaranteed to see the writes in the correct order. #li If you make currentpos volatile, then Java guarantees that that this is a synchronisation point which will be total-ordered wrt other synchronisation points. As in main the writes to x and y must happen before the write to currentpos, then any read of currentpos in another thread must see also the writes of x, y that happened before. Using final has the advantage that it makes the fields immutable, and thus allows the values to be cached. Using volatile leads to synchronisation on every write and read of currentpos, which might hurt performance. See chapter 17 of the Java Language Spec for the gory details: #a (Initial answer assumed a weaker memory model, as I was not sure the JLS guaranteed volatile was sufficient. Answer edited to reflect comment from assylias, pointing out the Java model is stronger - happens-before is transitive - and so volatile on currentpos also suffices).

Question-14491966, answer-14492043
'Random' is more than just about getting numbers.... what you have is pseudo-random If pseudo-random is good enough for your purposes, then sure, it's way faster (and XOR+Bitshift will be faster than what you have) Rolf Edit: OK, after being too hasty in this answer, let me answer the real reason why your code is faster: From the JavaDoc for Math.Random() #blockquote This is likely why your code is faster.

Question-14491966, answer-14492320
Your QuickRandom implementation hasn't really an uniform distribution. The frequencies are generally higher at the lower values while Math.random(_exprs) has a more uniform distribution. Here's a SSCCE which shows that: #pre The average result looks like this: #pre If you repeat the test, you'll see that the QR distribution varies heavily, depending on the initial seeds, while the MR distribution is stable. Sometimes it reaches the desired uniform distribution, but more than often it doesn't. Here's one of the more extreme examples, it's even beyond the borders of the graph: #pre

Question-14491966, answer-14495128
If the Math.Random(_exprs) function calls the operating system to get the time of day, then you cannot compare it to your function. Your function is a PRNG, whereas that function is striving for real random numbers. Apples and oranges. Your PRNG may be fast, but it does not have enough state information to achieve a long period before it repeats (and its logic is not sophisticated enough to even achieve the periods that are possible with that much state information). Period is the length of the sequence before your PRNG begins to repeat itself. This happens as soon as the PRNG machine makes a state transition to a state which is identical to some past state. From there, it will repeat the transitions which began in that state. Another problem with PRNG's can be a low number of unique sequences, as well as degenerate convergence on a particular sequence which repeats. There can also be undesirable patterns. For instance, suppose that a PRNG looks fairly random when the numbers are printed in decimal, but an inspection of the values in binary shows that bit 4 is simply toggling between 0 and 1 on each call. Oops! Take a look at the Mersenne Twister and other algorithms. There are ways to strike a balance between the period length and CPU cycles. One basic approach (used in the Mersenne Twister) is to cycle around in the state vector. That is to say, when a number is being generated, it is not based on the entire state, just on a few words from the state array subject to a few bit operations. But at each step, the algorithm also moves around in the array, scrambling the contents a little bit at a time.

Question-14491966, answer-14502259
There are many, many pseudo random number generators out there. For example Knuth's ranarray , the Mersenne twister , or look for LFSR generators. Knuth's monumental "Seminumerical algorithms" analizes the area, and proposes some linear congruential generators (simple to implement, fast). But I'd suggest you just stick to java.util.Random or Math.random , they fast and at least OK for occasional use (i.e., games and such). If you are just paranoid on the distribution (some Monte Carlo program, or a genetic algorithm), check out their implementation (source is available somewhere), and seed them with some truly random number, either from your operating system or from random.org . If this is required for some application where security is critical, you'll have to dig yourself. And as in that case you shouldn't believe what some colored square with missing bits spouts here, I'll shut up now.

Question-14491966, answer-14513598
java.util.Random is not much different, a basic LCG described by Knuth. However it has main 2 main advantages/differences: #li thread safe - each update is a CAS which is more expensive than a simple write and needs a branch (even if perfectly predicted single threaded). Depending on the CPU it could be significant difference. #li undisclosed internal state - this is very important for anything non-trivial. You wish the random numbers not to be predictable. Below it's the main routine generating 'random' integers in java.util.Random. #pre If you remove the AtomicLong and the undisclosed sate (i.e. using all bits of the long ), you'd get more performance than the double multiplication/modulo. Last note: Math.random should not be used for anything but simple tests, it's prone to contention and if you have even a couple of threads calling it concurrently the performance degrades. One little known historical feature of it is the introduction of CAS in java - to beat an infamous benchmark (first by IBM via intrinsics and then Sun made "CAS from Java")

Question-14491966, answer-14526729
The real problem with this is that it's output histogram is dependent on the initial seed far to much - much of the time it will end up with a near uniform output but a lot of the time will have distinctly un-uniform output. Inspired by this article about how bad php's rand() function is , I made some random matrix images using QuickRandom and System.Random . This run shows how sometimes the seed can have a bad effect (in this case favouring lower numbers) where as System.Random is pretty uniform. QuickRandom #img System.Random #img Even Worse If we initialise QuickRandom as new QuickRandom(_exprs) we get this image: #img The Code #pre

Question-14491966, answer-14563808
It is very unlikely that random number generation performance would be an issue for any use-case you came up with unless accessing a single Random instance from multiple threads (because Random is synchronized ). However, if that really is the case and you need lots of random numbers fast, your solution is far too unreliable. Sometimes it gives good results, sometimes it gives horrible results (based on the initial settings). If you want the same numbers that the Random class gives you, only faster, you could get rid of the synchronization in there: #pre I simply took the java.util.Random code and removed the synchronization which results in twice the performance compared to the original on my Oracle HotSpot JVM 7u9. It is still slower than your QuickRandom , but it gives much more consistent results. To be precise, for the same seed values and single threaded applications, it gives the same pseudo-random numbers as the original Random class would. This code is based on the current java.util.Random in OpenJDK 7u which is licensed under GNU GPL v2 . EDIT 10 months later: I just discovered that you don't even have to use my code above to get an unsynchronized Random instance. There's one in the JDK, too! Look at Java 7's ThreadLocalRandom class. The code inside it is almost identical to my code above. The class is simply a local-thread-isolated Random version suitable for generating random numbers quickly. The only downside I can think of is that you can't set its seed manually. Example usage: Random random = ThreadLocalRandom.current(_exprs);

Question-14809293, answer-14821488
The Collections.unmodifiableList has already been mentioned - the Arrays.asList(_exprs) strangely not! My solution would also be to use the list from the outside and wrap the array as follows: #pre The problem with copying the array is: if you're doing it every time you access the code and the array is big, you'll create a lot of work for the garbage collector for sure. So the copy is a simple but really bad approach - I'd say "cheap", but memory-expensive! Especially when you're having more than just 2 elements. If you look at the source code of Arrays.asList and Collections.unmodifiableList there is actually not much created. The first just wraps the array without copying it, the second just wraps the list, making changes to it unavailable.

Question-15156857, answer-15157021
You must use doThrow(_exprs).when(_exprs).methodReturningVoid(_exprs); and not doThrow(_exprs).when(_exprs); This is explained in the documentation

Question-16232833, answer-16250729
change your return type to ResponseEntity<String> , then you can use below for 400 return new ResponseEntity<String>(_exprs); and for correct request return new ResponseEntity<String>(_exprs); UPDATE 1 after spring 4.1 there are helper methods in ResponseEntity could be used as return ResponseEntity.status(_exprs).body(_exprs); and return ResponseEntity.ok(_exprs);

Question-14833008, answer-14833048
You need to escape the dot if you want to split on a literal dot: String extensionRemoved = filename.split(_exprs)[0]; Otherwise you are splitting on the regex . , which means "any character". Note the double backslash needed to create a single backslash in the regex. You're getting an ArrayIndexOutOfBoundsException because your input string is just a dot, ie _str , which is an edge case that produces an empty array when split on dot; split(regex) removes all trailing blanks from the result, but since splitting a dot on a dot leaves only two blanks, after trailing blanks are removed you're left with an empty array. To avoid getting an ArrayIndexOutOfBoundsException for this edge case, use the overloaded version of split(regex, limit) , which has a second parameter that is the size limit for the resulting array. When limit is negative , the behaviour of removing trailing blanks from the resulting array is disabled: _str.split(_exprs) // returns an array of two blanks, ie [_str, _str] ie, when filename is just a dot _str , calling filename.split(_exprs)[0] will return a blank, but calling filename.split(_exprs)[0] will throw an ArrayIndexOutOfBoundsException .

Question-14833008, answer-14833098
"." is a special character in java. You have to use "\." to escape this character : final String extensionRemoved = filename.split(_exprs)[0]; I hope this helps

Question-15430247, answer-15430259
If you know in advance what the size of the ArrayList is going to be, it is more efficient to specify the initial capacity. If you don't do this, the internal array will have to be repeatedly reallocated as the list grows. The larger the final list, the more time you save by avoiding the reallocations. That said, even without pre-allocation, inserting n elements at the back of an ArrayList is guaranteed to take total O(_exprs) time. In other words, appending an element is an amortized constant-time operation. This is achieved by having each reallocation increase the size of the array exponentially, typically by a factor of 1.5 . With this approach, the total number of operations can be shown to be O(n) .

Question-15430247, answer-15432106
This is to avoid possible efforts for reallocation for every single object. int newCapacity = (_exprs)/2 + 1; internally new Object[] is created. JVM needs effort to create new Object[] when you add element in the arraylist. If you don't have above code(any algo you think) for reallocation then every time when you invoke arraylist.add(_exprs) then new Object[] has to be created which is pointless and we are loosing time for increasing size by 1 for each and every objects to be added. So it is better to increase size of Object[] with following formula. (JSL has used forcasting formula given below for dynamically growing arraylist instead of growing by 1 every time. Because to grow it takes effort by JVM) int newCapacity = (_exprs)/2 + 1;

Question-15430247, answer-15446006
I actually wrote a blog post on the topic 2 months ago. The article is for C#'s List<T> but Java's ArrayList has a very similar implementation. Since ArrayList is implemented using a dynamic array, it increases in size on demand. So the reason for the capacity constructor is for optimisation purposes. When one of these resizings operation occurs, the ArrayList copies the contents of the array into a new array that is twice the capacity of the old one. This operation runs in O(n) time. Example Here is an example of how the ArrayList would increase in size: #pre So the list starts with a capacity of 10 , when the 11th item is added it is increase by 50% + 1 to 16 . On the 17th item the ArrayList is increased again to 25 and so on. Now consider the example where we're creating a list where the desired capacity is already known as 1000000 . Creating the ArrayList without the size constructor will call ArrayList.add 1000000 times which takes O(1) normally or O(n) on resize. #blockquote Compare this using the constructor and then calling ArrayList.add which is guaranteed to run in O(1) . #blockquote Java vs C# Java is as above, starting at 10 and increasing each resize at 50% + 1 . C# starts at 4 and increases much more aggressively, doubling at each resize. The 1000000 adds example from above for C# uses 3097084 operations. References #li My blog post on C#'s List<T> #li Java's ArrayList source code

Question-15430247, answer-25757711
ArrayList shrinks or expands dynamically. This is a effective but costly operation, if you know size of your array will be fixed then its better to mention the size of the array,If you don't, new array will be created at each insertion and reallocation will be done for each element. this will cost you. ensureCapacity(_exprs) this method help to avoid reallocations and helps increase size of arraylist. trimToSize(_exprs) this is reverse used to shrink size Arraylist has 3 constructors: #li ArrayList() -builds empty ArrayList #li ArrayList(Collection c) - builds a ArrayList initialized with elements of collection c. #li ArrayList(int capacity) -builds an ArrayList with specified initial capacity.

Question-14846920, answer-14846943
It is, in my personal experience admittedly, very useful in cases where an API requires a collection of parameters, but you have nothing to provide. For example you may have an API that looks something like this, and does not allow null references: public ResultSet executeQuery(_exprs); If you have a query that doesn't take any parameters, it's certainly a bit wasteful to create a HashMap, which involves allocating an array, when you could just pass in the 'Empty Map' which is effectively a constant, the way it's implemented in java.util.Collections .

Question-14846920, answer-14846960
From Effective Java , Item #43 - _str demonstrates returning an empty collection and perhaps even demonstrates using these emptyList(_exprs) , emptySet(_exprs) , and emptyMap(_exprs) methods on the Collections class to get an empty collection that also has the additional benefit of being immutable. From Item #15 _str . From Collections-emptySet-Collections-emptyList-Collections #blockquote Note: Below code is just an example (change it according to your use case): #pre These methods offer a couple of advantages: #li They're more concise because you don't need to explicitly type out the generic type of the collection - it's generally just inferred from the context of the method call. #li They're more efficient because they don't bother creating new objects; they just re-use an existing empty and immutable object. This effect is generally very minor, but it's occasionally (well, rarely) important.

Question-14846920, answer-14846963
#blockquote For the same reason you'd use Collections.unmodifiableMap() at some point. You want to return a Map instance that throws an exception if the user attempts to modify it. It's just a special case: the empty Map.

Question-14846920, answer-14846964
For one, you can get away with reference sharing. A new HashMap(_exprs) etc will require an allocated object, and possibly some extra elements to hold the data, but you only need one copy of an immutable empty collection (list, set, map, or any other such). This makes it an obvious choice when a method you're calling needs to accept a Map but does not need to edit it. I suggest checking out Josh Bloch's Effective Java , which lists some very nice attributes of immutable objects (including thread safety).

Question-14846920, answer-14847046
Most of the time we use a constructor to create a new empty map . But the Collections methods offer a couple of advantages to create an empty map using static method java.util.Collections.emptyMap(_exprs) #blockquote

Question-16252269, answer-16252296
Use util method of java.util.Collections class, i.e Collections.sort(_exprs) In fact, if you want to sort custom object you can use Collections.sort(_exprs) see collections api

Question-16252269, answer-29652714
You can use Collections.sort(_exprs) to sort list if your list contains Comparable elements. Otherwise I would recommend you to implement that interface like here: public class Circle implements Comparable<Circle> {} and of course provide your own realization of compareTo method like here: #pre And then you can again use Colection.sort(_exprs) as now list contains objects of Comparable type and can be sorted. Order depends on compareTo method. Check this #a for more detailed information.

Question-16252269, answer-30210459
Collections.sort allows you to pass an instance of a Comparator which defines the sorting logic. So instead of sorting the list in natural order and then reversing it, one can simply pass Collections.reverseOrder(_exprs) to sort in order to sort the list in reverse order: #pre As mentioned by @Marco13, apart from being more idiomatic (and possibly more efficient), using the reverse order comparator makes sure that the sort is stable (meaning that the order of elements will not be changed when they are equal according to the comparator, whereas reversing will change the order)

Question-14534767, answer-14534791
Escape should be done with \ , not / . So r.append(_exprs); or r.append(_exprs); will work ( StringBuilder has overloaded methods for char and String type).

Question-14534767, answer-14534798
It should be r.append(_exprs); But I recommend you to do as below, r.append(_exprs); System.getProperty(_exprs) gives you system-dependent newline in java. Also from Java 7 there's a method that returns the value directly: System.lineSeparator()

Question-14534767, answer-33687544
For HTML response below code can be used bufString.append(_exprs);

Question-14534767, answer-34073047
Another option is to use Apache Commons StrBuilder, which has the functionality that's lacking in StringBuilder. StrBuilder.appendLn()

Question-14602062, answer-14602092
From String.split() API Doc : #blockquote Overloaded String.split(regex, int) is more appropriate for your case.

Question-14602062, answer-14602155
From the documentation of String.split(String regex) : #blockquote So you will have to use the two argument version String.split(String regex, int limit) with a negative value: String[] split = data.split(_exprs); Doc: #blockquote This will not leave out any empty elements, including the trailing ones.

Question-15336477, answer-15336606
It got deprecated in version 4.3-alpha1 which you use because of the LATEST version specification. If you take a look at the javadoc of the class, it tells you what to use instead: HttpClientBuilder . In the latest stable version ( 4.2.3 ) the DefaultHttpClient is not deprecated yet.

Question-15336477, answer-32364785
Examples from Apache ( link ) use this: HttpClient httpclient = HttpClients.createDefault(_exprs); The class org.apache.http.impl.client.HttpClients is there since version 4.3. The code for HttpClients.createDefault(_exprs) is the same as the accepted answer in here.

Question-16098046, answer-16098086
You could use printf(_exprs) with %f : #pre This will print dexp: 12345678.000000 . If you don't want the fractional part, use System.out.printf(_exprs); This uses the format specifier language explained in the documentation . The default toString(_exprs) format used in your original code is spelled out here .

Question-16098046, answer-25307973
In short: If you want to get rid of trailing zeros and Locale problems, then you should use : #pre Explanation: Why other answers did not suit me : #li Double.toString(_exprs) or System.out.println or FloatingDecimal.toJavaFormatString uses scientific notations if double is less than 10^-3 or greater than or equal to 10^7 #li by using %f , the default decimal precision is 6, otherwise you can hardcode it but it results in extra zeros added if you have less decimals. Example : #pre #li by using setMaximumFractionDigits(_exprs); or %.0f you remove any decimal precision, which is fine for integers/longs but not for double #pre #li by using DecimalFormat, you are local dependent. In French locale, the decimal separator is a comma, not a point : #pre Using the ENGLISH locale makes sure you get a point for decimal separator, wherever your program will run Why using 340 then for setMaximumFractionDigits ? Two reasons : #li setMaximumFractionDigits accepts an integer but its implementation has a maximum digits allowed of DecimalFormat.DOUBLE_FRACTION_DIGITS which equals 340 #li Double.MIN_VALUE = 4.9E-324 so with 340 digits you are sure not to round your double and loose precision

Question-14526260, answer-14526289
just use File.getName() #pre using String methods : #pre

Question-14376807, answer-22092971
For those looking for a general strategy for reading and writing a string to file: First, get a file object You'll need the storage path. For the internal storage, use: File path = context.getFilesDir(_exprs); For the external storage (SD card), use: File path = context.getExternalFilesDir(_exprs); Then create your file object: File file = new File(_exprs); Write a string to the file #pre Or with Google Guava Files.write(_exprs); Read the file to a string #pre Or if you are using Google Guava String contents = Files.toString(_exprs); For completeness I'll mention String contents = new Scanner(_exprs).useDelimiter(_exprs).next(_exprs); which requires no libraries, but benchmarks 50% - 400% slower than the other options (in various tests on my Nexus 5). Notes For each of these strategies, you'll be asked to catch an IOException. The default character encoding on Android is UTF-8. If you are using external storage, you'll need to add to your manifest either: <uses-permission android:name=_str/> or <uses-permission android:name=_str/> Write permission implies read permission, so you don't need both.

Question-16324831, answer-16324963
Note: everything in this post is in the source of Java7-b147 Double.parseDouble(_exprs) goes into a Sun library (in sun.misc.FloatingDecimal ) the first important thing that happens is: #pre Integer.parseInt(_exprs) is done manually in the Integer class. The first important thing that happens is: #pre I would guess there are two different authors.

Question-14316487, answer-14316543
A very simple implementation with String.split() : #pre

Question-16148575, answer-16148593
Use Integer instead. HashMap<Integer, MyObject> myMap = new HashMap<Integer, MyObject>(_exprs); Java will automatically autobox your int primitive values to Integer objects. Read more about autoboxing from Oracle Java documentations.

Question-16148575, answer-16148619
use int as Object not as primitive type HashMap<Integer, myObject> myMap = new HashMap<Integer, myObject>(_exprs);

Question-16148575, answer-16148647
Please use HashMap<Integer, myObject> myMap = new HashMap<Integer, myObject>(_exprs);

Question-16148575, answer-16149127
You can't use a primitive because HashMap use object internally for the key. So you can only use an object that inherits from Object (that is any object). That is the function put() in HashMap and as you can see it uses Object for K: #pre The expression "k = e.key" should make it clear. I suggest to use a wrapper like Integer and autoboxing.

Question-16148575, answer-16149160
HashMap does not allow primitive data types as arguments. It can only accept objects so HashMap<int, myObject> myMap = new HashMap<int, myObject>(_exprs); will not work. You have to change the declaration to HashMap<Integer, myObject> myMap = new HashMap<Integer, myObject>(_exprs); so even when you do the following myMap.put(_exprs); The primitive data type is autoboxed to an Integer object. 8 (_exprs) === boxing ===> 8 (_exprs) You can read more on autoboxing here #a

Question-16148575, answer-18481387
If you code in Android, there is SparseArray , mapping integer to object.

Question-16148575, answer-20454885
For everybody who codes Java for Android devices and ends up here: use SparseArray for better performance private final SparseArray<myObject> myMap = new SparseArray<myObject>(_exprs); with this you can use int instead of Integer like #pre

Question-16148575, answer-32866218
The main reason with HashMap not allowing primitive as keys is that HashMap is designed in such a way that for comparing the keys, it makes use of equals() method, and a method can be called only on an object not on a primitive. Thus when int is autoboxed to Integer, Hashmap can call equals() method on Integer object. That is why, you should use Integer instead of int. I mean hashmap throws an error while putting int as a key (Don't know the meaning of the error that is thrown) And if you think that, you can make Map performance faster by making a primitive as a key, there is a library called FastUtil which contains a Map implementation with int type as a key. Because of this, it is much faster than Hashmap
