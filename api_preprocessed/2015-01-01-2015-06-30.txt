Question-30581531
What is the maximum number of parameters that a method in Java can have and why? I am using Java 1.8 on a 64-bit Windows system. All the answers on StackOverflow about this say that the technical limit is 255 parameters without specifying why. To be precise, 255 for static and 254 for non-static ( this will be the 255th in this case) methods. I thought this could be specified in some sort of specification and that this was simply a statically defined maximum number of parameters allowed. But this was only valid for int and all 4-bytes types . I did some tests with long parameters, and I was only able to declare 127 parameters in that case. With String parameters, the allowed number i deduced from testing is 255 (it may be because the reference size is 4 bytes in Java?). But since I am using a 64-bit system, references size should be 8 bytes wide and so with String parameters the maximum allowed number should be 127, similar to long types. How does this limit is exactly applied? Does the limit have anything to do with the stack size of the method? Note: I am not really going to use these many parameters in any method, but this question is only to clarify the exact behavior.

Question-29140402
I have a class defined as follows: #pre I tried to print an instance of my class: System.out.println(args); but I got the following output: com.foo.Person@2f92e0f4 . A similar thing happened when I tried to print an array of Person objects: #pre I got the output: [Lcom.foo.Person;@28a418fc What does this output mean? How do I change this output so it contains the name of my person? And how do I print collections of my objects? Note : this is intended as a canonical Q&A about this subject.

Question-28671903
I have a set – a HashSet I want to remove some items from it… none of the items in the "removals" collection will be in the original set. I specify the size of the "source" set and the size of the "removals" collection on the command line, and build both of them. The source set contains only non-negative integers; the removals set contains only negative integers. I measure how long it takes to remove all the elements using System.currentTimeMillis(), which isn’t the world most accurate stopwatch but is more than adequate in this case, as you’ll see. Here’s the code: #pre Let’s start off by giving it an easy job: a source set of 100 items, and 100 to remove: #pre Okay, That's fast as I expected. Next i tried source of one million items and 300,000 items to remove? #pre That still seems pretty speedy. Now make it a bit easier – 300,000 source items and 300,000 removals: #pre Nearly three minutes? Really confused !! can some one explain why this is happening.

Question-27908213
Are enum names interned in Java? I.e. is it guaranteed that enum1.name(args) == enum2.name(args) in case of the same name? And is it safe to compare enum.name(args) to a String that is guaranteed to be interned.

Question-27845223
I'm looking at a piece of Java code right now, and it takes a path as a String and gets its URL using URL resource = ClassLoader.getSystemClassLoader(args).getResource(args); , then calls String path = resource.getPath(args) and finally executes new File(args); . Oh, and there are also calls to URL url = resource.toURI(args); and String file = resource.getFile(args) . I'm totally confused right now - mostly because of the terminology, I guess. Can someone please walk me through the differences, or provide a few links to Dummy-proof material? Especially URI to URL and Resource to File ? To me, it feels like they should be the same thing, respectively... The difference between getFile(args) and getPath(args) is explained here: What's the difference between url.getFile() and getpath()? (Interestingly they both seem to return Strings, which probably adds a whole lot to my state of mind...) Now, if I have a locator that references a class or package in a jar file, will those two (i.e. path an file strings) differ? resource.toString(args) would give you jar:file:/C:/path/to/my.jar!/com/example/ , after all (note the exclamation mark). Is the difference between URI and URL in Java that the former doesn't encode spaces? Cf. Files, URIs, and URLs conflicting in Java (This answer explains the general, conceptual difference between the two terms fairly well: URIs identify and URLs locate; ) Lastly - and most importantly - why do I need File object; why isn't a Resource ( URL ) enough? (And is there a Resource object?) Sorry if this question is a bit unorganized; it just reflects the confusion I have... :)

Question-28584669
Why doesn't infinity comparison follow the logic applied to NaNs? This code prints out false three times: #pre However, if I change Double.NaN to Double.POSITIVE_INFINITY , I get true for equality, but false for the greater-than and less-than comparisons: #pre This seems dangerous. Assuming that infinite values result from overflows, I imagine it's more likely that two variables that ended up as infinities wouldn't actually be equal in perfect arithmetic.

Question-28840047
Some time ago, I've blogged about a Java 8 functional way of calculating fibonacci numbers recursively , with a ConcurrentHashMap cache and the new, useful computeIfAbsent(args) method: #pre I chose ConcurrentHashMap because I was thinking of making this example even more sophisticated by introducing parallelism (which I didn't in the end). Now, let's increase the number from 8 to 25 and observe what happens: System.out.println(args); The program never halts. Inside the method, there's a loop that just runs forever: #pre I'm using: #pre Matthias, a reader of that blog post also confirmed the issue (he actually found it) . This is weird. I would have expected any of the following two: #li It works #li It throws a ConcurrentModificationException But just never halting? That seems dangerous. Is it a bug? Or did I misunderstand some contract?

Question-29922296
I'm wondering what the best way is in Java 8 to work with all the values of an enum. Specifically when you need to get all the values and add it to somewhere, for example, supposing that we have the following enum: #pre I could of course do the following: #pre But, I could also add the following method to the enum definition: #pre And then replace the for from above with: Letter.stream(args).forEach(args); Is this approach OK or does it have some fault in design or performance? Moreover, why don't enums have a stream() method?

Question-28724850
Running the following stream example in Java8: System.out.println(args); yields: /a/b/c/d/e/f Which is - of course - no surprise. Due to #a it shouldn't matter whether the stream is executed sequentially or parallel: #blockquote AFAIK reduce(args) is deterministic and (s1, s2) -> s1 + StringLiteral + s2 is associative, so that adding parallel(args) should yield the same result: System.out.println(args); However the result on my machine is: /a//b//c//d//e//f What's wrong here? BTW: using (the preferred) .collect(args) instead of reduce(args) yields the same result a/b/c/d/e/f for sequential and parallel execution. JVM details: #pre

Question-30522698
The problem I would like to be able to override my apps resources such as R.colour.brand_colour or R.drawable.ic_action_start at runtime. My application connects to a CMS system that will provide branding colours and images. Once the app has downloaded the CMS data it needs to be able to re-skin itself. I know what you are about to say - overriding resources at runtime is not possible. Except that it kinda is. In particular I have found this Bachelor Thesis from 2012 which explains the basic concept - The Activity class in android extends ContextWrapper , which contains the attachBaseContext method. You can override attachBaseContext to wrap the Context with your own custom class which overrides methods such as getColor and getDrawable. Your own implementation of getColor could look the colour up however it wanted. The Calligraphy library uses a similar approach to inject a custom LayoutInflator which can deal with loading custom fonts. The code I have created a simple Activity which uses this approach to override the loading of a colour. #pre The problem is, it doesn't work! The logging shows calls to load resources such as layout/activity_main and mipmap/ic_launcher however color/theme_colour is never loaded. It seems that the context is being used to create the window and action bar, but not the activity's content view. My questions is - Where does the layout inflator load resources from, if not the activities context? I would also like to know - Is there a workable way to override the loading of colours and drawables at runtime? A word about alternative approaches I know its possible to theme an app from CMS data other ways - for example we could create a method getCMSColour(args) then inside our onCreate(args) we have a bunch of code along the lines of: myTextView.setTextColour(args) A similar approach could be taken for drawables, strings, etc. However this would result in a large amount of boilerplate code - all of which needs maintaining. When modifying the UI it would be easy to forget to set the colour on a particular view. Wrapping the Context to return our own custom values is 'cleaner' and less prone to breakage. I would like to understand why it doesn't work, before exploring alternative approaches.

Question-29333689
The following is taken from a job interview: #blockquote I thought about using an HashSet, but it might complicate everything... Any ideas of a simple solution?

Question-27747562
In the following code, a dowcast to an apparently incompatible type passes compilation: #pre Item and List<T> are disparate types so the cast can never succeed. Why did the compiler allow this?

Question-28770822
I understand the use of the @Native annotation. #blockquote However, while reading the java source code I noticed that in the class Integer and Long the SIZE constant is @Native while it is not for the Float, Byte, Double, Short and Character. Note that the SIZE constant represents the number of bits used to represent the actual value. #pre Edit : I just noticed that this also applies for MAX_VALUE and MIN_VALUE of the same classes. Edit 2 : I had spare time to do some research on this, and looking at the header files of the Long, Float etc. classes I had hope to figure out that the constants were not present in the other headers, but unfortunately they are. #pre Why is the SIZE constant only @Native for Integer and Long ?

Question-29183904
I used to define a set of related constants like Bundle keys together in an interface like below: #pre This provides me a nicer way to group related constants together and used them by making a static import (not implements). I know Android framework also uses the constants in same way like Toast.LENTH_LONG , View.GONE . However, I often feel that the Java Enums provide much better and powerful way to represent the constant. But is there a performence issue in using enums on Android ? With a bit of research I ended up in confusion. From this question "Avoid Enums Where You Only Need Ints” removed from Android's performance tips? it's clear that Google has removed "Avoid enums" from its performance tips, but from it's official training docs Be aware of memory overhead section it clearly says: "Enums often require more than twice as much memory as static constants. You should strictly avoid using enums on Android." Is this still holds good? (say in Java versions after 1.6) One more issue that I observed is to send enums across intents using Bundle I should send them by serializing (i.e putSerializable(args) , that I think an expensive operation compared to primitive putString(args) method, eventhough enums provides it for free). Can someone please clarify which one is the best way to represent the same in Android ? Should I strictly avoid using enums on Android ?

Question-28818506
I've been working with the new Optional type in Java 8 , and I've come across what seems like a common operation that isn't supported functionally: an "orElseOptional" Consider the following pattern: #pre There are many forms of this pattern, but it boils down to wanting an "orElse" on an optional that takes a function producing a new optional, called only if the current one does not exist. It's implementation would look like this: #pre I'm curious if there's a reason such a method doesn't exist, if I'm just using Optional in an unintended way, and what other ways people have come up with to deal with this case. I should say that I think that solutions involving custom utility classes/methods aren't elegant because people working with my code won't necessarily know they exist. Also, if anyone knows, will such a method be included in JDK 9, and where might I propose such a method? This seems like a pretty glaring omission to the API to me.

Question-29328785
#pre Compile >javac BigDecimalTest.java Execute >java BigDecimalTest Output #pre The value of z.toString(args) in the output is correct 4.883242e+888 / 7.115109e+302 = 6.863200e+585 as is the value of y.toString(args) , but notice that the value given for x.toString(args) is completely wrong. Why is this? Strangely, if the scale (i.e. desired decimal places) of the result of the division is changed BigDecimal z = x.divide(args); then x.toString(args) will produce the correct value for x . Or, if the operands are swapped BigDecimal z = y.divide(args); then x.toString(args) will also then produce the correct value. Or, if the exponent of x is changed from e+888 to e.g. e+878 then x.toString(args) will be correct. Or, if another x.toString(args) call is added above the divide operation, then both x.toString(args) calls will produce the correct value! On the machine I'm testing this, Windows 7 64 bit, the behaviour is the same using java 7 and 8, both 32bit and 64 bit versions, but testing online at #a produces different results for java 7 and java 8. Using java 7, the value of x is given correctly: #a , but using java 8 its value is incorrect: #a . Also, this behaviour is not unique to this particular value of x , as calling toString on other BigDecimals with more than about 1500 digits after passing them as the first operand to a divide operation will also produce incorrect values. What is the explanation for this? The divide operation seems to be mutating the value produced by subsequent toString calls on its operands. Does this happen on your platform? Edit: The issue seems to be with the java 8 runtime only, as the above program compiled with java 7 produces correct output when executed with the java 7 runtime, but incorrect output when executed with the java 8 runtime. Edit: I've tested with the early access jre1.8.0_60 and the bug does not appear, and according to Marco13's answer it was fixed in build 51. The Oracle JDK 8 product binaries are only at update 40 though so it may be some time before the fixed versions are widely used.

Question-30081520
I am aware that it is better to call the equals method over using the == operator (see this question ). I want two strings to compare as equal if they are both null or if they represent the same string. Unfortunately the equals method will throw an NPE if the strings are null . My code is currently: #pre This is inelegant. What is the correct way to perform this test?

Question-29229373
I have the following sample code: #pre The output is as follows: #pre From here I see that in first case stream really behaves lazily - we use findFirst(args) so once we have first element our filtering lambda is not invoked. However, in second case which uses flatMaps we see that despite first element which fulfils the filter condition is found (it's just any first element as lambda always returns true) further contents of the stream are still being fed through filtering function. I am trying to understand why it behaves like this rather than giving up after first element is calculated as in the first case. Any helpful information would be appreciated.

Question-29494800
I have trouble understanding the source code of Arrays.copyOf . #pre #li what is this line checking? (Object)newType == (Object)Object[].class #li What are the differences between (T[]) new Object[newLength] and (T[]) Array.newInstance(args) . why Array.newInstance not good enough for both cases? #li This following line compiles, but crashes at run time (as expected). When should I use this method? Integer[] nums = Arrays.copyOf(args)

Question-27949213
Since String in Java (like other languages) consumes a lot of memory because each character consumes two bytes, Java 8 has introduced a new feature called String Deduplication which takes advantage of the fact that the char arrays are internal to strings and final, so the JVM can mess around with them. I have read this example so far but since I am not a pro java coder, I am having a hard time grasping the concept. Here is what it says, #blockquote My First question, There is still a lack of resources on this topic since it is recently added in Java 8 update 20, could anyone here share some practical examples on how it help in reducing the memory consumed by String in Java ? Edit: The above link says, #blockquote My 2nd question, If hash code of two String are same then the Strings are already the same, then why compare them char by char once it is found that the two String have same hash code ?

Question-29095967
I have this list ( List<T> ): [StringLiteral, StringLiteral, null, StringLiteral, null, StringLiteral, StringLiteral] And I'd like something like this: [[StringLiteral, StringLiteral], [StringLiteral], [StringLiteral, StringLiteral]] In other words I want to split my list in sublists using the null value as separator, in order to obtain a list of lists ( List<T> ). I'm looking for a Java 8 solution. I've tried with Collectors.partitioningBy but I'm not sure it is what I'm looking for. Thanks!

Question-30727515, answer-30739477
Since this hasn’t addressed yet, here an explanation, why the translation of Unicode escapes happens before any other source code processing: The idea behind it was that it allows lossless translations of Java source code between different character encodings. Today, there is widespread Unicode support, and this doesn’t look like a problem, but back then it wasn’t easy for a developer from a western country to receive some source code from his Asian colleague containing Asian characters, make some changes (including compiling and testing it) and sending the result back, all without damaging something. So, Java source code can be written in any encoding and allows a wide range of characters within identifiers, character and String literals and comments. Then, in order to transfer it losslessly, all characters not supported by the target encoding are replaced by their Unicode escapes. This is a reversible process and the interesting point is that the translation can be done by a tool which doesn’t need to know anything about the Java source code syntax as the translation rule is not dependent on it. This works as the translation to their actual Unicode characters inside the compiler happens independently to the Java source code syntax as well. It implies that you can perform an arbitrary number of translation steps in both directions without ever changing the meaning of the source code. This is the reason for another weird feature which hasn’t even mentioned: the \uuuuuuxxxx syntax: When a translation tool is escaping characters and encounters a sequence that is already an escaped sequence, it should insert an additional u into the sequence, converting \ucafe to \uucafe . The meaning doesn’t change, but when converting into the other direction, the tool should just remove one u and replace only sequences containing a single u by their Unicode characters. That way, even Unicode escapes are retained in their original form when converting back and forth. I guess, no-one ever used that feature…

Question-27747562, answer-27747702
A List<T> could very well be an Item. See for example: #pre A cast tells the compiler: "I know you can't be sure that this is a object of type Item, but I know better than you, so please compile". The compiler will only refuse to compile that if it's impossible for the returned object to be an instance of Item (like, for example, Integer can't ever be a String ) At runtime, the type of the actual object returned by the method will be checked, and if it's not actually an object of type Item, you'll get a ClassCastException.

Question-29183904, answer-29972028
Use enum when you need its features. Don't avoid it strictly . Java enum is more powerful, but if you don't need its features, use constants, they occupy less space and they can be primitive itself. When to use enum: #li type checking - you can accept only listed values, and they are not continuous (see below what I call continuous here) #li method overloading - every enum constant has its own implementation of a method #pre #li more data - your one constant contains more than one information that cannot be put in one variable #li complicated data - your constant need methods to operate on the data When not to use enum: #li you can accept all values of one type, and your constants contain only these most used #li you can accept continuous data #pre #li for names (like in your example) #li for everything other that really doesn't need an enum Enum occupies more space #li a single reference to an enum constant occupy 4 bytes #li every enum constant occupy space that is a sum of its fields' sizes aligned to 8 bytes + overhead of the object #li the enum class itself occupy some space Constant occupies less space #li a constant doesn't have a reference so it's a pure data (even if it's a reference, then enum instance would be a reference to another reference) #li constants may be added to existing class - it's not necessary to add another class #li constants may be inlined; it brings extended compile-time features (such as null checking, finding dead code etc.)

Question-28818506, answer-28818627
Perhaps this is what you're after: Get value from one Optional or another Otherwise, you may want to have a look at Optional.orElseGet . Here's an example of what I think that you're after: result = Optional.ofNullable(args);

Question-28818506, answer-28821352
It's not pretty, but this will work: return serviceA(args) .map(args).orElseGet(args) .map(args).orElseGet(args) .map(args).orElseGet(args); .map(args).orElseGet(args) is a fairly handy pattern for use with Optional . It means "If this Optional contains value v , give me func(args) , otherwise give me sup.get(args) ". In this case, we call serviceA(args) and get an Optional<T> . If that Optional contains value v , we want to get Optional.of(args) , but if it is empty, we want to get serviceB(args) . Rinse-repeat with more alternatives. Other uses of this pattern are #li .map(args).orElseGet(args) #li .map(args).orElseGet(args)

Question-28818506, answer-28833677
The cleanest “try services” approach given the current API would be: Optional<T> o = Stream.<Supplier<T>>of(args) .map(args) .filter(args) .map(args) .findFirst(args); The important aspect is not the (constant) chain of operations you have to write once but how easy it is to add another service (or modify the list of services is general). Here, adding or removing a single ()->serviceX(args) is enough. Due to the lazy evaluation of streams, no service will be invoked if a preceding service returned a non-empty Optional .

Question-28818506, answer-32653133
This is looks like a good fit for pattern matching and a more traditional Option interface with Some and None implementations (such as those in Javaslang , FunctionalJava ) or a lazy Maybe implementation in cyclops-react .I'm the author of this library. With cyclops-react you can also use structural pattern matching on JDK types. For Optional you can match on the present and absent cases via the visitor pattern . it would look something like this - #pre

Question-28818506, answer-38560612
This is part of JDK 9 in the form of or , which takes a Supplier<T> . Your example would then be: return serviceA(args) .or(args) .or(args); For details see the (preliminary) Javadoc or this post I wrote.

Question-29328785, answer-29330741
It's not so hard to track down the reason for the odd behavior. The divide call goes to #pre This, internally, delegates to another divide method, based on the rounding mode: #pre In this case, the last call applies. Note that the intVal (which is a BigInteger that is stored in the BigDecimal ) is passed directly to this method as the first argument: #pre Finally, the path to the second divideAndRound is taken here, again passing the dividend on (which was the intVal of the original BigDecimal ), ending up with this code: #pre And this is where the error is introduced: The mdivididend is a mutable BigInteger , that was created as a mutable view on the mag array of the BigInteger that is stored in the BigDecimal x from the original call. The division modifies the mag field, and thus, the state of the (now not-so-immutable) BigDecimal . This is clearly a bug in the implementation of one of the divide methods. I already started tracking the change sets of the OpenJDK, but have not yet spotted the definite culprit. ( Edit: See updates below ) (A side note: Calling x.toString(args) before doing the division does not really avoid , but only hide the bug: It causes a string cache of the correct state to be created internally. The right value is printed, but the internal state is still wrong - which is concerning, to say the least...) #blockquote

Question-28770822, answer-28816219
Looking at the issue and fix, it looks like this has been done to resolve a handling of a header file generation for special classes in jigsaw Jigsaw is a module system designated to use in Java SE Platform and JDK. More details here Here is a corresponding changeset . You can see a comment, #blockquote From the changeset I see that for the sake of purpose in addition to java.lang.Integer and java.lang.Long , also some properties in java.net.SocketOptions , sun.nio.ch.IOStatus , java.io.FileSystem have been changed to @Native . So I assume only those were needed to solve the dependency with jigsaw.

Question-28770822, answer-28853754
TLDR : Jump to the conclusion #blockquote A brief history of @Native I made some search on the mailing lists. I found some interesting things. At first an annotation ( 1 2 ) javax.tools.annotation.ForceNativeHeader was introduced to #blockquote It is used by com.sun.tools.javac.processing.NativeapiVisitor . By looking at the code we can see that the native header is generated if the class declare some native methods or if the class is annotated @ForceNativeHeader . Later this annotation was renamed to GenerateNativeHeader ( 1 2 ). Then this annotation was added to several types (especially Integer and Long ) with an interresting comment: #pre But by adding this annotation it add a problematic dependency from base module to the module containing javax.tools. So the annotation were removed from Integer and Long and these files were explicitly added to the build process since the header were no more automatically generated... a "(hopefully temporary) hack" . So a new annotation java.lang.annotation.Native was created and used in Integer and Long . The annotation was set a TargetType FIELD . #blockquote All the purpose of this stuff is: #blockquote It is the case of Integer and Long this was a part of the JEP 139: Enhance javac to Improve Build Speed : #blockquote A basic experimentation I made a basic experimentation on the JDK. I clone the open-jdk forest and i successfully build it. As expected the header files where generated for Integer and Long (thanks to @Native ) and for Float and Double (thanks to their native methods) but not for Byte , Short ... #pre Then i tried to remove the @Native from the Integer fields and i tried to build again the jdk but i get an error: #pre logically since the header have not been generated. I have also confirmed that java_lang_Integer.h is included in several c and cpp files : #pre like Long #pre like Float #pre and like Double #pre but neither Short find . \( -name StringLiteral -o -name StringLiteral \) -exec grep StringLiteral {} \; -print nor Byte , nor Character . Conclusion Among all these types, only Integer , Long , Float , Double are used in the native source code of the jdk . And only the Integer and Long fields are annotated with @Native because they have no native methods (as opposed to Float and Double )

Question-28459498, answer-28459604
Background While the question appears simple, the actual answer requires some background to make sense. If you want to skip to the conclusion, scroll down... Pick your comparison point - Basic functionality Using basic concepts, C#'s IEnumerable concept is more closely related to Java's Iterable , which is able to create as many Iterators as you want. IEnumerables create IEnumerators . Java's Iterable create Iterators The history of each concept is similar, in that both IEnumerable and Iterable have a basic motivation to allow 'for-each' style looping over the members of data collections. That's an oversimplification as they both allow more than just that, and they also arrived at that stage via different progressions, but it is a significant common feature regardless. Let's compare that feature: in both languages, if a class implements the IEnumerable / Iterable , then that class must implement at least a single method (for C#, it's GetEnumerator and for Java it's iterator(args) ). In each case, the instance returned from that ( IEnumerator / Iterator ) allows you to access the current and subsequent members of the data. This feature is used in the for-each language syntax. Pick your comparison point - Enhanced functionality IEnumerable in C# has been extended to allow a number of other language features ( mostly related to Linq ). Features added include selections, projections, aggregations, etc. These extensions have a strong motivation from use in set-theory, similar to SQL and Relational Database concepts. Java 8 has also had functionality added to enable a degree of functional programming using Streams and Lambdas. Note that Java 8 streams are not primarily motivated by set theory, but by functional programming. Regardless, there are a lot of parallels. So, this is the second point. The enhancements made to C# were implemented as an enhancement to the IEnumerable concept. In Java, though, the enhancements made were implemented by creating new base concepts of Lambdas and Streams, and then also creating a relatively trivial way to convert from Iterators and Iterables to Streams, and visa-versa. So, comparing IEnumerable to Java's Stream concept is incomplete. You need to compare it to the combined Streams and Collections API's in Java. In Java, Streams are not the same as Iterables, or Iterators Streams are not designed to solve problems the same way that iterators are: #li Iterators are a way of describing the sequence of data. #li Streams are a way of describing a sequence of data transformations. With an Iterator , you get a data value, process it, and then get another data value. With Streams, you chain a sequence of functions together, then you feed an input value to the stream, and get the output value from the combined sequence. Note, in Java terms, each function is encapsulated in a single Stream instance. The Streams API allows you to link a sequence of Stream instances in a way that chains a sequence of transformation expressions. In order to complete the Stream concept, you need a source of data to feed the stream, and a terminal function that consumes the stream. The way you feed values in to the stream may in fact be from an Iterable , but the Stream sequence itself is not an Iterable , it is a compound function. A Stream is also intended to be lazy, in the sense that it only does work when you request a value from it. Note these significant assumptions and features of Streams: #li A Stream in Java is a transformation engine, it transforms a data item in one state, to being in another state. #li streams have no concept of the data order or position, the simply transform whatever they are asked to. #li streams can be supplied with data from many sources, including other streams, Iterators, Iterables, Collections, #li you cannot "reset" a stream, that would be like "reprogramming the transformation". Resetting the data source is probably what you want. #li there is logically only 1 data item 'in flight' in the stream at any time (unless the stream is a parallel stream, at which point, there is 1 item per thread). This is independent of the data source which may have more than the current items 'ready' to be supplied to the stream, or the stream collector which may need to aggregate and reduce multiple values. #li Streams can be unbound (infinite), limited only by the data source, or collector (which can be infinite too). #li Streams are 'chainable', the output of filtering one stream, is another stream. Values input to and transformed by a stream can in turn be supplied to another stream which does a different transformation. The data, in its transformed state flows from one stream to the next. You do not need to intervene and pull the data from one stream and plug it in to the next. C# Comparison When you consider that a Java Stream is just a part of a supply, stream, and collect system, and that Streams and Iterators are often used together with Collections, then it is no wonder that it is hard to relate to the same concepts which are almost all embedded in to a single IEnumerable concept in C#. Parts of IEnumerable (and close related concepts) are apparent in all of the Java Iterator, Iterable, Lambda, and Stream concepts. There are small things that the Java concepts can do that are harder in IEnumerable, and visa-versa. Conclusion #li There's no design problem here, just a problem in matching concepts between the languages. #li Streams solve problems in a different way #li Streams add functionality to Java (they add a different way of doing things, they do not take functionality away) Adding Streams gives you more choices when solving problems, which is fair to classify as 'enhancing power', not 'reducing', 'taking away', or 'restricting' it. Why are Java Streams once-off? This question is misguided, because streams are function sequences, not data. Depending on the data source that feeds the stream, you can reset the data source, and feed the same, or different stream. Unlike C#'s IEnumerable, where an execution pipeline can be executed as many times as we want, in Java a stream can be 'iterated' only once. Comparing an IEnumerable to a Stream is misguided. The context you are using to say IEnumerable can be executed as many times as you want, is best compared to Java Iterables , which can be iterated as many times as you want. A Java Stream represents a subset of the IEnumerable concept, and not the subset that supplies data, and thus cannot be 'rerun'. Any call to a terminal operation closes the stream, rendering it unusable. This 'feature' takes away a lot of power. The first statement is true, in a sense. The 'takes away power' statement is not. You are still comparing Streams it IEnumerables. The terminal operation in the stream is like a 'break' clause in a for loop. You are always free to have another stream, if you want, and if you can re-supply the data you need. Again, if you consider the IEnumerable to be more like an Iterable , for this statement, Java does it just fine. I imagine the reason for this is not technical. What were the design considerations behind this strange restriction? The reason is technical, and for the simple reason that a Stream a subset of what think it is. The stream subset does not control the data supply, so you should reset the supply, not the stream. In that context, it is not so strange. QuickSort example Your quicksort example has the signature: IEnumerable<T> QuickSort(args) You are treating the input IEnumerable as a data source: IEnumerable<T> lt = ints.Where(args); Additionally, return value is IEnumerable too, which is a supply of data, and since this is a Sort operation, the order of that supply is significant. If you consider the Java Iterable class to be the appropriate match for this, specifically the List specialization of Iterable , since List is a supply of data which has a guaranteed order or iteration, then the equivalent Java code to your code would be: #pre Note there is a bug (which I have reproduced), in that the sort does not handle duplicate values gracefully, it is a 'unique value' sort. Also note how the Java code uses data source ( List ), and stream concepts at different point, and that in C# those two 'personalities' can be expressed in just IEnumerable . Also, although I have use List as the base type, I could have used the more general Collection , and with a small iterator-to-Stream conversion, I could have used the even more general Iterable

Question-28459498, answer-28460301
Stream s are built around Spliterator s which are stateful, mutable objects. They don’t have a “reset” action and in fact, requiring to support such rewind action would “take away much power”. How would Random.ints() be supposed to handle such a request? On the other hand, for Stream s which have a retraceable origin, it is easy to construct an equivalent Stream to be used again. Just put the steps made to construct the Stream into a reusable method. Keep in mind that repeating these steps is not an expensive operation as all these steps are lazy operations; the actual work starts with the terminal operation and depending on the actual terminal operation entirely different code might get executed. It would be up to you, the writer of such a method, to specify what calling the method twice implies: does it reproduce exactly the same sequence, as streams created for an unmodified array or collection do, or does it produce a stream with a similar semantics but different elements like a stream of random ints or a stream of console input lines, etc. By the way, to avoid confusion, a terminal operation consumes the Stream which is distinct from closing the Stream as calling close(args) on the stream does (which is required for streams having associated resources like, e.g. produced by Files.lines(args) ). It seems that a lot of confusion stems from misguiding comparison of IEnumerable with Stream . An IEnumerable represents the ability to provide an actual IEnumerator , so its like an Iterable in Java. In contrast, a Stream is a kind of iterator and comparable to an IEnumerator so it’s wrong to claim that this kind of data type can be used multiple times in .NET, the support for IEnumerator.Reset is optional. The examples discussed here rather use the fact that an IEnumerable can be used to fetch new IEnumerator s and that works with Java’s Collection s as well; you can get a new Stream . If the Java developers decided to add the Stream operations to Iterable directly, with intermediate operations returning another Iterable , it was really comparable and it could work the same way. However, the developers decided against it and the decision is discussed in this question . The biggest point is the confusion about eager Collection operations and lazy Stream operations. By looking at the .NET API, I (yes, personally) find it justified. While it looks reasonable looking at IEnumerable alone, a particular Collection will have lots of methods manipulating the Collection directly and lots of methods returning a lazy IEnumerable , while the particular nature of a method isn’t always intuitively recognizable. The worst example I found (within the few minutes I looked at it) is List.Reverse() whose name matches exactly the name of the inherited (is this the right terminus for extension methods?) Enumerable.Reverse() while having an entirely contradicting behavior. Of course, these are two distinct decisions. The first one to make Stream a type distinct from Iterable / Collection and the second to make Stream a kind of one time iterator rather than another kind of iterable. But these decision were made together and it might be the case that separating these two decision never was considered. It wasn’t created with being comparable to .NET’s in mind. The actual API design decision was to add an improved type of iterator, the Spliterator . Spliterator s can be provided by the old Iterable s (which is the way how these were retrofitted) or entirely new implementations. Then, Stream was added as a high-level front-end to the rather low level Spliterator s. That’s it. You may discuss about whether a different design would be better, but that’s not productive, it won’t change, given the way they are designed now. There is another implementation aspect you have to consider. Stream s are not immutable data structures. Each intermediate operation may return a new Stream instance encapsulating the old one but it may also manipulate its own instance instead and return itself (that doesn’t preclude doing even both for the same operation). Commonly known examples are operations like parallel or unordered which do not add another step but manipulate the entire pipeline). Having such a mutable data structure and attempts to reuse (or even worse, using it multiple times at the same time) doesn’t play well… For completeness, here is your quicksort example translated to the Java Stream API. It shows that it does not really “take away much power”. #pre It can be used like #pre You can write it even more compact as #pre

Question-28459498, answer-28465634
I think there are very few differences between the two when you look closely enough. At it's face, an IEnumerable does appear to be a reusable construct: #pre However, the compiler is actually doing a little bit of work to help us out; it generates the following code: #pre Each time you would actually iterate over the enumerable, the compiler creates an enumerator. The enumerator is not reusable; further calls to MoveNext will just return false, and there is no way to reset it to the beginning. If you want to iterate over the numbers again, you will need to create another enumerator instance. To better illustrate that the IEnumerable has (can have) the same 'feature' as a Java Stream, consider a enumerable whose source of the numbers is not a static collection. For example, we can create an enumerable object which generates a sequence of 5 random numbers: #pre Now we have very similar code to the previous array-based enumerable, but with a second iteration over numbers : #pre The second time we iterate over numbers we will get a different sequence of numbers, which isn't reusable in the same sense. Or, we could have written the RandomNumberStream to thrown an exception if you try to iterate over it multiple times, making the enumerable actually unusable (like a Java Stream). Also, what does your enumerable-based quick sort mean when applied to a RandomNumberStream ? Conclusion So, the biggest difference is that .NET allows you to reuse an IEnumerable by implicitly creating a new IEnumerator in the background whenever it would need to access elements in the sequence. This implicit behavior is often useful (and 'powerful' as you state), because we can repeatedly iterate over a collection. But sometimes, this implicit behavior can actually cause problems. If your data source is not static, or is costly to access (like a database or web site), then a lot of assumptions about IEnumerable have to be discarded; reuse is not that straight-forward

Question-28459498, answer-28513908
I have some recollections from the early design of the Streams API that might shed some light on the design rationale. Back in 2012, we were adding lambdas to the language, and we wanted a collections-oriented or "bulk data" set of operations, programmed using lambdas, that would facilitate parallelism. The idea of lazily chaining operations together was well established by this point. We also didn't want the intermediate operations to store results. The main issues we needed to decide were what the objects in the chain looked like in the API and how they hooked up to data sources. The sources were often collections, but we also wanted to support data coming from a file or the network, or data generated on-the-fly, e.g., from a random number generator. There were many influences of existing work on the design. Among the more influential were Google's Guava library and the Scala collections library. (If anybody is surprised about the influence from Guava, note that Kevin Bourrillion , Guava lead developer, was on the JSR-335 Lambda expert group.) On Scala collections, we found this talk by Martin Odersky to be of particular interest: Future-Proofing Scala Collections: from Mutable to Persistent to Parallel . (Stanford EE380, 2011 June 1.) Our prototype design at the time was based around Iterable . The familiar operations filter , map , and so forth were extension (default) methods on Iterable . Calling one added an operation to the chain and returned another Iterable . A terminal operation like count would call iterator(args) up the chain to the source, and the operations were implemented within each stage's Iterator. Since these are Iterables, you can call the iterator(args) method more than once. What should happen then? If the source is a collection, this mostly works fine. Collections are Iterable, and each call to iterator(args) produces a distinct Iterator instance that is independent of any other active instances, and each traverses the collection independently. Great. Now what if the source is one-shot, like reading lines from a file? Maybe the first Iterator should get all the values but the second and subsequent ones should be empty. Maybe the values should be interleaved among the Iterators. Or maybe each Iterator should get all the same values. Then, what if you have two iterators and one gets farther ahead of the other? Somebody will have to buffer up the values in the second Iterator until they're read. Worse, what if you get one Iterator and read all the values, and only then get a second Iterator. Where do the values come from now? Is there a requirement for them all to be buffered up just in case somebody wants a second Iterator? Clearly, allowing multiple Iterators over a one-shot source raises a lot of questions. We didn't have good answers for them. We wanted consistent, predictable behavior for what happens if you call iterator(args) twice. This pushed us toward disallowing multiple traversals, making the pipelines one-shot. We also observed others bumping into these issues. In the JDK, most Iterables are collections or collection-like objects, which allow multiple traversal. It isn't specified anywhere, but there seemed to be an unwritten expectation that Iterables allow multiple traversal. A notable exception is the NIO DirectoryStream interface. Its specification includes this interesting warning: #blockquote [bold in original] This seemed unusual and unpleasant enough that we didn't want to create a whole bunch of new Iterables that might be once-only. This pushed us away from using Iterable. About this time, an article by Bruce Eckel appeared that described a spot of trouble he'd had with Scala. He'd written this code: #pre It's pretty straightforward. It parses lines of text into Registrant objects and prints them out twice. Except that it actually only prints them out once. It turns out that he thought that registrants was a collection, when in fact it's an iterator. The second call to foreach encounters an empty iterator, from which all values have been exhausted, so it prints nothing. This kind of experience convinced us that it was very important to have clearly predictable results if multiple traversal is attempted. It also highlighted the importance of distinguishing between lazy pipeline-like structures from actual collections that store data. This in turn drove the separation of the lazy pipeline operations into the new Stream interface and keeping only eager, mutative operations directly on Collections. Brian Goetz has explained the rationale for that. What about allowing multiple traversal for collection-based pipelines but disallowing it for non-collection-based pipelines? It's inconsistent, but it's sensible. If you're reading values from the network, of course you can't traverse them again. If you want to traverse them multiple times, you have to pull them into a collection explicitly. But let's explore allowing multiple traversal from collections-based pipelines. Let's say you did this: #pre (The into operation is now spelled collect(args) .) If source is a collection, then the first into(args) call will create a chain of Iterators back to the source, execute the pipeline operations, and send the results into the destination. The second call to into(args) will create another chain of Iterators, and execute the pipeline operations again . This isn't obviously wrong but it does have the effect of performing all the filter and map operations a second time for each element. I think many programmers would have been surprised by this behavior. As I mentioned above, we had been talking to the Guava developers. One of the cool things they have is an Idea Graveyard where they describe features that they decided not to implement along with the reasons. The idea of lazy collections sounds pretty cool, but here's what they have to say about it. Consider a List.filter(args) operation that returns a List : #blockquote To take a specific example, what's the cost of get(args) or size(args) on a List? For commonly used classes like ArrayList , they're O(1). But if you call one of these on a lazily-filtered list, it has to run the filter over the backing list, and all of a sudden these operations are O(n). Worse, it has to traverse the backing list on every operation. This seemed to us to be too much laziness. It's one thing to set up some operations and defer actual execution until you so "Go". It's another to set things up in such a way that hides a potentially large amount of recomputation. In proposing to disallow non-linear or "no-reuse" streams, Paul Sandoz described the potential consequences of allowing them as giving rise to "unexpected or confusing results." He also mentioned that parallel execution would make things even trickier. Finally, I'd add that a pipeline operation with side effects would lead to difficult and obscure bugs if the operation were unexpectedly executed multiple times, or at least a different number of times than the programmer expected. (But Java programmers don't write lambda expressions with side effects, do they? DO THEY??) So that's the basic rationale for the Java 8 Streams API design that allows one-shot traversal and that requires a strictly linear (no branching) pipeline. It provides consistent behavior across multiple different stream sources, it clearly separates lazy from eager operations, and it provides a straightforward execution model. With regard to IEnumerable , I am far from an expert on C# and .NET, so I would appreciate being corrected (gently) if I draw any incorrect conclusions. It does appear, however, that IEnumerable permits multiple traversal to behave differently with different sources; and it permits a branching structure of nested IEnumerable operations, which may result in some significant recomputation. While I appreciate that different systems make different tradeoffs, these are two characteristics that we sought to avoid in the design of the Java 8 Streams API. The quicksort example given by the OP is interesting, puzzling, and I'm sorry to say, somewhat horrifying. Calling QuickSort takes an IEnumerable and returns an IEnumerable , so no sorting is actually done until the final IEnumerable is traversed. What the call seems to do, though, is build up a tree structure of IEnumerables that reflects the partitioning that quicksort would do, without actually doing it. (This is lazy computation, after all.) If the source has N elements, the tree will be N elements wide at its widest, and it will be lg(N) levels deep. It seems to me -- and once again, I'm not a C# or .NET expert -- that this will cause certain innocuous-looking calls, such as pivot selection via ints.First(args) , to be more expensive than they look. At the first level, of course, it's O(1). But consider a partition deep in the tree, at the right-hand edge. To compute the first element of this partition, the entire source has to be traversed, an O(N) operation. But since the partitions above are lazy, they must be recomputed, requiring O(lg N) comparisons. So selecting the pivot would be an O(N lg N) operation, which is as expensive as an entire sort. But we don't actually sort until we traverse the returned IEnumerable . In the standard quicksort algorithm, each level of partitioning doubles the number of partitions. Each partition is only half the size, so each level remains at O(N) complexity. The tree of partitions is O(lg N) high, so the total work is O(N lg N). With the tree of lazy IEnumerables, at the bottom of the tree there are N partitions. Computing each partition requires a traversal of N elements, each of which requires lg(N) comparisons up the tree. To compute all the partitions at the bottom of the tree, then, requires O(N^2 lg N) comparisons. (Is this right? I can hardly believe this. Somebody please check this for me.) In any case, it is indeed cool that IEnumerable can be used this way to build up complicated structures of computation. But if it does increase the computational complexity as much as I think it does, it would seem that programming this way is something that should be avoided unless one is extremely careful.

Question-29229373, answer-29230939
When looking into the implementation ( ReferencePipeline.java ) we see the method [ link ] #pre which will be invoke for findFirst operation. The special thing to take care about is the sink.cancellationRequested(args) which allows to end the loop on the first match. Compare to [ link ] #pre The method for advancing one item ends up calling forEach on the sub-stream without any possibility for earlier termination and the comment at the beginning of the flatMap method even tells about this absent feature. Since this is more than just an optimization thing as it implies that the code simply breaks when the sub-stream is infinite, I hope that the developers soon prove that they “can do better than this”… To illustrate the implications, while Stream.iterate(args).findFirst(args) works as expected, Stream.of(args).flatMap(args).findFirst(args) will end up in an infinite loop. Regarding the specification, most of it can be found in the chapter “Stream operations and pipelines” of the package specification : #blockquote It’s clear that a short-circuiting operation doesn’t guaranty a finite time termination, e.g. when a filter doesn’t match any item the processing can’t complete, but an implementation which doesn’t support any termination in finite time by simply ignoring the short-circuiting nature of an operation is far off the specification.

Question-29229373, answer-33839551
In my free StreamEx library I introduced the short-circuiting collectors. When collecting sequential stream with short-circuiting collector (like MoreCollectors.first() ) exactly one element is consumed from the source. Internally it's implemented in quite dirty way: using a custom exception to break the control flow. Using my library your sample could be rewritten in this way: #pre The result is the following: #pre

Question-29815636, answer-29815698
The first form is called Javadoc . You use this when you're writing formal APIs for your code, which are generated by the javadoc tool. For an example, the Java 7 API page uses Javadoc and was generated by that tool. Some common elements you'd see in Javadoc include: #li @param : this is used to indicate what parameters are being passed to a method, and what value they're expected to have #li @return : this is used to indicate what result the method is going to give back #li @throws : this is used to indicate that a method throws an exception or error in case of certain input #li @since : this is used to indicate the earliest Java version this class or function was available in As an example, here's Javadoc for the compare method of Integer : #pre The second form is a block (multi-line) comment. You use this if you want to have multiple lines in a comment. I will say that you'd only want to use the latter form sparingly ; that is, you don't want to overburden your code with block comments that don't describe what behaviors the method/complex function is supposed to have. Since Javadoc is the more descriptive of the two, and you can generate actual documentation as a result of using it, using Javadoc would be more preferable to simple block comments.

Question-29140402, answer-29141638
Every class in java has toString(args) method in it by default, which is called by System.out.println(args) if you pass some object of a class to it. When you try to print object of a class, the System.out.println(args) method will call toString(args) of the class which returns the className@hashcode of that object. #pre You can override the toString method of a class to get different output. See this example #pre

Question-29140402, answer-36771568
In Eclipse, Go to your class, Right click->source->Generate toString(); It will override the toString() method and print the object of that class.

Question-29140402, answer-38628203
In intellij you can auto generate toString method by pressing alt+inset and then selecting toString() here is an out put for a test class: #pre As you can see, it generates a String by concatenating, several attributes of the class, for primitives it will print their values and for reference types it will use their class type (in this case to string method of Test2).

Question-29945627, answer-29945754
The lambda: () -> { System.out.println(args); }; actually represents an implementation for an interface like: #pre which is completely different than the one you've defined. That's why you get an error. Since you can't extend your @FunctionalInterface , nor introduce a brand new one, then I think you don't have much options. You can use the Optional<T> interfaces to denote that some of the values (return type or method parameter) is missing, though. However, this won't make the lambda body simpler.

Question-29945627, answer-29946155
The syntax you're after is possible with a little helper function that converts a Runnable into Action<T> (you can place it in Action for example): #pre

Question-27949213, answer-27949367
The strategy they describe is to simply reuse the internal character array of one String in possibly many equal Strings. There's no need for each String to have its own copy if they are equal. In order to more quickly determine if 2 strings are equal, the hash code is used as a first step, as it is a fast way to determine if Strings may be equal. Hence their statement: #blockquote This is to make a certain (but slower) comparison for equality once possible equality has been determined using the hash code. In the end, equal Strings will share a single underlying char array. Java has had String.intern(args) for a long time, to do more or less the same (i.e. save memory by deduplicating equal Strings). What's novel about this is that it happens during garbage collection time and can be externally controlled.

Question-27949213, answer-27949581
Imagine you have a phone book, which contains people, which have a String firstName and a String lastName . And it happens that in your phone book, 100,000 people have the same firstName = StringLiteral . Because you get the data from a database or a file those strings are not interned so your JVM memory contains the char array {CharLiteral, CharLiteral, CharLiteral, CharLiteral} 100 thousand times, one per John string. Each of these arrays takes, say, 20 bytes of memory so those 100k Johns take up 2 MB of memory. With deduplication, the JVM will realise that "John" is duplicated many times and make all those John strings point to the same underlying char array, decreasing the memory usage from 2MB to 20 bytes. You can find a more detailed explanation in the JEP . In particular: #blockquote

Question-27949213, answer-27950675
Since your first question has already been answered, I'll answer your second question. The String objects must be compared character by character, because though equal Object s implies equal hashes, the inverse is not necessarily true. As Holger said in his comment , this represents a hash collision. The applicable specifications for the hashcode(args) method are as follows: #blockquote This means that in order for them to guarantee equality, the comparison of each character is necessary in order for them to confirm the equality of the two objects. They start by comparing hashCode s rather than using equals since they are using a hash table for the references, and this improves performance.

Question-28276423, answer-28276463
If all of the streams have been instantiated then closing only the outermost is just fine. The documentation on Closeable interface states that close method: #blockquote The releasing system resources includes closing streams. It also states that: #blockquote So if you close them explicitly afterwards, nothing wrong will happen.

Question-28276423, answer-28276469
It will be fine if you only close the last stream - the close call will be send to the underlying streams, too.

Question-28276423, answer-28276493
You can close the outer most stream, in fact you don't need to retain all the streams wrapped and you can use Java 7 try-with-resources. #pre If you subscribe to YAGNI, or you-aint-gonna-need-it, you should be only adding code you actually need. You shouldn't be adding code you imagine you might need but in reality doesn't do anything useful. Take this example and imagine what could possibly go wrong if you didn't do this and what the impact would be? #pre Lets start with FileOutputStream which calls open to do all the real work. /** * Opens a file, with the specified name, for overwriting or appending. * @param name name of file to be opened * @param append whether the file is to be opened in append mode */ private native void open(args) throws FileNotFoundException; If the file is not found, there is no underlying resource to close, so closing it won't make any difference. If The file exists, it should be throwing a FileNotFoundException. So there is nothing to be gained by trying to close the resource from this line alone. The reason you need to close the file is when the file is opened successfully, but you later get an error. Lets look at the next stream GZIPOutputStream There is code which can throw an exception #pre This writes the header of the file. Now it would be very unusual for you to be able to open a file for writing but not be able to write even 8 bytes to it, but lets imagine this could happen and we don't close the file afterwards. What does happen to a file if it is not closed? You don't get any unflushed writes, they are discarded and in this case, there is no successfully written bytes to the stream which isn't buffered at this point anyway. But a file which is not closed doesn't live forever, instead FileOutputStream has #pre If you don't close a file at all, it gets closed anyway, just not immediately (and like I said, data which is left in a buffer will be lost this way, but there is none at this point) What is the consequence of not closing the file immediately? Under normal conditions, you potentially lose some data, and you potentially run out of file descriptors. But if you have a system where you can create files but you can't write anything to them, you have a bigger problem. i.e. it hard to imagine why you are repeatedly trying to create this file despite the fact you are failing. Both OutputStreamWriter and BufferedWriter don't throw IOException in their constructors, so it not clear what problem they would cause. In The case of BufferedWriter, you could get an OutOfMemoryError. In this case it will immediately trigger a GC, which as we have seen will close the file anyway.

Question-28276423, answer-28276497
No, the topmost level Stream or reader will ensure that all underlying streams / readers are closed. Check the close(args) method implementation of your topmost level stream.

Question-28276423, answer-28276498
Assuming all the streams get created okay, yes, just closing bw is fine with those stream implementations ; but that's a big assumption. I'd use try-with-resources ( tutorial ) so that any issues constructing the subsequent streams that throw exceptions don't leave the previous streams hanging, and so you don't have to rely on the stream implementation having the call to close the underlying stream: #pre Note you no longer call close at all. Important note : To have try-with-resources close them, you must assign the streams to variables as you open them, you cannot use nesting. If you use nesting, an exception during construction of one of the later streams (say, GZIPOutputStream ) will leave any stream constructed by the nested calls inside it open. From JLS §14.20.3 : #blockquote Note the word "variables" (my emphasis) . E.g., don't do this: #pre ...because an exception from the GZIPOutputStream(OutputStream) constructor (which says it may throw IOException , and writes a header to the underlying stream) would leave the FileOutputStream open. Since some resources have constructors that may throw and others don't, it's a good habit to just list them separately. We can double-check our interpretation of that JLS section with this program: #pre ...which has the output: #pre Note that there are no calls to close there. If we fix main : #pre then we get the appropriate close calls: #pre (Yes, two calls to InnerMost#close is correct; one is from Middle , the other from try-with-resources.)

Question-29494800, answer-29495065
#li It is checking if newType is array of Objects or not: #pre Why to do that? Because new Object[n] is faster than Array.newInstance #li Array.newInstance(args) creates an array of types defined by the first argument, eg String.class -> String[] . Note that String[].class.getComponentType(args) returns String.class #li You cannot use it like that, but it can be like this Integer[] nums = Arrays.copyOf(args); in this case it depends only on actual type of elements, eg Arrays.copyOf(args); will fail, you cannot write in Integer[] anything but Integer

Question-29494800, answer-29495113
#blockquote (Object)newType == (Object)Object[].class It is checking whether variable newType holds a reference to an instance of java.lang.Class representing type Object[] . The casts are unneeded. #blockquote As far as I can tell, Array.newInstance(args) could be used in both cases, but non-reflective ordinary array construction is likely a bit faster. Thus, I suppose that Object[] is called out as a special case for performance reasons, but I have no idea whether that case is exercised frequently enough for the optimization to be important. #blockquote Integer[] nums = Arrays.copyOf(args) You should use it when you need to copy an array to an array with a possibly different (but compatible) element type, especially when the element types are not statically known. If you know you want the copy to have the same element type as the original, then it's easier to use the original array's clone(args) method.

Question-29494800, answer-29500496
First of all, the cast in that line ((Object)newType == (Object)Object[].class) are absobuletly needed. Removing them will result in a compilation error : #pre Now to answer your question What is this line checking ? It simply verify if the given array is of object type, which is part of the answer for your other question Why Array.newInstance not good enough for both cases? In the first case, we already know that the array is of Object type so there is no point in calling the newInstance method to retrieve the correct type, this would only result in performance loss. As for your final example, Integer[] nums = Arrays.copyOf(args) That it does compile, it is true. Because the given arguments to the method are all valids. It will certainly fail at runtime; what would be the expected output of converting "a" to Integer type ? Now, when to use copyOf ? When you already know the both types, and already know that they are valid together. It main usage is to return a copy but truncated or padded with [null/default values] to the original array.

Question-29494800, answer-29502668
#blockquote It's checking simple equality (likely for the purpose of a micro-optimization, but more on that later). The unusual casting is necessary because Class<T> (the type of Object[].class ) and Class<T> are incomparable types. Basically, for an equality comparison with == to compile, one of the sides has to be a subtype or supertype of the other. I.e. we can't do: #pre The rules for generic types are a bit more complicated and there are a few cases where a comparison doesn't compile, but it may still evaluate to true. The reason Class<T> is not a supertype of Class<T> , despite Object[] being a supertype of all object array types, is that Java generics are invariant without the presence of a wildcard. Another way to do the comparison would be: (newType == (Class<T>)Object[].class) #blockquote #li new Object[...] creates an array the normal way, of a type that is statically known. Remember, the code has just checked that T[] is Object[] . #li Array.newInstance(args) uses reflection to dynamically create an array of the Class type passed in. #blockquote An operation using reflection is generally slower than its non-reflective counterpart. The reflection tutorial says: #blockquote Java SE is filled with micro-optimization like this. The writers of SE try to squeeze everything they can out of it. But I wouldn't be worried about a performance hit in this case: newInstance and copyOf are HotSpot intrinsics . This means that ideally calls to these methods get replaced with machine-specific assembly. Anecdotally, I ran some tests and found the difference between new Object[...] and Array.newInstance(args) to be negligible. The code in the question is probably a relic, although it may still be useful on less well-equipped JVMs. Reflection can also be disabled in certain contexts with strict security (such as an applet), but not typically for a normal desktop application. #blockquote In general, you will probably never use this overload. This overload is only useful if you want to change the type of the array. #li Widening: #pre #li Narrowing: #pre It's more typical to use Arrays.copyOf(T[], int) .

Question-29691513, answer-29727412
The JLS describes a few special behaviors of what it calls constant variables , which are final variables (whether static or not) which are initialized with constant expressions of String or primitive type. Constant variables have a major difference with respect to binary compatibility: the values of constant variables become part of the class's API, as far as the compiler is concerned. An example: #pre Here, XFOO is a "constant variable" and YFOO is not, but they are otherwise equivalent. Class Z prints out each of them. Compile those classes, then disassemble them with javap -v X Y Z , and here is the output: Class X: #pre Class Y: #pre Class Z: #pre Things to notice in the disassembly, which tell you the differences between X and Y run deeper than syntactic sugar: #li XFOO has a ConstantValue attribute, signifying that its value is a compile-time constant. Whereas YFOO does not, and uses a static block with a putstatic instruction to initialize the value at runtime. #li The String constant StringLiteral has become part of class Z 's constant pool , but StringLiteral has not. #li Z.main uses the ldc (load constant) instruction to load StringLiteral onto the stack directly from its own constant pool, but it uses a getstatic instruction to load the value of Y.YFOO . Other differences you will find: #li If you change the value of XFOO and recompile X.java but not Z.java , you have a problem: class Z is still using the old value. If you change the value of YFOO and recompile Y.java , class Z uses the new value whether you recompile Z.java or not. #li If you delete the X.class file entirely, class Z still runs correctly. Z has no runtime dependency on X . Whereas if you delete the Y.class file, class Z fails to initialize with a ClassNotFoundException: Y . #li If you generate documentation for the classes with javadoc, the "Constant Field Values" page will document the value of XFOO , but not the value of YFOO . The JLS describes the above effects constant variables have on compiled class files in §13.1.3 : #blockquote And in §13.4.9 : #blockquote The upshot is that if your public library exposes any constant variables, you must never change their values if your new library version is otherwise supposed to be compatible with code compiled against old versions of the library. It won't necessarily cause an error, but the existing code will probably malfunction since it will have outdated ideas about the values of constants. (If your new library version needs for classes which use it to be recompiled anyway, then changing constants doesn't cause this problem.) Thus, initializing a constant with a block gives you more freedom to change its value, because it prevents the compiler embedding the value into other classes.

Question-29262002, answer-29262126
The JLS specify that #blockquote Now let's see that in detail, Since your takeBiConsumer method is of void type, the lambda receiving new String(args) will interpret it as a block like #pre which is valid in a void, hence the first case compile. However, in the case where the lambda is -> StringLiteral , a block such as #pre is not valid syntax in java. Therefore the only thing to do with "hi" is to try and return it. #pre which is not valid in a void and explain the error message #pre For a better understanding, note that if you change the type of takeBiConsumer to a String, -> StringLiteral will be valid as it will simply try to directly return the string. Note that at first I tought the error was caused by the lambda being in a wrong invocation context, so I'll share this possibility with the community : JLS 15.27 #blockquote However in our case, we are in an invocation context which is correct.

Question-29262002, answer-29262511
The first case is ok because you are invoking a "special" method (a constructor) and you are no actually taking the created object. Just to make it more clear, I'll put the optional braces in your lambdas: #pre And more clear, I'll translate that to the older notation: #pre In the first case you are executing a constructor, but you are NOT returning the created object, in the second case you are attempting to return a String value, but your method in your interface BiConsumer returns void, hence the compiler error.

Question-29262002, answer-29262520
Basicly, new String(args) is an executable piece of code that actually does something (it creates a new String and then returns it). The returned value can be ignored and new String(args) can still be used in void-return lambda to create a new String. However, StringLiteral is just a constant that doesn't do anything on it's own. The only reasonable thing to do with it in lambda body is to return it. But the lambda method would have to have return type String or Object , but it returns void , hence the String cannot be casted to void error.

Question-28681737, answer-28684917
The short answer is: it's safe if you use them safely :) The snarky answer: tell me what you mean by traits, and maybe I'll give you a better answer :) In all seriousness, the term "trait" is not well-defined. Many Java developers are most familiar with traits as they are expressed in Scala, but Scala is far from the first language to have traits, either in name or in effect. For example, in Scala, traits are stateful (can have var variables); in Fortress they are pure behavior. Java's interfaces with default methods are stateless; does this mean they are not traits? (Hint: that was a trick question.) Again, in Scala, traits are composed through linearization; if class A extends traits X and Y , then the order in which X and Y are mixed in determines how conflicts between X and Y are resolved. In Java, this linearization mechanism is not present (it was rejected, in part, because it was too "un-Java-like".) The proximate reason for adding default methods to interfaces was to support interface evolution , but we were well aware that we were going beyond that. Whether you consider that to be "interface evolution++" or "traits--" is a matter of personal interpretation. So, to answer your question about safety ... so long as you stick to what the mechanism actually supports, rather than trying to wishfully stretch it to something it does not support, you should be fine. A key design goal was that, from the perspective of the client of an interface, default methods should be indistinguishable from "regular" interface methods. The default-ness of a method, therefore, is only interesting to the designer and implementor of the interface. Here are some use cases that are well within the design goals: #li Interface evolution. Here, we are adding a new method to an existing interface, which has a sensible default implementation in terms of existing methods on that interface. An example would be adding the forEach method to Collection , where the default implementation is written in terms of the iterator(args) method. #li "Optional" methods. Here, the designer of an interface is saying "Implementors need not implement this method if they are willing to live with the limitations in functionality that entails". For example, Iterator.remove was given a default which throws UnsupportedOperationException ; since the vast majority of implementations of Iterator have this behavior anyway, the default makes this method essentially optional. (If the behavior from AbstractCollection were expressed as defaults on Collection , we might do the same for the mutative methods.) #li Convenience methods. These are methods that are strictly for convenience, again generally implemented in terms of non-default methods on the class. The logger(args) method in your first example is a reasonable illustration of this. #li Combinators. These are compositional methods that instantiate new instances of the interface based on the current instance. For example, the methods Predicate.and(args) or Comparator.thenComparing(args) are examples of combinators. If you provide a default implementation, you should also provide some specification for the default (in the JDK, we use the @implSpec javadoc tag for this) to aid implementors in understanding whether they want to override the method or not. Some defaults, like convenience methods and combinators, are almost never overridden; others, like optional methods, are often overridden. You need to provide enough specification (not just documentation) about what the default promises to do, so the implementor can make a sensible decision about whether they need to override it.

Question-27888429, answer-27888447
You can use Arrays.stream E.g. Arrays.stream(args); You can also use Stream.of as mentioned by @fge , which looks like #pre But note Stream.of(args) will return Stream<T> whereas Arrays.stream(args) will return IntStream providing you pass an array of type int[] . So in a nutshell for primitives type you can observe the difference between 2 methods E.g. #pre When you pass primitive array to Arrays.stream , the following code is invoked #pre and when you pass primitive array to Stream.of the following code is invoked #pre Hence you get different results. Updated : As mentioned by Stuart Marks comment The subrange overload of Arrays.stream is preferable to using Stream.of(args).skip(args).limit(args) because the former results in a SIZED stream whereas the latter does not. The reason is that limit(args) doesn't know whether the size is m or less than m, whereas Arrays.stream does range checks and knows the exact size of the stream You can read the source code for stream implementation returned by Arrays.stream(args) here , whereas for stream implementation returned by Stream.of(args).skip(args).limit(args) is within this method .

Question-27888429, answer-27888452
Alternative to @sol4me's solution: Stream.of(args) Of the difference between this and Arrays.stream(args) : it does make a difference if your array is of a primitive type. For instance, if you do: Arrays.stream(args) where someArray is a long[] , it will return a LongStream . Stream.of(args) , on the other hand, will return a Stream<T> with a single element.

Question-27888429, answer-27888481
Stream.of(args) Or, if you are already have an array, you can also do Stream.of(args) For primitive types use IntStream.of or LongStream.of etc.

Question-29280857, answer-29282558
As you have mentioned there will be 1000-10000 concurrent transaction you expecting at a time than you can store accounts on which some transaction is going on and handle concurrency One Solution is to allow system to create only one object of particulate account id, means that if you want to make a transaction between account "123" and "456" than your thread will create account object and in that constructor of account class we will check if any other object of account is there with particulate account id, if other object of account is there with same account id means that some transaction is going on with particulate account id so you have to wait to get the account object. So we can do transaction between "123" and "456" and at same time we can do transaction between "abc" and "xyz" but if at same time some other thread will try to create object of account "123" than system will say please wait for reference you can see below code Please note : #li dont forgote to remove your account id from locks map by call to freeAccount(BigDecimal accId) from LockHolder class #li I have used HasMap instand of list because list will not be a good choice when you are randomly removing element from it(or when frequently you are updating it) #pre

Question-29280857, answer-29382161
As stated previously, you should lock on both accounts, always in the same order. The key part, however, is ensuring both high granularity and singularity across the VM instance. This can be done using String.intern() : #pre

Question-29095967, answer-29096777
The only solution I come up with for the moment is by implementing your own custom collector. Before reading the solution, I want to add a few notes about this. I took this question more as a programming exercise, I'm not sure if it can be done with a parallel stream. So you have to be aware that it'll silently break if the pipeline is run in parallel . This is not a desirable behavior and should be avoided . This is why I throw an exception in the combiner part (instead of (l1, l2) -> {l1.addAll(args); return l1;} ), as it's used in parallel when combining the two lists, so that you have an exception instead of a wrong result. Also this is not very efficient due to list copying (although it uses a native method to copy the underlying array). So here's the collector implementation: #pre and how to use it: List<T> ll = list.stream(args).collect(args); Output: [[a, b], [c], [d, e]] As the answer of Joop Eggen is out , it appears that it can be done in parallel (give him credit for that!). With that it reduces the custom collector implementation to: #pre which let the paragraph about parallelism a bit obsolete, however I let it as it can be a good reminder. Note that the Stream API is not always a substitute. There are tasks that are easier and more suitable using the streams and there are tasks that are not. In your case, you could also create a utility method for that: #pre and call it like List<T> list = splitBySeparator(args); . It can be improved for checking edge-cases.

Question-29095967, answer-29097694
Please do not vote. I do not have enough place to explain this in comments . This is a solution with a Stream and a foreach but this is strictly equivalent to Alexis's solution or a foreach loop (and less clear, and I could not get rid of the copy constructor) : #pre I understand that you want to find a more elegant solution with Java 8 but I truly think that it has not been designed for this case. And as said by Mr spoon, highly prefer the naive way in this case.

Question-29095967, answer-29098447
The solution is to use Stream.collect . To create a Collector using its builder pattern is already given as solution. The alternative is the other overloaded collect being a tiny bit more primitive. #pre As one sees, I make a list of string lists, where there always is at least one last (empty) string list. #li The first function creates a starting list of string lists. It specifies the result (typed) object. #li The second function is called to process each element. It is an action on the partial result and an element. #li The third is not really used, it comes into play on parallelising the processing, when partial results must be combined. A solution with an accumulator: As @StuartMarks points out, the combiner does not fullfill the contract for parallelism. Due to the comment of @ArnaudDenoyelle a version using reduce . #pre #li The first parameter is the accumulated object. #li The second function accumulates. #li The third is the aforementioned combiner.

Question-29095967, answer-29099896
This is a very interesting problem. I came up with a one line solution. It might not very performant but it works. #pre It is a similar idea that @Rohit Jain came up with. I'm grouping the space between the null values. If you really want a List<T> you may append: List<T> ll = cl.stream(args).collect(args);

Question-29095967, answer-29111023
Although there are several answers already, and an accepted answer, there are still a couple points missing from this topic. First, the consensus seems to be that solving this problem using streams is merely an exercise, and that the conventional for-loop approach is preferable. Second, the answers given thus far have overlooked an approach using array or vector-style techniques that I think improves the streams solution considerably. First, here's a conventional solution, for purposes of discussion and analysis: #pre This is mostly straightforward but there's a bit of subtlety. One point is that a pending sublist from prev to cur is always open. When we encounter null we close it, add it to the result list, and advance prev . After the loop we close the sublist unconditionally. Another observation is that this is a loop over indexes, not over the values themselves, thus we use an arithmetic for-loop instead of the enhanced "for-each" loop. But it suggests that we can stream using the indexes to generate subranges instead of streaming over values and putting the logic into the collector (as was done by Joop Eggen's proposed solution ). Once we've realized that, we can see that each position of null in the input is the delimiter for a sublist: it's the right end of the sublist to the left, and it (plus one) is the left end of the sublist to the right. If we can handle the edge cases, it leads to an approach where we find the indexes at which null elements occur, map them to sublists, and collect the sublists. The resulting code is as follows: #pre Getting the indexes at which null occurs is pretty easy. The stumbling block is adding -1 at the left and size at the right end. I've opted to use Stream.of to do the appending and then flatMapToInt to flatten them out. (I tried several other approaches but this one seemed like the cleanest.) It's a bit more convenient to use arrays for the indexes here. First, the notation for accessing an array is nicer than for a List: indexes[i] vs. indexes.get(args) . Second, using an array avoids boxing. At this point, each index value in the array (except for the last) is one less than the beginning position of a sublist. The index to its immediate right is the end of the sublist. We simply stream over the array and map each pair of indexes into a sublist and collect the output. Discussion The streams approach is slightly shorter than the for-loop version, but it's denser. The for-loop version is familiar, because we do this stuff in Java all the time, but if you're not already aware of what this loop is supposed to be doing, it's not obvious. You might have to simulate a few loop executions before you figure out what prev is doing and why the open sublist has to be closed after the end of the loop. (I initially forgot to have it, but I caught this in testing.) The streams approach is, I think, easier to conceptualize what's going on: get a list (or an array) that indicates the boundaries between sublists. That's an easy streams two-liner. The difficulty, as I mentioned above, is finding a way to tack the edge values onto the ends. If there were a better syntax for doing this, e.g., // Java plus pidgin Scala int[] indexes = [-1] ++ IntStream.range(args) .filter(args) ++ [input.size(args)]; it would make things a lot less cluttered. (What we really need is array or list comprehension.) Once you have the indexes, it's a simple matter to map them into actual sublists and collect them into the result list. And of course this is safe when run in parallel. UPDATE 2016-02-06 Here's a nicer way to create the array of sublist indexes. It's based on the same principles, but it adjusts the index range and adds some conditions to the filter to avoid having to concatenate and flatmap the indexes. #pre

Question-28671903, answer-28672583
The behaviour is (somewhat) documented in the javadoc : #blockquote What this means in practice, when you call source.removeAll(args); : #li if the removals collection is of a smaller size than source , the remove method of HashSet is called, which is fast. #li if the removals collection is of equal or larger size than the source , then removals.contains is called, which is slow for an ArrayList. Quick fix: Collection<T> removals = new HashSet<T>(); Note that there is an open bug that is very similar to what you describe. The bottom line seems to be that it is probably a poor choice but can't be changed because it is documented in the javadoc. For reference, this is the code of removeAll (in Java 8 - haven't checked other versions): #pre

Question-28319064, answer-28319148
I prefer the second way. When you use the first way, if you decide to use a parallel stream to improve performance, you'll have no control over the order in which the elements will be added to the output list by forEach . When you use toList , the Streams API will preserve the order even if you use a parallel stream.

Question-28319064, answer-28319221
Don't worry about any performance differences, they're going to be minimal in this case normally. Method 2 is preferable because #li it doesn't require mutating a collection that exists outside the lambda expression, #li it's more readable because the different steps that are performed in the collection pipeline are written sequentially (first a filter operation, then a map operation, then collecting the result), (for more info on the benefits of collection pipelines, see Martin Fowler's excellent article ) #li you can easily change the way values are collected by replacing the Collector that is used. In some cases you may need to write your own Collector , but then the benefit is that you can easily reuse that.

Question-28319064, answer-28323553
One of the main benefits of using streams is that it gives the ability to process data in a declarative way, that is, using a functional style of programming. It also gives multi-threading capability for free meaning there is no need to write any extra multi-threaded code to make your stream concurrent. Assuming the reason you are exploring this style of programming is that you want to exploit these benefits then your first code sample is potentially not functional since the foreach method is classed as being terminal (meaning that it can produce side-effects). The second way is preferred from functional programming point of view since the map function can accept stateless lambda functions. More explicitly, the lambda passed to the map function should be #li Non-interfering, meaning that the function should not alter the source of the stream if it is non-concurrent (e.g. ArrayList ). #li Stateless to avoid unexpected results when doing parallel processing (caused by thread scheduling differences). Another benefit with the second approach is if the stream is parallel and the collector is concurrent and unordered then these characteristics can provide useful hints to the reduction operation to do the collecting concurrently.

Question-28319064, answer-29000802
There is a third option - using stream(args).toArray(args) - see comments under why didn't stream have a toList method . It turns out to be slower than forEach() or collect(), and less expressive. It might be optimised in later JDK builds, so adding it here just in case. assuming List<T> myFinalList = Arrays.asList(args); with a micro-micro benchmark, 1M entries, 20% nulls and simple transform in doSomething() #pre the results are parallel: #pre sequential: #pre parallel without nulls and filter (so the stream is SIZED ): toArrays has the best performance in such case, and .forEach(args) fails with "indexOutOfBounds" on the recepient ArrayList, had to replace with .forEachOrdered(args) #pre

Question-30888581, answer-30897102
Once given more thought In hindsight it sounds like they were looking for the execute around pattern . They're usually used to do things like enforce closing of streams. This is also more relevant due to this line: #blockquote The idea is you give the thing that does the "executing around" some class to do somethings with. You'll probably use Runnable but it's not necessary. ( Runnable makes the most sense and you'll see why soon.) In your StopWatch class add some method like this #pre You would then call it like this #pre This makes it fool proof. You don't have to worry about handling stop before start or people forgetting to call one and not the other, etc. The reason Runnable is nice is because #li Standard java class, not your own or third party #li End users can put whatever they need in the Runnable to be done. (If you were using it to enforce stream closing then you could put the actions that need to be done with a database connection inside so the end user doesn't need to worry about how to open and close it and you simultaneously force them to close it properly.) If you wanted, you could make some StopWatchWrapper instead leave StopWatch unmodified. You could also make measureAction(args) not return a time and make getTime(args) public instead. The Java 8 way to calling it is even simpler #pre #blockquote Original answer #blockquote To me something like this seems to be good. #pre The reason I believe this to be good is the recording is during object creation so it can't be forgotten or done out of order (can't call stop(args) method if it doesn't exist). One flaw is probably the naming of stop(args) . At first I thought maybe lap(args) but that usually implies a restarting or some sort (or at least recording since last lap/start). Perhaps read(args) would be better? This mimics the action of looking at the time on a stop watch. I chose stop(args) to keep it similar to the original class. The only thing I'm not 100% sure about is how to get the time. To be honest that seems to be a more minor detail. As long as both ... in the above code obtain current time the same way it should be fine.

Question-27908213, answer-27908294
The Oracle documentation about Enum says (first line): #blockquote If this is true then, yes, your enum1.name(args) == enum2.name(args) is guaranteed to be true if the names are the same. Also, in the method name(args) javadoc: #blockquote For example, if you had two enums, Days and MyDays , where SUNDAY is a common value, == between the enum object values, SUNDAY will return true as you are comparing two strings - see the working example in #a . #pre

Question-27908213, answer-27908302
With strings you're going to want to use the equals method of Strings. That aside, you already have the enum that you could compare with the equality operator. Under what scenario would this arise? That being said, yes, the .equals method will return true if they are the same. I'm not sure about equality operator, and without looking it up, I can tell you that it's poor programming to use it if it is.

Question-27908213, answer-27908402
Although there is no explicit guarantee of this, the end result is bound to be such that the comparison always succeeds for enum constants with identical names: #pre The reason for this is that Java compiler constructs subclasses of Enum in such a way that they end up calling Enum 's sole protected constructor, passing it the name of enum value: protected Enum(args); The name is embedded into the generated code in the form of a string literal. According to String documentation, #blockquote This amounts to an implicit guarantee of your expression succeeding when names of enum constants are identical. However, I would not rely on this behavior, and use equals(args) instead, because anyone reading my code would be scratching his head, thinking that I made a mistake.

Question-27845223, answer-27846417
Pavel Horal's answer is nice. As he says, the word "file" has totally different (practically unrelated) meanings in URL#getFile vs java.io.File - may be that's part of the confusion. Just to add: #li A resource in Java is an abstract concept, a source of data that can be read. The location (or address) of a resource is represented in Java by a URL object. #li A resource can correspond to a regular file in the local filesystem (specifically, when its URL begins with file:// ). But a resource is more general (it can be also some file stored in a jar, or some data to be read from the network, or from memory, or...). And it's also more limited, because a File (besides being other things than a regular file: a directory, a link) can also be created and writen to. #li Remember in Java a File object does not really represents "a file" but the location (the full name, with path) of a file. So, a File object allows you to locate (and open) a file, as a URL allows you to access (and open) a resource. (There is no Resource class in Java to represent a resource, but neither there is one to represent a file! once more : File is not a file, it's the path of a file).

Question-30081520, answer-30081600
From Objects.equals() : return(args) || (a != null && a.equals(args)); Very simple, self-explaining and elegant.

Question-28584669, answer-28584932
Your reasoning is that Double.POSITIVE_INFINITY should not be equal to itself because it is “likely” to have been obtained as the result of a loss of accuracy. This line of reasoning applies to all of floating-point. Any finite value can be obtained as the result of an inaccurate operation. That did not push the IEEE 754 standardization committee to define == as always evaluating to false for finite values, so why should infinities be different? As defined, == is useful for people who understand what it does (that is, test the floating-point values that have been obtained , and certainly not the values that should have been obtained with real computations). For anyone who understands that, and you need to understand it to use floating-point even for computations that do not involve infinity, having Double.POSITIVE_INFINITY == Double.POSITIVE_INFINITY evaluate to true is convenient, if only to test if the floating-point result of a floating-point computation is Double.POSITIVE_INFINITY . That leaves the question of why NaN can afford to have special behavior, and infinities should follow the same general principles as finite values. NaN is different from infinities: the underlying principle of the IEEE 754 standard is that values are exactly what they are, but the result of an operation can be approximated with respect to the real result, and in this case, the resulting floating-point value is obtained according to the rounding mode. Forget for an instant that 1.0 / 0.0 is defined as +inf, which is an annoyance in this discussion. Think for the moment of Double.POSITIVE_INFINITY only as the result of operations such as 1.0e100 / 1.0e-300 or Double.MAX_VALUE + Double.MAX_VALUE . For these operations, +inf is the closest approximation of the real result, just like for operations that produce a finite result. By contrast, NaN is the result you obtain when the operation doesn't make sense. It is defensible to have NaN behave specially, but inf is just an approximation of all the values too large to represent. In reality, 1.0 / 0.0 also produces +inf, but that should be considered an exception. It would have been just as coherent to define the result of that operation as NaN, but defining it as +inf was more convenient in the implementation of some algorithms. An example is provided page 10 in Kahan's notes . More details than most will wish for are in the article “Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit” . I would also interpret the existence in IEEE 754 of a “division by zero” flag separate from the NaN flag as recognition that the user may want to treat division by zero specially although it is not defined as producing NaN.

Question-28584669, answer-28584954
Because thats the standard. Infinity represents a number greater than or less than Double.MAX_VALUE/-Double.MAX_VALUE. NaN represents the outcome of an operation that didn't make sense. That is, the operation didn't possibly come out with a number. I would guess the logic is once a number gets big enough (infinity) and because of the limitation of floating point numbers, adding numbers to it won't change the outcome, so its 'like' infinity. So if you want to compare to really big numbers, at some point you might just say those two big numbers are close enough for all intents and purposes. But if you want to compare two things that both aren't numbers, you can't compare them so its false. At least you couldn't compare them as a primitive.

Question-28584669, answer-28588854
The correct answer is a simple "because the standard (and the docs ) say so". But I'm not gonna be cynical because it's obvious that's not what you are after. In addition to the other answers here, I'll try to relate the infinities to saturating arithmetic. Other answers have already stated that the reason the comparisons on NaNs result in true , so I'm not gonna beat a dead horse. Let's say I have a saturating integer that represents grayscale colors. Why am I using saturating arithmetic? Because anything brighter than white is still white, and anything darker than black is still black (except orange ). That means BLACK - x == BLACK and WHITE + x == WHITE . Makes sense? Now, let's say we want to represent those grayscale colors with a (signed) 1s complement 8-bit integer where BLACK == -127 and WHITE == 127 . Why 1s complement? Because it gives us a signed zero like IEEE 754 floating point . And, because we are using saturating arithmetic, -127 - x == -127 and 127 + x == 127 . How does this relate to floating point infinities? Replace the integer with floating point, BLACK with NEGATIVE_INFINITY , and WHITE with POSITIVE_INFINITY and what do you get? NEGATIVE_INFINITY - x == NEGATIVE_INFINITY and POSITIVE_INFINITY + x == POSITIVE_INFINITY . Since you used POSITIVE_INFINITY , I'll use it also. First we need a class to represent our saturating integer-based color; let's call it SaturatedColor and assume it works like any other integer in Java. Now, let's take your code and replace double with our own SaturatedColor and Double.POSITIVE_INFINITY with SaturatedColor.WHITE : #pre As we established above, SaturatedColor.WHITE (just WHITE above) is 127 , so let's do that here: #pre Now we take the System.out.println statements you used and replace a and b with their value (values?): #pre It should be obvious what this will print.

Question-28584669, answer-28590286
Since Double.Nan.equals (Double.NaN) was mentioned: It's one thing what should happen when you perform arithmetic and compare numbers, it's a totally different thing when you consider how objects should behave. Two typical problem cases are: Sorting an array of numbers, and using hash values to implement dictionaries, sets, and so on. There are two exceptional cases where the normal ordering with <, = and > doesn't apply: One case is that +0 = -0 and the other is that NaN ≠ NaN, and x < NaN, x > NaN, x = NaN will always be false whatever x is. Sorting algorithms can get into trouble with this. A sorting algorithm may assume that x = x is always true. So if I know that x is stored in an array and look for it, I might not do any bounds check because the search for it must find something. Not if x is NaN. A sorting algorithm may assume that exactly one of a < b and a >= b must be true. Not if one is NaN. So a naive sorting algorithm may crash when NaNs are present. You'd have to decide where you want NaNs to end up when sorting the array, and then change your comparison code so that it works. Now dictionaries and sets and generally hashing: What if I use an NaN as the key? A set contains unique objects. If the set contains an NaN and I try to add another one, is it unique because it is not equal to the one that is already there? What about +0 and -0, should they be considered equal or different? There's the rule that any two items considered equal must have the same hash value. So the sensible thing is (probably) that a hash function returns one unique value for all NaNs, and one unique value for +0 and -0. And after the hash lookup when you need to find an element with the same hash value that is actually equal, two NaNs should be considered equal (but different from anything else). That's probably why Double.Nan.equal () behaves different from ==.

Question-28584669, answer-28596792
To me, it seems that "because it should behave the same as zero" would make a good answer. Arithmetic overflow and underflow should be handlable similarly. If you underflow from the largest near-infinitesimally small value which can be stored in a float, you get zero, and zeros compare as identical. If you overflow from the largest near-infinitely large value which can be stored in a float, you get INF, and INFs compare as identical. This means that code which handles numbers which are out-of-scope in both directions will not require separate special-casing for one or the other. Instead, either both or neither will need to be treated differently. And the simplest requirement is covered by the "neither" case: you want to check if something over/underflowed, you can compare it to zero/INF using just the normal arithmetic comparison operators, without needing to know you current language's special syntax for the checking command: is it Math.isInfinite(), Float.checkForPositiveInfinity(), hasOverflowed()...?

Question-30217236, answer-30217518
Presumably, you are not calling your generic method in a generic fashion, so it's treated like a call to x(args) . In this example: #pre The first call to x is not made generically so it compiles. The second call equates T to String , so it fails because 1 and 2 are not Strings . The third call compiles because it properly passes in Strings .

Question-30217236, answer-30219144
Why this should be problem in the first place is kind of nebulous to me. I suspect you've instead misunderstood something about the ways in which the type system is useful. What can we do with a <T> void x(args) ? Well, not a whole lot. Inside the body of x , T is the same as Object , so we could only do something like call toString on a and b to print them. There's really no practical reason a and b must have the same type. Just that they have some type in common, and that type is Object or a subtype of it. In fact, there's no clear reason why <T> void x(args) actually needs to be generic at all. #li The method body doesn't care what the actual types of a and b are because it couldn't use them anyway. #li The call site doesn't care what the actual types of a and b are because x is a void method so it's a black hole. It's more typical for a method to have a result, like <T> List<T> Arrays.asList(args) : // This will cause a compile error because // the type inferred must be compatible // with the return assignment. List<T> r = Arrays.asList(args); Or a bound: #pre Or a bound which asserts some kind of relation: #pre

Question-30384955, answer-30385298
Replacing the definition of TbinList with class TbinList<T> extends ArrayList<T> {} and defining test2 with TbinList<T> test2 = new TbinList<T>(); instead would solve the issue. With your definition you're ending up with an ArrayList<T> where T is any fixed class extending Base .

Question-30384955, answer-30385343
This happens because of the way capture conversion works: #blockquote Note the end bit. So, what this means is that, given a type like this: #pre Only "outside" wildcards are captured. The Map key wildcard is captured, but the List element wildcard is not. This is why, for example, we can add to a List<T> , but not a List<T> . The placement of the wildcard is what matters. Carrying this over to TbinList , if we have an ArrayList<T> , the wildcard is in a place where it does not get captured, but if we have a TbinList<T> , the wildcard is in a place where it gets captured. As I alluded to in the comments, one very interesting test is this: ArrayList<T> test3 = new TbinList<T>(); We get this error: #pre So there's no way to make it work as-is. One of the class declarations needs to be changed. Additionally, think about it this way. Suppose we had: #pre And since a wildcard allows subtyping, we can do this: TbinList<T> test4 = new TbinList<T>(); Should we be able to add a Tbin<T> to test4 ? No, this would be heap pollution. We might end up with Derived2 s floating around in a TbinList<T> .

Question-28840047, answer-28840048
This is of course a "feature" . The ConcurrentHashMap.computeIfAbsent() Javadoc reads: #blockquote The "must not" wording is a clear contract, which my algorithm violated, although not for the same concurrency reasons. What's still interesting is that there is no ConcurrentModificationException . Instead, the program just never halts - which still is a rather dangerous bug in my opinion (i.e. infinite loops. or: anything that can possibly go wrong, does ). The simplest use-site solution for this concrete problem would be to not use a ConcurrentHashMap , but just a HashMap instead: static Map<T> cache = new HashMap<T>(); Now, everything works fine. Note: The HashMap.computeIfAbsent() or Map.computeIfAbsent() Javadoc don't forbid such recursive computation, which is of course ridiculous as the type of the cache is Map<T> , not ConcurrentHashMap<T> . It is very dangerous for subtypes to drastically re-define super type contracts ( Set vs. SortedSet is greeting). It should thus be forbidden also in super types, to perform such recursion.

Question-29922296, answer-29922469
My guess is that enums are limited in size (i.e the size is not limited by language but limited by usage)and thus they don't need a native stream api. Streams are very good when you have to manipulate transform and recollect the elements in a stream; these are not common uses case for Enum (usually you iterate over enum values, but rarely you need to transform, map and collect them). If you need only to do an action over each elements perhaps you should expose only a forEach method #pre

Question-29922296, answer-29922831
Three questions: three-part answer: Is it okay from a design point of view? Absolutely. Nothing wrong with it. If you need to do lots of iterating over your enum, the stream API is the clean way to go and hiding the boiler plate behind a little method is fine. Although I’d consider OldCumudgeon ’s version even better. Is it okay from a performance point of view? It most likely doesn’t matter. Most of the time, enums are not that big. Therefore, whatever overhead there is for one method or the other probably doesn’t matter in 99.9% of the cases. Of course, there are the 0.1% where it does. In that case: measure properly, with your real-world data and consumers. If I had to bet, I’d expect the for each loop to be faster, since it maps more directly to the memory model, but don’t guess when talking performance, and don’t tune before there is actual need for tuning. Write your code in a way that is correct first, easy to read second and only then worry about performance of code style. Why aren’t Enums properly integrated into the Stream API? If you compare Java’s Stream API to the equivalent in many other languages, it appears seriously limited. There are various pieces that are missing (reusable Streams and Optionals as Streams, for example). On the other hand, implementing the Stream API was certainly a huge change for the API. It was postponed multiple times for a reason. So I guess Oracle wanted to limit the changes to the most important use cases. Enums aren’t used that much anyway. Sure, every project has a couple of them, but they’re nothing compared to the number of Lists and other Collections. Even when you have an Enum, in many cases you won’t ever iterate over it. Lists and Sets, on the other hand, are probably iterated over almost every time. I assume that these were the reasons why the Enums didn’t get their own adapter to the Stream world. We’ll see whether more of this gets added in future versions. And until then you always can use Arrays.stream .

Question-29922296, answer-29923793
I think the shortest code to get a Stream of enum constants is Stream.of(args) . It's not as nice as Letter.values(args).stream(args) but that's an issue with arrays, not specifically enums. #blockquote You are right that the nicest possible call would be Letter.stream(args) . Unfortunately a class cannot have two methods with the same signature, so it would not be possible to implicitly add a static method stream(args) to every enum (in the same way that every enum has an implicitly added static method values(args) ) as this would break every existing enum that already has a static or instance method without parameters called stream(args) . #blockquote I think so. The drawback is that stream is a static method, so there is no way to avoid code duplication; it would have to be added to every enum separately.

Question-29922296, answer-29926133
I'd go for EnumSet . Because forEach(args) is also defined on Iterable , you can avoid creating the stream altogether: EnumSet.allOf(args).forEach(args); Or with a method reference: EnumSet.allOf(args).forEach(args); Still, the oldschool for-loop feels a bit simpler: #pre

Question-30125296, answer-30125487
I suggest 2 more options: #pre The second one uses Collectors.summingInt() collector. Besides, Java 8 introduces a very effective LongAdder accumulator designed to speed-up summarizing in parallel streams and multi-thread environments. Here, it may be used as follows: #pre

Question-30125296, answer-35035659
You can use reduce method: long creditAmountSum = result.stream(args).map(args).reduce(args);

Question-29333689, answer-29337734
My intuition in Python is to use the collections.Counter as a multiset: #pre So we have a list of integers from 0-99 twice except for 66. #pre Which gets the least common (last item) from a list sorted by frequency. Such a function would be: #pre And usage: #pre This is a generalized solution that works for any number of items.

Question-29333689, answer-29341981
The best answer is already given (XOR-ing the elements), this is to provide an alternative, more general way. If the input array would be sorted (we can make it sorted), we could simply iterate over the elements in pairs (stepping by 2) and if the elements of the "pair" are different, we're done: #pre Note: This solution sorts the input array; if this is unwanted or not allowed, it can be cloned first: arr = arr.clone(args); If input array is sorted, the Arrays.sort(args) call can be left out of course. Generalization The advantage of this solution is that it can be applied to all types which are comparable and therefore can be sorted (types which implement Comparable ), for example String or Date . The XOR solution is limited to numbers only. Here is a slightly modified version which takes an input array of any element type which is comparable: #pre Note: In most cases you could also use arr[i].equals(args) to compare elements instead of using Comparable.compareTo() . For details read the linked javadoc. Quoting the relevant part: #blockquote Now you can call this with a String[] for example: System.out.println(args); Output: 2 Final notes: Starting from the problem statement it is not checked whether there are more than 2 occurrences of the elements, and neither is whether the array length is odd. Also the second example doesn't check for null values, these are to be added if necessary.
