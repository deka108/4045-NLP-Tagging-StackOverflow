Question-28671903
I have a set – a HashSet I want to remove some items from it… none of the items in the "removals" collection will be in the original set. I specify the size of the "source" set and the size of the "removals" collection on the command line, and build both of them. The source set contains only non-negative integers; the removals set contains only negative integers. 
I measure how long it takes to remove all the elements using System.currentTimeMillis(), which isn’t the world most accurate stopwatch but is more than adequate in this case, as you’ll see. Here’s the code:  Let’s start off by giving it an easy job:  Okay, That's fast as I expected. Next i tried source of one million items and 300,000 items to remove?  That still seems pretty speedy.
  Now make it a bit easier – 300,000 source items and 300,000 removals:   Really confused !! can some one explain why this is happening.

Question-28319064
I have a first list : myListToParse where i want to filter the elements and apply a method on each elements and add the result in another list : myFinalList With Java 8 I noticed that I can do it in 2 different way, I would like to know the more efficient way between them and understand why one way is better than the other one. I'm open for any suggestion about a third way. Method 1: myListToParse.stream(_exprs)
        .filter(_exprs)
        .forEach(_exprs); Method 2: 

Question-27908213
Are enum names interned in Java? I.e. is it guaranteed that enum1.name(_exprs) == enum2.name(_exprs) in case of the same name? And is it safe to compare enum.name(_exprs) to a String that is guaranteed to be interned.

Question-27845223
I'm looking at a piece of Java code right now, and it takes a path as a String and gets its URL using URL resource = ClassLoader.getSystemClassLoader(_exprs).getResource(_exprs); , then calls String path = resource.getPath(_exprs) and finally executes new File(_exprs); . Oh, and there are also calls to URL url = resource.toURI(_exprs); and String file = resource.getFile(_exprs) . I'm totally confused right now - mostly because of the terminology, I guess. Can someone please walk me through the differences, or provide a few links to Dummy-proof material? Especially URI to URL and ? To me, it feels like they should be the same thing, respectively... The difference between and is explained here: #a (Interestingly they both seem to return Strings, which probably adds a whole lot to my state of mind...) Now, if I have a locator that references a class or package in a jar file, will those two (i.e. path an file strings) differ? resource.toString(_exprs) would give you jar:file:/C:/path/to/my.jar!/com/example/ , after all (note the exclamation mark). Is the difference between and that the former doesn't encode spaces? Cf. #a (This answer explains the difference between the two terms fairly well: #a ) Lastly - and most importantly - (And is there a Resource object?) Sorry if this question is a bit unorganized; it just reflects the confusion I have... :)

Question-28840047
Some time ago, #a , with a ConcurrentHashMap cache and the new, useful computeIfAbsent(_exprs) method:  I chose ConcurrentHashMap because I was thinking of making this example even more sophisticated by introducing parallelism (which I didn't in the end). Now, let's increase the number from 8 to 25 and observe what happens: System.out.println(_exprs); The program never halts. Inside the method, there's a loop that just runs forever:  I'm using:  #a . This is weird. I would have expected any of the following two: #li It works #li It throws a ConcurrentModificationException But just never halting? That seems dangerous. Is it a bug? Or did I misunderstand some contract?

Question-28724850
Running the following stream example in Java8: System.out.println(_exprs); yields: /a/b/c/d/e/f Which is - of course - no surprise.
Due to #a it shouldn't matter whether the stream is executed sequentially or parallel: #blockquote AFAIK reduce(_exprs) is deterministic and (_exprs) -> s1 + _str + s2 is associative, so that adding parallel(_exprs) should yield the same result: System.out.println(_exprs); However the result on my machine is: /a//b//c//d//e//f What's wrong here? BTW: using (the preferred) .collect(_exprs) instead of reduce(_exprs) yields the same result a/b/c/d/e/f for sequential and parallel execution. JVM details: 

Question-29328785
 Compile >javac BigDecimalTest.java Execute >java BigDecimalTest Output  The value of z.toString(_exprs) in the output is correct 4.883242e+888 / 7.115109e+302 = 6.863200e+585 as is the value of y.toString(_exprs) , but notice that the value given for x.toString(_exprs) is completely wrong. Why is this? Strangely, if the scale (i.e. desired decimal places) of the result of the division is changed BigDecimal z = x.divide(_exprs); then x.toString(_exprs) will produce the correct value for x . Or, if the operands are swapped BigDecimal z = y.divide(_exprs); then x.toString(_exprs) will also then produce the correct value. Or, if the exponent of x is changed from e+888 to e.g. e+878 then x.toString(_exprs) will be correct. Or, if another x.toString(_exprs) call is added above the divide operation, then x.toString(_exprs) calls will produce the correct value! On the machine I'm testing this, Windows 7 64 bit, the behaviour is the same using java 7 and 8, both 32bit and 64 bit versions, but testing online at #a produces different results for java 7 and java 8. Using java 7, the value of x is given correctly: #a , but using java 8 its value is incorrect: #a . Also, this behaviour is not unique to this particular value of x , as calling toString on other BigDecimals with more than about 1500 digits after passing them as the first operand to a divide operation will also produce incorrect values. What is the explanation for this? The divide operation seems to be mutating the value produced by subsequent toString calls on its operands. Does this happen on your platform?  The issue seems to be with the java 8 runtime only, as the above program compiled with java 7 produces correct output when executed with the java 7 runtime, but incorrect output when executed with the java 8 runtime.  I've tested with the early access #a and the bug does not appear, and according to Marco13's answer it was fixed in build 51. The Oracle JDK 8 product binaries are only at update 40 though so it may be some time before the fixed versions are widely used.

Question-29494800
I have trouble understanding the source code of Arrays.copyOf .  #li what is this line checking? (_exprs)newType == (_exprs)Object[].class #li What are the differences between (_exprs) new Object[newLength] and (_exprs) Array.newInstance(_exprs) . why Array.newInstance not good enough for both cases? #li This following line compiles, but crashes at run time (as expected). When should I use this method? Integer[] nums = Arrays.copyOf(_exprs)

Question-28319064, answer-28355204
If you use #a (formerly #a ), you can use the collectIf(_exprs) method.  It evaluates eagerly and should be a bit faster than using a Stream. I am a committer for Eclipse Collections.

Question-28319064, answer-29000802
There is a third option - using stream(_exprs).toArray(_exprs) - see comments under #a . It turns out to be slower than forEach() or collect(), and less expressive. It might be optimised in later JDK builds, so adding it here just in case. assuming List<String> myFinalList = Arrays.asList(_exprs); with a micro-micro benchmark, 1M entries, 20% nulls and simple transform in doSomething()  the results are parallel:  sequential:  parallel without nulls and filter (so the stream is SIZED ):
toArrays has the best performance in such case, and .forEach(_exprs) fails with "indexOutOfBounds" on the recepient ArrayList, had to replace with .forEachOrdered(_exprs) 

Question-29494800, answer-29495065
#li It is checking if newType is array of Objects or not: Why to do that? Because new Object[n] is faster than Array.newInstance #li Array.newInstance(_exprs) creates an array of types defined by the first argument, eg String.class -> String[] . Note that String[].class.getComponentType(_exprs) returns String.class #li You cannot use it like that, but it can be like this Integer[] nums = Arrays.copyOf(_exprs); in this case it depends only on actual type of elements, eg Arrays.copyOf(_exprs); will fail, you cannot write in Integer[] anything but Integer

Question-29494800, answer-29495113
#blockquote (_exprs)newType == (_exprs)Object[].class It is checking whether variable newType holds a reference to an instance of java.lang.Class representing type Object[] .  The casts are unneeded. #blockquote As far as I can tell, Array.newInstance(_exprs) be used in both cases, but non-reflective ordinary array construction is likely a bit faster.  Thus, I suppose that Object[] is called out as a special case for performance reasons, but I have no idea whether that case is exercised frequently enough for the optimization to be important. #blockquote Integer[] nums = Arrays.copyOf(_exprs) You should use it when you need to copy an array to an array with a possibly different (but compatible) element type, especially when the element types are not statically known.  If you know you want the copy to have the same element type as the original, then it's easier to use the original array's clone(_exprs) method.

Question-29494800, answer-29500496
First of all, the cast in that line (_exprs) are absobuletly needed. Removing them will result in a compilation error :  Now to answer your question It simply verify if the given array is of object type, which is part of the answer for your other question In the first case, we already know that the array is of Object type so there is no point in calling the newInstance method to retrieve the correct type, this would only result in performance loss. As for your final example, Integer[] nums = Arrays.copyOf(_exprs) That it does compile, it is true. Because the given arguments to the method are all valids. It will certainly fail at runtime; what would be the expected output of converting "a" to Integer type ? Now, when to use copyOf ? When you already know the both types, and already know that they are valid together. It main usage is to return a copy but truncated or padded with [null/default values] to the original array.

Question-29494800, answer-29502668
#blockquote It's checking simple equality (likely for the purpose of a micro-optimization, but more on that later). The unusual casting is necessary because Class<Object[]> (the type of Object[].class ) and Class<? extends T[]> are incomparable types. Basically, for an equality comparison with == to compile, one of the sides has to be a subtype or supertype of the other. I.e. we can't do:  The rules for generic types are a bit more complicated and there are a few cases where a comparison doesn't compile, but it may still evaluate to true. The reason Class<Object[]> is not a supertype of Class<? extends T[]> , despite Object[] being a supertype of all object array types, is that #a without the presence of a wildcard. Another way to do the comparison would be: (_exprs) #blockquote #li new Object[...] creates an array the normal way, of a type that is statically known. Remember, the code has just checked that T[] is Object[] . #li Array.newInstance(_exprs) uses reflection to dynamically create an array of the Class type passed in. #blockquote An operation using reflection is #a than its non-reflective counterpart. The #a says: #blockquote Java SE is filled with micro-optimization like this. The writers of SE try to squeeze everything they can out of it. But I wouldn't be worried about a performance hit in this case: #a are #a . This means that ideally calls to these methods get replaced with machine-specific assembly. Anecdotally, I ran some tests and found the difference between new Object[...] and Array.newInstance(_exprs) to be negligible. The code in the question is probably a relic, although it may still be useful on less well-equipped JVMs. Reflection can also be disabled in certain contexts with strict security (such as an applet), but not typically for a normal desktop application. #blockquote In general, you will probably never use this overload. This overload is only useful if you want to change the type of the array. #li Widening: #li Narrowing: It's more typical to use #a .

Question-29328785, answer-29330741
It's not so hard to track down the for the odd behavior. The divide call goes to  This, internally, delegates to another divide method, based on the rounding mode:  In this case, the last call applies. Note that the intVal (which is a BigInteger that is stored in the BigDecimal ) is passed directly to this method as the first argument:  Finally, the path to the divideAndRound is taken here, again passing the dividend on (which was the intVal of the original BigDecimal ), ending up with this code:  And this is where the error is introduced: The mdivididend is a BigInteger , that was created as a mutable view on the mag array of the BigInteger that is stored in the BigDecimal x from the original call. The division modifies the mag field, and thus, the state of the (now not-so-immutable) BigDecimal . This is clearly a bug in the implementation of one of the divide methods. I already started tracking the change sets of the OpenJDK, but have not yet spotted the definite culprit. ( ) (A side note: Calling x.toString(_exprs) before doing the division does not really , but only the bug: It causes a string cache of the correct state to be created internally. The right value is printed, but the internal state is still wrong - which is concerning, to say the least...) 

Question-27908213, answer-27908294
The Oracle documentation about Enum says (first line): #blockquote If this is true then, yes, your enum1.name(_exprs) == enum2.name(_exprs) is guaranteed to be true if the names are the same. Also, in the method name(_exprs) javadoc: #blockquote For example, if you had two enums, Days and MyDays , where SUNDAY is a common value, == between the enum object values, SUNDAY will return true as you are comparing two strings - see the working example in #a . 

Question-28459498, answer-28460301
Stream s are built around Spliterator s which are stateful, mutable objects. They don’t have a “reset” action and in fact, requiring to support such rewind action would “take away much power”. How would #a be supposed to handle such a request? On the other hand, for Stream s which have a retraceable origin, it is easy to construct an equivalent Stream to be used again. Just put the steps made to construct the Stream into a reusable method. Keep in mind that repeating these steps is not an expensive operation as all these steps are lazy operations; the actual work starts with the terminal operation and depending on the actual terminal operation entirely different code might get executed. It would be up to you, the writer of such a method, to specify what calling the method twice implies: does it reproduce exactly the same sequence, as streams created for an unmodified array or collection do, or does it produce a stream with a similar semantics but different elements like a stream of random ints or a stream of console input lines, etc. 

Question-28459498, answer-28465634
I think there are very few differences between the two when you look closely enough. At it's face, an IEnumerable does appear to be a reusable construct:  However, the compiler is actually doing a little bit of work to help us out; it generates the following code:  Each time you would actually iterate over the enumerable, the compiler creates an enumerator.  The enumerator is not reusable; further calls to MoveNext will just return false, and there is no way to reset it to the beginning.  If you want to iterate over the numbers again, you will need to create another enumerator instance. 

Question-28459498, answer-28513908
I have some recollections from the early design of the Streams API that might shed some light on the design rationale. Back in 2012, we were adding lambdas to the language, and we wanted a collections-oriented or "bulk data" set of operations, programmed using lambdas, that would facilitate parallelism. The idea of lazily chaining operations together was well established by this point. We also didn't want the intermediate operations to store results. The main issues we needed to decide were what the objects in the chain looked like in the API and how they hooked up to data sources. The sources were often collections, but we also wanted to support data coming from a file or the network, or data generated on-the-fly, e.g., from a random number generator. There were many influences of existing work on the design. Among the more influential were Google's #a library and the Scala collections library. (If anybody is surprised about the influence from Guava, note that #a , Guava lead developer, was on the #a expert group.) On Scala collections, we found this talk by Martin Odersky to be of particular interest: #a . (Stanford EE380, 2011 June 1.) Our prototype design at the time was based around Iterable . The familiar operations filter , map , and so forth were extension (default) methods on Iterable . Calling one added an operation to the chain and returned another Iterable . A terminal operation like count would call iterator(_exprs) up the chain to the source, and the operations were implemented within each stage's Iterator. Since these are Iterables, you can call the iterator(_exprs) method more than once. What should happen then? If the source is a collection, this mostly works fine. Collections are Iterable, and each call to iterator(_exprs) produces a distinct Iterator instance that is independent of any other active instances, and each traverses the collection independently. Great. Now what if the source is one-shot, like reading lines from a file? Maybe the first Iterator should get all the values but the second and subsequent ones should be empty. Maybe the values should be interleaved among the Iterators. Or maybe each Iterator should get all the same values. Then, what if you have two iterators and one gets farther ahead of the other? Somebody will have to buffer up the values in the second Iterator until they're read. Worse, what if you get one Iterator and read all the values, and only get a second Iterator. Where do the values come from now? Is there a requirement for them all to be buffered up somebody wants a second Iterator? Clearly, allowing multiple Iterators over a one-shot source raises a lot of questions. We didn't have good answers for them. We wanted consistent, predictable behavior for what happens if you call iterator(_exprs) twice. This pushed us toward disallowing multiple traversals, making the pipelines one-shot. We also observed others bumping into these issues. In the JDK, most Iterables are collections or collection-like objects, which allow multiple traversal. It isn't specified anywhere, but there seemed to be an unwritten expectation that Iterables allow multiple traversal. A notable exception is the NIO #a interface. Its specification includes this interesting warning: #blockquote [bold in original] This seemed unusual and unpleasant enough that we didn't want to create a whole bunch of new Iterables that might be once-only. This pushed us away from using Iterable. About this time, an #a appeared that described a spot of trouble he'd had with Scala. He'd written this code:  It's pretty straightforward. It parses lines of text into Registrant objects and prints them out twice. Except that it actually only prints them out once. It turns out that he thought that registrants was a collection, when in fact it's an iterator. The second call to foreach encounters an empty iterator, from which all values have been exhausted, so it prints nothing. This kind of experience convinced us that it was very important to have clearly predictable results if multiple traversal is attempted. It also highlighted the importance of distinguishing between lazy pipeline-like structures from actual collections that store data. This in turn drove the separation of the lazy pipeline operations into the new Stream interface and keeping only eager, mutative operations directly on Collections. #a the rationale for that. What about allowing multiple traversal for collection-based pipelines but disallowing it for non-collection-based pipelines? It's inconsistent, but it's sensible. If you're reading values from the network, you can't traverse them again. If you want to traverse them multiple times, you have to pull them into a collection explicitly. But let's explore allowing multiple traversal from collections-based pipelines. Let's say you did this:  (The into operation is now spelled collect(_exprs) .) If source is a collection, then the first into(_exprs) call will create a chain of Iterators back to the source, execute the pipeline operations, and send the results into the destination. The second call to into(_exprs) will create another chain of Iterators, and execute the pipeline operations . This isn't obviously wrong but it does have the effect of performing all the filter and map operations a second time for each element. I think many programmers would have been surprised by this behavior. As I mentioned above, we had been talking to the Guava developers. One of the cool things they have is an #a where they describe features that they decided to implement along with the reasons. The idea of lazy collections sounds pretty cool, but here's what they have to say about it. Consider a List.filter(_exprs) operation that returns a List : #blockquote To take a specific example, what's the cost of get(_exprs) or size(_exprs) on a List? For commonly used classes like ArrayList , they're O(1). But if you call one of these on a lazily-filtered list, it has to run the filter over the backing list, and all of a sudden these operations are O(n). Worse, it has to traverse the backing list on operation. This seemed to us to be laziness. It's one thing to set up some operations and defer actual execution until you so "Go". It's another to set things up in such a way that hides a potentially large amount of recomputation. In proposing to disallow non-linear or "no-reuse" streams, #a described the #a of allowing them as giving rise to "unexpected or confusing results." He also mentioned that parallel execution would make things even trickier. Finally, I'd add that a pipeline operation with side effects would lead to difficult and obscure bugs if the operation were unexpectedly executed multiple times, or at least a different number of times than the programmer expected. (But Java programmers don't write lambda expressions with side effects, do they? DO THEY??) So that's the basic rationale for the Java 8 Streams API design that allows one-shot traversal and that requires a strictly linear (no branching) pipeline. It provides consistent behavior across multiple different stream sources, it clearly separates lazy from eager operations, and it provides a straightforward execution model. 

Question-29229373, answer-33839551
In my free #a library I introduced the short-circuiting collectors. When collecting sequential stream with short-circuiting collector (like #a ) exactly one element is consumed from the source. Internally it's implemented in quite dirty way: using a custom exception to break the control flow. Using my library your sample could be rewritten in this way:  The result is the following: 

Question-29815636, answer-29815698
The first form is called #a .  You use this when you're writing formal APIs for your code, which are generated by the javadoc tool.  For an example, #a uses Javadoc and was generated by that tool. Some common elements you'd see in Javadoc include: #li @param :  this is used to indicate what parameters are being passed to a method, and what value they're expected to have #li @return :  this is used to indicate what result the method is going to give back #li @throws :  this is used to indicate that a method throws an exception or error in case of certain input #li @since :  this is used to indicate the earliest Java version this class or function was available in As an example, here's Javadoc for the compare method of Integer :  The second form is a block (multi-line) comment.  You use this if you want to have multiple lines in a comment. I will say that you'd only want to use the latter form ; that is, you don't want to overburden your code with block comments that don't describe what behaviors the method/complex function is supposed to have. Since Javadoc is the more descriptive of the two, and you can generate actual documentation as a result of using it, using Javadoc would be more preferable to simple block comments.

Question-29140402, answer-29140403
 All Java objects have a toString(_exprs) method, which is invoked when you try and print the object. System.out.println(_exprs);  // invokes myObject.toString(_exprs) This method is defined in the #a class (the superclass of all Java objects). The #a method returns a fairly ugly looking string, composed of the name of the class, an @ symbol and the #a of the object in hexadecimal. The code for this looks like:  A result such as com.foo.MyType@2f92e0f4 can therefore be explained as: #li com.foo.MyType - the name of the class, i.e. the class is MyType in the package com.foo . #li @ - joins the string together #li 2f92e0f4 the hashcode of the object. The name of array classes look a little different, which is explained well in the Javadocs for #a . For instance, [Ljava.lang.String means: #li [ - an single-dimensional array (as opposed to [[ or [[[ etc.) #li L - the array contains a class or interface #li java.lang.String - the type of objects in the array 

Question-29140402, answer-29141638
Every class in java has toString(_exprs) method in it by default, which is called by System.out.println(_exprs) if you pass some object of a class to it. When you try to print object of a class, the System.out.println(_exprs) method will call toString(_exprs) of the class which returns the className@hashcode of that object.  You can override the toString method of a class to get different output. See this example 

Question-30217236, answer-30219144
Why this should be problem in the first place is kind of nebulous to me. I suspect you've instead misunderstood something about the ways in which the type system is useful. What can we do with a <T> void x(_exprs) ? Well, not a whole lot. Inside the body of x , T is the same as Object , so we could only do something like call toString on a and b to print them. There's really no practical a and b must have the same type. Just that they have some type in common, and that type is Object or a subtype of it. In fact, there's no clear reason why <T> void x(_exprs) actually needs to be generic at all. #li The method body doesn't care what the actual types of a and b are because it couldn't use them anyway. #li The call site doesn't care what the actual types of a and b are because x is a void method so it's a black hole. It's more typical for a method to have a result, like <T> List<T> Arrays.asList(_exprs) : // This will cause a compile error because
// the type inferred must be compatible
// with the return assignment.
List<Integer> r = Arrays.asList(_exprs); Or a bound:  Or a bound which asserts some kind of relation: 

Question-28276423, answer-28276493
You can close the outer most stream, in fact you don't need to retain all the streams wrapped and you can use Java 7 try-with-resources.  

Question-28276423, answer-28276497
No, the topmost level Stream or reader will ensure that all streams / readers are closed. Check the close(_exprs) method of your topmost level stream.

Question-28276423, answer-28276498
Assuming all the streams get created okay, yes, just closing bw is fine ; but that's a big assumption. I'd use #a ( #a ) so that any issues constructing the subsequent streams that throw exceptions don't leave the previous streams hanging, and so you don't have to rely on the stream implementation having the call to close the underlying stream:  Note you no longer call close at all. : To have try-with-resources close them, you assign the streams to variables as you open them, you cannot use nesting. If you use nesting, an exception during construction of one of the later streams (say, GZIPOutputStream ) will leave any stream constructed by the nested calls inside it open. From #a : #blockquote Note the word "variables" . E.g., don't do this:  ...because an exception from the #a constructor (which says it may throw IOException , and writes a header to the underlying stream) would leave the FileOutputStream open. Since some resources have constructors that may throw and others don't, it's a good habit to just list them separately. We can double-check our interpretation of that JLS section with this program:  ...which has the output:  Note that there are no calls to close there. If we fix main :  then we get the appropriate close calls:  (Yes, two calls to InnerMost#close is correct; one is from Middle , the other from try-with-resources.)

Question-28840047, answer-28840048
This is of course a . The #a Javadoc reads: #blockquote The wording is a clear contract, which my algorithm violated, although not for the same concurrency reasons. What's still interesting is that there is no ConcurrentModificationException . Instead, the program just never halts - which still is a rather dangerous bug in my opinion (i.e. #a ). The simplest use-site solution for this concrete problem would be to not use a ConcurrentHashMap , but just a HashMap instead: static Map<Integer, Integer> cache = new HashMap<>(_exprs); Now, everything works fine.  The #a or #a Javadoc don't forbid such recursive computation, which is of course ridiculous as the type of the cache is Map<Integer, Integer> , not ConcurrentHashMap<Integer, Integer> . It is very dangerous for subtypes to drastically re-define super type contracts ( Set vs. SortedSet is greeting).

Question-29922296, answer-29922831
Three questions: three-part answer:  Absolutely. Nothing wrong with it. If you need to do lots of iterating over your enum, the stream API is the clean way to go and hiding the boiler plate behind a little method is fine. Although I’d consider #a ’s #a even better.  It most likely doesn’t matter. Most of the time, enums are not that big. Therefore, whatever overhead there is for one method or the other probably doesn’t matter in 99.9% of the cases. Of course, there are the 0.1% where it does. In that case: measure properly, with your real-world data and consumers. If I had to bet, I’d expect the for each loop to be faster, since it maps more directly to the memory model, but don’t guess when talking performance, and don’t tune before there is actual need for tuning. Write your code in a way that is correct first, easy to read second and only then worry about performance of code style.  If you compare Java’s Stream API to the equivalent in many other languages, it appears seriously limited. There are various pieces that are missing (reusable Streams and Optionals as Streams, for example). On the other hand, implementing the Stream API was certainly a huge change for the API. It was postponed multiple times for a reason. So I guess Oracle wanted to limit the changes to the most important use cases. Enums aren’t used that much anyway. Sure, every project has a couple of them, but they’re nothing compared to the number of Lists and other Collections. Even when you have an Enum, in many cases you  won’t ever iterate over it. Lists and Sets, on the other hand, are probably iterated over almost every time. I assume that these were the reasons why the Enums didn’t get their own adapter to the Stream world. We’ll see whether more of this gets added in future versions. And until then you always can use Arrays.stream .

Question-29922296, answer-29923793
I think the shortest code to get a Stream of enum constants is Stream.of(_exprs) . It's not as nice as Letter.values(_exprs).stream(_exprs) but that's an issue with arrays, not specifically enums. #blockquote You are right that the nicest possible call would be Letter.stream(_exprs) . Unfortunately a class cannot have two methods with the same signature, so it would not be possible to implicitly add a static method stream(_exprs) to every enum (in the same way that every enum has an implicitly added static method values(_exprs) ) as this would break every existing enum that already has a static or instance method without parameters called stream(_exprs) . #blockquote I think so. The drawback is that stream is a static method, so there is no way to avoid code duplication; it would have to be added to every enum separately.

Question-30125296, answer-30125423
From the #a #blockquote So, for a map you would use: integers.values(_exprs).stream(_exprs).mapToInt(_exprs).reduce(_exprs); Or: integers.values(_exprs).stream(_exprs).reduce(_exprs);

Question-28681737, answer-28684917
The short answer is: it's safe if you use them safely :) The snarky answer: tell me what mean by traits, and maybe I'll give you a better answer :) In all seriousness, the term "trait" is not well-defined.  Many Java developers are most familiar with traits as they are expressed in Scala, but Scala is far from the first language to have traits, either in name or in effect. For example, in Scala, traits are stateful (can have var variables); in Fortress they are pure behavior.  Java's interfaces with default methods are stateless; does this mean they are not traits?  (Hint: that was a trick question.) Again, in Scala, traits are composed through linearization; if class A extends traits X and Y , then the order in which X and Y are mixed in determines how conflicts between X and Y are resolved.  In Java, this linearization mechanism is not present (it was rejected, in part, because it was too "un-Java-like".) The proximate reason for adding default methods to interfaces was to support , but we were well aware that we were going beyond that.  Whether you consider that to be "interface evolution++" or "traits--" is a matter of personal interpretation.  So, to answer your question about safety ... so long as you stick to what the mechanism actually supports, rather than trying to wishfully stretch it to something it does not support, you should be fine. A key design goal was that, from the perspective of the of an interface, default methods should be indistinguishable from "regular" interface methods.  The default-ness of a method, therefore, is only interesting to the and of the interface. Here are some use cases that are well within the design goals: #li Interface evolution.  Here, we are adding a new method to an existing interface, which has a sensible default implementation in terms of existing methods on that interface.  An example would be adding the forEach method to Collection , where the default implementation is written in terms of the iterator(_exprs) method. #li "Optional" methods.  Here, the designer of an interface is saying "Implementors need not implement this method if they are willing to live with the limitations in functionality that entails".  For example, Iterator.remove was given a default which throws UnsupportedOperationException ; since the vast majority of implementations of Iterator have this behavior anyway, the default makes this method essentially optional.  (If the behavior from AbstractCollection were expressed as defaults on Collection , we might do the same for the mutative methods.) #li Convenience methods.  These are methods that are strictly for convenience, again generally implemented in terms of non-default methods on the class.  The logger(_exprs) method in your first example is a reasonable illustration of this. #li Combinators.  These are compositional methods that instantiate new instances of the interface based on the current instance.  For example, the methods Predicate.and(_exprs) or Comparator.thenComparing(_exprs) are examples of combinators. If you provide a default implementation, you should also provide some specification for the default (in the JDK, we use the @implSpec javadoc tag for this) to aid implementors in understanding whether they want to override the method or not.  Some defaults, like convenience methods and combinators, are almost never overridden; others, like optional methods, are often overridden.  You need to provide enough specification (not just documentation) about what the default promises to do, so the implementor can make a sensible decision about whether they need to override it.

Question-27888429, answer-27888447
You can use Arrays.stream E.g. Arrays.stream(_exprs); You can also use Stream.of as mentioned by @fge , which looks like  But note Stream.of(_exprs) will return Stream<int[]> whereas Arrays.stream(_exprs) will return IntStream providing you pass an array of type int[] . So in a nutshell for primitives type you can observe the difference between 2 methods E.g.  When you pass primitive array to Arrays.stream , the following code is invoked  and when you pass primitive array to Stream.of the following code is invoked  Hence you get different results. : As mentioned by #a comment
The subrange overload of Arrays.stream is preferable to using Stream.of(_exprs).skip(_exprs).limit(_exprs) because the former results in a SIZED stream whereas the latter does not. The reason is that limit(_exprs) doesn't know whether the size is m or less than m, whereas Arrays.stream does range checks and knows the exact size of the stream
You can read the source code for stream implementation returned by Arrays.stream(_exprs) #a , whereas for stream implementation returned by Stream.of(_exprs).skip(_exprs).limit(_exprs) is within #a .

Question-27888429, answer-27888452
Alternative to @sol4me's solution: Stream.of(_exprs) Of the difference between this and Arrays.stream(_exprs) : it make a difference if your array is of a primitive type. For instance, if you do: Arrays.stream(_exprs) where someArray is a long[] , it will return a LongStream . Stream.of(_exprs) , on the other hand, will return a Stream<long[]> with a single element.

Question-27888429, answer-27888481
Stream.of(_exprs) Or, if you are already have an array, you can also do Stream.of(_exprs) For primitive types use IntStream.of or LongStream.of etc.

Question-29280857, answer-29382161
As stated previously, you should lock on both accounts, always in the same order. The key part, however, is ensuring both high granularity and singularity across the VM instance. This can be done using #a : 

Question-29333689, answer-29341981
The best answer is already given (XOR-ing the elements), this is to provide an alternative, more general way. If the input array would be sorted (we can make it sorted), we could simply iterate over the elements in pairs (stepping by 2) and if the elements of the "pair" are different, we're done:  This solution sorts the input array; if this is unwanted or not allowed, it can be cloned first: arr = arr.clone(_exprs); If input array is sorted, the Arrays.sort(_exprs) call can be left out of course.  The advantage of this solution is that it can be applied to all types which are comparable and therefore can be sorted (types which implement #a ), for example #a or #a . The XOR solution is limited to numbers only. Here is a slightly modified version which takes an input array of any element type which is comparable:   #blockquote Now you can call this with a String[] for example: System.out.println(_exprs); Output: 2  Starting from the problem statement it is not checked whether there are more than 2 occurrences of the elements, and neither is whether the array length is odd. Also the second example doesn't check for null values, these are to be added if necessary.
