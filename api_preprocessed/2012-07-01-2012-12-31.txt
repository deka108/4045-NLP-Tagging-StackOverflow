Question-13102045
I am using Scanner methods nextInt(_exprs) and nextLine(_exprs) for reading input. Basically, it looks like this: #pre The problem is that after entering the numerical value, the first input.nextLine(_exprs) is skipped and the second input.nextLine(_exprs) is executed, so that my output looks like this: #pre I tested my application and it looks like the problem lies in using input.nextInt(_exprs) . If I delete it, then both string1 = input.nextLine(_exprs) and string2 = input.nextLine(_exprs) are executed as I want them to be.

Question-11359187
For the first time in my life I find myself in a position where I'm writing a Java API that will be open sourced. Hopefully to be included in many other projects. For logging I (and indeed the people I work with) have always used JUL (java.util.logging) and never had any issues with it. However now I need to understand in more detail what I should do for my API development. I've done some research on this and with the information I've got I just get more confused. Hence this post. Since I come from JUL I'm biased on that. My knowledge of the rest is not that big. From the research I've done I've come up with these reasons why people do not like JUL: #li "I started developing in Java long before Sun released JUL and it was just easier for me to continue with logging-framework-X rather than to learn something new" . Hmm. I'm not kidding, this is actually what people say. With this argument we could all be doing COBOL. (however I can certainly relate to this being a lazy dude myself) #li "I don't like the names of the logging levels in JUL" . Ok, seriously, this is just not enough of a reason to introduce a new dependency. #li "I don't like the standard format of the output from JUL" . Hmm. This is just configuration. You do not even have to do anything code-wise. (true, back in old days you may have had to create your own Formatter class to get it right). #li "I use other libraries that also use logging-framework-X so I thought it easier just to use that one" . This is a cyclic argument, isn't ? Why does 'everybody' use logging-framework-X and not JUL? #li "Everybody else is using logging-framework-X" . This to me is just a special case of the above. Majority is not always right. So the real big question is why not JUL? . What is it I have missed ? The raison d'être for logging facades (SLF4J, JCL) is that multiple logging implementations have existed historically and the reason for that really goes back to the era before JUL as I see it. If JUL was perfect then logging facades wouldn't exist, or what? Rather than embracing them shouldn't we question why they were necessary in the first place? (and see if those reasons still exist) Ok, my research so far has led to a couple of things that I can see may be real issues with JUL: #li Performance . Some say that performance in SLF4J is superior to the rest. This seems to me to be a case of premature optimization. If you need to log hundreds of megabytes per second then I'm not sure you are on the right path anyway. JUL has also evolved and the tests you did on Java 1.4 may no longer be true. You can read about it here and this fix has made it into Java 7. Many also talk about the overhead of string concatenation in logging methods. However template based logging avoids this cost and it exist also in JUL. Personally I never really write template based logging. Too lazy for that. For example if I do this with JUL: log.finest(_exprs)); my IDE will warn me and ask permission that it should change it to: log.log(_exprs); .. which I will of course accept. Permission granted ! Thank you for your help. So I don't actually write such statements myself, that is done by the IDE. In conclusion on the issue of performance I haven't found anything that would suggest that JUL's performance is not ok compared to the competition. #li Configuration from classpath . Out-of-the-box JUL cannot load a configuration file from the classpath. It is a few lines of code to make it do so. I can see why this may be annoying but the solution is short and simple. #li Availability of output handlers . JUL comes with 5 output handlers out-of-the-box: console, file stream, socket and memory. These can be extended or new ones can be written. This may for example be writing to UNIX/Linux Syslog and Windows Event Log. I have personally never had this requirement nor have I seen it used but I can certainly relate to why it may be a useful feature. Logback comes with an appender for Syslog for example. Still I would argue that #li 99.5% of the needs for output destinations are covered by what is in JUL out-of-the-box. #li Special needs could be catered for by custom handlers on top of JUL rather than on top of something else. There's nothing to me that suggests that it takes more time to write a Syslog output handler for JUL than it does for another logging framework. I'm really concerned that there's something I've overlooked. The use of logging facades and logging implementations other than JUL is so widespread that I have to come to the conclusion that it's me who just doesn't understand. That wouldn't be the first time, I'm afraid. :-) So what should I do with my API? I want it to become successful. I can of course just "go with the flow" and implement SLF4J (which seems the most popular these days) but for my own sake I still need to understand exactly what is wrong with the JUL of today that warrants all the fuzz? Will I sabotage myself by choosing JUL for my library ? Testing performance (section added by nolan600 on 07-JUL-2012) There's a reference below from Ceki about SLF4J's parametrization being 10 times or more faster than JUL's. So I've started doing some simple tests. At first glance the claim is certainly correct. Here are the preliminary results (but read on!): #li Execution time SLF4J, backend Logback: 1515 #li Execution time SLF4J, backend JUL: 12938 #li Execution time JUL: 16911 The numbers above are msecs so less is better. So 10 times performance difference is by first actually pretty close. My initial reaction: That is a lot ! Here is the core of the test. As can be seen an integer and a string is construted in a loop which is then used in the log statement: #pre (I wanted the log statement to have both a primitive data type (in this case an int) and a more complex data type (in this case a String). Not sure it matters but there you have it.) The log statement for SLF4J: logger.info(_exprs); The log statement for JUL: logger.log(_exprs); The JVM was 'warmed up' with the same test executed once before the actual measurement was done. Java 1.7.03 was used on Windows 7. Latest versions of SLF4J (v1.6.6) and Logback (v1.0.6) was used. Stdout and stderr was redirected to null device. However, careful now, it turns out JUL is spending most of its time in getSourceClassName(_exprs) because JUL by default prints the source class name in the output, while Logback doesn't. So we are comparing apples and oranges. I have to do the test again and configure the logging implementations in a similar manner so that they actually output the same stuff. I do however suspect that SLF4J+Logback will still come out on top but far from the initial numbers as given above. Stay tuned. Btw: The test was first time I've actually worked with SLF4J or Logback. A pleasant experience. JUL is certainly a lot less welcoming when you are starting out. Testing performance (part 2) (section added by nolan600 on 08-JUL-2012) As it turns out it doesn't really matter for performance how you configure your pattern in JUL, i.e. whether or not it includes the source name or not. I tried with a very simple pattern: java.util.logging.SimpleFormatter.format=_str and that did not change the above timings at all. My profiler revealed that the logger still spent a lot of time in calls to getSourceClassName(_exprs) even if this was not part of my pattern. The pattern doesn't matter. I'm therefore concluding on the issue of performance that at least for the tested template based log statement there seems to be roughly a factor of 10 in real performance difference between JUL (slow) and SLF4J+Logback (quick). Just like Ceki said. I can also see another thing namely that SLF4J's getLogger(_exprs) call is a lot more expensive than JUL's ditto. (95 ms vs 0.3 ms if my profiler is accurate). This makes sense. SLF4J has to do some time on the binding of the underlying logging implementation. This doesn't scare me. These calls should be somewhat rare in the lifetime of an application. The fastness should be in the actual log calls. Final conclusion (section added by nolan600 on 08-JUL-2012) Thank you for all your answers. Contrary to what I initially thought I've ended up deciding to use SLF4J for my API. This is based on a number of things and your input: #li It gives flexibility to choose log implementation at deployment time. #li Issues with lack of flexibility of JUL's configuration when run inside an application server. #li SLF4J is certainly a lot faster as detailed above in particular if you couple it with Logback. Even if this was just a rough test I have reason to believe that a lot more effort has gone into optimization on SLF4J+Logback than on JUL. #li Documentation. The documentation for SLF4J is simply a lot more comprehensive and precise. #li Pattern flexibility. As I did the tests I set out to have JUL mimic the default pattern from Logback. This pattern includes the name of the thread. It turns out JUL cannot do this out of the box. Ok, I haven't missed it until now, but I don't think it is a thing that should be missing from a log framework. Period! #li Most (or many) Java projects today use Maven so adding a dependency is not that big a thing especially if that dependency is rather stable, i.e. doesn't constantly change its API. This seems to be true for SLF4J. Also the SLF4J jar and friends are small in size. So the strange thing that happened was that I actually got quite upset with JUL after having worked a bit with SLF4J. I still regret that it has to be this way with JUL. JUL is far from perfect but kind of does the job. Just not quite well enough. The same can be said about Properties as an example but we do not think about abstracting that so people can plug in their own configuration library and what have you. I think the reason is that Properties comes in just above the bar while the opposite is true for JUL of today ... and in the past it came in at zero because it didn't exist.

Question-12317668
According to the JLS, an int array should be filled by zeros just after initialization. However, I am faced with a situation where it is not. Such a behavior occurs first in JDK 7u4 and also occurs in all later updates (I use 64-bit implementation). The following code throws exception: #pre The exception occurs after the JVM performs compilation of the code block and does not arise with -Xint flag. Additionally, the Arrays.fill(_exprs) statement (as all other statements in this code) is necessary, and the exception does not occurs if it is absent. It is clear that this possible bug is bounded with some JVM optimization. Any ideas for the reason of such a behavior? Update: I see this behavior on HotSpot 64-bit server VM, Java version from 1.7.0_04 to 1.7.0_10 on Gentoo Linux, Debian Linux (both kernel 3.0 version) and MacOS Lion. This error can always be reproduced with the code above. I did not test this problem with a 32-bit JDK or on Windows. I already sent a bug report to the Oracle (bug id 7196857) and it will appear in public Oracle bug database in few days. Update: Oracle published this bug at their public bug database: #a

Question-12146298
I would like to know what is the cleanest and best way to perform form validation of user inputs. I have seen some developers implement org.springframework.validation.Validator . A question about that: I saw it validates a class. Does the class have to be filled manually with the values from the user input, and then passed to the validator? I am confused about the cleanest and best way to validate the user input. I know about the traditional method of using request.getParameter(_exprs) and then manually checking for nulls , but I don't want to do all the validation in my Controller . Some good advice on this area will be greatly appreciated. I am not using Hibernate in this application.

Question-12539365
I write jUnit test cases for 3 purposes: #li To ensure that my code satisfies all of the required functionality, under all (or most of) the input combinations/values. #li To ensure that I can change the implementation, and rely on JUnit test cases to tell me that all my functionality is still satisfied. #li As a documentation of all the use cases my code handles, and act as a spec for refactoring - should the code ever need to be rewritten. (Refactor the code, and if my jUnit tests fail - you probably missed some use case). I do not understand why or when Mockito.verify(_exprs) should be used. When I see verify(_exprs) being called, it is telling me that my jUnit is becoming aware of the implementation. (Thus changing my implementation would break my jUnits, even though my functionality was unaffected). I'm looking for: #li What should be the guidelines for appropriate usage of Mockito.verify(_exprs) ? #li Is it fundamentally correct for jUnits to be aware of, or tightly coupled to, the implementation of the class under test?

Question-12552863
The Java 7 try-with-resources syntax (also known as ARM block ( Automatic Resource Management )) is nice, short and straightforward when using only one AutoCloseable resource. However, I am not sure what is the correct idiom when I need to declare multiple resources that are dependent on each other, for example a FileWriter and a BufferedWriter that wraps it. Of course, this question concerns any case when some AutoCloseable resources are wrapped, not only these two specific classes. I came up with the three following alternatives: 1) The naive idiom I have seen is to declare only the top-level wrapper in the ARM-managed variable: #pre This is nice and short, but it is broken. Because the underlying FileWriter is not declared in a variable, it will never be closed directly in the generated finally block. It will be closed only through the close method of the wrapping BufferedWriter . The problem is, that if an exception is thrown from the bw 's constructor, it's close will not be called and therefore the underlying FileWriter will not be closed . 2) #pre Here, both the underlying and the wrapping resource are declared in the ARM-managed variables, so both of them will be certainly closed, so certainly that the underlying fw.close(_exprs) will be called twice , first directly and for the second time through the wrapping bw.close(_exprs) . This should not be a problem for these two specific classes that both implement Closeable (which is a subtype of AutoCloseable ), whose contract states that multiple calls to close are permitted: #blockquote However, in a general case, I can have resources that do implement only AutoCloseable (and not Closeable ), which doesn't guarantee that the close can be called multiple times: #blockquote 3) #pre This version should be theoretically correct, because only the fw represents a real resource that needs to be cleaned up. The bw does't itself hold any resource, it only delegates to the fw , so it should be sufficient to only close the underlying fw . On the other hand, the syntax is a bit irregular and also, Eclipse issues a warning, which I believe is a false alarm, but it is still a warning that one has to deal with: #blockquote So, which approach to go for? Or have I missed some other idiom that is the correct one?

Question-13195797
I want to delete all files inside ABC directory. When I tried with FileUtils.deleteDirectory(_exprs); it also deletes folder ABC. Is there a one liner solution where I can delete files inside directory but not directory?

Question-12544479
I am looking for the easiest and simplest way to bind and convert data in Spring MVC. If possible, without doing any xml configuration. So far I've been using PropertyEditors like so : #pre and #pre It is simple : both conversion are defined in the same class, and the binding is straightforward. If I wanted to do a general binding across all my controllers, I could still add 3 lines in my xml config . But Spring 3.x introduced a new way to do it, using Converters : #blockquote So let's say I want to use Converters because it is "the latest alternative". I would have to create two converters : #pre First drawback : I have to make two classes. Benefit : no need to cast thanks to genericity. Then, how do I simply data bind the converters ? Second drawback : I haven't found any simple way (annotations or other programmatic facilities) to do it in a controller : nothing like someSpringObject.registerCustomConverter(_exprs); . The only ways I've found would be tedious, not simple, and only about general cross-controller binding : #li XML config : #pre #li Java config ( only in Spring 3.1+ ) : #pre With all these drawbacks, why using Converters ? Am I missing something ? Are there other tricks that I am not aware of ? I am tempted to go on using PropertyEditors... Binding is much easier and quicker.

Question-13291076
If you look in the enum api at the method name(_exprs) it says that: #blockquote Why is better to use toString(_exprs) ? I mean toString may be overridden when name() is already final. So if you use toString and someone overrides it to return a hard-coded value your whole application is down... Also if you look in the sources the toString() method returns exactly and just the name. It's the same thing.

Question-13744450
The interviewer asked me: #blockquote I wasn't aware of these terms. So, when I came back to home, then I started looking into Google about Observer and Observable and found some point from different resources : #blockquote I found this example: #pre But I'm still unable to figure out why do we need Observer and Observable ? What are the setChanged(_exprs) and notifyObservers(_exprs) methods for?

Question-12732422
I'm trying to add header for my request using HttpUrlConnection but the method setRequestProperty(_exprs) doesn't seem working. The server side doesn't receive any request with my header. Can anyone help me? Thanks in advance. #pre

Question-11821801
I'm trying to understand what makes the lock in concurrency so important if one can use synchronized (_exprs) . In the dummy code below, I can do either: #li synchronized the entire method or synchronize the vulnerable area (synchronized(this){...}) #li OR lock the vulnerable code area with a ReentrantLock . Code: #pre

Question-13659217
I declare an enum as : enum Sex {MALE,FEMALE}; And then, iterate enum as shown below : #pre I checked the Java API but can't find the values() method? I'm curious as to where this method comes from? API link : #a

Question-14014086, answer-20784007
Ken's answer is basically right but I'd like to chime in on the "why would you want to use one over the other?" part of your question. Basics The base interface you choose for your repository has two main purposes. First, you allow the Spring Data repository infrastructure to find your interface and trigger the proxy creation so that you inject instances of the interface into clients. The second purpose is to pull in as much functionality as needed into the interface without having to declare extra methods. The common interfaces The Spring Data core library ships with two base interfaces that expose a dedicated set of functionalities: #li CrudRepository - CRUD methods #li PagingAndSortingRepository - methods for pagination and sorting (extends CrudRepository ) Store-specific interfaces The individual store modules (e.g. for JPA or MongoDB) expose store-specific extensions of these base interfaces to allow access to store-specific functionality like flushing or dedicated batching that take some store specifics into account. An example for this is deleteInBatch(_exprs) of JpaRepository which is different from delete(_exprs) as it uses a query to delete the given entities which is more performant but comes with the side effect of not triggering the JPA-defined cascades (as the spec defines it). We generally recommend not to use these base interfaces as they expose the underlying persistence technology to the clients and thus tighten the coupling between them and the repository. Plus, you get a bit away from the original definition of a repository which is basically "a collection of entities". So if you can, stay with PagingAndSortingRepository . Custom repository base interfaces The downside of directly depending on one of the provided base interfaces is two-fold. Both of them might be considered as theoretical but I think they're important to be aware of: #li Depending on a a Spring Data repository interface couples your repository interface to the library. I don't think this is a particular issue as you'll probably use abstractions like Page or Pageable in your code anyway. Spring Data is not any different from any other general purpose library like commons-lang or Guava. As long as it provides reasonable benefit, it's just fine. #li By extending e.g. CrudRepository , you expose a complete set of persistence method at once. This is probably fine in most circumstances as well but you might run into situations where you'd like to gain more fine-grained control over the methods expose, e.g. to create a ReadOnlyRepository that doesn't include the save(_exprs) and delete(_exprs) methods of CrudRepository . The solution to both of these downsides is to craft your own base repository interface or even a set of them. In a lot of applications we've seen something like this: #pre The first repository interface is some general purpose base interface that actually only fixes point 1 but also ties the ID type to be Long for consistency. The second interface usually has all the find…(_exprs) methods copied from CrudRepository and PagingAndSortingRepository but does not expose the manipulating ones. Read more on that approach in the reference documentation . Summary - tl;dr The repository abstraction allows you to pickt the base repository totally driven by you architectural and functional needs. Use the ones provided out of the box if they suit, craft your own repository base interfaces if necessary. Stay away from the store specific repository interfaces unless unavoidable.

Question-12317668, answer-12375312
Here we are faced with a bug in the JIT-compiler. Compiler determines that the allocated array is filled after allocation in Arrays.fill(_exprs) , but the check for uses between the allocation and the fill is faulty. So, compiler performs an illegal optimization - it skips zeroing of allocated array. This bug is placed in Oracle bug tracker ( bug id 7196857 ). Unfortunately, I did not wait for any clarifications from Oracle about the following points. As I see, this bug is OS-specific: it absolutely reproducible on 64-bit Linux and Mac, but, as I see from comments, it reproduces not regularly on Windows (for similar versions of JDK). Additionally it would be nice to know when this bug will be fixed. There is only advice at the moment: do not use JDK1.7.0_04 or later if you depend on JLS for newly declared arrays. Update at October 5: In the new Build 10 of the JDK 7u10 (early access) released at October 04, 2012, this bug was fixed at least for Linux OS (I did not test for other). Thanks to @Makoto, who found that this bug is no longer available for public access in Oracle bug database. Unfortunately, I do not know for the reasons Oracle removed it from public access, but it is available in Google cache . Also, this bug has caught the attention of Redhat: the CVE identifiers CVE-2012-4420 ( bugzilla ) and CVE-2012-4416 ( bugzilla ) were assigned to this flaw.

Question-13604703, answer-13604748
Lambdas are purely a call-site construct: the recipient of the lambda does not need to know that a Lambda is involved, instead it accepts an Interface with the appropriate method. In other words, you define or use a functional interface (i.e. an interface with a single method) that accepts and returns exactly what you want. For this Java 8 comes with a set of commonly-used interface types in java.util.function (thanks to Maurice Naftalin for the hint about the JavaDoc). For this specific use case there's java.util.function.IntBinaryOperator with a single int applyAsInt(int left, int right) method , so you could write your method like this: #pre But you can just as well define your own interface and use it like this: #pre Using your own interface has the advantage that you can have names that more clearly indicate the intent.

Question-13744450, answer-13744816
In very simple terms (because the other answers are referring you to all the official design patterns anyway, so look at them for further details): If you want to have a class which is monitored by other classes in the ecosystem of your program you say that you want the class to be observable. I.e. there might be some changes in its state which you would want to broadcast to the rest of the program. Now, to do this we have to call some kind of method. We don't want the Observable class to be tightly coupled with the classes that are interested in observing it. It doesn't care who it is as long as it fulfils certain criteria. (Imagine it is a radio station, it doesn't care who is listening as long as they have an FM radio tuned on their frequency). To achieve that we use an interface, referred to as the Observer. Therefore, the Observable class will have a list of Observers (i.e. instances implementing the Observer interface methods you might have). Whenever it wants to broadcast something, it just calls the method on all the observers, one after the other. The last thing to close the puzzle is how will the Observable class know who is interested? So the Observable class must offer some mechanism to allow Observers to register their interest. A method such as addObserver(_exprs) internally adds the Observer to the list of observers, so that when something important happens, it loops through the list and calls the respective notification method of the Observer interface of each instance in the list. It might be that in the interview they did not ask you explicitly about the java.util.Observer and java.util.Observable but about the generic concept. The concept is a design pattern, which Java happens to provide support for directly out of the box to help you implement it quickly when you need it. So I would suggest that you understand the concept rather than the actual methods/classes (which you can look up when you need them). UPDATE In response to your comment, the actual java.util.Observable class offers the following facilities: #li Maintaining a list of java.util.Observer instances. New instances interested in being notified can be added through addObserver(_exprs) , and removed through deleteObserver(_exprs) . #li Maintaining an internal state, specifying whether the object has changed since the last notification to the observers. This is useful because it separates the part where you say that the Observable has changed, from the part where you notify the changes. (E.g. Its useful if you have multiple changes happening and you only want to notify at the end of the process rather than at each small step). This is done through setChanged(_exprs) . So you just call it when you changed something to the Observable and you want the rest of the Observers to eventually know about it. #li Notifying all observers that the specific Observable has changed state. This is done through notifyObservers(_exprs) . This checks if the object has actually changed (i.e. a call to setChanged(_exprs) was made) before proceeding with the notification. There are 2 versions, one with no arguments and one with an Object argument, in case you want to pass some extra information with the notification. Internally what happens is that it just iterates through the list of Observer instances and calls the update(Observable o, Object arg) method for each of them. This tells the Observer which was the Observable object that changed (you could be observing more than one), and the extra Object arg to potentially carry some extra information (passed through notifyObservers(_exprs) .

Question-11408427, answer-11408477
The + operator is usually replaced by a StringBuilder at compile time. Check this answer for more details on that matter.

Question-11408427, answer-11408482
The Java language provides special support for the string concatenation operator (+) and for conversion of other objects to strings. String concatenation is implemented through the StringBuilder (or StringBuffer ) class and its append method.

Question-11408427, answer-11408526
Let's look at the following simple expressions in Java #pre The compiler converts _str+x; into a StringBuilder internally and uses .append(_exprs) to "add" the integer to the string. 5.1.11. String Conversion #blockquote 15.18.1. #blockquote The optimized version will not actually do a full wrapped String conversion first. This is a good illustration of an optimized version used by the compiler, albeit without the conversion of a primitive, where you can see the compiler changing things into a StringBuilder in the background: #a This java code: #pre Generates this - see how the two concatenation styles lead to the very same bytecode: #pre Looking at the example above and how the byte code based on the source code in the given example is generated, you will be able to notice that the compiler has internally transformed the following statement cip+ciop; into new StringBuilder(_exprs).append(_exprs).toString(_exprs); In other words, the operator + in string concatenation is effectively a shorthand for the more verbose StringBuilder idiom.

Question-13950338, answer-17066803
Grant Vibration Permission Before you start implementing any vibration code, you have to give your application the permission to vibrate: <uses-permission android:name=_str/> Make sure to include this line in your AndroidManifest.xml file. Import the Vibration Library Most IDEs will do this for you, but here is the import statement if yours doesn't: import android.os.Vibrator; Make sure this in the activity where you want the vibration to occur. How to Vibrate for a Given Time In most circumstances, you'll be wanting to vibrate the device for a short, predetermined amount of time. You can achieve this by using the vibrate(_exprs) method. Here is a quick example: #pre That's it, simple! How to Vibrate Indefinitely It may be the case that you want the device to continue vibrating indefinitely. For this, we use the vibrate(_exprs) method: #pre When you're ready to stop the vibration, just call the cancel(_exprs) method: v.cancel(_exprs); How to use Vibration Patterns If you want a more bespoke vibration, you can attempt to create your own vibration patterns: #pre More Complex Vibrations There are multiple SDKs that offer a more comprehensive range of haptic feedback. One that I use for special effects is Immersion's Haptic Development Platform for Android . Troubleshooting If your device won't vibrate, first make sure that it can vibrate: #pre Secondly, please ensure that you've given your application the permission to vibrate! Refer back to the first point.

Question-13291076, answer-13291108
Use name(_exprs) when you want to make a comparison or use the hardcoded value for some internal use in your code. Use toString(_exprs) when you want to present information to a user (including a developper looking at a log). Never rely in your code on toString(_exprs) giving a specific value. Never test it against a specific string. If your code breaks when someone correctly changes the toString(_exprs) return, then it was already broken. From the javadoc (emphasis mine) : #blockquote

Question-13291076, answer-13291109
It really depends on what you want to do with the returned value: #li If you need to get the exact name used to declare the enum constant, you should use name(_exprs) as toString may have been overriden #li If you want to print the enum constant in a user friendly way, you should use toString which may have been overriden (or not!). When I feel that it might be confusing, I provide a more specific getXXX method, for example: #pre

Question-13291076, answer-13291166
name(_exprs) is a "built-in" method of enum . It is final and you cannot change its implementation. It returns the name of enum constant as it is written, e.g. in upper case, without spaces etc. Compare MOBILE_PHONE_NUMBER and Mobile phone number . Which version is more readable? I believe the second one. This is the difference: name(_exprs) always returns MOBILE_PHONE_NUMBER , toString(_exprs) may be overriden to return Mobile phone number .

Question-13291076, answer-31886637
name() is literally the textual name in the java code of the enum. That means it is limited to strings that can actually appear in your java code, but not all desirable strings are expressible in code. For example, you may need a string that begins with a number. name() will never be able to obtain that string for you.

Question-12576156, answer-12576308
Have you considered @Autowired ing the constructor or a setter and String.split(_exprs) ing in the body? #pre I tend to prefer doing my autowiring in one of these ways to enhance the testability of my code.

Question-12576156, answer-12576394
By specifying the the my.list.of.strings=ABC,CDE,EFG in .properties file and using @Value(_exprs) private String[] myString; You can get the arrays of strings. And using CollectionUtils.addAll(_exprs) , you can get the list of strings.

Question-12539365, answer-12562433
This is great question! I think the root cause of it is the following, we are using JUnit not only for unit testing. So the question should be splited up: #li Should I use Mockito.verify() in my integration (or any other higher-than-unit testing) testing? #li Should I use Mockito.verify() in my black-box unit-testing? #li Should I use Mockito.verify() in my white-box unit-testing? so if we will ignore higher-than-unit testing, the question can be rephrased " Using white-box unit-testing with Mockito.verify() creates great couple between unit test and my could implementation, can I make some "grey-box" unit-testing and what rules of thumb I should use for this ". Now, let's go through all of this step-by-step. *- Should I use Mockito.verify() in my integration (or any other higher-than-unit testing) testing?* I think the answer is clearly no, moreover you shouldn't use mocks for this. Your test should be as close to real application as possible. You are testing complete use case, not isolated part of the application. * black-box vs white-box unit-testing black-box state If you are using white-box approach what is you really doing, you're testing the behaviour of your unit. In this approach calling to Mockito.verify() is essential, you should verify that your unit behaves as you're expecting to. rules of thumbs for grey-box-testing The problem with white-box testing is it creates a high coupling. One possible solution is to do grey-box-testing, not white-box-testing. This is sort of combination of black&white box testing. You are really testing the behaviour of your unit like in white-box testing, but in general you make it implementation-agnostic when possible . When it is possible, you will just make a check like in black-box case, just asserts that output is what is your expected to be. So, the essence of your question is when it is possible. This is really hard. I don't have a good example, but I can give you to examples. In the case that was mentioned above with equals() vs equalsIgnoreCase() you shouldn't call Mockito.verify(), just assert the output. If you couldn't do it, break down your code to the smaller unit, until you can do it. On the other hand, suppose you have some @Service and you are writting @Web-Service that is essentially wrapper upon your @Service - it delegates all calls to the @Service (and making some extra error handling). In this case calling to Mockito.verify() is essential, you shouldn't duplicate all of your checks that you did for the @Serive, verifying that you're calling to @Service with correct parammeter list is sufficient.

Question-12539365, answer-29349910
I must say, that you are absolutely right from a classical approach's point of view: #li If you first create (or change) business logic of your application and then cover it with (adopt) tests ( Test-Last approach ), then it will be very painful and dangerous to let tests know anything about how your software works, other than checking inputs and outputs. #li If you are practicing a Test-Driven approach , then your tests are the first to be written, to be changed and to reflect the use cases of your software's functionality. The implementation depends on tests. That sometimes mean, that you want your software to be implemented in some particular way, e.g. rely on some other component's method or even call it a particular amount of times. That is where Mockito.verify() comes in handy! It is important to remember, that there are no universal tools. The type of software, it's size, company goals and market situation, team skills and many other things influence the decision on which approach to use at your particular case.

Question-12862659, answer-12862662
use the second argument on the Mockito.verify method, as in: verify(_exprs).someMethod(_exprs)

Question-13102045, answer-7056786
It's because when you enter a number then press Enter , input.nextInt(_exprs) consumes only the number, not the "end of line". When input.nextLine(_exprs) executes, it consumes the "end of line" still in the buffer from the first input. Instead, use input.nextLine(_exprs) immediately after input.nextInt(_exprs)

Question-13102045, answer-15046253
It does that because input.nextInt(_exprs); doesn't capture the newline. you could do like the others proposed by adding an input.nextLine(_exprs); underneath. Alternatively you can do it C# style and parse a nextLine to an integer like so: int number = Integer.parseInt(_exprs); Doing this works just as well, and it saves you a line of code.

Question-13102045, answer-22593693
There seem to be many questions about this issue with java.util.Scanner . I think a more readable/idiomatic solution would be to call scanner.skip(_exprs) to drop any newline characters after calling nextInt(_exprs) .

Question-13102045, answer-24907860
Instead of input.nextLine(_exprs) use input.next(_exprs) , that should solve the problem. Modified code: #pre

Question-11774099, answer-11774137
iterate through the whole 65k chars and ask Character.isJavaIdentifierStart(_exprs) . The answer is : "undertie" decimal 8255

Question-11411022, answer-34052602
The best way is the clearest way: * Thread.currentThread(_exprs) == Looper.getMainLooper(_exprs).getThread(_exprs) Or, if the runtime platform is API level 23 (Marshmallow 6.0) or higher: Looper.getMainLooper(_exprs).isCurrentThread(_exprs) See the Looper API . Note that calling Looper.getMainLooper(_exprs) involves synchonization (see the source ). You might want to avoid the overhead by storing the return value and reusing it. * credit greg7gkb

Question-12552863, answer-12619799
I would say don't use ARM and go on with Closeable. Use method like, #pre Also you should consider calling close of BufferedWriter as it is not just delegating the close to FileWriter , but it does some cleanup like flushBuffer .

Question-12552863, answer-12665271
Here's my take on the alternatives: 1) #pre For me, the best thing coming to Java from traditional C++ 15 years ago was that you could trust your program. Even if things are in the muck and going wrong, which they often do, I want the rest of the code to be on best behaviour and smelling of roses. Indeed, the BufferedWriter might throw an exception here. Running out of memory wouldn't be unusual, for instance. For other decorators, do you know which of the java.io wrapper classes throw a checked exception from their constructors? I don't. Doesn't do code understandability much good if you rely upon that sort of obscure knowledge. Also there's the "destruction". If there is an error condition, then you probably don't want to be flushing rubbish to a file that needs deleting (code for that not shown). Although, of course, deleting the file is also another interesting operation to do as error handling. Generally you want finally blocks to be as short and reliable as possible. Adding flushes does not help this goal. For many releases some of the buffering classes in the JDK had a bug where an exception from flush within close caused close on the decorated object not be called. Whilst that has been fixed for some time, expect it from other implementations. 2) #pre We're still flushing in the implicit finally block (now with repeated close - this gets worse as you add more decorators), but the construction is safe and we have to implicit finally blocks so even a failed flush doesn't prevent resource release. 3) #pre There's a bug here. Should be: #pre Some poorly implemented decorators are in fact resource and will need to be closed reliably. Also some streams may need to be closed in a particular way (perhaps they are doing compression and need to write bits to finish off, and can't just flush everything. Verdict Although 3 is a technically superior solution, software development reasons make 2 the better choice. However, try-with-resource is still an inadequate fix and you should stick with the Execute Around idiom, which should have a clearer syntax with closures in Java SE 8.

Question-12552863, answer-16229988
My solution is to do a "extract method" refactoring, as following: #pre printToFile can be written either #pre or #pre For class lib designers, I will suggest them extend the AutoClosable interface with an additional method to suppress the close. In this case we can then manually control the close behavior. For language designers, the lesson is that adding a new feature could mean adding a lot others. In this Java case, obviously ARM feature will work better with a resource ownership transfer mechanism. UPDATE Originally the code above requires @SuppressWarning since the BufferedWriter inside the function requires close(_exprs) . As suggested by a comment, if flush(_exprs) to be called before close the writer, we need to do so before any return (implicit or explicit) statements inside the try block. There is currently no way to ensure the caller doing this I think, so this must be documented for writeFileWriter . UPDATE AGAIN The above update makes @SuppressWarning unnecessary since it require the function to return the resource to the caller, so itself does not necessary being closed. Unfortunately, this pull us back to the beginning of the situation: the warning is now moved back to the caller side. So to properly solve this, we need a customised AutoClosable that whenever it closes, the underline BufferedWriter shall be flush(_exprs) ed. Actually, this shows us another way to bypass the warning, since the BufferWriter is never closed in either way.

Question-12552863, answer-18496449
The first style is the one suggested by Oracle . BufferedWriter doesn't throw checked exceptions, so if any exception is thrown, the program is not expected to recover from it, making resource recover mostly moot. Mostly because it could happen in a thread, with the thread dieing but the program still continuing -- say, there was a temporary memory outage that wasn't long enough to seriously impair the rest of the program. It's a rather corner case, though, and if it happens often enough to make resource leak a problem, the try-with-resources is the least of your problems.

Question-12552863, answer-32302353
To concur with earlier comments: simplest is (2) to use Closeable resources and declare them in order in the try-with-resources clause. If you only have AutoCloseable, you can wrap them in another (nested) class that just checks that close is only called once (Facade Pattern), e.g. by having private bool isClosed; . In practice even Oracle just (1) chains the constructors and doesn't correctly handle exceptions partway through the chain. Alternatively, you can manually create a chained resource, using a static factory method; this encapsulates the chain, and handle cleanup if it fails part-way: #pre The complexity comes from handling multiple exceptions; otherwise it's just "close resources that you've acquired so far". A common practice seems to be to first initialize the variable that holds the object that holds the resource to null (here fileWriter ), and then include a null check in the cleanup, but that seems unnecessary: if the constructor fails, there's nothing to clean up, so we can just let that exception propagate, which simplifies the code a little. You could probably do this generically: #pre

Question-13195797, answer-13195890
FileUtils.cleanDirectory(_exprs); There is this method available in the same file. This will also recursively deletes all sub-folders and files under them. Docs: org.apache.commons.io.FileUtils.cleanDirectory

Question-13195797, answer-20043507
Peter Lawrey's answer is great because it is simple and not depending on anything special, and it's the way you should do it. If you need something that removes subdirectories and their contents as well, use recursion: #pre To spare subdirectories and their contents (part of your question), modify as follows: #pre Or, since you wanted a one-line solution: for (_exprs) if (_exprs) file.delete(_exprs); Using an external library for such a trivial task is not a good idea unless you need this library for something else anyway, in which case it is preferrable to use existing code. You appear to be using the Apache library anyway so use its FileUtils.cleanDirectory(_exprs) method.

Question-11821801, answer-11821900
A ReentrantLock is unstructured , unlike synchronized constructs -- i.e. you don't need to use a block structure for locking and can even hold a lock across methods. An example: #pre Such flow is impossible to represent via a single monitor in a synchronized construct. Aside from that, ReentrantLock supports lock polling and interruptible lock waits that support time-out . ReentrantLock also has support for configurable fairness policy , allowing more flexible thread scheduling. #blockquote ReentrantLock may also be more scalable , performing much better under higher contention. You can read more about this here . This claim has been contested, however; see the following comment: #blockquote When should you use ReentrantLock s? According to that developerWorks article... #blockquote

Question-11821801, answer-36827191
From oracle documentation page about ReentrantLock : #blockquote #li A ReentrantLock is owned by the thread last successfully locking, but not yet unlocking it. A thread invoking lock will return, successfully acquiring the lock, when the lock is not owned by another thread. The method will return immediately if the current thread already owns the lock. #li The constructor for this class accepts an optional fairness parameter. When set true, under contention, locks favor granting access to the longest-waiting thread . Otherwise this lock does not guarantee any particular access order. ReentrantLock key features as per this article #li Ability to lock interruptibly. #li Ability to timeout while waiting for lock. #li Power to create fair lock. #li API to get list of waiting thread for lock. #li Flexibility to try for lock without blocking. You can use ReentrantReadWriteLock.ReadLock, ReentrantReadWriteLock.WriteLock to further acquire control on granular locking on read and write operations. Have a look at this article by Benjamen on usage of different type of ReentrantLocks

Question-11359187, answer-11359320
#li java.util.logging was introduced in Java 1.4. There were uses for logging before that, that's why many other logging APIs exist. Those APIs where used heavily before Java 1.4 and thus had a great marketshare that didn't just drop to 0 when 1.4 was release. #li JUL didn't start out all that great, many of the things you mentioned where a lot worse in 1.4 and only got better in 1.5 (and I guess in 6 as well, but I'm not too sure). #li JUL isn't well suited for multiple applications with different configurations in the same JVM (think multiple web applications that should not interact). Tomcat needs to jump through some hoops to get that working (effectively re-implementing JUL if I understood that correctly). #li You can't always influence what logging framework your libraries use. Therefore using SLF4J (which is actually just a very thin API layer above other libraries) helps keeping a somewhat consistent picture of the entire logging world (so you can decide the underlying logging framework while still having library logging in the same system). #li Libraries can't easily change. If a previous version of a library used to use logging-library-X it can't easily switch to logging-library-Y (for example JUL), even if the latter is clearly superious: any user of that library would need to learn the new logging framework and (at least) reconfigure their logging. That's a big no-no, especially when it brings no apparent gain to most people. Having said all that I think JUL is at least a valid alternative to other logging frameworks these days.

Question-12468764, answer-13368831
Actual Answer: The default deserializer for enums uses .name(_exprs) to deserialize, so it's not using the @JsonValue . So as @OldCurmudgeon pointed out, you'd need to pass in {_str: _str} to match the .name(_exprs) value. An other option (assuming you want the write and read json values to be the same)... More Info: There is (yet) another way to manage the serialization and deserialization process with Jackson. You can specify these annotations to use your own custom serializer and deserializer: #pre Then you have to write MySerializer and MyDeserializer which look like this: MySerializer #pre MyDeserializer #pre Last little bit, particularly for doing this to an enum JsonEnum that serializes with the method getYourValue(_exprs) , your serializer and deserializer might look like this: #pre

Question-12468764, answer-24537575
I've found a very nice and concise solution, especially useful when you cannot modify enum classes as it was in my case. Then you should provide a custom ObjectMapper with a certain feature enabled. Those features are available since Jackson 1.6. So you only need to write toString(_exprs) method in your enum. #pre There are more enum-related features available, see here: #a #a

Question-13659217, answer-13659231
You can't see this method in javadoc because it's added by the compiler. Documented in three places : #li Enum Types , The Java Tutorials #blockquote #li Enum.valueOf class (The special implicit values method is mentioned in description of valueOf method) #blockquote #li Enum Types, Section 8.9 , Java Language Specification The values function simply list all values of the enumeration.

Question-12575990, answer-12576219
A Java Date is a container for the number of milliseconds since January 1, 1970, 00:00:00 GMT. When you use something like System.out.println(_exprs) , Java uses Date.toString(_exprs) to print the contents. The only way to change it is to override Date and provide your own implementation of Date.toString(_exprs) . Now before you fire up your IDE and try this, I wouldn't; it will only complicate matters. You are better off formatting the date to the format you want to use (or display). What you can do, is format the date. #pre These are actually the same date, represented differently.
