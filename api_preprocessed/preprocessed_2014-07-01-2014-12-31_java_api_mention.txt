{
    "items": [
        {
            "cleaned": "I don't know whether I am asking the right question. I was exploring RecyclerView and I was surprised to see that RecyclerView does not have onItemClickListener(_exprs) . Because RecyclerView extends #blockquote and ListView extends #blockquote .  However I solved my problem by writing onClick in my RecyclerView.Adapter : #pre But still I want to know why Google removed onItemClickListener(_exprs) ? Is there a performance issue or something else?",
            "question_id": 24885223
        },
        {
            "cleaned": "I am looking for a concise way to convert an #a to a #a or more specifically to \"view\" the iterator as a stream. For performance reason, I would like to avoid a copy of the iterator in a new list: #pre ",
            "question_id": 24511052
        },
        {
            "cleaned": "In Java 8, what's the difference between #a and #a methods?",
            "question_id": 26684562
        },
        {
            "cleaned": "The Java language specification specifies that #blockquote So, considering arrays are objects \u2014 why did the Java designers make the decision not to allow inherit and override from it, for example, #a or equals(_exprs) ? The current syntax wouldn't allow creating anonymous classes with an array as the base class, but I don't think was the reason for their decision.",
            "question_id": 27504925
        },
        {
            "cleaned": "I just studied about generic programming, the List<E> interface, and ArrayList , so I can understand the statement below. #pre But I don't understand the next statement which I saw while surfing the web. #pre #li What is Collections ? Why isn't it Collections<E> or Collections<String> ? #li Why is <String> placed before the method name emptyList ? (Isn't emptyList<String>(_exprs) correct for Generic?) #li What does the statement mean?",
            "question_id": 27683759
        },
        {
            "cleaned": "Why is i++ not atomic in Java? To get a bit deeper in Java I tried to count how often the loop in threads are executed. So I used a #pre in the main class. I have two threads. #li Thread 1: Prints System.out.println(_exprs); #li Thread 2: Prints System.out.println(_exprs); And I count the lines printed by thread 1 and thread 2. But the lines of thread 1 + lines of thread 2 don't match the total number of lines printed out. Here is my code: #pre",
            "question_id": 25168062
        },
        {
            "cleaned": "I was being taught that Marker interface in Java is an empty interface and is used to signal to compiler or JVM that the objects of the class implementing this interface must be treated in a special way, like serializing, cloning, etc. But lately I have learned that it actually has nothing to do with the compiler or the JVM. For example, in case of Serializable interface the method writeObject(_exprs) of ObjectOutputStream does something like instanceOf Serializable to detect whether the class implements Serializable & throws NotSerializableException accordingly.\nEverything is handled in the code and this seems to be a design-pattern so I think we can define our own marker interfaces. Now my doubts: #li Is the definition of a marker interface mentioned above in 1st point wrong? How can we define a Marker interface then? #li And instead of using the instanceOf operator why can't the method be something like writeObject(_exprs) so that there is a compile-time type checking rather than runtime? #li How are Annotations better than Marker Interfaces?",
            "question_id": 25850328
        },
        {
            "cleaned": "The Java 8 Collectors.toMap throws a NullPointerException if one of the values is 'null'. I don't understand this behaviour, maps can contain null pointers as value without any problems. Is there a good reason why values cannot be null for Collectors.toMap ? Also, is there a nice Java 8 way of fixing this, or should I revert to plain old for loop? An example of my problem: #pre Stacktrace: #pre",
            "question_id": 24630963
        },
        {
            "cleaned": "How is the compiler not complaining when I write the following code? #pre Even though it is an instance of the same class in which testPrivate is written, shouldn't it give a compilation error at System.out.println(_exprs) ? After all, I am trying to access a private variable directly.",
            "question_id": 27482579
        },
        {
            "cleaned": "Google Guava has a predicate that #a . Does Java 8 have something similar for its Predicate ? I know I could use (_exprs)->{return true;} , but I want something pre-made, analogous to Collections.emptySet(_exprs) .",
            "question_id": 26549659
        },
        {
            "cleaned": "I am getting following exception while running the tests. I am using Mockito for mocking. The hints mentioned by Mockito library are not helping. #pre Test Code from DomainTestFactory. When I run the following test, I see the exception #pre",
            "question_id": 26318569
        },
        {
            "cleaned": "In Java 8, the Collection interface was extended with two methods that return Stream<E> : stream(_exprs) , which returns a sequential stream, and parallelStream(_exprs) , which returns a possibly-parallel stream.  Stream itself also has a parallel(_exprs) method that returns an equivalent parallel stream (either mutating the current stream to be parallel or creating a new stream). The duplication has obvious disadvantages: #li It's confusing.  A question asks #a , given that parallelStream() may return a sequential stream.  Why does parallelStream() exist if it can't make a guarantee?  The other way around is also confusing -- if parallelStream() returns a sequential stream, there's probably a reason (e.g., an inherently sequential data structure for which parallel streams are a performance trap); what should Stream.parallel() do for such a stream?  (UnsupportedOperationException is not allowed by parallel()'s specification.) #li Adding methods to an interface risks conflicts if an existing implementation has a similarly-named method with an incompatible return type.  Adding parallelStream() in addition to stream() doubles the risk for little gain.  (Note that parallelStream() was at one point just named parallel(), though I don't know if it was renamed to avoid name clashes or for another reason.) Why does Collection.parallelStream() exist when calling Collection.stream().parallel() does the same thing?",
            "question_id": 24603186
        },
        {
            "cleaned": "What's the main difference between scheduleAtFixedRate and scheduleWithFixedDelay methods of #a ? #pre they print exact the same time, seems they are executed at exact the same interval.",
            "question_id": 24649842
        },
        {
            "cleaned": "Thanks to @marmor, I updated my answer. I think it's a good solution to handle the onClick() in the class constructor and pass it to the parent class via interface.  #pre Usage of adapter in other classes:  #pre",
            "question_id": 24885223,
            "answer_id": 29841832
        },
        {
            "cleaned": "Use the #a method from #a : #pre",
            "question_id": 26791441,
            "answer_id": 26791724
        },
        {
            "cleaned": "Search for the first word to be replaced. If it's in the string, recurse on the the part of the string before the occurrence, and on the part of the string after the occurrence. Otherwise, continue with the next word to be replaced. A naive implementation might look like this #pre Sample usage: #pre Output: #pre ",
            "question_id": 26791441,
            "answer_id": 26791826
        },
        {
            "cleaned": " If there is only one occurrence of each of the swapable strings in the input, you can do the following: Before proceeding to any replace, get the indices of the occurrences of the words. After that we only replace the word found at these indexes, and not all occurrences. This solution uses StringBuilder and does not produce intermediate String s like String.replace(_exprs) . One thing to note: if the swapable words have different lengths, after the first replace the second index might change (if the 1st word occurs before the 2nd) exactly with the difference of the 2 lengths. So aligning the second index will ensure this works even if we're swapping words with different lengths. #pre  Analogous to the previous case we will first collect the indexes (occurrences) of the words, but in this case it will a list of integers for each word, not just one int . For this we will use the following utility method: #pre And using this we will replace the words with the other one by decreasing index (which might require to alternate between the 2 swapable words) so that we won't even have to correct the indices after a replace: #pre",
            "question_id": 26791441,
            "answer_id": 26796725
        },
        {
            "cleaned": "This is not an easy problem. And the more search-replacement parameters you have, the trickier it gets. You have several options, scattered on the palette of ugly-elegant, efficient-wasteful: #li Use StringUtils.replaceEach from Apache Commons as #a recommended. This is a good option if you're free to add new dependencies in your project. You might get lucky: the dependency might be included already in your project #li Use a temporary placeholder as #a suggested, and perform the replacement in 2 steps: #li Replace all search patterns with a unique tag that doesn't exist in the original text #li Replace the placeholders with the real target replacement This is not a great approach, for several reasons: it needs to ensure that the tags used in the first step are really unique; it performs more string replacement operations than really necessary #li Build a regex from all the patterns and use the method with #a as suggested by #a . This is not terrible, but not that great either, as building the regex is kind of hackish, and it involves StringBuffer which went out of fashion a while ago in favor of StringBuilder . #li Use a recursive solution proposed by #a , by splitting the string at the matched patterns, and recursing on the remaining segments. This is a fine solution, compact and quite elegant. Its weakness is the potentially many substring and concatenation operations, and the stack size limits that apply to all recursive solutions #li Split the text to words and use Java 8 streams to perform the replacements elegantly as #a suggested, but of course that only works if you are ok with splitting at word boundaries, which makes it not suitable as a general solution Here's my version, based on ideas borrowed from #a . It's neither simple nor elegant, but it works, and should be relatively efficient, without unnecessary steps. In a nutshell, it works like this: repeatedly find the next matching search pattern in the text, and use a StringBuilder to accumulate the unmatched segments and the replacements. #pre Unit tests: #pre",
            "question_id": 26791441,
            "answer_id": 26819171
        },
        {
            "cleaned": "It's easy to write a method to do this using #a : #pre Testing: #pre Output: #blockquote It is not immediately obvious, but a function like this can still be dependent on the order in which the replacements are specified. Consider: #pre Output: #blockquote But reverse the replacements: #pre Output: #blockquote Oops! :) Therefore it is sometimes useful to make sure to look for the match (as PHP's #a function does, for example). This version of the method will do that: #pre Note that the above methods are case-sensitive. If you need a case-insensitive version it is easy to modify the above because String.regionMatches can take an ignoreCase parameter.",
            "question_id": 26791441,
            "answer_id": 26826286
        },
        {
            "cleaned": "What I can only share is my own method. You can use a temporary String temp = _str; or String.Format(_exprs); This is my example code created in console application via #a . #pre Or you can also use the String.Format(_exprs); #pre time upon a Once, there was a bar and a foo.",
            "question_id": 26791441,
            "answer_id": 26877910
        },
        {
            "cleaned": "You are performing multiple search-replace operations on the input. This will produce undesired results when the replacement strings contain search strings. Consider the foo->bar, bar-foo example, here are the results for each iteration: #li Once upon a time, there was a foo and a bar. (input) #li Once upon a time, there was a bar and a bar. (foo->bar) #li Once upon a time, there was a foo and a foo. (bar->foo, output) You need to perform the replacement in one iteration without going back. A brute-force solution is as follows: #li Search the input from current position to end for multiple search strings until a match is found #li Replace the matched search string with corresponding replace string #li Set current position to the next character after the replaced string #li Repeat A function such as String.indexOfAny(_exprs) -> int[]{index, whichString} would be useful. Here is an example (not the most efficient one): #pre Some tests: #pre #a",
            "question_id": 26791441,
            "answer_id": 26886202
        },
        {
            "cleaned": " a) Both libraries use immutable types. Joda-Time also offers additional mutable types like MutableDateTime . b) Furthermore: Both libraries are inspired by the design study #a or ideas from #a so they strive more or less for a #a (although not always perfect ;-)). c) With both libraries we get a real calendar date type (called LocalDate ), a real wall time type (called LocalTime ) and the composition (called LocalDateTime ). That is a very big win compared with old java.util.Calendar and java.util.Date . d) Both libraries use a method-centric approach meaning they encourage the user to use getDayOfYear(_exprs) instead of get(_exprs) . This causes a lot of extra methods compared with java.util.Calendar (although latter is not type-safe at all due to excessive use of ints).  See the other answer by @OO7 pointing to the analysis of Mikhail Vorontsov although point 3 (exception catching) is probably obsolete - see #a . The different performance (which is in general favour of #a ) is mainly due to the fact that the internal implementation of #a always use a machine-time-like long-primitive (in milliseconds).  Joda-Time often use NULL as default for system timezone, default locale, current timestamp etc. while JSR-310 almost always rejects NULL values.  JSR-310 handles #a precision while Joda-Time is limited to #a precision.  An overview about supported fields in Java-8 (JSR-310) is given by some classes in the temporal-package (for example #a and #a ) while Joda-Time is rather weak on this area - see #a . The biggest lack of Joda-Time is here the absence of localized week-related fields. A common feature of both field implementation design is that both are based on values of type long (no other types, not even enums).  JSR-310 offers #a like DayOfWeek or Month while Joda-Time does not offer this because it was mainly developed in years 2002-2004 before #a .  a) JSR-310 offers more timezone features than Joda-Time. Latter is not able to yield a programmatical access to the history of timezone offset transitions while JSR-310 is capable to do this. b) For your information: JSR-310 has moved its internal timezone repository to a new location and a different format. The old library folder lib/zi does not exist any more.  JSR-310 has introduced the TemporalAdjuster -interface as a formalized way to externalize temporal calculations and manipulations, especially for library or framework-writers this is a nice and relative easy way to embed new extensions of JSR-310 (a kind of equivalent to static helper classes for former java.util.Date ). For most users however, this feature has very limited value because the burden to write code is still with the user. Built-in solutions based on the new TemporalAdjuster -concept are not so many, there is currently only the helper class TemporalAdjusters with a limited set of manipulations (and the enums Month or other temporal types). Joda-Time offers a field-package but practice has shown evidence that new field implementations are very hard to code. On the other side Joda-Time offers so-called properties which make some manipulations much easier and more elegant than in JSR-310, for example #a .  JSR-310 offers 4 extra calendar systems. The most interesting one is #a (used in Saudi Arabia). The other 3 are: #a (Taiwan), Japanese (only the modern calendar since 1871!) and #a (only correct after 1940). Joda-Time offers an #a based on calculatory base - not a sighting-based calendar like Umalqura. Thai-Buddhist is also offered by Joda-Time in a similar form, Minguo and the japanese one not. Otherwise Joda-Time offers coptic and ethiopic calendar, too (but without any support for internationalization). More interesting for Europeans: Joda-Time also offers a #a , #a and mixed-gregorian-julian calendar. However, the practical value for real historical calculations is limited because important features like different year starts in date history are not supported at all (the same criticism is valid for old java.util.GregorianCalendar ). Other calendars like #a or #a or #a are completely missing in both libraries.  JSR-310 has the class #a while Joda-Time (version 2.0) offers some helper methods in the class #a .  JSR-310 has no interface (a design mistake) but an abstract class java.time.Clock which can be used for any clock dependency injection. Joda-Time offers the interface #a and some helper methods in #a instead. So this way Joda-Time is also capable of supporting test-driven models with different clocks (mocking etc.).  Both libraries support the calculation of time distances in one or more temporal units. However, when handling single-unit-durations the JSR-310-style is obviously nicer (and long-based instead of using int): JSR-310 => long days = ChronoUnit.DAYS.between(_exprs); Joda-Time => int days = DAYS.daysBetween(_exprs).getDays(_exprs); Handling of multiple-unit-durations are also different. Even the calculation results can differ - see this closed #a . While JSR-310 use a very simple and limited approach to use just the classes Period (duration based on years, months and days) and Duration (based on seconds and nanoseconds), Joda-Time uses a more sophisticated way using the class PeriodType in order to control in which units a duration (Joda-Time call it \"Period\") shall be expressed. While the PeriodType -API is somehow awkward to use a similar way is not offered by JSR-310 at all. Especially it is not yet possible in JSR-310 to define mixed date and time durations (based on days and hours for example). So be warned if it comes to migration from one library to another. The libraries in discussion are incompatible - despite of partially same class names.  JSR-310 does not support this feature while Joda-Time has limited support. See also this #a .  Best way to compare both libraries is to view the equal-named classes #a (JSR-310) and #a (Joda-Time). The JSR-310-variant is a little bit more powerful (can also handle any kind of TemporalField provided the field implementor has managed to code some extension points like #a ). Most important difference is however - in my opinion: JSR-310 can much better parse timezone names (format pattern symbol z) while Joda-Time could not do this at all in its earlier versions and now only in a very limited way. Another advantage of JSR-310 is support for standalone month names which is important in languages like Russian or Polish etc. Joda-Time has #a - not even on Java-8 platforms. The pattern syntax in JSR-310 is also more flexible than in Joda-Time, allows for optional sections (using square brackets), is more orientated towards CLDR-standard and offers padding (letter symbol p) and more fields. Otherwise it should be noted that Joda-Time can format durations using #a . JSR-310 cannot do this. ",
            "question_id": 24631909,
            "answer_id": 24635657
        },
        {
            "cleaned": "Because nextDouble works like this: ( #a ) #pre next(_exprs) makes x random bits. Now why does this matter? Because about half the numbers generated by the first part (before the division) are less than 1L << 52 , and therefore their significand doesn't entirely fill the 53 bits that it could fill, meaning the least significant bit of the significand is always zero for those. ",
            "question_id": 27625611,
            "answer_id": 27625722
        },
        {
            "cleaned": "It is an integer overflow. The int data type is 4 bytes, or 32 bits.  Therefore, numbers larger than 2^(32 - 1) - 1 (2,147,483,647) cannot be stored in this data type.  Your numerical values will be incorrect. For very large numbers, you will want to import and use the class java.math.BigInteger: #pre NOTE: For numerical values that are still too large for the int data type, but small enough to fit within 8 bytes (absolute value less than or equal to 2^(64 - 1) - 1), you should probably use the long primitive. HackerRank's practice problems (www.hackerrank.com), such as the Algorithms practice section, ( #a ) include some very good large-number questions that give good practice about how to think about the appropriate data type to use.",
            "question_id": 26375932,
            "answer_id": 26407637
        },
        {
            "cleaned": "Both map and flatMap can be applied to a Stream<T> and they both return a Stream<R> . The difference is that the map operation produces one output value for each input value, whereas the flatMap operation produces an arbitrary number (zero or more) values for each input value. This is reflected in the arguments to each operation. The map operation takes a Function , which is called for each value in the input stream and produces one result value, which is sent to the output stream. The flatMap operation takes a function that conceptually wants to consume one value and produce an arbitrary number of values. However, in Java, it's cumbersome for a method to return an arbitrary number of values, since methods can return only zero or one value. One could imagine an API where the mapper function for flatMap takes a value and returns an array or a List of values, which are then sent to the output. Given that this is the streams library, a particularly apt way to represent an arbitrary number of return values is for the mapper function itself to return a stream! The values from the stream returned by the mapper are drained from the stream and are passed to the output stream. The \"clumps\" of values returned by each call to the mapper function are not distinguished at all in the output stream, thus the output is said to have been \"flattened.\" Typical use is for the mapper function of flatMap to return Stream.empty(_exprs) if it wants to send zero values, or something like Stream.of(_exprs) if it wants to return several values. But of course any stream can be returned.",
            "question_id": 26684562,
            "answer_id": 26684710
        },
        {
            "cleaned": "A possible use case is in testing primality of a given number (at test which in itself has many uses). The isProbablePrime algorithm will run much faster than an exact algorithm, so if the number fails isProbablePrime , then one need not go to the expense of running the more expensive algorithm.",
            "question_id": 27430092,
            "answer_id": 27430172
        },
        {
            "cleaned": "The standard use case for #a is in cryptography.  Specifically, certain cryptographic algorithms, such as #a , require randomly chosen large primes.  Importantly, however, these algorithms don't really require these numbers to be to be prime \u2014 they just need to be prime with a high probability. How high is very high?  Well, in a crypto application, one would typically call .isProbablePrime(_exprs) with an argument somewhere between 128 and 256.  Thus, the probability of a non-prime number passing such a test is less than one in 2 or 2 . Let's put that in perspective: if you had 10 billion computers, each generating 10 billion probable prime numbers per second (which would mean less than one clock cycle per number on any modern CPU), and the primality of those numbers was tested with .isProbablePrime(_exprs) , you would, on average, expect one non-prime number to slip in . That is, that would be the case, if those 10 billion computers could somehow all run for hundreds of billions of years without experiencing hardware failures.  In practice, though, of .isProbablePrime(_exprs) from false to true, without causing any other detectable effects, than it is for a non-prime number to actually pass the probabilistic primality test at that certainty level. Of course, the same risk of random cosmic rays and other hardware faults also applies to deterministic primality tests like #a .  Thus, in practice, even these tests have a (very small) baseline false positive rate due to random hardware failures (not to mention all other possible sources of errors, such as implementation bugs). Since it's easy to push the intrinsic false positive rate of the #a used by .isProbablePrime(_exprs) far below this baseline rate, simply by repeating the test sufficiently many times, and since, even repeated so many times, the Miller\u2013Rabin test is still much faster in practice than the best known deterministic primality tests like AKS, it remains the standard primality test for cryptographic applications. (Besides, even if you happened to accidentally select a strong pseudoprime as one of the factors of your RSA modulus, it would not generally lead to a catastrophic failure.  Typically, such pseudoprimes would be products of two (or rarely more) primes of approximately half the length, which means that you'd end up with a #a .  As long as none of the factors were small (and if they were, the primality test should've caught them), the RSA algorithm will still work just fine, and the key, although somewhat weaker against certain types of attacks than normal RSA keys of the same length, should still be reasonably secure if you didn't needlessly skimp on the key length.)",
            "question_id": 27430092,
            "answer_id": 27449187
        },
        {
            "cleaned": "OK, first of all, your setExits(_exprs) method will not work as intended, chained if-elseif will maximally execute 1 branch of code, for example: #pre Even if e contains both N and W , only bexits[0] will be set. Also this method will only add exits (for example calling setExits(_exprs) will not delete any existing exits. I would change that method to: #pre Also, i definetly wouldn't remember that north is on index 0, west in on 1, ... so a common practice is to name your indexes using final static constants: #pre Then you can write in your setExits method: #pre (much more readible) Finally, if you want your code even more well-arranged, you can make a Exits class representing avaliable exits, and backed by boolean array. Then on place where you create your String, you could create this class instead and save yourself work with generating and then parsing a string. EDIT: as @gexicide answers, there is a really handy class EnumSet which would be probably better for representing the exits than bollean array.",
            "question_id": 24651403,
            "answer_id": 24651855
        },
        {
            "cleaned": "If you will read comment above the code, It is mentioned that... If the caller is not a ForkJoinTask , this method is behaviorally equivalent to #pre So it is just another form to implement above code in else part...!! In #a it is mentioned that, #blockquote And if you will see implementation of #a , It is updating the lock and returns true if blocking is unnecessary.  Blank while loops are used to provide an interrupt until some condition reset to true/false. Here, do { } while(_exprs) is a blocker/interrupt until blocker.block(_exprs) will be true when blocker.isReleasable(_exprs) is false . Loop will continue execution while blocker is not releasable ( !blocker.isReleasable(_exprs) ) and blocker is not blocked !! Execution will be out of loop as soon as blocker.block(_exprs) will set to true. Note that, do{ } while(_exprs) does not update CAS variable, but it guarantee that program will wait until variable gets updated (force to wait until variable gets updated).",
            "question_id": 24609564,
            "answer_id": 24609841
        },
        {
            "cleaned": "I'd like to point out this #a . It seems as though arrays and objects follow different opcodes. I can't honestly summarize it more than that however it seems, arrays are simply not treated as Objects like we're normally used to so they don't inherit Object methods. Full credits to the author of that post as it's a very interesting read, both short & detailed. ",
            "question_id": 27504925,
            "answer_id": 27505121
        },
        {
            "cleaned": "std::list in C++ is a linked list, whereas java.util.ArrayList is an array. Try replacing std::list by std::vector . Also, be sure to compile with optimization turned on.",
            "question_id": 24914525,
            "answer_id": 24914586
        },
        {
            "cleaned": "That line creates an empty list of strings by calling a static method with a generic type parameter. Inside the Collections class, there is a static method emptyList declared like: #pre This has a generic type parameter T . We call call this method by using: #pre and T is infered to be a String because of the type of list . We can also specify the type of T by putting it in angle brackets when calling emptyList . This may be needed if we want a more specific type than is inferred: #pre emptyList<String>(_exprs) is not correct because that placement is only valid when creating\ninstances of generic classes, not calling methods. When using new there are two possible\ntype parameters, the ones before the class name are for the constructor only, and the ones after the class name are for the whole instance, so with the class: #pre We can call its constructor where A is String and B is Integer like: #pre or by using type inference: #pre  #li #a #li #a",
            "question_id": 27683759,
            "answer_id": 27684040
        },
        {
            "cleaned": " Let's look at the expression bit by bit. #a is the name of a class. From the #a : #blockquote #a is the name of a static method defined in the Collections class ( #a ). It is a generic method, and the <String> in Collections.<String>emptyList(_exprs) specifies the generic type argument. The method returns a List<T> , which in this case is List<String> : a list of strings. More specifically, it returns an , list of strings.",
            "question_id": 27683759,
            "answer_id": 27684042
        },
        {
            "cleaned": "The answer is, as always, \"it depends\".  It depends on how big the returned collection will be.  It depends on whether the result changes over time, and how important consistency of the returned result is.  And it depends very much on how the user is likely to use the answer. First, note that you can always get a Collection from a Stream, and vice versa: #pre So the question is, which is more useful to your callers. If your result might be infinite, there's only one choice: Stream. If your result might be very large, you probably prefer Stream, since there may not be any value in materializing it all at once, and doing so could create significant heap pressure. If all the caller is going to do is iterate through it (search, filter, aggregate), you should prefer Stream, since Stream has these built-in already and there's no need to materialize a collection (especially if the user might not process the whole result.)  This is a very common case. Even if you know that the user will iterate it multiple times or otherwise keep it around, you still may want to return a Stream instead, for the simple fact that whatever Collection you choose to put it in (e.g., ArrayList) may not be the form they want, and then the caller has to copy it anyway.  if you return a stream, they can do collect(_exprs) and get it in exactly the form they want. The above \"prefer Stream\" cases mostly derive from the fact that Stream is more flexible; you can late-bind to how you use it without incurring the costs and constraints of materializing it to a Collection. The one case where you must return a Collection is when there are strong consistency requirements, and you have to produce a consistent snapshot of a moving target.  Then, you will want put the elements into a collection that will not change. So I would say that most of the time, Stream is the right answer -- it is more flexible, it doesn't impose usually-unnecessary materialization costs, and can be easily turned into the Collection of your choice if needed.  But sometimes, you may have to return a Collection (say, due to strong consistency requirements), or you may want to return Collection because you know how the user will be using it and know this is the most convenient thing for them.",
            "question_id": 24676877,
            "answer_id": 24679745
        },
        {
            "cleaned": "I have a few points to add to #a . It's quite common to return a Stream from a \"getter\" style method call. See the #a in the Java 8 javadoc and look for \"methods... that return Stream\" for the packages other than java.util.Stream . These methods are usually on classes that represent or can contain multiple values or aggregations of something. In such cases, APIs typically have returned collections or arrays of them. For all the reasons that Brian noted in his answer, it's very flexible to add Stream-returning methods here. Many of these classes have collections- or array-returning methods already, because the classes predate the Streams API. If you're designing a new API, and it makes sense to provide Stream-returning methods, it might not be necessary to add collection-returning methods as well. Brian mentioned the cost of \"materializing\" the values into a collection. To amplify this point, there are actually two costs here: the cost of storing values in the collection (memory allocation and copying) and also the cost of creating the values in the first place. The latter cost can often be reduced or avoided by taking advantage of a Stream's laziness-seeking behavior. A good example of this are the APIs in java.nio.file.Files : #pre Not only does readAllLines have to hold the entire file contents in memory in order to store it into the result list, it also has to read the file to the very end before it returns the list. The lines method can return almost immediately after it has performed some setup, leaving file reading and line breaking until later when it's necessary -- or not at all. This is a huge benefit, if for example, the caller is interested only in the first ten lines: #pre Of course considerable memory space can be saved if the caller filters the stream to return only lines matching a pattern, etc. An idiom that seems to be emerging is to name stream-returning methods after the plural of the name of the things that it represents or contains, without a get prefix. Also, while stream(_exprs) is a reasonable name for a stream-returning method when there is only one possible set of values to be returned, sometimes there are classes that have aggregations of multiple types of values. For example, suppose you have some object that contains both attributes and elements. You might provide two stream-returning APIs: #pre",
            "question_id": 24676877,
            "answer_id": 24681959
        },
        {
            "cleaned": "#pre It's not quite as nice as the list code. You can't construct new Map.Entry s in a map(_exprs) call so the work is mixed into the collect(_exprs) call.",
            "question_id": 25903137,
            "answer_id": 25903190
        },
        {
            "cleaned": "Here are some variations on #a , which was pretty good to begin with (+1). Consider the following: #pre A couple points here. First is the use of wildcards in the generics; this makes the function somewhat more flexible. A wildcard would be necessary if, for example, you wanted the output map to have a key that's a superclass of the input map's key: #pre (There is also an example for the map's values, but it's really contrived, and I admit that having the bounded wildcard for Y only helps in edge cases.) A second point is that instead of running the stream over the input map's entrySet , I ran it over the keySet . This makes the code a little cleaner, I think, at the cost of having to fetch values out of the map instead of from the map entry. Incidentally, I initially had key -> key as the first argument to toMap(_exprs) and this failed with a type inference error for some reason. Changing it to (_exprs) -> key worked, as did Function.identity(_exprs) . Still another variation is as follows: #pre This uses Map.forEach(_exprs) instead of streams. This is even simpler, I think, because it dispenses with the collectors, which are somewhat clumsy to use with maps. The reason is that Map.forEach(_exprs) gives the key and value as separate parameters, whereas the stream has only one value -- and you have to choose whether to use the key or the map entry as that value. On the minus side, this lacks the rich, streamy goodness of the other approaches. :-)",
            "question_id": 25903137,
            "answer_id": 25905196
        },
        {
            "cleaned": "Documentation: For: System.out.println(_exprs); #blockquote It will simply return the entire string that's why it returns 1. ",
            "question_id": 25056607,
            "answer_id": 25056821
        },
        {
            "cleaned": "We can take a look into #a which is behind String.split . Way down the rabbit hole the method #pre is invoked.  For input _str this method is called as #pre #a : #pre And that is what happens: #a .  For input _str #a #pre Here resultSize == 0 and limit == 0 so #a .",
            "question_id": 25056607,
            "answer_id": 25056878
        },
        {
            "cleaned": "#li - It is correct in the parts that (1) a marker interface must be empty, and (2) implementing it is meant to imply some special treatment of the implementing class. The part that is incorrect is that it implies that JVM or the compiler would treat the objects of that class differently: you are correct in observing that it is the code of Java class library that treats these objects as cloneable, serializable, etc. It has nothing to do with the compiler or the JVM. #li - This lets you avoid polluting your code with the name of the marker interface when a \"plain Object \" is needed. For example, if you make a class that needs to be serializable, and has object members, you would be forced to either do casting or make your objects Serializable at compile time. This is inconvenient, because the interface is devoid of any functionality. #li - They let you achieve the same purpose of conveying metadata about the class to its consumers without creating a separate type for it. Annotations are more powerful, too, letting programmers pass more sophisticated information to classes that \"consume\" it.",
            "question_id": 25850328,
            "answer_id": 25850576
        },
        {
            "cleaned": "a. \nI have always seen them as a design pattern and nothing JVM-Special I have used that pattern in several situations. c. \nI beleive that using Annotations to mark something is a better solution then using marker interfaces. Simply because Interfaces are in the first place aimed to define common interfaces of Types / Classes. They are part of the class-hierachy. Annotations are aimed to provide Meta-Informations to Code, and I think that marker are meta-informations. So they are exactly for that use-case.",
            "question_id": 25850328,
            "answer_id": 25850622
        },
        {
            "cleaned": "I would argue first-off that Serializable and Cloneable are bad examples of marker interfaces. Sure, they're interfaces with methods, but they methods, such as writeObject(_exprs) . (The compiler will create a writeObject(_exprs) method for you if you don't override it, and all objects already have clone(_exprs) , but the compiler will again create a real clone(_exprs) method for you but with caveats. Both of these are weird edge cases that really aren't good design examples.) Marker interfaces are generally used for one of two purposes: 1) As a shortcut to avoid an excessively long type, which can happen with lots of generics. For instance, say you have this method signature: #pre That's messy and annoying to type, and more importantly, difficult to understand. Consider this instead: #pre Then your method looks like this: #pre Not only is it clearer, but you can now Javadoc the Widget interface, and it's also easier to search for all occurrences in your code of Widget. 2) Marker interfaces can also be used as a way around Java's lack of intersection types. With a marker interface, you can require something to be of two different types, such as in a method signature. Say you have some interface Widget in your application, like we described above. If you have a method that requires a Widget that also happens to let you iterate over it (it's contrived, but work with me here), your only good solution is to create a marker interface that extends both interfaces: #pre And in your code: #pre",
            "question_id": 25850328,
            "answer_id": 25853827
        },
        {
            "cleaned": "The main purpose of marker interfaces is to create special types where types themselves have no behavior of their own. #pre Here save method makes sure that only the objects of classes that implement the MarkerEntity interface are saved, for other types InvalidEntityFoundException is thrown. So here MarkerEntity marker interface is defining a type that adds special behavior to the classes implementing it. Though annotations can also used now to mark classes for some special treatments but marker annotations are replacement for naming pattern not for Marker interfaces. But marker annotations can't fully replace the marker interfaces because; marker interfaces are used to define type (as already explained above) where as marker annotations do not. #a",
            "question_id": 25850328,
            "answer_id": 31838310
        },
        {
            "cleaned": "There are several issues here. #li Any solution based on reading lines is going to process each character twice. Compilers for example don't do this, they read one character at a time and despatch on it directly. #li Any solution based on readLine(_exprs) is going to create Strings. #li You are using different buffer sizes. #li You are using different I/O technologies. #li In some cases you are using character conversion, while in others you aren't. #li You're over-analyzing the file. You don't really care where the white space is, or how much of it there is, as long as it separates the numbers from each other. My solution: #pre",
            "question_id": 25606833,
            "answer_id": 25757572
        },
        {
            "cleaned": "It's a curious omission. You would have to mark the field as transient and provide your own custom writeObject(_exprs) method that wrote the get(_exprs) result itself, and a readObject(_exprs) method that restored the Optional by reading that result from the stream. Not forgetting to call defaultWriteObject(_exprs) and defaultReadObject(_exprs) respectively.",
            "question_id": 24547673,
            "answer_id": 24547831
        },
        {
            "cleaned": "I wrote a Collector which, unlike the default java one, does not crash when you have null values: #pre Just replace your Collectors.toMap(_exprs) call to a call to this function and it'll fix the problem.",
            "question_id": 24630963,
            "answer_id": 32648397
        },
        {
            "cleaned": "I think you're just looking for the #a which takes another Collector to specify what to do with each group... and then Collectors.counting(_exprs) to do the counting: #pre Result: #pre (There's also the possibility of using groupingByConcurrent for more efficiency. Something to bear in mind for your real code, if it would be safe in your context.)",
            "question_id": 25441088,
            "answer_id": 25441208
        },
        {
            "cleaned": "To begin with ( ), instantiating the Application class by yourself does not seem to be its intended use. From what one can read from its #a , you are rather expected to use the static instance returned by getApplication(_exprs) . Now let's get to the error Eclipse reports. I've ran into a similar issue recently: Access restriction: The method ... is not API (_exprs) . I called the method in question as a method of an object which inherited that method from a super class. All I had to do was to add the package the super class was in to the packages imported by my plugin. However, there is a lot of different causes for errors based on \". Similar to the problem described above, the type you are using might have dependencies to packages that are not exported by the library or might not be exported itself. In that case you can try to track down the missing packages and export them my yourself, as suggested #a , or try Access Rules. Other possible scenarios include: #li Eclipse wants to keep you from using available packages that are not part of the public Java API (solution #a , #a ) #li Dependencies are satisfied by multiple sources, versions are conflicting etc. (solution #a , #a , #a ) #li Eclipse is using a JRE where a JDK is necessary (which might be the case here, from what your errors say; #a ) or JRE/JDK version in project build path is not the right one This ended up as more like a medley of restriction-related issues than an actual answer. But since is such a versatile error to be reported, the perfect recipe is probably still to be found.",
            "question_id": 25222811,
            "answer_id": 26676046
        },
        {
            "cleaned": "You can use a Callback to get onSuccess and onError events. Just add a new Callback to your request like so: #pre Then you can perform any alterations and modifications in the onSuccess callback.",
            "question_id": 26548660,
            "answer_id": 26548894
        },
        {
            "cleaned": "#blockquote . It will never throw a compilation error. This is much similar to what a simple getter and setter does or a copy constructor does. Remember we can access private members using this. #pre Your testPrivate method accepts an instance of MyClass. Since testPrivate is a method inside MyClass , it will have access to private properties. #pre Methods defined inside the class will always have access to it's private members, through this. and instance variable. But if you define testPrivate outside of MyClass then, you won't have access to private members. There you will have to use a method or a setter or a getter.",
            "question_id": 27482579,
            "answer_id": 27483288
        },
        {
            "cleaned": "You're nesting mocking inside of mocking.  You're calling getSomeList(_exprs) , which does some mocking, before you've finished the mocking for MyMainModel .  Mockito doesn't like it when you do this. Replace #pre with #pre To understand why this causes a problem, you need to know a little about how Mockito works, and also be aware in what order expressions and statements are evaluated in Java. Mockito can't read your source code, so in order to figure out what you are asking it to do, it relies a lot on static state.  When you call a method on a mock object, Mockito records the details of the call in an internal list of invocations.  The when method reads the last of these invocations off the list and records this invocation in the OngoingStubbing object it returns. The line #pre causes the following interactions with Mockito: #li Mock method mainModel.getList(_exprs) is called, #li Static method when is called, #li Method thenReturn is called on the OngoingStubbing object returned by the when method. The thenReturn method can then instruct the mock it received via the OngoingStubbing method to handle any suitable call to the getList method to return someModelList . In fact, as Mockito can't see your code, you can also write your mocking as follows: #pre This style is somewhat less clear to read, especially since in this case the null has to be casted, but it generates the same sequence of interactions with Mockito and will achieve the same result as the line above. However, the line #pre causes the following interactions with Mockito: #li Mock method mainModel.getList(_exprs) is called, #li Static method when is called, #li A new mock of SomeModel is created (inside getSomeList(_exprs) ), #li Mock method model.getName(_exprs) is called, At this point Mockito gets confused.  It thought you were mocking mainModel.getList(_exprs) , but now you're telling it you want to mock the model.getName(_exprs) method.  To Mockito, it looks like you're doing the following: #pre This looks silly to Mockito as it can't be sure what you're doing with mainModel.getList(_exprs) . Note that we did not get to the thenReturn method call, as the JVM needs to evaluate the parameters to this method before it can call the method.  In this case, this means calling the getSomeList(_exprs) method. Generally it is a bad design decision to rely on static state, as Mockito does, because it can lead to cases where the Principle of Least Astonishment is violated.  However, Mockito's design does make for clear and expressive mocking, even if it leads to astonishment sometimes. Finally, recent versions of Mockito add an extra line to the error message above.  This extra line indicates you may be in the same situation as this question: #blockquote",
            "question_id": 26318569,
            "answer_id": 26319364
        },
        {
            "cleaned": "The Javadocs for Collection.(_exprs)tream(_exprs) and Stream itself don't answer the question, so it's off to the mailing lists for the rationale.  I went through the lambda-libs-spec-observers archives and found #a and another thread that touched on whether #a to match (or actually, whether it should be removed).  There was no once-and-for-all conclusion, so perhaps I've missed something from another list or the matter was settled in private discussion.  (Perhaps #a , one of the principals of this discussion, can fill in anything missing.) The participants made their points well, so this answer is mostly just an organization of the relevant quotes, with a few clarifications in , presented in order of importance (as I interpret it).  #a in the first thread, explaining why Collections.parallelStream(_exprs) is valuable enough to keep even after other parallel stream factory methods have been removed: #blockquote #a stands by this position in the later discussion about Arrays.parallelStream(_exprs) : #blockquote  #a : #blockquote In response to Kevin Bourrillion's skepticism about whether the effect is significant, #a : #blockquote #a , but hedges his position: #blockquote Indeed, the later discussion about Arrays.parallelStream(_exprs) #a .  At the time of the discussion, switching a stream from sequential to parallel and back could be interleaved with other stream operations. #a , explains why sequential/parallel mode switching may complicate future development of the Java platform: #blockquote This mode switching was #a .  In the current version of the library, a stream pipeline is either sequential or parallel; last call to sequential(_exprs) / parallel(_exprs) wins.  Besides side-stepping the statefulness problem, this change also improved the performance of using parallel(_exprs) to set up a parallel pipeline from a sequential stream factory.  #a , in response to #a that Stream.parallel(_exprs) allows programmers to understand streams sequentially before going parallel: #blockquote",
            "question_id": 24603186,
            "answer_id": 24603187
        },
        {
            "cleaned": "Try adding a Thread.sleep(_exprs); call within your run(_exprs) method... Basically it's the difference between scheduling something based on when the previous execution and when it (logically) . For example, suppose I schedule an alarm to go off with a fixed of once an hour, and every time it goes off, I have a cup of coffee, which takes 10 minutes. Suppose that starts at midnight, I'd have: #pre If I schedule with a fixed of one hour, I'd have: #pre Which one you want depends on your task.",
            "question_id": 24649842,
            "answer_id": 24649939
        }
    ]
}